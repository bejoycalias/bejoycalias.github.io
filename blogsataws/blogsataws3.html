<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/blogsataws1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li class="active"><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li class="active"><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li class="active"><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="blogsataws1.html">Page 1</a>|<a href="blogsataws2.html">Page 2</a>|<a href="blogsataws3.html">Page 3</a>|<a href="blogsataws4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/dl_arch_1.png" /> 
<b class="lb-b blog-post-title" property="name headline">AWS DeepLens – Get Hands-On Experience with Deep Learning With Our New Video Camera</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr">Jeff Barr</a> | on 
<time property="datePublished" datetime="2017-11-29T09:42:23+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/artificial-intelligence/" title="View all posts in Artificial Intelligence*"><span property="articleSection">Artificial Intelligence*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/artificial-intelligence/aws-deeplens/" title="View all posts in AWS DeepLens"><span property="articleSection">AWS DeepLens</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/deeplens/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-22104" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=22104&amp;disqus_title=AWS+DeepLens+%26%238211%3B+Get+Hands-On+Experience+with+Deep+Learning+With+Our+New+Video+Camera&amp;disqus_url=https://aws.amazon.com/blogs/aws/deeplens/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22104');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>As I have mentioned a time or two in the past, I am a strong believer in life-long learning. Technological change is coming along faster than ever and you need to do the same in order to keep your skills current.</p> 
<p>For most of my career, artificial intelligence has been an academic topic, with practical applications and real-world deployment “just around the corner.” I think it is safe to say, with the number of practical applications for machine learning, including <a href="https://aws.amazon.com/rekognition/">computer vision</a> and <a href="https://aws.amazon.com/deep-learning/">deep learning</a>, that we’ve turned the corner and that now is the time to start getting some hands-on experience and polishing your skills! Also, while both are more recent and spent far less time gestating, it is safe to say that <a href="https://aws.amazon.com/iot/">IoT</a> and <a href="https://aws.amazon.com/serverless/">serverless</a> computing are here to stay and should be on your list.</p> 
<p><span style="text-decoration: underline"><strong>New AWS DeepLens</strong></span><br /> <img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_arch_1.png">AI</a> and infrastructure services in your app.</p> 
<p>Let’s start with the hardware. We have packed a lot of power into this device. There’s a 4 megapixel camera that can capture 1080P video, accompanied by a 2D microphone array. An Intel Atom<sup>&reg;</sup> Processor provides over 100 GFLOPS of compute power, enough to run tens of frames of incoming video through on-board deep learning models every second. <span title="">DeepLens</span> is well-connected, with dual-band Wi-Fi, USB and micro HDMI ports. Wrapping it all up, 8 gigabytes of memory for your pre-trained models and your code, makes this a powerful yet compact device.</p> 
<p>On the software side, the <span title="">AWS DeepLens</span> runs Ubuntu 16.04 and is preloaded with the Greengrass Core (Lambda runtime, message manager, and <a href="http://docs.aws.amazon.com/greengrass/latest/developerguide/what-is-gg.html">more</a>). There’s also a device-optimized version of <a href="https://aws.amazon.com/mxnet/" title="">MXNet</a>, and the flexibility to use other frameworks such as <a href="https://www.tensorflow.org/" title="">TensorFlow</a> and <a href="https://caffe2.ai/" title="">Caffe2</a>. The <a href="https://github.com/intel/clDNN">Intel<sup>&reg;</sup> clDNN</a> library provide a set of set of deep learning primitives for computer vision and other AI workloads, taking advantage of special features of the Intel Atom<sup>&reg;</sup> Processor for accelerated inferencing.</p> 
<p>We also give you data! When you build an app that runs on your <span title="">AWS DeepLens</span>, you can take advantage of a set of pre-trained models for image detection and recognition. These models will help you detect cats and dogs, faces, a wide array of household and everyday objects, motions and actions, and even hot dogs. We will continue to train these models, making them better and better over time. Here’s the initial set of models:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_models_1.png" /></p> 
<p>All of this hardware, software, and data come together to make the <span title="">AWS DeepLens</span> a prime example of an edge device. With eyes, ears, and a fairly powerful brain that are all located out in the field and close to the action, it can run incoming video and audio through on-board deep learning models quickly and with low latency, making use of the cloud for more compute-intensive higher-level processing. For example, you can do face detection on the <span title="">DeepLens</span> and then let <a href="https://aws.amazon.com/rekognition/" title="">Amazon Rekognition</a> take care of the face recognition.</p> 
<p>This is an epic learning opportunity in a box! We’ve also included tons of sample code (Lambda functions) that you can use as-is, pick apart and study, and use as the basis for your own functions. Once you have built something cool and useful, you can deploy it in production form. We’ve made sure that <span title="">AWS DeepLens</span> is robust and secure, with unique certificates for each device and fine-grained, IAM-powered control over access to AWS services and resources.</p> 
<p><span style="text-decoration: underline"><strong>Registering a Device</strong></span><br /> Let’s walk through the process of registering a device and getting it ready for use, starting from the DeepLens Console. I open it up and click on <strong>Register device</strong>:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_console_splash_2.png" /></p> 
<p>Then I give my camera a name and click on <strong>Next</strong>:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_reg_cam_1.png" /></p> 
<p>Then I click on <strong>Download certificate</strong> and save it away in a safe place:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_download_cert_1.png" /></p> 
<p>Next, I create the necessary IAM roles (the console makes it easy) and select each one in the appropriate menu:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_set_permissions_3.png" /></p> 
<p>Now I am ready to go hands-on with my DeepLens! I power up, connect my laptop to the device’s network, and access the built-in portal to complete the process. The console outlines the steps:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_setup_guide_1.png" /></p> 
<p>At this point my DeepLens is a fully-functional edge device. The certificate on the device allows it to make secure, signed calls to AWS. The Greengrass Core is running, ready to accept and run a Lambda function.</p> 
<p><span style="text-decoration: underline"><strong>Creating a DeepLens Project</strong></span><br /> With everything connected and set up, I can create my first project. I navigate to the <strong>Projects</strong> and click on <strong>Create new project</strong>:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_projects_none_1.png" /></p> 
<p>Then I choose a project template or start with a blank project. I’ll go for <strong>cat and dog recognition</strong>, selecting it and clicking on <strong>Next</strong>:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_choose_project_1.png" /></p> 
<p>The console give me the opportunity to name and customize the project. As you can see, the project refers to a Lambda function and one of the pre-trained models that I listed above. The defaults are all good, so I simply click on <strong>Create</strong>:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_raining_cats_and_dogs_1.png" /></p> 
<p>Now I simply deploy the project to my camera:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/dl_deploy_prep_1.png" /></p> 
<p><span style="text-decoration: underline"><strong>Training Cats and Dogs</strong></span><br /> The functions run on the camera and publish their output to an MQTT topic. Here’s an excerpt from the inner loop of the cat and dog recognition function (I removed some error handing):</p> 
<code class="lang-python">while doInfer:
# Get a frame from the video stream
ret, frame = awscam.getLastFrame()
numFrames += 1
# Resize frame to fit model input requirement
frameResize = cv2.resize(frame, (224, 224))
# Run model inference on the resized frame
inferOutput = model.doInference(frameResize)
# Publish a message to the cloud for every 100 frames
if numFrames &gt;= 10:
msg = &quot;Infinite inference is running. Sample result of the last frame is\n&quot;
# Output inference result of the last frame to cloud
# The awsca module can parse the output from some known models
outputProcessed = model.parseResult(modelType, inferOutput)
# Get top 5 results with highest probiblities
topFive = outputProcessed[modelType][0:2]
msg += &quot;label    prob&quot;
for obj in topFive:
msg += &quot;\n{}   {}&quot;.format(outMap[obj[&quot;label&quot;]], obj[&quot;prob&quot;])
client.publish(topic=iotTopic, payload=msg)
numFrames = 0;
</code> 
<p>Like I said, you can modify this or you can start from scratch. Either way, as you can see, it is pretty easy to get started.</p> 
<p>I can’t wait to see what you come up with once you get a <span title="">DeepLens</span> in your hands. To learn more and to have the opportunity to take a <span title="">AWS DeepLens</span> home with you, be sure to attend one of the sixteen <a href="https://www.portal.reinvent.awsevents.com/connect/search.ww#loadSearch-searchPhrase=mcl212&amp;searchType=session&amp;tc=0&amp;sortBy=abbreviationSort&amp;sessionTypeID=1000&amp;p=">DeepLens Workshops</a> at <a href="https://reinvent.awsevents.com/" title="">AWS re:Invent</a>.</p> 
<p><span style="text-decoration: underline"><strong>Pre-Order One Today</strong></span><br /> We’ll start shipping the <span title="">AWS DeepLens</span> in 2018, initially in the US. To learn more about pricing and availability, or to <a href="https://www.amazon.com/dp/B075Y3CK37">pre-order</a> one of your own, visit the <a href="https://aws.amazon.com/deeplens">DeepLens page</a>.</p> 
<p>— <a href="https://twitter.com/jeffbarr">Jeff</a>;</p> 
<p>&nbsp;</p> 
<footer> 
<img width="100%" src="https://media.amazonwebservices.com/blog/2017/jeffbarr_purple_2017_400x400.jpg" /> 
<h3 class="lb-h4"> <a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr" property="name">Jeff Barr</a> </h3> 
<p property="description">Jeff Barr is Chief Evangelist for AWS. He started this blog in 2004 and has been writing posts just about non-stop ever since.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22104');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/sagemaker_3-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Amazon SageMaker – Accelerating Machine Learning</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by Randall Hunt | on 
<time property="datePublished" datetime="2017-11-29T09:34:15+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/artificial-intelligence/" title="View all posts in Artificial Intelligence*"><span property="articleSection">Artificial Intelligence*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/artificial-intelligence/sagemaker/" title="View all posts in SageMaker"><span property="articleSection">SageMaker</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/top-posts/" title="View all posts in Top Posts*"><span property="articleSection">Top Posts*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/sagemaker/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-22131" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=22131&amp;disqus_title=Amazon+SageMaker+%26%238211%3B+Accelerating+Machine+Learning&amp;disqus_url=https://aws.amazon.com/blogs/aws/sagemaker/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22131');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Machine Learning is a pivotal technology for many startups and enterprises. Despite decades of investment and improvements, the process of developing, training, and maintaining machine learning models has still been cumbersome and ad-hoc. The process of incorporating machine learning into an application often involves a team of experts tuning and tinkering for months with inconsistent setups. Businesses and developers want an end-to-end, development to production pipeline for machine learning.</p> 
<b>Introducing Amazon SageMaker</b> 
<p><a href="https://aws.amazon.com/sagemaker/">Amazon SageMaker</a> is a fully managed end-to-end machine learning service that enables data scientists, developers, and machine learning experts to quickly build, train, and host machine learning models at scale. This drastically accelerates all of your machine learning efforts and allows you to add machine learning to your production applications quickly.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/please_work.png" /></p> 
<p>There are 3 main components of Amazon SageMaker:</p> 
<li>Authoring: Zero-setup hosted Jupyter notebook IDEs for data exploration, cleaning, and preprocessing. You can run these on general instance types or GPU powered instances.</li> 
<li>Model Training: A distributed model building, training, and validation service. You can use built-in common supervised and unsupervised learning algorithms and frameworks or create your own training with Docker containers. The training can scale to tens of instances to support faster model building. Training data is read from S3 and model artifacts are put into S3. The model artifacts are the data dependent model parameters, not the code that allows you to make inferences from your model. This separation of concerns makes it easy to deploy Amazon SageMaker trained models to other platforms like IoT devices.</li> 
<li>Model Hosting: A model hosting service with HTTPs endpoints for invoking your models to get realtime inferences. These endpoints can scale to support traffic and allow you to A/B test multiple models simultaneously. Again, you can construct these endpoints using the built-in SDK or provide your own configurations with Docker images.</li> 
<p>Each of these components can be used in isolation which makes it really easy to adopt Amazon SageMaker to fill in the gaps in your existing pipelines. That said, there are some really powerful things that are enabled when you use the service end-to-end.</p> 
<b>Working with SageMaker</b> 
<p>I want to build, train, and deploy an <a href="https://mxnet.incubator.apache.org/">Apache MXNet</a> based image classifier. I’ll use the <a href="http://gluon.mxnet.io/">Gluon</a> language, the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> dataset, and a <a href="https://arxiv.org/abs/1512.03385">ResNet V2</a> model architecture.</p> 
<h3>Authoring with Jupyter Notebooks</h3> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/27/sagemaker_2.png" /><br /> When I create a notebook instance it launches an ML compute instance that comes with Anaconda packages and libraries common in deep learning, a 5GB ML storage volume, and several example notebooks demonstrating various algorithms. I can optionally configure VPC support which creates an ENI in my VPC for easy and secure access to my resources.</p> 
<p>Once my instance is provisioned I’m able to open my notebook and start writing some code!</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/27/sagemaker_3.png" /></p> 
<h3>Model Training</h3> 
<p>I’m going to leave out the actual model training code here for brevity, but in general for any kind of Amazon SageMaker common framework training you can implement a simple training interface that looks something like this:</p> 
<code class="lang-python">def train(
channel_input_dirs, hyperparameters, output_data_dir,
model_dir, num_gpus, hosts, current_host):
pass
def save(model):
pass</code> 
<p>I want to create a distributed training job on 4 ml.p2.xlarge instances in my Amazon SageMaker infrastructure. I’ve already downloaded all of the data I need locally.</p> 
<code class="lang-python">import sagemaker
from sagemaker.mxnet import MXNet
m = MXNet(&quot;cifar10.py&quot;, role=role, 
train_instance_count=4, train_instance_type=&quot;ml.p2.xlarge&quot;,
hyperparameters={'batch_size': 128, 'epochs': 50, 
'learning_rate': 0.1, 'momentum': 0.9})</code> 
<p>Now that we’ve constructed our model training job we can feed it data by calling: <code>m.fit(&quot;s3://randall-likes-sagemaker/data/gluon-cifar10&quot;)</code>.</p> 
<p>If I navigate to the jobs console I can see that my job is running!</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/27/sagemaker_4.png" /></p> 
<h3>Hosting and Real Time Inferences</h3> 
<p>Now that my model has finished training I can start to generate predictions! Using the same code from earlier I’ll create and launch an endpoint.</p> 
<code class="lang-python">
predictor = m.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')
</code> 
<p>Then invoking the endpoint is as simple as running: <code class="lang-python">predictor.predict(img_input)</code>!</p> 
<p>That’s an end-to-end Machine Learning pipeline in fewer than 100 lines of code.</p> 
<p>I want to walk through one more example that shows how you could use just the model hosting components of Amazon SageMaker.</p> 
<h3>Using Custom Docker Containers</h3> 
<p>Amazon SageMaker defines a simple spec for Docker containers that allows you to easily write your own training algorithms or your own inference containers.</p> 
<p>I have an existing model based on the architecture described <a href="https://github.com/multimedia-berkeley/tutorials">here</a> and I want to host this model for real time inferences.</p> 
<p>I’ve created a simple Dockerfile and flask app to serve my inferences.</p> 
<p>I’ve left my code that loads the models and generates predictions out here because yours will be different. Essentially, I built a method that would download an image from an input URL and then pass that image data onto the MXNet model for predictions.</p> 
<code class="lang-python">from flask import Flask, request, jsonify
import predict
app = Flask(__name__)
@app.route('/ping')
def ping():
return (&quot;&quot;, 200)
@app.route('/invocations', methods=[&quot;POST&quot;])
def invoke():
data = request.get_json(force=True)
return jsonify(predict.download_and_predict(data['url']))
if __name__ == '__main__':
app.run(port=8080)</code> 
<code>FROM mxnet/python:latest
WORKDIR /app
COPY *.py /app/
COPY models /app/models
RUN pip install -U numpy flask scikit-image
ENTRYPOINT [&quot;python&quot;, &quot;app.py&quot;]
EXPOSE 8080</code> 
<p>I push this image to ECR and then I navigate to the models console in Amazon SageMaker to create a new model.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/27/sagemaker_5.png" /></p> 
<p>After creating a new model I’ll provision an endpoint as well.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/27/sagemaker_6.png" /></p> 
<p>Now I can invoke the endpoint right from <a href="https://aws.amazon.com/lambda/" title="">AWS Lambda</a> or any other application! In fact I setup a twitter account to showcase this model. Just tweet <a href="https://twitter.com/WhereMl/">@WhereML</a> a picture to see if it can guess the location!</p> 
<code class="lang-python">
import boto3
import json
sagemaker = boto3.client('runtime.sagemaker')
data = {'url': 'https://pbs.twimg.com/media/DPwe4kMUMAAWCd_.jpg'}
result = sagemaker.invoke_endpoint(
EndpointName='predict',
Body=json.dumps(data)
)
</code> 
<h3>Pricing</h3> 
<p>As part of the <a href="https://aws.amazon.com/free/">AWS Free Tier</a>, you can get started with Amazon SageMaker for free. For the first two months of usage each month you’re provided 250 hours of t2.medium notebook usage, 50 hours of m4.xlarge usage for training, and 125 hours of m4.xlarge usage for hosting. Beyond the free tier, the pricing differs by region but is billed per-second of instance usage, per-GB of storage, and per-GB of Data transfer into and out of the service.</p> 
<p>Before writing blog posts for re:Invent this year <a href="https://twitter.com/jeffbarr">Jeff</a> told me not to pick favorites. Well, I failed. Of some absolutely amazing launches, Amazon SageMaker is my favorite service of re:Invent 2017. I absolutely cannot wait to see what our customers are able to accomplish with such an exciting suite of tools.</p> 
<p>– <a href="https://twitter.com/jrhunt/">Randall</a></p> 
<footer> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/08/09/headshot-square-small.jpeg" /> 
<h3 class="lb-h4" property="name">Randall Hunt</h3> 
<p property="description">Senior Technical Evangelist at AWS. Formerly of NASA, SpaceX, and MongoDB.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22131');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/s3_select-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">S3 Select and Glacier Select – Retrieving Subsets of Objects</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by Randall Hunt | on 
<time property="datePublished" datetime="2017-11-29T09:21:33+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/storage/amazon-glacier/" title="View all posts in Amazon Glacier*"><span property="articleSection">Amazon Glacier*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/storage/" title="View all posts in Storage*"><span property="articleSection">Storage*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/top-posts/" title="View all posts in Top Posts*"><span property="articleSection">Top Posts*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/s3-glacier-select/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-22348" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=22348&amp;disqus_title=S3+Select+and+Glacier+Select+%26%238211%3B+Retrieving+Subsets+of+Objects&amp;disqus_url=https://aws.amazon.com/blogs/aws/s3-glacier-select/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22348');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/s3/" title="">Amazon Simple Storage Service (S3)</a> stores data for millions of applications used by market leaders in every industry. Many of these customers also use <a href="https://aws.amazon.com/glacier/" title="">Amazon Glacier</a> for secure, durable, and extremely low-cost archival storage. With S3, I can store as many objects as I want and individual objects can be as large as 5 terabytes. Data in object storage have traditionally been accessed as a whole entities, meaning when you ask for a 5 gigabyte object you get all 5 gigabytes. It’s the nature of object storage. Today we’re challenging that paradigm by announcing two new capabilities for S3 and Glacier that allow you to use simple SQL expressions to pull out only the bytes you need from those objects. This fundamentally enhances virtually every application that accesses objects in S3 or Glacier.</p> 
<b>S3 Select</b> 
<p><a href="https://aws.amazon.com/s3/details/#s3-select">S3 Select</a>, launching in preview, enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases – in many cases you can get as much as a 400% improvement.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/28/s3_select.png" /></p> 
<p>As an example let’s imagine you’re a developer at a large retailer and you need to analyze the weekly sales data from a single store, but the data for all 200 stores is saved in a new GZIP-ed CSV every day. Without S3 Select, you would need to download, decompress and process the entire CSV to get the data you needed. With S3 Select, you can use a simple SQL expression to return only the data from the store you’re interested in, instead of retrieving the entire object. This means you’re dealing with an order of magnitude less data which improves the performance of your underlying applications.</p> 
<p>Let’s look at a quick Python example.</p> 
<code class="lang-python">import boto3
from s3select import ResponseHandler
class PrintingResponseHandler(ResponseHandler):
def handle_records(self, record_data):
print(record_data.decode('utf-8'))
handler = PrintingResponseHandler()
s3 = boto3.client('s3')
response = s3.select_object_content(
Bucket=&quot;super-secret-reinvent-stuff&quot;,
Key=&quot;stuff.csv&quot;,
SelectRequest={
'ExpressionType': 'SQL',
<b>'Expression': 'SELECT s._1 FROM S3Object AS s'',</b>
'InputSerialization': {
'CompressionType': 'NONE',
'CSV': {
'FileHeaderInfo': 'IGNORE',
'RecordDelimiter': '\n',
'FieldDelimiter': ',',
}
},
'OutputSerialization': {
'CSV': {
'RecordDelimiter': '\n',
'FieldDelimiter': ',',
}
}
}
)
handler.handle_response(response['Body'])</code> 
<p>Pretty cool! To enable this behavior S3 Select uses a binary wire-protocol to return the objects. For now, that requires the use of a small additional library to help with deserialization.</p> 
<p>We expect customers to use S3 Select to accelerate all sorts of applications. For example, this partial data retrieval ability is especially useful for serverless applications built with <a href="https://aws.amazon.com/lambda/" title="">AWS Lambda</a>. When we modified the <a href="https://github.com/awslabs/lambda-refarch-mapreduce">Serverless MapReduce</a> reference architecture to retrieve only the data needed using S3 Select we saw a 2X improvement in performance and an 80% reduction in cost.</p> 
<p>The S3 Select team also created a Presto connector which can provide an immediate performance boost for <a href="https://aws.amazon.com/elasticmapreduce/" title="">Amazon EMR</a> with no changes to your queries. We put this connector to test by running a complex query that filtered almost 99% of the data retrieved from S3. With S3 Select disabled, Presto had to scan and filter entire objects from S3 but with S3 Select enabled, Presto leveraged S3 Select to retrieve only the data needed for the query.</p> 
<code class="lang-bash">[hadoop@ip-172-31-19-123 ~]$ time presto-cli --catalog hive --schema default --session hive.s3_optimized_select_enabled=false -f query.sql
&quot;31.965496&quot;,&quot;127178&quot;,&quot;5976&quot;,&quot;70.89902&quot;,&quot;130147&quot;,&quot;6996&quot;,&quot;37.17715&quot;,&quot;138092&quot;,&quot;8678&quot;,&quot;135.49536&quot;,&quot;103926&quot;,&quot;11446&quot;,&quot;82.35177&quot;,&quot;116816&quot;,&quot;8484&quot;,&quot;67.308304&quot;,&quot;135811&quot;,&quot;10104&quot;
real  0m35.910s
user  0m2.320s
sys   0m0.124s
[hadoop@ip-172-31-19-123 ~]$ time presto-cli --catalog hive --schema default --session hive.s3_optimized_select_enabled=true -f query.sql
&quot;31.965496&quot;,&quot;127178&quot;,&quot;5976&quot;,&quot;70.89902&quot;,&quot;130147&quot;,&quot;6996&quot;,&quot;37.17715&quot;,&quot;138092&quot;,&quot;8678&quot;,&quot;135.49536&quot;,&quot;103926&quot;,&quot;11446&quot;,&quot;82.35177&quot;,&quot;116816&quot;,&quot;8484&quot;,&quot;67.308304&quot;,&quot;135811&quot;,&quot;10104&quot;
real  0m6.566s
user  0m2.136s
sys   0m0.088s</code> 
<p>That query took 35.9 seconds without S3 Select and only 6.5 seconds with S3 Select. That’s 5X faster!</p> 
<h3>Things To Know</h3> 
<li>While in preview S3 Select supports CSV or JSON files with or without GZIP compression. During the preview objects that are encrypted at rest are not supported.</li> 
<li>There are no charges for S3 Select while in preview.</li> 
<li><a href="https://aws.amazon.com/athena" title="">Amazon Athena</a>, <a href="https://aws.amazon.com/redshift/" title="">Amazon Redshift</a>, and <a href="https://aws.amazon.com/elasticmapreduce/" title="">Amazon EMR</a> as well as partners like Cloudera, DataBricks, and Hortonworks will all support S3 Select.</li> 
<b>Glacier Select</b> 
<p>Some companies in highly regulated industries like Financial Services, Healthcare, and others, write data directly to Amazon Glacier to satisfy compliance needs like SEC Rule 17a-4 or HIPAA. Many S3 users have lifecycle policies designed to save on storage costs by moving their data into Glacier when they no longer need to access it on a regular basis. Most legacy archival solutions, like on premise tape libraries, have highly restricted data retrieval throughput and are unsuitable for rapid analytics or processing. If you want to make use of data stored on one of those tapes you might have to wait for weeks to get useful results. In contrast, cold data stored in Glacier can now be easily queried within minutes.</p> 
<p>This unlocks a lot of exciting new business value for your archived data. <a href="https://aws.amazon.com/glacier/details/#amazon-glacier-select">Glacier Select </a>allows you to to perform filtering directly against a Glacier object using standard SQL statements.</p> 
<p>Glacier Select works just like any other retrieval job except it has an additional set of parameters you can pass in initiate job request. <code>SelectParameters</code></p> 
<p>Here a quick example:</p> 
<code class="lang-python">import boto3
glacier = boto3.client(&quot;glacier&quot;)
jobParameters = {
&quot;Type&quot;: &quot;select&quot;, &quot;ArchiveId&quot;: &quot;ID&quot;,
&quot;Tier&quot;: &quot;Expedited&quot;,
&quot;SelectParameters&quot;: {
&quot;InputSerialization&quot;: {&quot;csv&quot;: {}},
&quot;ExpressionType&quot;: &quot;SQL&quot;,
<b>&quot;Expression&quot;: &quot;SELECT * FROM archive WHERE _5='498960'&quot;</b>,
&quot;OutputSerialization&quot;: {
&quot;csv&quot;: {}
}
},
&quot;OutputLocation&quot;: {
&quot;S3&quot;: {&quot;BucketName&quot;: &quot;glacier-select-output&quot;, &quot;Prefix&quot;: &quot;1&quot;}
}
}
glacier.initiate_job(vaultName=&quot;reInventSecrets&quot;, jobParameters=jobParameters)
</code> 
<h3>Things To Know</h3> 
<p>Glacier Select is generally available in all commercial regions that have Glacier.</p> 
<p>Glacier is priced in 3 dimensions.</p> 
<li>GB of Data Scanned</li> 
<li>GB of Data Returned</li> 
<li>Select Requests</li> 
<p>Pricing for each dimension is determined by the speed at which you want your results returned: expedited (1-5 minutes), standard (3-5 hours), and bulk (5-12 hours).</p> 
<p>Soon, in 2018, Athena will be integrated with Glacier using Glacier Select.</p> 
<p>I hope you’re able to get started enhancing your applications or building new ones with these capabilities.&nbsp;You can apply for this preview at the <a href="https://pages.awscloud.com/amazon-s3-select-preview.html">S3 Select Preview Application Page</a>.</p> 
<p>– <a href="https://twitter.com/jrhunt/">Randall</a></p> 
<footer> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/08/09/headshot-square-small.jpeg" /> 
<h3 class="lb-h4" property="name">Randall Hunt</h3> 
<p property="description">Senior Technical Evangelist at AWS. Formerly of NASA, SpaceX, and MongoDB.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22348');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/nept.png" /> 
<b class="lb-b blog-post-title" property="name headline">Amazon Neptune – A Fully Managed Graph Database Service</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by Randall Hunt | on 
<time property="datePublished" datetime="2017-11-29T08:57:02+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/database/amazon-neptune/" title="View all posts in Amazon Neptune"><span property="articleSection">Amazon Neptune</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/database/amazon-rds/" title="View all posts in Amazon RDS*"><span property="articleSection">Amazon RDS*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/database/" title="View all posts in Database*"><span property="articleSection">Database*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/amazon-neptune-a-fully-managed-graph-database-service/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-22144" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=22144&amp;disqus_title=Amazon+Neptune+%26%238211%3B+A+Fully+Managed+Graph+Database+Service&amp;disqus_url=https://aws.amazon.com/blogs/aws/amazon-neptune-a-fully-managed-graph-database-service/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22144');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Of all the data structures and algorithms we use to enable our modern lives, <b>graphs</b> are changing the world everyday. Businesses continuously create and ingest rich data with complex relationships. Yet developers are still forced to model these complex relationships in traditional databases. This leads to frustratingly complex queries with high costs and increasingly poor performance as you add relationships. We want to make it easy for you to deal with these modern and increasingly complex datasets, relationships, and patterns.</p> 
<b>Hello, Amazon Neptune</b> 
<p>Today we’re launching a limited preview (sign up here)&nbsp;of <a href="https://aws.amazon.com/neptune/">Amazon Neptune</a>, a fast and reliable graph database service that makes it easy to gain insights from relationships among your highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds of latency. Delivered as a fully managed database, Amazon Neptune frees customers to focus on their applications rather than tedious undifferentiated operations like maintenance, patching, backups, and restores. The service supports fast-failover, point-in-time recovery, and Multi-AZ deployments for high availability. With support for up to 15 read replicas you can scale query throughput to 100s of thousands of queries per second. Amazon Neptune runs within your <a href="https://aws.amazon.com/vpc/" title="">Amazon Virtual Private Cloud</a> and allows you to encrypt your data at rest, giving you complete control over your data integrity in transit and at rest.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/console_1.png" /></p> 
<p>There are <b>a lot</b> of interesting features in this service but graph databases may be an unfamiliar topic for many of you so lets make sure we’re using the same vocabulary.</p> 
<b>Graph Databases</b> 
<p>A graph database is a store of vertices (nodes) and edges (relationships or connections) which can both have properties stored as key-value pairs. Graph databases are useful for connected, contextual, relationship-driven data. Some examples applications are social media networks, recommendation engines, driving directions, logistics, diagnostics, fraud detection, and genomic sequencing.</p> 
<p>Amazon Neptune supports two open standards for describing and querying your graphs:</p> 
<li><a href="https://tinkerpop.apache.org/">Apache TinkerPop3</a> style Property Graphs queried with Gremlin. Gremlin is a graph traversal language where a query is a traversal made up of discrete steps following an edge to a node.&nbsp;Your existing tools and clients that are designed to work with TinkerPop&nbsp;allow you to quickly get started with Neptune.</li> 
<li>Resource Description Framework (RDF) queried with SPARQL. SPARQL is a declarative language based on <a href="https://www.w3.org/standards/semanticweb/">Semantic Web</a> standards from W3C. It follows a subject-&gt;predicate-&gt;object model. Specifically Neptune supports the following standards: RDF 1.1., SPARQL Query 1.1., SPARQL Update 1.1, and the SPARQL Protocol 1.1.</li> 
<p>If you have existing applications that work with SPARQL or TinkerPop you should be able to start using Neptune by simply updating the endpoint your applications connect to.</p> 
<p>Let’s walk through launching Amazon Neptune.</p> 
<b>Launching Amazon Neptune</b> 
<p>Start by navigating to the Neptune console then click “Launch Neptune” to start the launch wizard.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/console_start_redux.png" /></p> 
<p>On this first screen you simply name your instance and select an instance type. Next we configure the advanced options. Many of these may look familiar to you if you’ve launched an instance-based AWS database service before, like <a href="https://aws.amazon.com/rds/" title="">Amazon Relational Database Service (RDS)</a> or <a href="https://aws.amazon.com/elasticache/" title="">Amazon ElastiCache</a>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/console_4.png" /></p> 
<p>Amazon Neptune runs securely in your VPC and can&nbsp;create its own security group that you can add your EC2 instances to for easy-access.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/console_3-1.png" /></p> 
<p>Next, we are able to configure some additional options like the parameter group, port, and a cluster name.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/console_5.png" /></p> 
<p>On this next screen we can enable KMS based encryption-at-rest, failover priority, and a backup retention time.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/console_6-1.png" /></p> 
<p>Similar to RDS maintenance of the&nbsp;database can be handled by the service.</p> 
<p>Once the instances are done provisioning you can find your connection endpoint on the Details page of the cluster. In my case it’s <code>triton.cae1ofmxxhy7.us-east-1.rds.amazonaws.com</code>.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/25/cluster_endpoint_1.png" /></p> 
<b>Using Amazon Neptune</b> 
<p>As stated above there are two different query engines that you can use with Amazon Neptune.</p> 
<p>To connect to the gremlin endpoint you can use the endpoint with <code>/gremlin</code> to do something like:</p> 
<code class="lang-bash">
curl -X POST -d '{&quot;gremlin&quot;:&quot;g.V()&quot;}' https://your-neptune-endpoint:8182/gremlin
</code> 
<p>You can similarly connect to the SPARQL endpoint with <code>/sparql</code></p> 
<code class="lang-bash">
curl -G https://your-neptune-endpoint:8182/sparql --data-urlencode 'query=select ?s ?p ?o where {?s ?p ?o}'
</code> 
<p>Before we can query data we need to populate our database. Let’s imagine we’re modeling AWS re:Invent and use the bulk loading API to insert some data.<br /> For Property Graph, Neptune supports CSVs stored in <a href="https://aws.amazon.com/s3/" title="">Amazon Simple Storage Service (S3)</a> for loading node, node properties, edges, and edge properties.</p> 
<p>A typical CSV for vertices looks like this:</p> 
<code class="lang-csv">~label,name,email,title,~id
Attendee,George Harrison,george@thebeatles.com,Lead Guitarist,1
Attendee,John Lennon,john@thebeatles.com,Guitarist,2
Attendee,Paul McCartney,paul@thebeatles.com,Lead Vocalist,3
</code> 
<p>The edges CSV looks something like this:</p> 
<code class="lang-csv">~label,~from,~to ,~id
attends,2,ARC307,attends22
attends,3,SRV422,attends27
</code> 
<p>Now to load a similarly structured CSV into Neptune we run something like this:</p> 
<code class="lang-bash">curl -H 'Content-Type: application/json' \
https://neptune-endpoint:8182/loader -d '
{
&quot;source&quot;: &quot;s3://super-secret-reinvent-data/vertex.csv&quot;,
&quot;format&quot;: &quot;csv&quot;,
&quot;region&quot;: &quot;us-east-1&quot;,
&quot;accessKey&quot;: &quot;AKIATHESEARENOTREAL&quot;,
&quot;secretKey&quot;: &quot;ThEseARE+AlsoNotRea1K3YSl0l1234coVFefE12&quot;  
}'
</code> 
<p>Which would return:</p> 
<code class="lang-json">{
&quot;status&quot; : &quot;200 OK&quot;,
&quot;payload&quot; : {
&quot;loadId&quot; : &quot;2cafaa88-5cce-43c9-89cd-c1e68f4d0f53&quot;
}
}
</code> 
<p>I could take that result and query the loading status: <code>curl https://neptune-endpoint:8182/loader/2cafaa88-5cce-43c9-89cd-c1e68f4d0f53</code></p> 
<code class="lang-json">{
&quot;status&quot; : &quot;200 OK&quot;,
&quot;payload&quot; : {
&quot;feedCount&quot; : [{&quot;LOAD_COMPLETED&quot; : 1}],
&quot;overallStatus&quot; : {
&quot;fullUri&quot; : &quot;s3://super-secret-reinvent-data/stuff.csv&quot;,
&quot;runNumber&quot; : 1,
&quot;retryNumber&quot; : 0,
&quot;status&quot; : &quot;LOAD_COMPLETED&quot;,
&quot;totalTimeSpent&quot; : 1,
&quot;totalRecords&quot; : 987,
&quot;totalDuplicates&quot; : 0,
&quot;parsingErrors&quot; : 0,
&quot;datatypeMismatchErrors&quot; : 0,
&quot;insertErrors&quot; : 0
}
}
}</code> 
<p>For this particular data serialization format I’d repeat this loading process for my edges as well.</p> 
<p>For RDF, Neptune supports four serializations: Turtle, N-Triples, N-Quads, and RDF/XML. I could load all of these through the same loading API.</p> 
<p>Now that I have my data in my database I can run some queries. In Gremlin, we write our queries as Graph Traversals. I’m a big Paul McCartney fan so I want to find all of the sessions he’s attending:<br /> <code>g.V().has(&quot;name&quot;,&quot;Paul McCartney&quot;).out(&quot;attends&quot;).id()</code></p> 
<p>This defines a graph traversal that finds all of the nodes that have the property “name” with the value “Paul McCartney” (there’s only one!). Next it follows all of the edges from that node that are of the type “attends” and gets the ids of the resulting nodes.</p> 
<code>
==&gt;ENT332
==&gt;SRV422
==&gt;DVC201
==&gt;GPSBUS216
==&gt;ENT323
</code> 
<p>Paul looks like a busy guy.</p> 
<p>Hopefully this gives you a brief overview of the capabilities of graph databases. Graph databases open up a new set of possibilities for a lot of customers and Amazon Neptune makes it easy to store and query your data at scale. I’m excited to see what amazing new products our customers build. Sign up for the <a href="https://pages.awscloud.com/NeptunePreview.html">preview</a> today.</p> 
<p>– <a href="https://twitter.com/jrhunt/">Randall</a></p> 
<p>P.S. Major thanks to Brad Bebee and Divij Vaidya for helping to create this post!</p> 
<footer> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/08/09/headshot-square-small.jpeg" /> 
<h3 class="lb-h4" property="name">Randall Hunt</h3> 
<p property="description">Senior Technical Evangelist at AWS. Formerly of NASA, SpaceX, and MongoDB.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22144');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/ddb_add_1_region_3.png" /> 
<b class="lb-b blog-post-title" property="name headline">Amazon DynamoDB Update – Global Tables and On-Demand Backup</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr">Jeff Barr</a> | on 
<time property="datePublished" datetime="2017-11-29T08:52:21+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/database/amazon-dynamodb/" title="View all posts in Amazon DynamoDB*"><span property="articleSection">Amazon DynamoDB*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/re-invent/" title="View all posts in AWS re:Invent"><span property="articleSection">AWS re:Invent</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/new-for-amazon-dynamodb-global-tables-and-on-demand-backup/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-21642" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=21642&amp;disqus_title=Amazon+DynamoDB+Update+%26%238211%3B+Global+Tables+and+On-Demand+Backup&amp;disqus_url=https://aws.amazon.com/blogs/aws/new-for-amazon-dynamodb-global-tables-and-on-demand-backup/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-21642');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS customers in a wide variety of industries use <a href="https://aws.amazon.com/dynamodb/"></a><a href="https://aws.amazon.com/dynamodb/" title="">Amazon DynamoDB</a> to store mission-critical data. Financial services, commerce, AdTech, IoT, and gaming applications (to name a few) make millions of requests per second to individual tables that contain hundreds of terabytes of data and trillions of items, and count on DynamoDB to return results in single-digit milliseconds.</p> 
<p>Today we are introducing two powerful new features that I know you will love:</p> 
<p><a href="https://aws.amazon.com/dynamodb/global-tables/"><strong>Global Tables</strong></a> – You can now create tables that are automatically replicated across two or more AWS Regions, with full support for multi-master writes, with a couple of clicks. This gives you the ability to build fast, massively scaled applications for a global user base without having to manage the replication process.</p> 
<p><a href="https://aws.amazon.com/dynamodb/backup-restore/"><strong>On-Demand Backup</strong></a> – You can now create full backups of your DynamoDB tables with a single click, and with zero impact on performance or availability. Your application remains online and runs at full speed. Backups are suitable for long-term retention and archival, and can help you to comply with regulatory requirements.</p> 
<p><span style="text-decoration: underline"><strong>Global Tables<br /> </strong></span>DynamoDB already replicates your tables across three Availability Zones to provide you with durable, highly available storage. Now you can use Global Tables to replicate your tables across two or more AWS Regions, setting it up with a couple of clicks. You get fast read and write performance that can scale to meet the needs of the most demanding global apps.</p> 
<p><img style="padding-left: 8px;padding-bottom: 8px;float: right" src="https://media.amazonwebservices.com/blog/2017/ddb_create_table_my_button_1.png" />You do not need to make any changes to your existing code. You simply send write requests and eventually consistent read requests to a DynamoDB endpoint in any of the designated Regions (writes that are associated with strongly consistent reads should share a common endpoint). Behind the scenes, DynamoDB implements multi-master writes and ensures that the last write to a particular item prevails. When you use Global Tables, each item will include a timestamp attribute representing the time of the most recent write. Updates are propagated to other Regions asynchronously via DynamoDB Streams and are typically complete within one second (you can track this using the new&nbsp;<strong>ReplicationLatency</strong> and&nbsp;<strong>PendingReplicationCount</strong> metrics).</p> 
<p>Getting started is simple. You create a table in the usual way, and then do one-click adds to arrange for replication to other Regions. You must start out with empty tables, all with the same name and key configuration (hash and optional sort). All of the tables should also share a consistent set of Auto Scaling, TTL, Local Secondary Index, Global Secondary Index, provisioned throughput settings, and IAM policies. For convenience, Auto Scaling is enabled automatically for new Global Tables.</p> 
<p>If you are not using <a href="https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/">DynamoDB Auto Scaling</a>, you should provision enough read capacity to deal with local reads along with enough write capacity to accommodate writes from all of the tables in the group and for an additional <em>system</em> write for each application write that originates in the local region. The system write is used to support the last-writer-wins model.</p> 
<p>Let’s create a Global Table that spans three Regions. I create my table in the usual way and then click on the <strong>Global Tables</strong> tab:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_global_tables_tab_3.png" /></p> 
<p>DynamoDB checks the table to make sure that it meets the requirements, and indicates that I need to enable DynamoDB Streams, which I do. Now I click on <strong>Add region, </strong>chose <strong>EU (Frankfurt), </strong>and click on<strong> Continue:</strong></p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_add_1_region_3.png" /></p> 
<p>The table is created in a matter of seconds:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_rep_not_rapped_2.png" /></p> 
<p>I do this a second time and I now have a global table that spans three AWS Regions:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_we_are_the_world_1.png" /></p> 
<p>I create an item in <span title="">EU (Ireland)</span>:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_item_dub_2.png" /></p> 
<p>And it shows up in <span title="">EU (Frankfurt)</span> right away:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_item_fra_2.png" /></p> 
<p>The cross-region replication process adds the <strong>aws:rep:updateregion </strong>and the <strong>aws:rep:updatetime</strong> attributes; they are visible to your application but should not be modified.</p> 
<p>Global Tables are available in the US East (Ohio), US East (N. Virginia), US West (Oregon), EU (Ireland), and EU (Frankfurt) Regions today, with more Regions in the works for 2018. You pay the usual DynamoDB prices for read capacity and storage, along with data transfer charges for cross-region replication. Write capacity is billed in terms of replicated write capacity units.</p> 
<p><span style="text-decoration: underline"><strong>On-Demand Backup</strong></span><br /> This feature is designed to help you to comply with regulatory requirements for long-term archival and data retention. You can create a backup with a click (or an API call) without consuming your provisioned throughput capacity or impacting the responsiveness of your application. Backups are stored in a highly durable fashion and can be used to create fresh tables.</p> 
<p>The DynamoDB Console now includes a <strong>Backups</strong> section:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_backups_tab_2.png" /></p> 
<p>I simply click on <strong>Create backup</strong> and give my backup a name:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_create_backup_2.png" /></p> 
<p>The backup is available right away! It is encrypted with an Amazon-managed key and includes all of the table data, provisioned capacity settings, and settings for Local and Global Secondary Indexes. It does not include Auto Scaling or TTL settings, tags, IAM policies, CloudWatch metrics, or CloudWatch Alarms.</p> 
<p>You may be wondering how this operation can be instant, given that some of our customers have tables approaching half of a petabyte. Behind the scenes, DynamoDB takes full snapshots and saves all change logs. Taking a backup is as simple as saving a timestamp along with the current metadata for the table.</p> 
<p>Here is my backup:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_my_backup_2.png" /></p> 
<p>And here’s how I restore it to a new table:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/ddb_restore_ashore_more_2.png" /></p> 
<p>Here are a couple of things to keep in mind about DynamoDB backups:</p> 
<p><strong>Setup </strong>– After you create a new table, DynamoDB has to do some setup work (basically enough time to eat lunch at your desk) before you can create your first backup.</p> 
<p><strong>Restoration</strong> – Restoration time depends on the size of the table, with times ranging from 30 minutes to several hours for very large tables.</p> 
<p><strong>Availability</strong> – We are rolling this new feature out on an account-by-account basis as quickly as possible, with initial availability in the <span title="">US East (Northern Virginia)</span>, <span title="">US East (Ohio)</span>, <span title="">US West (Oregon)</span>, and <span title="">EU (Ireland)</span> Regions.</p> 
<p><strong>Pricing</strong> – You pay for backup storage by the gigabyte-month and restore based on the amount of data that you restore.</p> 
<p>— <a href="https://twitter.com/jeffbarr">Jeff</a>;</p> 
<p>&nbsp;</p> 
<footer> 
<img width="100%" src="https://media.amazonwebservices.com/blog/2017/jeffbarr_purple_2017_400x400.jpg" /> 
<h3 class="lb-h4"> <a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr" property="name">Jeff Barr</a> </h3> 
<p property="description">Jeff Barr is Chief Evangelist for AWS. He started this blog in 2004 and has been writing posts just about non-stop ever since.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-21642');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/18/aurora_serverless_arch_1-741x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">In The Works – Amazon Aurora Serverless</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr">Jeff Barr</a> | on 
<time property="datePublished" datetime="2017-11-29T08:47:57+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/database/amazon-aurora/" title="View all posts in Amazon Aurora*"><span property="articleSection">Amazon Aurora*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/top-posts/" title="View all posts in Top Posts*"><span property="articleSection">Top Posts*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/in-the-works-amazon-aurora-serverless/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-21760" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=21760&amp;disqus_title=In+The+Works+%26%238211%3B+Amazon+Aurora+Serverless&amp;disqus_url=https://aws.amazon.com/blogs/aws/in-the-works-amazon-aurora-serverless/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-21760');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>You may already know about <a href="https://aws.amazon.com/rds/aurora/" title="">Amazon Aurora</a>. Available in editions that are either MySQL-compatible or PostgreSQL-compatible, Aurora is fully-managed and automatically scales to up to 64 TB of database storage. When you create an Aurora Database Instance, you choose the desired instance size and have the option to increase read throughput using read replicas. If your processing needs or your query rate changes you have the option to modify the instance size or to alter the number of read replicas as needed. This model works really well in an environment where the workload is predictable, with bounds on the request rate and processing requirement.</p> 
<p>In some cases the workloads can be intermittent and/or unpredictable, with bursts of requests that might span just a few minutes or hours per day or per week. Flash sales, infrequent or one-time events, online gaming, reporting workloads (hourly or daily), dev/test, and brand-new applications all fit the bill. Arranging to have just the right amount of capacity can be a lot work; paying for it on steady-state basis might not be sensible.</p> 
<p><span style="text-decoration: underline"><strong>Get Ready for Amazon Aurora Serverless</strong></span><br /> Today we are launching a preview (<a href="https://pages.awscloud.com/amazon-aurora-serverless-preview.html">sign up now</a>) of <a href="https://aws.amazon.com/rds/aurora/serverless/">Amazon Aurora Serverless</a>. Designed for workloads that are highly variable and subject to rapid change, this new configuration allows you to pay for the database resources you use, on a second-by-second basis.</p> 
<p>This serverless model builds on the clean separation of processing and storage that’s an intrinsic part of the Aurora architecture (read <a href="https://media.amazonwebservices.com/blog/2017/aurora-design-considerations-paper.pdf">Design Considerations for High-Throughput Cloud-Native Relational Databases</a> to learn more). Instead of choosing your database instance size up front, you create an endpoint, set the desired minimum and maximum capacity if you like, and issue queries to the endpoint. The endpoint is a simple proxy that routes your queries to a rapidly scaled fleet of database resources. This allows your connections to remain intact even as scaling operations take place behind the scenes. Scaling is rapid, with new resources coming online within 5 seconds. Here’s how it all fits together:</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/aurora_serverless_arch_1.png" /></p> 
<p>Because storage and processing are separate, you can scale all the way down to zero and pay only for storage. I think this is really cool, and I expect it to lead to the creations of new kinds of instant-on, transient applications. Scaling happens in seconds, building upon a pool of “warm” resources that are raring to go and eager to serve your requests. Special care is taken to build upon existing cached and buffered content so that newly added resources operate at full speed. You will be able to make your existing Aurora databases serverless with almost no effort.</p> 
<p>Billing is based on Aurora Capacity Units, each representing a combination of compute power and memory. It is metered in 1-second increments, with a 1-minute minimum for each newly added resource.</p> 
<p><span style="text-decoration: underline"><strong>Stay Tuned</strong></span><br /> I’ll be able to more information about Amazon Aurora Serverless in early 2018. Our current plan is to make it available in production form with MySQL compatibility in the first half, and to follow up with PostgreSQL compatibility later in the year. Today, you can <a href="https://pages.awscloud.com/amazon-aurora-serverless-preview.html">sign up for the preview</a>.</p> 
<p>— <a href="https://twitter.com/jeffbarr">Jeff</a>;</p> 
<footer> 
<img width="100%" src="https://media.amazonwebservices.com/blog/2017/jeffbarr_purple_2017_400x400.jpg" /> 
<h3 class="lb-h4"> <a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr" property="name">Jeff Barr</a> </h3> 
<p property="description">Jeff Barr is Chief Evangelist for AWS. He started this blog in 2004 and has been writing posts just about non-stop ever since.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-21760');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/eks_eat_my_donuts_now_1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Amazon Elastic Container Service for Kubernetes</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr">Jeff Barr</a> | on 
<time property="datePublished" datetime="2017-11-29T08:43:26+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/top-posts/" title="View all posts in Top Posts*"><span property="articleSection">Top Posts*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-22571" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=22571&amp;disqus_title=Amazon+Elastic+Container+Service+for+Kubernetes&amp;disqus_url=https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22571');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>My colleague Deepak Singh has a lot to say about containers!</p> 
<p>— <a href="https://twitter.com/jeffbarr">Jeff</a>;</p> 
<hr /> 
<p>We have a lot of AWS customers who run Kubernetes on AWS. In fact, according to the Cloud Native Computing Foundation, 63% of Kubernetes workloads run on AWS. While AWS is a popular place to run Kubernetes, there’s still a lot of manual configuration that customers need to manage their Kubernetes clusters. You have to install and operate the Kubernetes master and configure a cluster of Kubernetes workers. In order to achieve high availability in you Kubernetes clusters, you have to run at least three Kubernetes masters across different AZs. Each master needs to be configured to talk to each, reliably share information, load balance, and failover to the other masters if one experiences a failure. Then once you have it all set up and running you still have to deal with upgrades and patches of the masters and workers software. This all requires a good deal of operational expertise and effort, and customers asked us to make this easier.</p> 
<p><strong>Introducing Amazon EKS</strong><br /> <a href="https://aws.amazon.com/eks/">Amazon Elastic Container Service for Kubernetes (Amazon EKS)</a> is a fully managed service that makes it easy for you to use <a href="https://kubernetes.io/">Kubernetes </a>on AWS without having to be an expert in managing Kubernetes clusters. There are few things that we think developers will really like about this service. First, Amazon EKS runs the upstream version of the open-source Kubernetes software, so you can use all the existing plugins and tooling from the Kubernetes community. Applications running on Amazon EKS are fully compatible with applications running on any standard Kubernetes environment, whether running in on-premises datacenters or public clouds. This means that you can easily migrate your Kubernetes application to Amazon EKS with zero code changes. Second, Amazon EKS automatically runs K8s with three masters across three AZs to protect against a single point of failure. This multi-AZ architecture delivers resiliency against the loss of an AWS Availability Zone.</p> 
<p><img class="aligncenter size-medium" src="https://media.amazonwebservices.com/blog/2017/eks_s1_sushi_donut_1.png" /></p> 
<p>Third, Amazon EKS also automatically detects and replaces unhealthy masters, and it provides automated version upgrades and patching for the masters. Last, Amazon EKS is integrated with a number of key AWS features such as Elastic Load Balancing for load distribution, IAM for authentication, Amazon VPC for isolation, AWS PrivateLink for private network access, and AWS CloudTrail for logging.</p> 
<p><span style="text-decoration: underline"><strong>How it Works</strong></span><br /> Now, let’s see how some of this works. Amazon EKS integrates IAM authentication with Kubernetes RBAC (the native role based access control system for Kubernetes) through a collaboration with <a href="https://github.com/heptio/authenticator">Heptio</a>.</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/eks_sticker_donut_1.png" /></p> 
<p>You can assign RBAC roles directly to each IAM entity allowing you to granularly control access permissions to your Kubernetes masters. This allows you to easily manage your Kubernetes clusters using standard Kubernetes tools, such as <strong>kubectl</strong>.</p> 
<p>You can also use <a href="https://aws.amazon.com/blogs/aws/new-aws-privatelink-endpoints-kinesis-ec2-systems-manager-and-elb-apis-in-your-vpc/">PrivateLink</a> if you want to access your Kubernetes masters directly from your own Amazon VPC. With PrivateLink, your Kubernetes masters and the Amazon EKS service endpoint appear as an elastic network interface with private IP addresses in your Amazon VPC.</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/eks_mmmmm_donuts_1.png" /></p> 
<p>This allows you to access the Kubernetes masters and the Amazon EKS service directly from within your own Amazon VPC, without using public IP addresses or requiring the traffic to traverse the internet.</p> 
<p>Finally, we also built an open source <a href="https://github.com/aws/amazon-vpc-cni-k8s/">CNI plugin</a> that anyone can use with their Kubernetes clusters on AWS. This allows you to natively use Amazon VPC networking with your Kubernetes pods.</p> 
<p><img width="100%" src="https://media.amazonwebservices.com/blog/2017/eks_eat_my_donuts_now_1.png" /></p> 
<p>With Amazon EKS, launching a Kubernetes cluster is as easy as a few clicks in the AWS Management Console. Amazon EKS handles the rest, the upgrades, patching, and high availability. Amazon EKS is available in <a href="https://pages.awscloud.com/amazon-eks-preview.html">Preview</a>. We look forward to hearing your feedback.</p> 
<p>— Deepak Singh, General Manager of AWS Container Services</p> 
<footer> 
<img width="100%" src="https://media.amazonwebservices.com/blog/2017/jeffbarr_purple_2017_400x400.jpg" /> 
<h3 class="lb-h4"> <a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr" property="name">Jeff Barr</a> </h3> 
<p property="description">Jeff Barr is Chief Evangelist for AWS. He started this blog in 2004 and has been writing posts just about non-stop ever since.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22571');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/12/01/stuff.png" /> 
<b class="lb-b blog-post-title" property="name headline">Introducing AWS Fargate – Run Containers without Managing Infrastructure</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by Randall Hunt | on 
<time property="datePublished" datetime="2017-11-29T08:33:01+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/compute/aws-fargate/" title="View all posts in AWS Fargate"><span property="articleSection">AWS Fargate</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/top-posts/" title="View all posts in Top Posts*"><span property="articleSection">Top Posts*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/aws-fargate/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-22151" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=22151&amp;disqus_title=Introducing+AWS+Fargate+%26%238211%3B+Run+Containers+without+Managing+Infrastructure&amp;disqus_url=https://aws.amazon.com/blogs/aws/aws-fargate/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22151');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Containers are&nbsp;a powerful way for developers to develop, package, and deploy their applications. At AWS we have over a hundred thousand active ECS clusters and hundreds of millions of new containers started each week. That’s 400+% customer growth since 2016. Container orchestration solutions, like <a href="https://aws.amazon.com/ecs/" title="">Amazon ECS</a> and Kubernetes make it easier to deploy, manage, and scale these container workloads increasing your agility. However, with each of these&nbsp;container management solutions you’re still responsible for the availability, capacity, and maintenance of the underlying infrastructure. At AWS we saw this as an opportunity to remove some undifferentiated heavy lifting. We want to let you take full advantage of the speed, agility, and immutability that containers offer so you can focus on building your applications rather than managing your infrastructure.</p> 
<b>AWS Fargate</b> 
<p><a href="https://aws.amazon.com/fargate/">AWS Fargate</a> is an easy way to deploy your containers on AWS. To put it simply, Fargate is like EC2 but instead of giving you a virtual machine you get a container. It’s a technology that allows you to use containers as a fundamental compute primitive without having to manage the underlying instances. All you need to do is build your container image, specify the CPU and memory requirements, define your networking and IAM policies, and launch. With Fargate, you have flexible configuration options to closely match your application needs and you’re billed with per-second granularity.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/Picture1.png" /></p> 
<p>The best part? You can still use all of the same ECS primitives, APIs, and AWS integrations. Fargate provides native integrations with <a href="https://aws.amazon.com/vpc/" title="">Amazon Virtual Private Cloud</a>, <a href="https://aws.amazon.com/iam/" title="">AWS Identity and Access Management (IAM)</a>, <a href="https://aws.amazon.com/cloudwatch/" title="">Amazon CloudWatch</a> and load balancers. Fargate tasks use the AWSVPC networking mode and provision an Elastic Network Interface (ENI) in your VPC to communicate securely with your resources. With the <a href="https://aws.amazon.com/cli/" title="">AWS Command Line Interface (CLI)</a> launching a Fargate task is simple.</p> 
<code class="lang-bash">aws ecs run-task --launch-type FARGATE --cluster BlogCluster --task-definition blog --network-configuration &quot;awsvpcConfiguration={subnets=[subnet-b563fcd3]}&quot;</code> 
<p>It’s also easy to use the console to create task definitions and run tasks with the Fargate launch type.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/28/fargate_2.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/28/fargate_1.png" /></p> 
<p>Once we’ve launched a few tasks we can see them running in our cluster:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/fargate.png" /></p> 
<p>You’ll notice that ECS clusters are heterogeneous. They can contain tasks running in Fargate and on EC2.</p> 
<p>If we dive a little deeper and look at a task we can see some useful information including the ENI that&nbsp;Fargate provisioned in our VPC and all of the containers used by that task.&nbsp;The logs tab gives me easy access to my CloudWatch Logs for that task as well.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/29/fargate_4.jpg" /></p> 
<p>Let’s take a look at&nbsp;the configuration options and pricing details for Fargate.</p> 
<h3>Pricing</h3> 
<p>AWS Fargate uses an on-demand pricing model. You pay per per-second for the amount of vCPU and memory resources consumed by your applications. Price per vCPU is $0.0506 per hour and per GB memory is $0.0127 per hour. With Fargate you have 50 configuration options for vCPU and Memory to support a wide range of workloads. The configuration options are below.</p> 
<p>&nbsp;</p> 
<table> 
<tbody> 
<tr> 
<th>CPU (vCPU)</th> 
<th>Memory Values (GB)</th> 
</tr> 
<tr> 
<td>0.25</td> 
<td>0.5, 1, 2</td> 
</tr> 
<tr> 
<td>0.5</td> 
<td>1, 2, 3</td> 
</tr> 
<tr> 
<td>1</td> 
<td>Min. 2GB and Max. 8GB, in 1GB increments</td> 
</tr> 
<tr> 
<td>2</td> 
<td>Min. 4GB and Max. 16GB, in 1GB increments</td> 
</tr> 
<tr> 
<td>4</td> 
<td>Min. 8GB and Max. 30GB in 1GB increments</td> 
</tr> 
</tbody> 
</table> 
<h3>Things To Know</h3> 
<li>You can configure Fargate to closely meet your application’s resource requirements and pay only for resources required by your containers. You can launch tens or tens of thousands of containers in seconds.</li> 
<li>Fargate tasks&nbsp;run&nbsp;similarly to&nbsp;tasks running on EC2. You can add them to VPCs, configure load balancers, and assign IAM roles.</li> 
<h3>On the Roadmap</h3> 
<p>I won’t spill all the beans, but we have a really exciting roadmap for AWS Fargate. I will tell you that we plan to support launching containers on Fargate using Amazon EKS in 2018. As always, we love your feedback. Please leave a note in the <a href="https://forums.aws.amazon.com/forum.jspa?forumID=187">Amazon ECS forum</a> letting us know what you think.</p> 
<p>Fargate is available today in the US East (Northern Virginia) region.</p> 
<p>– <a href="https://twitter.com/jrhunt/">Randall</a></p> 
<footer> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/08/09/headshot-square-small.jpeg" /> 
<h3 class="lb-h4" property="name">Randall Hunt</h3> 
<p property="description">Senior Technical Evangelist at AWS. Formerly of NASA, SpaceX, and MongoDB.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-22151');
});
</script> 
</article> 
<b class="error-code">404</b>
<b>Sorry, the page you tried cannot be found.</b>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">M5 – The Next Generation of General-Purpose EC2 Instances</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr">Jeff Barr</a> | on 
<time property="datePublished" datetime="2017-11-28T21:50:59+00:00">28 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/aws/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/aws/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/aws/m5-the-next-generation-of-general-purpose-ec2-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-21565" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-official-blog&amp;disqus_identifier=21565&amp;disqus_title=M5+%26%238211%3B+The+Next+Generation+of+General-Purpose+EC2+Instances&amp;disqus_url=https://aws.amazon.com/blogs/aws/m5-the-next-generation-of-general-purpose-ec2-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-21565');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>I always advise new EC2 users to start with our general-purpose instances, run some stress tests, and to get a really good feel for the compute, memory, and networking profile of their application before taking a look at other <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>. With a broad selection of instances optimized for compute, memory, and storage, our customers have many options and the flexibility to choose the instance type that is the best fit for their needs.</p> 
<p>As you can see from my <a href="https://aws.amazon.com/blogs/aws/ec2-instance-history/">EC2 Instance History</a> post, the general-purpose (M) instances go all the way back to <a href="https://aws.amazon.com/blogs/aws/amazon_ec2_beta/">2006</a> when we launched the <strong>m1.small</strong>. We continued to evolve along this branch of our family tree, launching the the M2 (<a href="https://aws.amazon.com/blogs/aws/two-new-ec2-instance-types-additional-memory/">2009</a>), M3 (<a href="https://aws.amazon.com/blogs/aws/aws-update-new-m3-features-reduced-ebs-prices-reduced-s3-prices/">2012</a>), and the M4 (<a href="https://aws.amazon.com/blogs/aws/the-new-m4-instance-type-bonus-price-reduction-on-m3-c4/">2015</a>) instances. Our customers use the general-purpose instances to run web &amp; app servers, host enterprise applications, support online games, and build cache fleets.</p> 
<p><span style="text-decoration: underline"><strong>New M5 Instances</strong></span><br /> Today we are taking the next step forward with the launch of our new <a href="https://aws.amazon.com/ec2/instance-types/m5/">M5 instances</a>. These instances benefit from our commitment to continued innovation and offer even better price-performance than their predecessors. Based on Custom Intel<sup>&reg;</sup> Xeon<sup>&reg;</sup> Platinum 8175M series processors running at 2.5 GHz, the M5 instances are designed for highly demanding workloads and will deliver 14% better price/performance than the M4 instances on a per-core basis. Applications that use the AVX-512 instructions will crank out twice as many FLOPS per core. We’ve also added a new size at the high-end, giving you even more options.</p> 
<p>Here are the M5 instances (all VPC-only, HVM-only, and EBS-Optimized):</p> 
<table style="margin-left: auto;margin-right: auto;border: 1px solid black;border-collapse: collapse" cellpadding="8"> 
<tbody> 
<tr style="background-color: #e0e0e0"> 
<td style="border-bottom: 1px solid black;text-align: center"><strong>Instance Name</strong></td> 
<td style="border-bottom: 1px solid black;text-align: center"><strong>vCPUs<br /> </strong></td> 
<td style="border-bottom: 1px solid black" align="center"><strong>RAM<br /> </strong></td> 
<td style="border-bottom: 1px solid black" align="center"><strong>Network Bandwidth</strong></td> 
<td style="border-bottom: 1px solid black" align="center"><strong>EBS-Optimized Bandwidth</strong></td> 
</tr> 
<tr style="border-bottom: 1px solid #ddd"> 
<td align="left"><strong>m5.large</strong></td> 
<td align="center">2</td> 
<td align="center">8 GiB</td> 
<td align="center">Up to 10 Gbps</td> 
<td align="center">Up to 2120 Mbps</td> 
</tr> 
<tr style="border-bottom: 1px solid #ddd"> 
<td align="left"><strong>m5.xlarge</strong></td> 
<td align="center">4</td> 
<td align="center">16 GiB</td> 
<td align="center">Up to 10 Gbps</td> 
<td align="center">Up to 2120&nbsp;Mbps</td> 
</tr> 
<tr style="border-bottom: 1px solid #ddd"> 
<td align="left"><strong>m5.2xlarge</strong></td> 
<td align="center">8</td> 
<td align="center">32 GiB</td> 
<td align="center">Up to 10 Gbps</td> 
<td align="center">Up to 2120&nbsp;Mbps</td> 
</tr> 
<tr style="border-bottom: 1px solid #ddd"> 
<td align="left"><strong>m5.4xlarge</strong></td> 
<td align="center">16</td> 
<td align="center">64 GiB</td> 
<td align="center">Up to 10 Gbps</td> 
<td align="center">2120 Mbps</td> 
</tr> 
<tr style="border-bottom: 1px solid #ddd"> 
<td align="left"><strong>m5.12xlarge</strong></td> 
<td align="center">48</td> 
<td align="center">192 GiB</td> 
<td align="center">10 Gbps</td> 
<td align="center">5000 Mbps</td> 
</tr> 
<tr> 
<td align="left"><strong>m5.24xlarge</strong></td> 
<td align="center">96</td> 
<td align="center">384 GiB</td> 
<td align="center">25 Gbps</td> 
<td align="center">10000 Mbps</td> 
</tr> 
</tbody> 
</table> 
<p>At the top end of the lineup, the <strong>m5.24xlarge</strong> is second only to the <a href="https://aws.amazon.com/blogs/aws/ec2-in-memory-processing-update-instances-with-4-to-16-tb-of-memory-scale-out-sap-hana-to-34-tb/">X</a> instances when it comes to vCPU count, giving you more room to scale up and to consolidate workloads. The instances support <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html">Enhanced Networking</a>, and can deliver up to 25 Gbps when used within a Placement Group.</p> 
<p>In addition to dedicated, EBS-Optimized bandwidth to EBS, access to EBS storage is enhanced by the use of NVMe (you’ll need to install the proper drivers if you are using older AMIs). The combination of more bandwidth and NVMe will increase the amount of data that your M5 instances can chew through.</p> 
<p><span style="text-decoration: underline"><strong>Launch One Today</strong></span><br /> You can launch M5 instances today in the <span title="">US East (Northern Virginia)</span>, <span title="">US West (Oregon)</span>, and <span title="">EU (Ireland)</span> Regions in On-Demand and Spot form (Reserved Instances are also available), with additional Regions in the works.</p> 
<p>One quick note: the current NVMe driver is not optimized for high-performance sequential workloads and we don’t recommend the use of M5 instances in conjunction with <strong>sc1</strong> or <strong>st1</strong> volumes. We are aware of this issue and have been working to optimize the driver for this important use case.</p> 
<p>— <a href="https://twitter.com/jeffbarr">Jeff</a>;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<footer> 
<img width="100%" src="https://media.amazonwebservices.com/blog/2017/jeffbarr_purple_2017_400x400.jpg" /> 
<h3 class="lb-h4"> <a href="https://aws.amazon.com/blogs/aws/author/jbarr/" title="Posts by Jeff Barr" property="name">Jeff Barr</a> </h3> 
<p property="description">Jeff Barr is Chief Evangelist for AWS. He started this blog in 2004 and has been writing posts just about non-stop ever since.</p> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-21565');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li class="active"><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
