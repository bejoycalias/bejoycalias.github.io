<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/14/Longer-ID-Console-screenshot-1022x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Longer Resource IDs in 2018 for Amazon EC2, Amazon EBS, and Amazon VPC</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-12-14T14:25:22+00:00">14 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-elastic-block-storage-ebs/" title="View all posts in Amazon Elastic Block Storage (EBS)*"><span property="articleSection">Amazon Elastic Block Storage (EBS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-vpc/" title="View all posts in Amazon VPC*"><span property="articleSection">Amazon VPC*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-vpc-networking-content-delivery/" title="View all posts in Amazon VPC*"><span property="articleSection">Amazon VPC*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/" title="View all posts in Networking &amp; Content Delivery*"><span property="articleSection">Networking &amp; Content Delivery*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/" title="View all posts in Storage*"><span property="articleSection">Storage*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/longer-resource-ids-in-2018-for-amazon-ec2-amazon-ebs-and-amazon-vpc/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3513" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3513&amp;disqus_title=Longer+Resource+IDs+in+2018+for+Amazon+EC2%2C+Amazon+EBS%2C+and+Amazon+VPC&amp;disqus_url=https://aws.amazon.com/blogs/compute/longer-resource-ids-in-2018-for-amazon-ec2-amazon-ebs-and-amazon-vpc/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3513');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post contributed by Laura Thomson, Senior Product Manager for Amazon EC2.</em></p> 
<p>As you start planning for the new year, I want to give you a heads up that <a href="https://aws.amazon.com/ec2">Amazon EC2</a> is migrating to longer format, 17-character resource IDs. Instances and volumes currently already receive this ID format. Beginning in July 2018, all newly created EC2 resources receive longer IDs as well.</p> 
<p>The switch-over will not impact most customers. However, I wanted to make you aware so that you can schedule time at the beginning of 2018 to test your systems with the longer format. If you have a system that parses or stores resource IDs, you may be affected.</p> 
<p>From January 2018 through the end of June 2018, there will be a transition period, during which you can opt in to receive longer IDs. To make this easy, AWS will provide an option to opt in with one click for all regions, resources, and users. AWS will also provide more granular controls via API operations and console support. More information on the opt-in process will be sent out in January.</p> 
<p>We need to do this given how fast AWS is continuing to grow. We will start to run low on IDs for certain resources within a year or so. In order to enable the long-term, uninterrupted creation of new resources, we need to move to the longer ID format.</p> 
<p>The current format is a resource identifier followed by an eight-character string. The new format is the same resource identifier followed by a 17-character string. For example, your current VPCs have resource identifiers such as “vpc-1234abc0”. Starting July 2018, new VPCs will be assigned an identifier such as “vpc-1234567890abcdef0”. You can continue using the existing eight-character IDs for your existing resources, which won’t change and will continue to be supported. Only new resources will receive the 17-character IDs and only after you opt in to the new format.</p> 
<p>For more information, see <a href="https://aws.amazon.com/ec2/faqs/#longer-ids">Longer EC2, EBS, and Storage Gateway Resource IDs</a>.&nbsp; If you have any questions, contact AWS Support on the community forums and via <a href="https://console.aws.amazon.com/support/">AWS Support</a>.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3513');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/concurrency.png" /> 
<b class="lb-b blog-post-title" property="name headline">Managing AWS Lambda Function Concurrency</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2017-12-11T10:50:02+00:00">11 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3483" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3483&amp;disqus_title=Managing+AWS+Lambda+Function+Concurrency&amp;disqus_url=https://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3483');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>One of the key benefits of <a href="https://aws.amazon.com/serverless/">serverless applications</a> is the ease in which they can scale to meet traffic demands or requests, with little to no need for capacity planning. In <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, which is the core of the serverless platform at AWS, the unit of scale is a <a href="https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html">concurrent execution</a>. This refers to the number of executions of your function code that are happening at any given time.</p> 
<p>Thinking about concurrent executions as a unit of scale is a fairly unique concept. In this post, I dive deeper into this and talk about how you can make use of per function concurrency limits in Lambda.<br /> <span id="more-3483"></span></p> 
<b>Understanding concurrency in Lambda</b> 
<p>Instead of diving right into the guts of how Lambda works, here’s an appetizing analogy: <strong>a magical pizza</strong>.<br /> Yes, a magical pizza!</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/pizza.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/pizza-300x300.png" /></a></p> 
<p>This magical pizza has some unique properties:</p> 
<li>It has a fixed maximum number of slices, such as 8.</li> 
<li>Slices automatically re-appear after they are consumed.</li> 
<li>When you take a slice from the pizza, it does not re-appear until it has been completely consumed.</li> 
<li>One person can take multiple slices at a time.</li> 
<li>You can easily ask to have the number of slices increased, but they remain fixed at any point in time otherwise.</li> 
<p>Now that the magical pizza’s properties are defined, here’s a hypothetical situation of some friends sharing this pizza.</p> 
<p>Shawn, Kate, Daniela, Chuck, Ian and Avleen get together every Friday to share a pizza and catch up on their week. As there is just six of them, they can easily all enjoy a slice of pizza at a time. As they finish each slice, it re-appears in the pizza&nbsp;pan&nbsp;and they can take another slice again. Given the magical properties of their pizza, they can continue to eat all they want, but with two very important constraints:</p> 
<li>If any of them take too many slices at once, the others may not get as much as they want.</li> 
<li>If they take too many slices, they might also eat too much and get sick.</li> 
<p>One particular week, some of the friends are hungrier than the rest, taking two slices at a time instead of just one. If more than two of them try to take two pieces at a time, this can cause contention for pizza slices. Some of them would wait hungry for the slices to re-appear. They could ask for a pizza with more slices, but then run the same risk again later if more hungry friends join than planned for.</p> 
<p>What can they do?</p> 
<p>If the friends agreed to accept a limit for the maximum number of slices they each eat concurrently, both of these issues are avoided. Some could have a maximum of 2 of the 8 slices, or other concurrency limits that were more or less. Just so long as they kept it at or under eight total slices to be eaten at one time. This would keep any from going hungry or eating too much. The six friends can happily enjoy their magical pizza without worry!</p> 
<b>Concurrency in Lambda</b> 
<p>Concurrency in Lambda actually works similarly to the magical pizza model. Each AWS Account has an overall <a href="https://docs.aws.amazon.com/lambda/latest/dg/API_GetAccountSettings.html">AccountLimit</a> value that is fixed at any point in time, but can be easily increased as needed, just like the count of slices in the pizza. <a href="https://aws.amazon.com/about-aws/whats-new/2017/05/aws-lambda-raises-default-concurrent-execution-limit/">As of May 2017</a>, the default limit is 1000 “slices” of concurrency per AWS Region.</p> 
<p>Also like the magical pizza, each concurrency “slice” can only be consumed individually one at a time. After consumption, it becomes available to be consumed again. Services invoking Lambda functions can consume multiple slices of concurrency at the same time, just like the group of friends can take multiple slices of the pizza.</p> 
<p>Let’s take our example of the six friends and bring it back to AWS services that commonly invoke Lambda:</p> 
<li>Amazon S3</li> 
<li>Amazon Kinesis</li> 
<li>Amazon DynamoDB</li> 
<li>Amazon Cognito</li> 
<p>In a single account with the default concurrency limit of 1000 concurrent executions, any of these four services could invoke enough functions to consume the entire limit or some part of it. Just like with the pizza example, there is the possibility for two issues to pop up:</p> 
<li>One or more of these services could invoke enough functions to consume a majority of the available concurrency capacity. This could cause others to be starved for it, causing failed invocations.</li> 
<li>A service could consume too much concurrent capacity and cause a downstream service or database to be overwhelmed, which could cause failed executions.</li> 
<p>For Lambda functions that are launched in a VPC, you have the potential to consume the available IP addresses in a subnet or the maximum number of elastic network interfaces to which your account has access. For more information, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/vpc.html">Configuring a Lambda Function to Access Resources in an Amazon VPC</a>. For information about elastic network interface limits, see <a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html#vpc-limits-enis">Network Interfaces</a> section in the Amazon VPC Limits topic.</p> 
<p>One way to solve both of these problems is applying a concurrency limit to the Lambda functions in an account.</p> 
<b>Configuring per function concurrency limits</b> 
<p>You can now set a concurrency limit on individual Lambda functions in an account. The concurrency limit that you set reserves a portion of your account level concurrency for a given function. All of your functions’ concurrent executions count against this account-level limit by default.</p> 
<p>If you set a concurrency limit for a specific function, then that function’s concurrency limit allocation is deducted from the shared pool and assigned to that specific function. AWS also reserves 100 units of concurrency for all functions that don’t have a specified concurrency limit set. This helps to make sure that future functions have capacity to be consumed.</p> 
<p>Going back to the example of the consuming services, you could set throttles for the functions as follows:</p> 
<p>Amazon S3 function = 350<br /> Amazon Kinesis function = 200<br /> Amazon DynamoDB function = 200<br /> Amazon Cognito function = 150<br /> Total = 900</p> 
<p>With the 100 reserved for all non-concurrency reserved functions, this totals the account limit of 1000.</p> 
<p>Here’s how this works. To start, create a basic Lambda function that is invoked via <a href="https://aws.amazon.com/api-gateway">Amazon API Gateway</a>. This Lambda function returns a single “Hello World” statement with an added sleep time between 2 and 5 seconds. The sleep time simulates an API providing some sort of capability that can take a varied amount of time. The goal here is to show how an API that is underloaded can reach its concurrency limit, and what happens when it does.<br /> <strong>To create the example function</strong></p> 
<ol> 
<li>Open the <a href="https://console.aws.amazon.com/lambda/home">Lambda console</a>.</li> 
<li>Choose <strong>Create Function</strong>.</li> 
<li>For <strong>Author from scratch</strong>, enter the following values: 
<ol> 
<li>For <strong>Name</strong>, enter a value (such as concurrencyBlog01).</li> 
<li>For <strong>Runtime</strong>, choose <strong>Python 3.6</strong>.</li> 
<li>For <strong>Role</strong>, choose <strong>Create new role from template</strong> and enter a name aligned with this function, such as concurrencyBlogRole.</li> 
</ol> </li> 
<li>Choose <strong>Create function</strong>.</li> 
<li>The function is created with some basic example code. Replace that code with the following:</li> 
</ol> 
<code class="lang-python">
import time
from random import randint
seconds = randint(2, 5)
def lambda_handler(event, context):
time.sleep(seconds)
return {&quot;statusCode&quot;: 200,
&quot;body&quot;: (&quot;Hello world, slept &quot; + str(seconds) + &quot; seconds&quot;),
&quot;headers&quot;:
{
&quot;Access-Control-Allow-Headers&quot;: &quot;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token&quot;,
&quot;Access-Control-Allow-Methods&quot;: &quot;GET,OPTIONS&quot;,
}}
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/code-editor.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/code-editor.png" /></a>
</code> 
<ol start="6"> 
<li>Under <strong>Basic settings</strong>, set <strong>Timeout</strong> to 10 seconds. While this function should only ever take up to 5-6 seconds (with the 5-second max sleep), this gives you a little bit of room if it takes longer.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/basic-settings.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/basic-settings.png" /></a></p> 
<ol start="7"> 
<li>Choose <strong>Save</strong> at the top right.</li> 
</ol> 
<p>At this point, your function is configured for this example. Test it and confirm this in the console:</p> 
<ol> 
<li>Choose <strong>Test</strong>.</li> 
<li>Enter a name (it doesn’t matter for this example).</li> 
<li>Choose <strong>Create</strong>.</li> 
<li>In the console, choose <strong>Test</strong> again.</li> 
<li>You should see output similar to the following:</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/test-success.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/test-success.png" /></a></p> 
<p>Now configure API Gateway so that you have an HTTPS endpoint to test against.</p> 
<ol> 
<li>In the Lambda console, choose <strong>Configuration</strong>.</li> 
<li>Under <strong>Triggers</strong>, choose <strong>API Gateway</strong>.</li> 
<li>Open the API Gateway icon now shown as attached to your Lambda function:</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/triggers.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/triggers.png" /></a></p> 
<ol start="4"> 
<li>Under <strong>Configure triggers</strong>, leave the default values for <strong>API Name</strong> and <strong>Deployment stage</strong>. For <strong>Security</strong>, choose <strong>Open</strong>.</li> 
<li>Choose <strong>Add, Save</strong>.</li> 
</ol> 
<p>API Gateway is now configured to invoke Lambda at the Invoke URL shown under its configuration. You can take this URL and test it in any browser or command line, using tools such as “curl”:</p> 
<code class="lang-bash">
$ curl https://ofixul557l.execute-api.us-east-1.amazonaws.com/prod/concurrencyBlog01
Hello world, slept 2 seconds
</code> 
<h3>Throwing load at the function</h3> 
<p>Now start throwing some load against your API Gateway + Lambda function combo. Right now, your function is only limited by the total amount of concurrency available in an account. For this example account, you might have 850 unreserved concurrency out of a full account limit of 1000 due to having configured a few concurrency limits already (also the 100 concurrency saved for all functions without configured limits). You can find all of this information on the main <strong>Dashboard</strong> page of the Lambda console:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/lambda-dash.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/lambda-dash.png" /></a></p> 
<p>For generating load in this example, use an open source tool called “hey” (<a href="https://github.com/rakyll/hey">https://github.com/rakyll/hey</a>), which works similarly to ApacheBench (<a href="https://httpd.apache.org/docs/2.4/programs/ab.html">ab</a>). You test from an Amazon EC2 instance running the default Amazon Linux AMI from the EC2 console. For more help with configuring an EC2 instance, follow the steps in the <a href="https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LaunchInstanceWizard:">Launch Instance Wizard</a>.</p> 
<p>After the EC2 instance is running, SSH into the host and run the following:</p> 
<code class="lang-bash">
sudo yum install go
go get -u github.com/rakyll/hey
</code> 
<p>“hey” is easy to use. For these tests, specify a total number of tests (5,000) and a concurrency of 50 against the API Gateway URL as follows(replace the URL here with your own):</p> 
<code class="lang-bash">
$ ./go/bin/hey -n 5000 -c 50 https://ofixul557l.execute-api.us-east-1.amazonaws.com/prod/concurrencyBlog01
</code> 
<p>The output from “hey” tells you interesting bits of information:</p> 
<code class="lang-bash">
$ ./go/bin/hey -n 5000 -c 50 https://ofixul557l.execute-api.us-east-1.amazonaws.com/prod/concurrencyBlog01
Summary:
Total: 381.9978 secs
Slowest: 9.4765 secs
Fastest: 0.0438 secs
Average: 3.2153 secs
Requests/sec: 13.0891
Total data: 140024 bytes
Size/request: 28 bytes
Response time histogram:
0.044 [1] |
0.987 [2] |
1.930 [0] |
2.874 [1803] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
3.817 [1518] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
4.760 [719] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
5.703 [917] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
6.647 [13] |
7.590 [14] |
8.533 [9] |
9.477 [4] |
Latency distribution:
10% in 2.0224 secs
25% in 2.0267 secs
50% in 3.0251 secs
75% in 4.0269 secs
90% in 5.0279 secs
95% in 5.0414 secs
99% in 5.1871 secs
Details (average, fastest, slowest):
DNS+dialup: 0.0003 secs, 0.0000 secs, 0.0332 secs
DNS-lookup: 0.0000 secs, 0.0000 secs, 0.0046 secs
req write: 0.0000 secs, 0.0000 secs, 0.0005 secs
resp wait: 3.2149 secs, 0.0438 secs, 9.4472 secs
resp read: 0.0000 secs, 0.0000 secs, 0.0004 secs
Status code distribution:
[200] 4997 responses
[502] 3 responses
</code> 
<p>You can see a helpful histogram and latency distribution. Remember that this Lambda function has a random sleep period in it and so isn’t entirely representational of a real-life workload. Those three 502s warrant digging deeper, but could be due to Lambda cold-start timing and the “second” variable being the maximum of 5, causing the Lambda functions to time out. AWS X-Ray and the Amazon CloudWatch logs generated by both API Gateway and Lambda could help you troubleshoot this.</p> 
<h3>Configuring a concurrency reservation</h3> 
<p>Now that you’ve established that you can generate this load against the function, I show you how to limit it and protect a backend resource from being overloaded by all of these requests.</p> 
<ol> 
<li>In the console, choose <strong>Configure</strong>.</li> 
<li>Under <strong>Concurrency</strong>, for <strong>Reserve concurrency</strong>, enter 25.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/concurrency.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/concurrency.png" /></a></p> 
<ol start="3"> 
<li>Click on <strong>Save</strong> in the top right corner.</li> 
</ol> 
<p>You could also set this with the AWS CLI using the Lambda&nbsp;<a href="https://docs.aws.amazon.com/cli/latest/reference/lambda/put-function-concurrency.html">put-function-concurrency</a> command or see your current concurrency configuration via Lambda get-function. Here’s an example command:</p> 
<code class="lang-bash">
$ aws lambda get-function --function-name concurrencyBlog01 --output json --query Concurrency
{
&quot;ReservedConcurrentExecutions&quot;: 25
}
</code> 
<p>Either way, you’ve set the Concurrency Reservation to 25 for this function. This acts as both a limit and a reservation in terms of making sure that you can execute 25 concurrent functions at all times. Going above this results in the throttling of the Lambda function. Depending on the invoking service, throttling can result in a number of different outcomes, as shown in the documentation on <a href="https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html">Throttling Behavior</a>. This change has also reduced your unreserved account concurrency for other functions by 25.</p> 
<p>Rerun the same load generation as before and see what happens. Previously, you tested at 50 concurrency, which worked just fine. By limiting the Lambda functions to 25 concurrency, you should see rate limiting kick in. Run the same test again:</p> 
<code class="lang-bash">
$ ./go/bin/hey -n 5000 -c 50 https://ofixul557l.execute-api.us-east-1.amazonaws.com/prod/concurrencyBlog01
</code> 
<p>While this test runs, refresh the <strong>Monitoring</strong> tab on your function detail page. You see the following warning message:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/warning.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/11/warning.png" /></a></p> 
<p>This is great! It means that your throttle is working as configured and you are now protecting your downstream resources from too much load from your Lambda function.</p> 
<p>Here is the output from a new “hey” command:</p> 
<code class="lang-bash">
$ ./go/bin/hey -n 5000 -c 50 https://ofixul557l.execute-api.us-east-1.amazonaws.com/prod/concurrencyBlog01
Summary:
Total: 379.9922 secs
Slowest: 7.1486 secs
Fastest: 0.0102 secs
Average: 1.1897 secs
Requests/sec: 13.1582
Total data: 164608 bytes
Size/request: 32 bytes
Response time histogram:
0.010 [1] |
0.724 [3075] |∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎∎
1.438 [0] |
2.152 [811] |∎∎∎∎∎∎∎∎∎∎∎
2.866 [11] |
3.579 [566] |∎∎∎∎∎∎∎
4.293 [214] |∎∎∎
5.007 [1] |
5.721 [315] |∎∎∎∎
6.435 [4] |
7.149 [2] |
Latency distribution:
10% in 0.0130 secs
25% in 0.0147 secs
50% in 0.0205 secs
75% in 2.0344 secs
90% in 4.0229 secs
95% in 5.0248 secs
99% in 5.0629 secs
Details (average, fastest, slowest):
DNS+dialup: 0.0004 secs, 0.0000 secs, 0.0537 secs
DNS-lookup: 0.0002 secs, 0.0000 secs, 0.0184 secs
req write: 0.0000 secs, 0.0000 secs, 0.0016 secs
resp wait: 1.1892 secs, 0.0101 secs, 7.1038 secs
resp read: 0.0000 secs, 0.0000 secs, 0.0005 secs
Status code distribution:
[502] 3076 responses
[200] 1924 responses
</code> 
<p>This looks fairly different from the last load test run. A large percentage of these requests failed fast due to the concurrency throttle failing them (those with the 0.724 seconds line). The timing shown here in the histogram represents the entire time it took to get a response between the EC2 instance and API Gateway calling Lambda and being rejected. It’s also important to note that this example was configured with an <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-api-migration.html">edge-optimized endpoint</a> in API Gateway. You see under <strong>Status code distribution</strong> that 3076 of the 5000 requests failed with a 502, showing that the backend service from API Gateway and Lambda failed the request.</p> 
<b>Other uses</b> 
<p>Managing function concurrency can be useful in a few other ways beyond just limiting the impact on downstream services and providing a reservation of concurrency capacity. Here are two other uses:</p> 
<li>Emergency kill switch</li> 
<li>Cost controls</li> 
<h3>Emergency kill switch</h3> 
<p>On occasion, due to issues with applications I’ve managed in the past, I’ve had a need to disable a certain function or capability of an application. By setting the concurrency reservation and limit of a Lambda function to zero, you can do just that.</p> 
<p>With the reservation set to zero every invocation of a Lambda function results in being throttled. You could then work on the related parts of the infrastructure or application that aren’t working, and then reconfigure the concurrency limit to allow invocations again.</p> 
<h3>Cost controls</h3> 
<p>While I mentioned how you might want to use concurrency limits to control the downstream impact to services or databases that your Lambda function might call, another resource that you might be cautious about is money. Setting the concurrency throttle is another way to help control costs during development and testing of your application.</p> 
<p>You might want to prevent against a function performing a recursive action too quickly or a development workload generating too high of a concurrency. You might also want to protect development resources connected to this function from generating too much cost, such as APIs that your Lambda function calls.</p> 
<b>Conclusion</b> 
<p>Concurrent executions as a unit of scale are a fairly unique characteristic about Lambda functions. Placing limits on how many concurrency “slices” that your function can consume can prevent a single function from consuming all of the available concurrency in an account. Limits can also prevent a function from overwhelming a backend resource that isn’t as scalable.</p> 
<p>Unlike monolithic applications or even microservices where there are mixed capabilities in a single service, Lambda functions encourage a sort of “nano-service” of <a href="https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html">small business logic</a> directly related to the integration model connected to the function. I hope you’ve enjoyed this post and configure your concurrency limits today!</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/deployment/" rel="tag">deployment</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3483');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/ECS_windows_container-two.png" /> 
<b class="lb-b blog-post-title" property="name headline">Running Windows Containers on Amazon ECS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-12-06T10:33:55+00:00">06 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/running-windows-containers-on-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3464" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3464&amp;disqus_title=Running+Windows+Containers+on+Amazon+ECS&amp;disqus_url=https://aws.amazon.com/blogs/compute/running-windows-containers-on-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3464');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post was developed and written by Jeremy Cowan, Thomas Fuller, Samuel Karp, and Akram Chetibi.<br /> </em></p> 
<p>—</p> 
<p>Containers have revolutionized the way that developers build, package, deploy, and run applications. Initially, containers only supported code and tooling for Linux applications. With the release of Docker Engine for Windows Server 2016, Windows developers have started to realize the gains that their Linux counterparts have experienced for the last several years.</p> 
<p>This week, <a href="https://aws.amazon.com/about-aws/whats-new/2017/12/amazon-ecs-support-for-windows-server-containers-ga/">we’re adding support</a> for running production workloads in Windows containers using Amazon Elastic Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>). Now, Amazon ECS provides an ECS-Optimized&nbsp;Windows Server Amazon Machine Image (AMI). This AMI is based on the EC2 Windows Server 2016&nbsp;AMI, and includes Docker 17.06 Enterprise Edition and the ECS Agent 1.16. This AMI provides improved&nbsp;instance and container launch time performance.&nbsp;It’s based on Windows Server&nbsp;2016 Datacenter and includes Docker 17.06.2-ee-5, along with a new version of the ECS agent that now runs as a native Windows service.<span id="more-3464"></span></p> 
<p>In this post, I discuss the benefits of this new support, and walk you through getting started running Windows containers with Amazon ECS.</p> 
<p>When AWS released the Windows Server 2016 Base with Containers AMI, the ECS agent ran as a process that made it difficult to monitor and manage. As a service, the agent can be health-checked, managed, and restarted no differently than other Windows services. The AMI also includes pre-cached images for Windows Server&nbsp;Core 2016 and Windows Server&nbsp;Nano&nbsp;Server 2016. By caching the images in the AMI, launching new Windows containers is significantly faster. When Docker images include a layer that’s already cached on the instance, Docker re-uses that layer instead of pulling it from the Docker registry.</p> 
<p>The ECS agent and an accompanying ECS PowerShell module used to install, configure, and run the agent come pre-installed on the AMI. This guarantees there is a specific platform version available on the container instance at launch. Because the software is included, you don’t have to download it from the internet. This saves startup time.</p> 
<p>The Windows-compatible ECS-optimized AMI also reports CPU and memory utilization and reservation metrics to <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch</a>. Using the CloudWatch integration with ECS, you can create alarms that trigger dynamic scaling events to automatically add or remove capacity to your EC2 instances and ECS tasks.</p> 
<b>Getting started</b> 
<p>To help you get started running Windows containers on ECS, I’ve forked the <a href="https://github.com/awslabs/ecs-refarch-cloudformation">ECS reference architecture</a>, to build an ECS cluster comprised of Windows instances instead of Linux instances. You can pull the latest version of the <a href="https://github.com/aws-samples/ecs-refarch-cloudformation-windows">reference architecture for Windows</a>.</p> 
<p>The reference architecture is a layered CloudFormation stack, in that it calls other stacks to create the environment. Within the stack, the ecs-windows-cluster.yaml file contains the instructions for bootstrapping the Windows instances and configuring the ECS cluster. To configure the instances outside of AWS CloudFormation (for example, through the CLI or the console), you can add the following commands to your instance’s user data:</p> 
<code class="lang-yaml">Import-Module ECSTools
Initialize-ECSAgent</code> 
<p>Or</p> 
<code class="lang-yaml">Import-Module ECSTools
Initialize-ECSAgent –Cluster MyCluster -EnableIAMTaskRole</code> 
<p>If you don’t specify a cluster name when you initialize the agent, the instance is joined to the default cluster.</p> 
<p>Adding <code class="lang-bash">-EnableIAMTaskRole</code> when initializing the agent adds support for IAM roles for tasks. Previously, enabling this setting meant running a complex script and setting an environment variable before you could assign roles to your ECS tasks.</p> 
<p>When you enable IAM roles for tasks on Windows, it consumes port 80 on the host. If you have tasks that listen on&nbsp;port 80 on the host, I recommend configuring a service for them that uses load balancing. You can use port 80 on the load balancer, and the traffic can be routed to another host port on your container instances. For more information, see <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html">Service Load Balancing</a>.</p> 
<h3>Create a cluster</h3> 
<p>To create a new ECS cluster, choose Launch stack, or pull the GitHub project to your local machine and run the following command:</p> 
<p><code class="lang-bash">aws cloudformation create-stack –template-body file://&lt;path to master-windows.yaml&gt; --stack-name &lt;name&gt;</code></p> 
<h3>Upload your container image</h3> 
<p>Now that you have a cluster running, step through how to build and push an image into a container repository. You use a repository hosted in Amazon Elastic Container Registry (<a href="https://aws.amazon.com/ecr">Amazon ECR</a>) for this, but you could also use Docker Hub. To build and push an image to a repository, install Docker on your Windows* workstation. You also create a repository and assign the necessary permissions to the account that pushes your image to Amazon ECR. For detailed instructions, see <a href="http://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Pushing an Image</a>.</p> 
<p><em>* If you are building an image that is based on Windows layers, then you must use a Windows environment to build and push your image to the registry.</em></p> 
<h3>Write your task definition</h3> 
<p>Now that your image is built and ready, the next step is to run your Windows containers using a task.</p> 
<p>Start by creating a new task definition based on the <a href="https://hub.docker.com/r/microsoft/iis">windows-simple-iis</a> image from Docker Hub.</p> 
<ol> 
<li>Open the <a href="https://console.aws.amazon.com/ecs/home">ECS console</a>.</li> 
<li>Choose <strong>Task Definitions</strong>, <strong>Create new task definition</strong>.</li> 
<li>Scroll to the bottom of the page and choose <strong>Configure via JSON</strong>.</li> 
<li>Copy and paste the following JSON into that field.</li> 
<li>Choose <strong>Save, Create</strong>.</li> 
</ol> 
<code class="lang-json">{
&quot;family&quot;: &quot;windows-simple-iis&quot;,
&quot;containerDefinitions&quot;: [
{
&quot;name&quot;: &quot;windows_sample_app&quot;,
&quot;image&quot;: &quot;microsoft/iis&quot;,
&quot;cpu&quot;: 100,
&quot;entryPoint&quot;:[&quot;powershell&quot;, &quot;-Command&quot;],
&quot;command&quot;:[&quot;New-Item -Path C:\\inetpub\\wwwroot\\index.html -Type file -Value '&lt;html&gt;&lt;head&gt;&lt;title&gt;Amazon ECS Sample App&lt;/title&gt; &lt;style&gt;body {margin-top: 40px; background-color: #333;} &lt;/style&gt; &lt;/head&gt;&lt;body&gt; &lt;div style=color:white;text-align:center&gt;&lt;b&gt;Amazon ECS Sample App&lt;/b&gt; &lt;b&gt;Congratulations!&lt;/b&gt; &lt;p&gt;Your application is now running on a container in Amazon ECS.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;'; C:\\ServiceMonitor.exe w3svc&quot;],
&quot;portMappings&quot;: [
{
&quot;protocol&quot;: &quot;tcp&quot;,
&quot;containerPort&quot;: 80,
&quot;hostPort&quot;: 8080
}
],
&quot;memory&quot;: 500,
&quot;essential&quot;: true
}
]
}</code> 
<p>You can now go back into the Task Definition page and see <code class="lang-bash">windows-simple-iis</code> as an available task definition.</p> 
<p>There are a&nbsp;few important aspects of the task definition file to note when working with Windows containers. First, the hostPort is configured as 8080, which is necessary because the ECS agent currently uses port 80 to enable IAM roles for tasks required for least-privilege security configurations.</p> 
<p>There are also some fairly standard task parameters that are intentionally not included. For example, network mode is not available with Windows at the time of this release, so keep that setting blank to allow Docker to configure WinNAT, the only option available today.</p> 
<p>Also, some parameters work differently with Windows than they do with Linux. The CPU limits that you define in the task definition are absolute, whereas on Linux they are weights. For information about other task parameters that are supported or possibly different with Windows, see the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/windows_task_definitions.html#windows_sample_task_defs">documentation</a>.</p> 
<h3>Run your containers</h3> 
<p>At this point, you are ready to run containers. There are two options to run containers with ECS:</p> 
<ol> 
<li>Task</li> 
<li>Service</li> 
</ol> 
<p>A task is typically a short-lived process that ECS creates. It can’t be configured to actively monitor or scale. A service is meant for longer-running containers and can be configured to use a load balancer, minimum/maximum capacity settings, and a number of other knobs and switches to help ensure that your code keeps running. In both cases, you are able to pick a placement strategy and a specific IAM role for your container.</p> 
<ol> 
<li>Select the task definition that you created above and choose Action, Run Task.</li> 
<li>Leave the settings on the next page to the default values.</li> 
<li>Select the ECS cluster created when you ran the CloudFormation template.</li> 
<li>Choose Run Task to start the process of scheduling a Docker container on your ECS cluster.</li> 
</ol> 
<p>You can now go to the cluster and watch the status of your task. It may take 5–10 minutes for the task to go from <strong>PENDING</strong> to <strong>RUNNING</strong>, mostly because it takes time to download all of the layers necessary to run the&nbsp;<em>microsoft/iis</em> image. After the status is <strong>RUNNING</strong>, you should see the following results:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/17-12-Windows-GA-1.png" /></p> 
<p>You may have noticed that the example task definition is&nbsp;named <em>windows-simple-iis:2</em>. This is because I created a second version of the task definition, which is one of the powerful capabilities of using ECS. You can make the task definitions part of your source code and then version them. You can also roll out new versions and practice blue/green deployment, switching to reduce downtime and improve the velocity of your deployments!</p> 
<p>After the task has moved to <strong>RUNNING</strong>, you can see your website hosted in ECS. Find the public IP or DNS for your ECS host. Remember that you are hosting on port 8080. Make sure that the security group allows ingress from your client IP address to that port and that your VPC has an internet gateway associated with it. You should see a page that looks like the following:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/17-12-Windows-GA-2.png" /></p> 
<p>This is a nice start to deploying a simple single instance task, but what if you had a Web API to be scaled out and in based on usage? This is where you could look at defining a service and collecting CloudWatch data to add and remove both instances of the task. You could also use CloudWatch alarms to add more ECS container instances and keep up with the demand. The former is built into the configuration of your service.</p> 
<ol> 
<li>Select the task definition and choose&nbsp;<strong>Create Service</strong>.</li> 
<li>Associate a load balancer.</li> 
<li>Set up Auto Scaling.</li> 
</ol> 
<p>The following screenshot shows an example where you would add an additional task instance when the <strong>CPU Utilization</strong> CloudWatch metric is over 60% on average over three consecutive measurements. This may not be aggressive enough for your requirements; it’s meant to show you the option to scale tasks the same way you scale ECS instances with an Auto Scaling group. The difference is that these tasks start much faster because all of the base layers are already on the ECS host.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/17-12-Windows-GA-3.png" /></p> 
<p>Do not confuse task dynamic scaling with ECS instance dynamic scaling. To add additional hosts, see <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/cloudwatch_alarm_autoscaling.html">Tutorial: Scaling Container Instances with CloudWatch Alarms</a>.</p> 
<b>Conclusion</b> 
<p>This is just scratching the surface of the flexibility that you get from using containers and Amazon ECS. For more information, see the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html">Amazon ECS Developer Guide</a> and <a href="https://aws.amazon.com/ecs/resources">ECS Resources</a>.</p> 
<p>– Jeremy, Thomas, Samuel, Akram</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/net/" rel="tag">.net</a>, <a href="https://aws.amazon.com/blogs/compute/tag/containers/" rel="tag">Containers</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/windows/" rel="tag">Windows</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3464');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/reinvent_2017_banner_1.png" /> 
<b class="lb-b blog-post-title" property="name headline">The re:Invent 2017 Containers After-party Guide</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tiffany Jernigan</span></span> and 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-12-05T22:28:10+00:00">05 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-fargate/" title="View all posts in AWS Fargate"><span property="articleSection">AWS Fargate</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/the-reinvent-2017-containers-after-party-guide/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3448" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3448&amp;disqus_title=The+re%3AInvent+2017+Containers+After-party+Guide&amp;disqus_url=https://aws.amazon.com/blogs/compute/the-reinvent-2017-containers-after-party-guide/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3448');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Feeling uncontainable? re:Invent 2017 might be over, but the containers party doesn’t have to stop. Here are some ways you can keep learning about containers on AWS.</p> 
<b>Learn about containers in Austin and New York</b> 
<p>Come join AWS this week at <a href="http://events.linuxfoundation.org/events/kubecon-and-cloudnativecon-north-america">KubeCon</a> in Austin, Texas! We’ll be sharing best practices for running Kubernetes on AWS and talking about Amazon ECS, AWS Fargate, and Amazon EKS. Want to take Amazon EKS for a test drive? <a href="https://pages.awscloud.com/amazon-eks-preview.html">Sign up for the preview</a>.</p> 
<p>We’ll also be talking Containers at the NYC Pop-up Loft during <em>AWS Compute Evolved: Containers Day</em> on December 13th. <a href="https://awscomputeevolvedweekny122017.splashthat.com/">Register to attend</a>.</p> 
<b>Join an upcoming webinar</b> 
<p>Didn’t get to attend re:Invent or want to hear a recap? Join our upcoming webinar, <strong>What You Missed at re:Invent 2017</strong>, on December 11th from 12:00 PM – 12:40 PM PT (3:00 PM – 3:40 PM ET). <a href="https://pages.awscloud.com/registration_121117_Containers-on-AWS-What-You-Missed-at-reInvent-2017.html">Register to attend</a>.</p> 
<b>Start (or finish) a workshop</b> 
<p>All of the containers workshops given at re:Invent are available online. Get comfortable, fire up your browser, and start building!</p> 
<li><a href="https://interstella.trade/">Interstella GTC</a></li> 
<li><a href="http://docs.catsndogs.lol/">Cats ‘n Dogs</a></li> 
<li><a href="https://github.com/aws-samples/aws-workshop-for-kubernetes">Mastering Kubernetes on AWS</a></li> 
<li><a href="https://aws.amazon.com/getting-started/container-microservices-tutorial/">Break the Monolith!</a></li> 
<b>re:Watch your favorite talks</b> 
<p>All of the keynote and breakouts from re:Invent are available to watch on our <a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukc6GguNSQ9W0ylmx5Rckll7">YouTube playlist</a>. Slides can be found as they are uploaded on the <a href="https://www.slideshare.net/AmazonWebServices/tagged/awsreinvent2017">AWS Slideshare</a>. Just slip into your pajamas, make some popcorn, and start watching!</p> 
<li><a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukc6GguNSQ9W0ylmx5Rckll7">re:Invent containers breakout recordings</a></li> 
<li><a href="https://www.slideshare.net/AmazonWebServices/tagged/awsreinvent2017">re:Invent containers slides</a></li> 
<b>Learn more about what’s new</b> 
<p>Andy Jassy announced two big updates to the container landscape at re:Invent, AWS Fargate and Amazon EKS. Here are some resources to help you learn more about all the new features and products we announced, why we built them, and how they work.</p> 
<h3>AWS Fargate</h3> 
<p>AWS Fargate is a technology that allows you to run containers without having to manage servers or clusters.</p> 
<li><a href="https://www.youtube.com/watch?v=8i82i9QYUGs">Keynote</a></li> 
<li><a href="https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-aws-fargate-a-technology-to-run-containers-without-managing-infrastructure">What’s New</a></li> 
<li><a href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes">AWS News Blog</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/aws-fargate-a-product-overview">Compute Blog</a></li> 
<li><a href="https://www.youtube.com/watch?v=0SceSgOTyrw">CON 214 | Introduction to AWS Fargate</a></li> 
<li><a href="https://www.youtube.com/watch?v=CdxdjDpF8Eo">CON 333 | Deep Dive into AWS Fargate</a></li> 
<li><a href="https://github.com/aws/amazon-ecs-cli/releases/tag/v1.1.0">ECS CLI v1.1.0</a></li> 
<li><a href="https://aws.amazon.com/fargate">Product Page</a></li> 
<h3>Amazon Elastic Container Service for Kubernetes (Amazon EKS)</h3> 
<p>Amazon Elastic Container Service for Kubernetes (Amazon EKS) is a managed service that makes it easy for you to run Kubernetes on AWS without needing to configure and operate your own Kubernetes clusters.</p> 
<li><a href="https://www.youtube.com/watch?v=gUFtUU7qvSQ">Keynote</a></li> 
<li><a href="https://aws.amazon.com/about-aws/whats-new/2017/11/introducing-amazon-elastic-container-service-for-kubernetes">What’s New</a></li> 
<li><a href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes">Blog</a></li> 
<li><a href="https://www.youtube.com/watch?v=WHTejF3W0s4">CON 215 | Intro to Amazon Elastic Container Service for Kubernetes (EKS)</a></li> 
<li><a href="https://aws.amazon.com/eks">Product Page</a></li> 
<p>We hope you had a great re:Invent and look forward to seeing what you build on AWS in 2018!</p> 
<p>– The AWS Containers Team</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-ecs/" rel="tag">Amazon ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/containers/" rel="tag">Containers</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/kubernetes/" rel="tag">Kubernetes</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3448');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/SGK_Arch_Diagram_Resized-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Implementing Dynamic ETL Pipelines Using AWS Step Functions</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> and 
<span property="author" typeof="Person"><span property="name">Wangechi Doble</span></span> | on 
<time property="datePublished" datetime="2017-12-05T16:44:24+00:00">05 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-integration/" title="View all posts in Application Integration"><span property="articleSection">Application Integration</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/implementing-dynamic-etl-pipelines-using-aws-step-functions/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3409" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3409&amp;disqus_title=Implementing+Dynamic+ETL+Pipelines+Using+AWS+Step+Functions&amp;disqus_url=https://aws.amazon.com/blogs/compute/implementing-dynamic-etl-pipelines-using-aws-step-functions/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3409');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h5><strong>This post contributed by:<br /> Wangechi Dole, AWS Solutions Architect<em><br /> </em>Milan Krasnansky, ING, Digital Solutions Developer, SGK<em><br /> </em>Rian Mookencherry, Director – Product Innovation, SGK</strong></h5> 
<p>Data processing and transformation is a common use case you see in our <a href="https://aws.amazon.com/solutions/case-studies/">customer case studies and success stories</a>. Often, customers deal with complex data from a variety of sources that needs to be transformed and customized through a series of steps to make it useful to different systems and stakeholders. This can be difficult due to the ever-increasing volume, velocity, and variety of data. Today, data management challenges cannot be solved with traditional databases.</p> 
<p>Workflow automation helps you build solutions that are repeatable, scalable, and reliable. You can use <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> for this. A great example is how <a href="http://www.sgkinc.com/">SGK</a> used Step Functions to automate the ETL processes for their client. With Step Functions, SGK has been able to automate changes within the data management system, substantially reducing the time required for data processing.</p> 
<p>In this post, SGK shares the details of how they used Step Functions to build a robust data processing system based on highly configurable business transformation rules for ETL processes.</p> 
<b>SGK: Building dynamic ETL pipelines</b> 
<p>SGK is a subsidiary of Matthews International Corporation, a diversified organization focusing on brand solutions and industrial technologies. SGK’s Global Content Creation Studio network creates compelling content and solutions that connect brands and products to consumers through multiple assets including photography, video, and copywriting.</p> 
<p>We were recently contracted to build a sophisticated and scalable data management system for one of our clients. We chose to build the solution on AWS to leverage advanced, managed services that help to improve the speed and agility of development.</p> 
<p>The data management system served two main functions:</p> 
<ol> 
<li>Ingesting a large amount of complex data to facilitate both reporting and product funding decisions for the client’s global marketing and supply chain organizations.</li> 
<li>Processing the data through normalization and applying complex algorithms and data transformations. The system goal was to provide information in the relevant context—such as strategic marketing, supply chain, product planning, etc. —to the end consumer through automated data feeds or updates to existing ETL systems.</li> 
</ol> 
<p>We were faced with several challenges:</p> 
<li>Output data that needed to be refreshed at least twice a day to provide fresh datasets to both local and global markets. That constant data refresh posed several challenges, especially around data management and replication across multiple databases.</li> 
<li>The complexity of reporting business rules that needed to be updated on a constant basis.</li> 
<li>Data that could not be processed as contiguous blocks of typical time-series data. The measurement of the data was done across seasons (that is, combination of dates), which often resulted with up to three overlapping seasons at any given time.</li> 
<li>Input data that came from 10+ different data sources. Each data source ranged from 1–20K rows with as many as 85 columns per input source.</li> 
<p>These challenges meant that our small Dev team heavily invested time in frequent configuration changes to the system and data integrity verification to make sure that everything was operating properly. Maintaining this system proved to be a daunting task and that’s when we turned to Step Functions—along with other AWS services—to automate our ETL processes.</p> 
<h3>Solution overview</h3> 
<p>Our solution included the following AWS services:</p> 
<li><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a>: Before Step Functions was available, we were using multiple Lambda functions for this use case and running into memory limit issues. With Step Functions, we can execute steps in parallel simultaneously, in a cost-efficient manner, without running into memory limitations.</li> 
<li><a href="https://aws.amazon.com/lambda/">AWS Lambda</a>: The Step Functions state machine uses Lambda functions to implement the Task states. Our Lambda functions are implemented in Java 8.</li> 
<li><a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a> provides us with an easy and flexible way to manage business rules. We specify our rules as Keys. These are key-value pairs stored in a DynamoDB table.</li> 
<li><a href="https://aws.amazon.com/rds/">Amazon RDS</a>: Our ETL pipelines consume source data from our RDS MySQL database.</li> 
<li><a href="https://aws.amazon.com/redshift/">Amazon Redshift</a>: We use Amazon Redshift for reporting purposes because it integrates with our BI tools. Currently we are using Tableau for reporting which integrates well with Amazon Redshift.</li> 
<li><a href="https://aws.amazon.com/s3/">Amazon S3</a>: We store our raw input files and intermediate results in S3 buckets.</li> 
<li><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a>: Our users expect results at a specific time. We use CloudWatch Events to trigger Step Functions on an automated schedule.</li> 
<h3>Solution architecture</h3> 
<p>This solution uses a&nbsp;declarative&nbsp;approach to defining business transformation rules that are applied by the underlying Step Functions state machine as data moves from RDS to Amazon Redshift.&nbsp;An S3 bucket is used to store intermediate results. A CloudWatch Event rule triggers the Step Functions state machine on a schedule. The following diagram illustrates our architecture:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/SGK_Arch_Diagram_Resized-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/SGK_Arch_Diagram_Resized-1.png" /></a></p> 
<p>Here are more details for the above diagram:</p> 
<ol> 
<li>A rule in CloudWatch Events triggers the state machine execution on an automated schedule.</li> 
<li>The state machine invokes the first Lambda function.</li> 
<li>The Lambda function deletes all existing records in Amazon Redshift. Depending on the dataset, the Lambda function can create a new table in Amazon Redshift to hold the data.</li> 
<li>The same Lambda function then retrieves Keys from a DynamoDB table. Keys represent specific marketing campaigns or seasons and map to specific records in RDS.</li> 
<li>The state machine executes the second Lambda function using the Keys from DynamoDB.</li> 
<li>The second Lambda function retrieves the referenced dataset from RDS. The records retrieved represent the entire dataset needed for a specific marketing campaign.</li> 
<li>The second Lambda function executes in parallel for each Key retrieved from DynamoDB and stores the output in CSV format temporarily in S3.</li> 
<li>Finally, the Lambda function uploads the data into Amazon Redshift.</li> 
</ol> 
<p>To understand the above data processing workflow, take a closer look at the Step Functions state machine for this example.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/SGK_statemacine_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/SGK_statemacine_1.png" /></a></p> 
<p>We walk you through the state machine in more detail in the following sections.</p> 
<h3>Walkthrough</h3> 
<p>To get started, you need to:</p> 
<li>Create a schedule in CloudWatch Events</li> 
<li>Specify conditions for RDS data extracts</li> 
<li>Create Amazon Redshift input files</li> 
<li>Load data into Amazon Redshift</li> 
<p><strong>Step 1: Create a schedule in CloudWatch Events</strong><br /> Create rules in CloudWatch Events to trigger the Step Functions state machine on an automated schedule. The following is an example cron expression to automate your schedule:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/EventSource_resized.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/EventSource_resized.png" /></a></p> 
<p>In this example, the cron expression invokes the Step Functions state machine at 3:00am and 2:00pm (UTC) every day.</p> 
<p><strong>Step 2: Specify conditions for RDS data extracts</strong><br /> We use DynamoDB to store Keys that determine which rows of data to extract from our RDS MySQL database. An example Key is MCS2017, which stands for, Marketing Campaign Spring 2017. Each campaign has a specific start and end date and the corresponding dataset is stored in RDS MySQL. A record in RDS contains about 600 columns, and each Key can represent up to 20K records.</p> 
<p>A given day can have multiple campaigns with different start and end dates running simultaneously. In the following example DynamoDB item, three campaigns are specified for the given date.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/campaigns_resized.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/05/campaigns_resized.png" /></a></p> 
<p>The state machine example shown above&nbsp;uses Keys 31, 32, and 33 in the first ChoiceState and Keys 21 and 22 in the second ChoiceState. These keys represent marketing campaigns for a given day. For example, on Monday, there are only two campaigns requested. The ChoiceState with Keys 21 and 22 is executed. If three campaigns are requested on Tuesday, for example, then ChoiceState with Keys 31, 32, and 33 is executed. MCS2017 can be represented by Key 21 and Key 33 on Monday and Tuesday, respectively. This approach gives us the flexibility to add or remove campaigns dynamically.</p> 
<p><strong>Step 3: Create Amazon Redshift input files</strong><br /> When the state machine begins execution, the first Lambda function is invoked as the resource for FirstState, represented in the Step Functions state machine as follows:</p> 
<code class="lang-json">&quot;Comment&quot;: ” AWS Amazon States Language.&quot;, 
&quot;StartAt&quot;: &quot;FirstState&quot;,
 &quot;States&quot;: { 
&quot;FirstState&quot;: {
 &quot;Type&quot;: &quot;Task&quot;,
 &quot;Resource&quot;: &quot;arn:aws:lambda:xx-xxxx-x:XXXXXXXXXXXX:function:Start&quot;,
&quot;Next&quot;: &quot;ChoiceState&quot; 
} </code> 
<p>As described in the solution architecture, the purpose of this Lambda function is to delete existing data in Amazon Redshift and retrieve keys from DynamoDB. In our use case, we found that deleting existing records was more efficient and less time-consuming than finding the delta and updating existing records. On average, an Amazon Redshift table can contain about 36 million cells, which translates to roughly 65K records. The following is the code snippet for the first Lambda function in Java 8:</p> 
<code class="lang-java">public class LambdaFunctionHandler implements RequestHandler&lt;Map&lt;String,Object&gt;,Map&lt;String,String&gt;&gt; {
Map&lt;String,String&gt; keys= new HashMap&lt;&gt;();
public Map&lt;String, String&gt; handleRequest(Map&lt;String, Object&gt; input, Context context){
Properties config = getConfig(); 
// 1. Cleaning Redshift Database
new RedshiftDataService(config).cleaningTable(); 
// 2. Reading data from Dynamodb
List&lt;String&gt; keyList = new DynamoDBDataService(config).getCurrentKeys();
for(int i = 0; i &lt; keyList.size(); i++) {
keys.put(”key&quot; + (i+1), keyList.get(i)); 
}
keys.put(”key&quot; + T,String.valueOf(keyList.size()));
// 3. Returning the key values and the key count from the “for” loop
return (keys);
}</code> 
<p>The following JSON represents ChoiceState.</p> 
<code class="lang-json">&quot;ChoiceState&quot;: {
&quot;Type&quot; : &quot;Choice&quot;,
&quot;Choices&quot;: [ 
{ 
&quot;Variable&quot;: &quot;$.keyT&quot;,
&quot;StringEquals&quot;: &quot;3&quot;,
&quot;Next&quot;: &quot;CurrentThreeKeys&quot; 
}, 
{ 
&quot;Variable&quot;: &quot;$.keyT&quot;,
&quot;StringEquals&quot;: &quot;2&quot;,
&quot;Next&quot;: &quot;CurrentTwooKeys&quot; 
} 
], 
&quot;Default&quot;: &quot;DefaultState&quot;
}
</code> 
<p>The variable $.keyT represents the number of keys retrieved from DynamoDB. This variable determines which of the parallel branches should be executed. At the time of publication, Step Functions does not support dynamic parallel state. Therefore, choices under ChoiceState are manually created and assigned hardcoded StringEquals values. These values represent the number of parallel executions for the second Lambda function.</p> 
<p>For example, if $.keyT equals 3, the second Lambda function is executed three times in parallel with keys, $key1, $key2 and $key3 retrieved from DynamoDB. Similarly, if $.keyT equals two, the second Lambda function is executed twice in parallel. &nbsp;The following JSON represents this parallel execution:</p> 
<code class="lang-json">&quot;CurrentThreeKeys&quot;: { 
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;Next&quot;: &quot;NextState&quot;,
&quot;Branches&quot;: [ 
{ 
&quot;StartAt&quot;: “key31&quot;,
&quot;States&quot;: { 
“key31&quot;: { 
&quot;Type&quot;: &quot;Task&quot;,
&quot;InputPath&quot;: &quot;$.key1&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:xx-xxxx-x:XXXXXXXXXXXX:function:Execution&quot;,
&quot;End&quot;: true 
} 
} 
}, 
{ 
&quot;StartAt&quot;: “key32&quot;,
&quot;States&quot;: { 
“key32&quot;: { 
&quot;Type&quot;: &quot;Task&quot;,
&quot;InputPath&quot;: &quot;$.key2&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:xx-xxxx-x:XXXXXXXXXXXX:function:Execution&quot;,
&quot;End&quot;: true 
} 
} 
}, 
{ 
&quot;StartAt&quot;: “key33&quot;,
&quot;States&quot;: { 
“key33&quot;: { 
&quot;Type&quot;: &quot;Task&quot;,
&quot;InputPath&quot;: &quot;$.key3&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:xx-xxxx-x:XXXXXXXXXXXX:function:Execution&quot;,
&quot;End&quot;: true 
} 
} 
} 
] 
} </code> 
<p><code class="lang-json"></code></p> 
<p><strong>Step 4: Load data into Amazon Redshift</strong><br /> The second Lambda function in the state machine extracts records from RDS associated with keys retrieved for DynamoDB. It processes the data then loads into an Amazon Redshift table. The following is code snippet for the second Lambda function in Java 8.</p> 
<code class="lang-java">public class LambdaFunctionHandler implements RequestHandler&lt;String, String&gt; {
public static String key = null;
 public String handleRequest(String input, Context context) { 
key=input; 
//1. Getting basic configurations for the next classes + s3 client Properties
config = getConfig(); 
AmazonS3 s3 = AmazonS3ClientBuilder.defaultClient(); 
// 2. Export query results from RDS into S3 bucket 
new RdsDataService(config).exportDataToS3(s3,key); 
// 3. Import query results from S3 bucket into Redshift 
new RedshiftDataService(config).importDataFromS3(s3,key); 
System.out.println(input); 
return &quot;SUCCESS&quot;; 
} 
}</code> 
<p>After the data is loaded into Amazon Redshift, end users can visualize it using their preferred business intelligence tools.</p> 
<h3>Lessons learned</h3> 
<li>At the time of publication, the 1.5–GB memory hard limit for Lambda functions was inadequate for processing our complex workload. Step Functions gave us the flexibility to chunk our large datasets and process them in parallel, saving on costs and time.</li> 
<li>In our previous implementation, we assigned each key a dedicated Lambda function along with CloudWatch rules for schedule automation. This approach proved to be inefficient and quickly became an operational burden. Previously, we processed each key sequentially, with each key adding about five minutes to the overall processing time. For example, processing three keys meant that the total processing time was three times longer. With Step Functions, the entire state machine executes in about five minutes.</li> 
<li>Using DynamoDB with Step Functions gave us the flexibility to manage keys efficiently. In our previous implementations, keys were hardcoded in Lambda functions, which became difficult to manage due to frequent updates. DynamoDB is a great way to store dynamic data that changes frequently, and it works perfectly with our serverless architectures.</li> 
<b>Conclusion</b> 
<p>With Step Functions, we were able to fully automate the frequent configuration updates to our dataset resulting in significant cost savings, reduced risk to data errors due to system downtime, and more time for us to focus on new product development rather than support related issues. We hope that you have found the information useful and that it can serve as a jump-start to building your own ETL processes on AWS with managed AWS services.</p> 
<p>For more information about how Step Functions makes it easy to coordinate the components of distributed applications and microservices in any workflow, see the <a href="https://aws.amazon.com/step-functions/getting-started/">use case examples</a> and then build your first state machine in under five minutes in the <a href="http://console.aws.amazon.com/states/home">Step Functions console</a>.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-dynamodb/" rel="tag">Amazon DynamoDB</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-rds/" rel="tag">Amazon RDS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-redshift/" rel="tag">Amazon Redshift</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloudwatch-events/" rel="tag">CloudWatch Events</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3409');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/30/invocation-alias-count.png" /> 
<b class="lb-b blog-post-title" property="name headline">Implementing Canary Deployments of AWS Lambda Functions with Alias Traffic Shifting</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2017-11-30T11:25:26+00:00">30 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-codedeploy/" title="View all posts in AWS CodeDeploy*"><span property="articleSection">AWS CodeDeploy*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3399" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3399&amp;disqus_title=Implementing+Canary+Deployments+of+AWS+Lambda+Functions+with+Alias+Traffic+Shifting&amp;disqus_url=https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambda-functions-with-alias-traffic-shifting/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3399');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Ryan Green, Software Development Engineer, AWS Serverless</em></p> 
<p>The concepts of <a href="https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf">blue/green</a> and canary deployments have been around for a while now and have been well-established as best-practices for reducing the risk of software deployments.</p> 
<p>In a traditional, horizontally scaled application, copies of the application code are deployed to multiple nodes (instances, containers, on-premises servers, etc.), typically behind a load balancer. In these applications, deploying new versions of software to too many nodes at the same time can impact application availability as there may not be enough healthy nodes to service requests during the deployment. This aggressive approach to deployments also drastically increases the blast radius of software bugs introduced in the new version and does not typically give adequate time to safely assess the quality of the new version against production traffic.</p> 
<p>In such applications, one commonly accepted solution to these problems is to <a href="http://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html">slowly and incrementally</a> roll out application software across the nodes in the fleet while simultaneously verifying application health (canary deployments). Another solution is to stand up an entirely different fleet and weight (or flip) traffic over to the new fleet after verification, ideally with some production traffic (blue/green). Some teams deploy to a single host (“one box environment”), where the new release can bake for some time before promotion to the rest of the fleet. Techniques like this enable the maintainers of complex systems to safely test in production while minimizing customer impact.</p> 
<b>Enter Serverless</b> 
<p>There is somewhat of an impedance mismatch when mapping these concepts to a serverless world. You can’t incrementally deploy your software across a fleet of servers when there are no servers!* In fact, even the term “deployment” takes on a different meaning with functions as a service (FaaS). In <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, a “deployment” can be roughly modeled as a call to <a href="http://docs.aws.amazon.com/cli/latest/reference/lambda/create-function.html">CreateFunction</a>, <a href="http://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-code.html">UpdateFunctionCode</a>, or <a href="http://docs.aws.amazon.com/cli/latest/reference/lambda/update-alias.html">UpdateAlias</a> (I won’t get into the semantics of whether updating configuration counts as a deployment), all of which may affect the version of code that is invoked by clients.</p> 
<p>The abstractions provided by Lambda remove the need for developers to be concerned about servers and Availability Zones, and this provides a powerful opportunity to greatly simplify the process of deploying software.<br /> <em>*Of course there are servers, but they are abstracted away from the developer.</em></p> 
<b>Traffic shifting with Lambda aliases</b> 
<p>Before the release of <a href="http://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html">traffic shifting for Lambda aliases</a>, deployments of a Lambda function could only be performed in a single “flip” by updating function code for version $LATEST, or by updating an alias to target a different function version. After the update propagates, typically within a few seconds, 100% of function invocations execute the new version. Implementing canary deployments with this model required the development of an additional routing layer, further adding development time, complexity, and invocation latency.<br /> While rolling back a bad deployment of a Lambda function is a trivial operation and takes effect near instantaneously, deployments of new versions for critical functions can still be a potentially nerve-racking experience.</p> 
<p>With the introduction of alias traffic shifting, it is now possible to trivially implement canary deployments of Lambda functions. By updating additional version weights on an alias, invocation traffic is routed to the new function versions based on the weight specified. Detailed <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/working_with_metrics.html">CloudWatch metric</a>s for the alias and version can be analyzed during the deployment, or other health checks performed, to ensure that the new version is healthy before proceeding.</p> 
<p>Note: Sometimes the term “canary deployments” refers to the release of software to a subset of users. In the case of alias traffic shifting, the new version is released to some percentage of all users. It’s not possible to shard based on identity without adding an additional routing layer.</p> 
<h3>Examples</h3> 
<p>The simplest possible use of a canary deployment looks like the following:</p> 
<code># Update $LATEST version of function
aws lambda update-function-code --function-name myfunction ….
# Publish new version of function
aws lambda publish-version --function-name myfunction
# Point alias to new version, weighted at 5% (original version at 95% of traffic)
aws lambda update-alias --function-name myfunction --name myalias --routing-config '{&quot;AdditionalVersionWeights&quot; : {&quot;2&quot; : 0.05} }'
# Verify that the new version is healthy
…
# Set the primary version on the alias to the new version and reset the additional versions (100% weighted)
aws lambda update-alias --function-name myfunction --name myalias --function-version 2 --routing-config '{}'
</code> 
<p>This is begging to be automated! Here are a few options.</p> 
<h3>Simple deployment automation</h3> 
<p>This simple Python script runs as a Lambda function and deploys another function (how meta!) by incrementally increasing the weight of the new function version over a prescribed number of steps, while checking the health of the new version. If the health check fails, the alias is rolled back to its initial version. The health check is implemented as a simple check against the existence of Errors metrics in CloudWatch for the alias and new version.</p> 
<p><a href="https://github.com/awslabs/aws-lambda-deploy">GitHub aws-lambda-deploy repo</a></p> 
<p><strong>Install:</strong></p> 
<code>git clone <a href="https://github.com/awslabs/aws-lambda-deploy">https://github.com/awslabs/aws-lambda-deploy</a>
cd aws-lambda-deploy
export BUCKET_NAME=[YOUR_S3_BUCKET_NAME_FOR_BUILD_ARTIFACTS]
./install.sh
</code> 
<p><strong>Run:</strong></p> 
<code># Rollout version 2 incrementally over 10 steps, with 120s between each step
aws lambda invoke --function-name SimpleDeployFunction --log-type Tail --payload \
'{&quot;function-name&quot;: &quot;MyFunction&quot;,
&quot;alias-name&quot;: &quot;MyAlias&quot;,
&quot;new-version&quot;: &quot;2&quot;,
&quot;steps&quot;: 10,
&quot;interval&quot; : 120,
&quot;type&quot;: &quot;linear&quot;
}' output
</code> 
<p><strong>Description of input parameters</strong></p> 
<li><strong>function-name</strong>: The name of the Lambda function to deploy</li> 
<li><strong>alias-name</strong>: The name of the alias used to invoke the Lambda function</li> 
<li><strong>new-version</strong>: The version identifier for the new version to deploy</li> 
<li><strong>steps</strong>: The number of times the new version weight is increased</li> 
<li><strong>interval</strong>: The amount of time (in seconds) to wait between weight updates</li> 
<li><strong>type</strong>: The function to use to generate the weights. Supported values: “linear”</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/30/invocation-alias-count.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/30/invocation-alias-count.png" /></a></p> 
<p>Because this runs as a Lambda function, it is subject to the maximum timeout of 5 minutes. This may be acceptable for many use cases, but to achieve a slower rollout of the new version, a different solution is required.</p> 
<h3>Step Functions workflow</h3> 
<p>This <a href="https://github.com/awslabs/aws-lambda-deploy/blob/master/template.yml#L119">state machine</a> performs essentially the same task as the simple deployment function, but it runs as an asynchronous workflow in <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a>. A nice property of Step Functions is that the maximum deployment timeout has now increased from 5 minutes to 1 year!</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/30/step-function-lambda-aliases-workflow.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/30/step-function-lambda-aliases-workflow.png" /></a></p> 
<p>The step function incrementally updates the new version weight based on the steps parameter, waiting for some time based on the interval parameter, and performing health checks between updates. If the health check fails, the alias is rolled back to the original version and the workflow fails.</p> 
<p>For example, to execute the workflow:</p> 
<code>export STATE_MACHINE_ARN=`aws cloudformation describe-stack-resources --stack-name aws-lambda-deploy-stack --logical-resource-id DeployStateMachine --output text | cut&nbsp; -d$'\t' -f3`
aws stepfunctions start-execution --state-machine-arn $STATE_MACHINE_ARN --input '{
&quot;function-name&quot;: &quot;MyFunction&quot;,
&quot;alias-name&quot;: &quot;MyAlias&quot;,
&quot;new-version&quot;: &quot;2&quot;,
&quot;steps&quot;: 10,
&quot;interval&quot;: 120,
&quot;type&quot;: &quot;linear&quot;}'
</code> 
<h3>Getting feedback on the deployment</h3> 
<p>Because the state machine runs asynchronously, retrieving feedback on the deployment requires polling for the execution status using <a href="http://docs.aws.amazon.com/step-functions/latest/apireference/API_DescribeExecution.html">DescribeExecution</a> or implementing an asynchronous notification (using SNS or email, for example) from the Rollback or Finalize functions. A <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html">CloudWatch alarm</a> could also be created to alarm based on the “ExecutionsFailed” metric for the state machine.</p> 
<h3>A note on health checks and observability</h3> 
<p>Weighted rollouts like this are considerably more successful if the code is being exercised and monitored continuously. In this example, it would help to have some automation continuously invoking the alias and reporting metrics on these invocations, such as client-side success rates and latencies.</p> 
<p>The absence of Lambda Errors metrics used in these examples can be misleading if the function is not getting invoked. It’s also recommended to instrument your Lambda functions with custom metrics, in addition to Lambda’s built-in metrics, that can be used to monitor health during deployments.</p> 
<b>Extensibility</b> 
<p>These examples could be easily extended in various ways to support different use cases. For example:</p> 
<li><strong>Health check implementations</strong>: CloudWatch alarms, automatic invocations with payload assertions, querying external systems, etc.</li> 
<li><strong>Weight increase functions</strong>: Exponential, geometric progression, single canary step, etc.</li> 
<li><strong>Custom success/failure notifications</strong>: SNS, email, CI/CD systems, service discovery systems, etc.</li> 
<b>Traffic shifting with SAM and CodeDeploy</b> 
<p>Using the Lambda <a href="http://docs.aws.amazon.com/cli/latest/reference/lambda/update-alias.html">UpdateAlias</a> operation with additional version weights provides a powerful primitive for you to implement custom traffic shifting solutions for Lambda functions.</p> 
<p>For those not interested in building custom deployment solutions, <a href="https://aws.amazon.com/codedeploy/">AWS CodeDeploy</a> provides an intuitive turn-key implementation of this functionality integrated directly into the <a href="https://github.com/awslabs/serverless-application-model">Serverless Application Model</a>. Traffic-shifted deployments can be declared in a SAM template, and CodeDeploy manages the function rollout as part of the CloudFormation stack update. CloudWatch alarms can also be configured to trigger a stack rollback if something goes wrong.</p> 
<p>i.e.</p> 
<code>MyFunction:
Type: AWS::Serverless::Function
Properties:
FunctionName: MyFunction
AutoPublishAlias: MyFunctionInvokeAlias
DeploymentPreference:
Type: Linear10PercentEvery1Minute
Role:
Fn::GetAtt: [ DeploymentRole, Arn ]
Alarms:
- { Ref: MyFunctionErrorsAlarm }
...
</code> 
<p>For more information about using CodeDeploy with SAM, see <a href="http://docs.aws.amazon.com/lambda/latest/dg/automating-updates-to-serverless-apps.html">Automating Updates to Serverless Apps</a>.</p> 
<b>Conclusion</b> 
<p>It is often the simple features that provide <a href="https://en.wikipedia.org/wiki/Pareto_principle">the most value</a>. As I demonstrated in this post, serverless architectures allow the complex deployment orchestration used in traditional applications to be replaced with a simple Lambda function or Step Functions workflow. By allowing invocation traffic to be easily weighted to multiple function versions, Lambda alias traffic shifting provides a simple but powerful feature that I hope empowers you to easily implement safe deployment workflows for your Lambda functions.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/canary/" rel="tag">canary</a>, <a href="https://aws.amazon.com/blogs/compute/tag/codedeploy/" rel="tag">codedeploy</a>, <a href="https://aws.amazon.com/blogs/compute/tag/sam/" rel="tag">SAM</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3399');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/ReInvent_HA_Fargate_SOCIAL-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">AWS Fargate: A Product Overview</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Deepak Dayama</span></span> | on 
<time property="datePublished" datetime="2017-11-29T08:45:18+00:00">29 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/aws-fargate-a-product-overview/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3387" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3387&amp;disqus_title=AWS+Fargate%3A++A+Product+Overview&amp;disqus_url=https://aws.amazon.com/blogs/compute/aws-fargate-a-product-overview/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3387');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>It was just about three years ago that AWS announced Amazon Elastic Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>), to run and manage containers at scale on AWS. With Amazon ECS, you’ve been able to run your workloads at high scale and availability without having to worry about running your own cluster management and container orchestration software.</p> 
<p>Today, AWS announced the availability of <a href="https://aws.amazon.com/fargate">AWS Fargate</a> – a technology that enables you to use containers as a fundamental compute primitive without having to manage the underlying instances. With Fargate, you don’t need to provision, configure, or scale virtual machines in your clusters to run containers. Fargate can be used with Amazon ECS today, with plans to support Amazon Elastic Container Service for Kubernetes (<a href="https://aws.amazon.com/eks">Amazon EKS</a>) in the future.</p> 
<p>Fargate has flexible configuration options so you can closely match your application needs and granular, per-second billing.</p> 
<b>Amazon ECS with Fargate</b> 
<p>Amazon ECS enables you to run containers at scale. This service also provides native integration into the AWS platform with VPC networking, load balancing, IAM, Amazon CloudWatch Logs, and CloudWatch metrics. These deep integrations make the Amazon ECS <em>task</em> a first-class object within the AWS platform.</p> 
<p>To run tasks, you first need to stand up a cluster of instances, which involves picking the right types of instances and sizes, setting up Auto Scaling, and right-sizing the cluster for performance. With Fargate, you can leave all that behind and focus on defining your application and policies around permissions and scaling.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/29/fargate-1.png" /></p> 
<p>The same container management capabilities remain available so you can continue to scale your container deployments. With Fargate, the only entity to manage is the task. You don’t need to manage the instances or supporting software like Docker daemon or the Amazon ECS agent.</p> 
<p>Fargate capabilities are available natively within Amazon ECS. This means that you don’t need to learn new API actions or primitives to run containers on Fargate.</p> 
<p>Using Amazon ECS, Fargate is a launch type option. You continue to define the applications the same way by using task definitions. In contrast, the EC2 launch type gives you more control of your server clusters and provides a broader range of customization options.</p> 
<p>For example, a RunTask command example is pasted below with the Fargate launch type:</p> 
<p><code class="lang-bash">aws ecs run-task --launch-type FARGATE --cluster fargate-test --task-definition nginx --network-configuration</code><br /> <code class="lang-bash">&quot;awsvpcConfiguration={subnets=[subnet-b563fcd3]}&quot; </code></p> 
<b>Key features of Fargate</b> 
<p><strong>Resource-based pricing and per second billing</strong><br /> You pay by the task size and only for the time for which resources are consumed by the task. The price for CPU and memory is charged on a per-second basis. There is a one-minute minimum charge.</p> 
<p><strong>Flexible configurations options</strong><br /> Fargate is available with 50 different combinations of CPU and memory to closely match your application needs. You can use 2 GB per vCPU anywhere up to 8 GB per vCPU for various configurations. Match your workload requirements closely, whether they are general purpose, compute, or memory optimized.</p> 
<p><strong>Networking</strong><br /> All Fargate tasks run within your own VPC. Fargate supports the recently launched <a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">awsvpc networking mode</a> and the elastic network interface for a task is visible in the subnet where the task is running. This provides the separation of responsibility so you retain full control of networking policies for your applications via VPC features like security groups, routing rules, and NACLs. Fargate also supports public IP addresses.</p> 
<p><strong>Load Balancing</strong><br /> ECS Service Load Balancing &nbsp;for the Application Load Balancer and Network Load Balancer is supported. For the Fargate launch type, you specify the IP addresses of the Fargate tasks to register with the load balancers.</p> 
<p><strong>Permission tiers</strong><br /> Even though there are no instances to manage with Fargate, you continue to group tasks into logical clusters. This allows you to manage who can run or view services within the cluster. The task IAM role is still applicable. Additionally, there is a new Task Execution Role that grants Amazon ECS permissions to perform operations such as pushing logs to CloudWatch Logs or pulling image from Amazon Elastic Container Registry (Amazon ECR).</p> 
<p><strong>Container Registry Support</strong><br /> Fargate provides seamless authentication to help pull images from Amazon ECR via the Task Execution Role. Similarly, if you are using a public repository like DockerHub, you can continue to do so.</p> 
<p><strong>Amazon ECS CLI<br /> </strong>The Amazon ECS CLI provides high-level commands to help simplify to create and run Amazon ECS clusters, tasks, and services. The latest version of the CLI now supports running tasks and services with Fargate.</p> 
<p><strong>EC2 and Fargate Launch Type Compatibility<br /> </strong>All Amazon ECS clusters are heterogeneous – you can run both Fargate and Amazon ECS tasks in the same cluster. This enables teams working on different applications to choose their own cadence of moving to Fargate, or to select a launch type that meets their requirements without breaking the existing model. You can make an existing ECS task definition compatible with the Fargate launch type and run it as a Fargate service, and vice versa. Choosing a launch type is not a one-way door!</p> 
<p><strong>Logging and Visibility</strong><br /> With Fargate, you can send the application logs to CloudWatch logs. Service metrics (CPU and Memory utilization) are available as part of CloudWatch metrics. AWS partners for visibility, monitoring and application performance management including Datadog, Aquasec, Splunk, Twistlock, and New Relic also support Fargate tasks.</p> 
<b>Conclusion</b> 
<p>Fargate enables you to run containers without having to manage the underlying infrastructure. Today, Fargate is availabe for Amazon ECS, and in 2018, Amazon EKS. Visit the <a href="https://aws.amazon.com/fargate">Fargate product page</a> to learn more, or get started in the <a href="https://console.aws.amazon.com/ecs/home?region=us-east-1#/getStarted">AWS Console</a>.</p> 
<p>–Deepak Dayama</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3387');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Simplify Your Pub/Sub Messaging with Amazon SNS Message Filtering</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-22T11:56:01+00:00">22 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/simplify-pubsub-messaging-with-amazon-sns-message-filtering/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3362" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3362&amp;disqus_title=Simplify+Your+Pub%2FSub+Messaging+with+Amazon+SNS+Message+Filtering&amp;disqus_url=https://aws.amazon.com/blogs/compute/simplify-pubsub-messaging-with-amazon-sns-message-filtering/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3362');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by:&nbsp;Stephen Liedig, Senior Solutions Architect, ANZ Public Sector, and&nbsp;Otavio Ferreira, Manager, Amazon Simple Notification Service</em></p> 
<p>Want to make your cloud-native applications scalable, fault-tolerant, and highly available? Recently, we wrote a couple of posts about using AWS messaging services <a href="https://aws.amazon.com/sqs/">Amazon SQS</a>&nbsp;and&nbsp;<a href="https://aws.amazon.com/sns/">Amazon SNS</a> to address messaging patterns for loosely coupled communication between highly cohesive components. For more information, see:</p> 
<li><a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/">Messaging Fanout Pattern for Serverless Architectures Using Amazon SNS</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/event-driven-computing-with-amazon-sns-compute-storage-database-and-networking-services/">Event-Driven Computing with Amazon SNS and AWS Compute, Storage, Database, and Networking Services</a></li> 
<p>Today, AWS is releasing a new message filtering functionality for SNS.&nbsp;This new feature simplifies the pub/sub messaging architecture by offloading the filtering logic from subscribers, as well as the routing logic from publishers, to SNS.</p> 
<p>In this post, we walk you through the new message filtering feature, and how to use it to clean up unnecessary logic in your components, and reduce the number of topics in your architecture.<span id="more-3362"></span></p> 
<b>Topic-based filtering</b> 
<p>SNS is a fully managed <a href="https://aws.amazon.com/pub-sub-messaging/">pub/sub messaging</a> service that lets you fan out messages to large numbers of recipients at one time, using topics. SNS topics support a variety of subscription types, allowing you to push messages to SQS&nbsp;queues, <a href="https://aws.amazon.com/lambda">AWS Lambda</a> functions, HTTP endpoints, email addresses, and mobile devices (SMS, push).</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_1.png" /></a></p> 
<p>In the above scenario, every subscriber receives the same message published to the topic, allowing them to process the message independently. For many use cases, this is sufficient.</p> 
<p>However, in more complex scenarios, the subscriber may only be interested in a subset of the messages being published. The onus, in that case, is on each subscriber to ensure that they are filtering and only processing those messages in which they are actually interested.</p> 
<p>To avoid this additional filtering logic on each subscriber, many organizations have adopted a practice in which the publisher is now responsible for routing different types of messages to different topics. However, as depicted in the following diagram, this topic-based filtering practice can lead to overly complicated publishers, topic proliferation, and additional overhead in provisioning and&nbsp;managing your SNS topics.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_2.png" /></a></p> 
<b>Attribute-based filtering</b> 
<p>To leverage the new message filtering capability, SNS requires the publisher to set message attributes and each subscriber to set a subscription attribute (a&nbsp;subscription filter policy). When the publisher posts a new message to the topic, SNS attempts to match the incoming message attributes to the filter policy set on each subscription, to determine whether a particular subscriber is interested in that incoming event. If there is a match, SNS then pushes the message to the subscriber in question. The new attribute-based message filtering approach is depicted in the following diagram.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_3.png" /></a></p> 
<b>Message filtering in action</b> 
<p>Look at how message filtering works. The following example is based on a sports merchandise ecommerce website, which publishes a variety of events to an SNS topic. The events range from checkout events (triggered when orders are placed or canceled) to buyers’ navigation events (triggered when product pages are visited).&nbsp;The code below is based on the existing <a href="https://aws.amazon.com/tools/#sdk">AWS SDK for Python</a>.</p> 
<p>First, create the single SNS topic to which all shopping events are published.</p> 
<code class="lang-python">topic_arn = sns.create_topic(
Name='ShoppingEvents'
)['TopicArn']</code> 
<p>Next, subscribe the endpoints that will be listening to those shopping events. The first subscriber is an SQS queue that is processed by a payment gateway, while the second subscriber is a Lambda function that indexes the buyer’s shopping interests against a search engine.</p> 
<p>A subscription filter policy is set as a subscription attribute, by the subscription owner, as a simple JSON object, containing a set of key-value pairs. This object defines the kind of event in which the subscriber is interested.</p> 
<code class="lang-python">payment_gateway_subscription_arn = sns.subscribe(
TopicArn = topic_arn,
Protocol = 'sqs',
Endpoint = 'arn:aws:sqs:ap-southeast-2:123456789012:PaymentQueue'
)['SubscriptionArn']
sns.set_subscription_attributes(
SubscriptionArn = payment_gateway_subscription_arn, 
AttributeName = 'FilterPolicy', 
AttributeValue = '{&quot;event_type&quot;: [&quot;order_placed&quot;, &quot;order_cancelled&quot;]}'
)
search_engine_subscription_arn = sns.subscribe(
TopicArn = topic_arn,
Protocol = 'lambda',
Endpoint = 'arn:aws:lambda:ap-southeast-2:123456789012:function:SearchIndex'
)['SubscriptionArn']
sns.set_subscription_attributes(
SubscriptionArn = search_engine_subscription_arn,
AttributeName ='FilterPolicy', 
AttributeValue ='{&quot;event_type&quot;: [&quot;product_page_visited&quot;]}'
)</code> 
<p>You’re now ready to start publishing events with attributes!</p> 
<p>Message attributes allow you to provide structured metadata items (such as time stamps, geospatial data, event type, signatures, and identifiers) about the message. Message attributes are optional and separate from, but sent along with, the message body. You can include up to 10 message attributes with your message.</p> 
<p>The first message published in this example is related to an order that has been placed on the ecommerce website. The message attribute “<strong>event_type</strong>” with the value “<strong>order_placed</strong>” matches only the filter policy associated with the payment gateway subscription. Therefore, only the SQS queue subscribed to the SNS topic is notified about this checkout event.</p> 
<code class="lang-python">message = '{&quot;order&quot;: {&quot;id&quot;: 5678, &quot;status&quot;: &quot;confirmed&quot;, &quot;items&quot;: [' \
'{&quot;code&quot;: &quot;P-9012&quot;, &quot;product&quot;: &quot;Santos FC Jersey&quot;, &quot;units&quot;: 1},' \
'{&quot;code&quot;: &quot;P-3156&quot;, &quot;product&quot;: &quot;Soccer Ball&quot;, &quot;units&quot;: 2}]},' \
' &quot;buyer&quot;: {&quot;id&quot;: 4454}}'
sns.publish(
TopicArn = topic_arn,
Subject = 'Order Placed #5678',
Message = message,
MessageAttributes = {
'event_type': {
'DataType': 'String',
'StringValue': 'order_placed'
}
}
)</code> 
<p>The second message published is related to a buyer’s navigation activity on the ecommerce website. The message attribute “<strong>event_type</strong>” with the value “<strong>product_page_visited</strong>” matches only the filter policy associated with the search engine subscription. Therefore, only the Lambda function subscribed to the SNS topic is notified about this navigation event.</p> 
<code class="lang-python">message = '{&quot;product&quot;: {&quot;id&quot;: 1251, &quot;status&quot;: &quot;in_stock&quot;},' \
' &quot;buyer&quot;: {&quot;id&quot;: 4454}}'
sns.publish(
TopicArn = topic_arn,
Subject = 'Product Visited #1251',
Message = message,
MessageAttributes = {
'event_type': {
'DataType': 'String',
'StringValue': 'product_page_visited'
}
}
)</code> 
<p>The following diagram represents the architecture for this ecommerce website, with the message filtering mechanism in action. As described earlier, checkout events are pushed only to the SQS queue, whereas navigation events are pushed to the Lambda function only.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_4.png" /></a></p> 
<b>Message filtering criteria</b> 
<p>It is important to remember the following things about subscription filter policy matching:</p> 
<li>A subscription filter policy either matches an incoming message, or it doesn’t. It’s Boolean logic.</li> 
<li>For a filter policy to match a message, the message must contain all the attribute keys listed in the policy.</li> 
<li>Attributes of the message not mentioned in the filtering policy are ignored.</li> 
<li>The value of each key in the filter policy is an array containing one or more values. The policy matches if any of the values in the array match the value in the corresponding message attribute.</li> 
<li>If the value in the message attribute is an array, then the filter policy matches if the intersection of the policy array and the message array is non-empty.</li> 
<li>The matching is exact (character-by-character), without case-folding or any other string normalization.</li> 
<li>The values being matched follow JSON rules: Strings enclosed in quotes, numbers, and the unquoted keywords true, false, and null.</li> 
<li>Number matching is at the string representation level. Example: 300, 300.0, and 3.0e2 aren’t considered equal.</li> 
<b>When should I use message filtering?</b> 
<p>We recommend using message filtering and grouping subscribers into a single topic only when all of the following is true:</p> 
<li>Subscribers are semantically related to each other</li> 
<li>Subscribers consume similar types of events</li> 
<li>Subscribers are supposed to share the same access permissions on the topic</li> 
<p>Technically, you could get away with creating a single topic for your entire domain to handle all event processing, even unrelated use cases, but this wouldn’t be recommended. This option could result in an unnecessarily large topic, which could potentially impact your message delivery latency. Also, you would lose the ability to implement fine-grained access control on your topics.</p> 
<p>Finally, if you already use SNS, but had to add filtering logic in your subscribers or routing logic in your publishers (topic-based filtering), you can now immediately benefit from message filtering. This new approach lets you clean up any unnecessary logic in your components, and reduce the number of topics in your architecture.</p> 
<b>Summary</b> 
<p>As we’ve shown in this post, the new message filtering capability in Amazon SNS gives you a great amount of flexibility in your messaging pattern. It allows you to really simplify your pub/sub infrastructure requirements.</p> 
<p>Message filtering can be implemented easily with existing <a href="https://aws.amazon.com/tools/#sdk">AWS SDKs</a>&nbsp;by applying message and subscription attributes across all SNS supported protocols (Amazon SQS, AWS Lambda, HTTP, SMS, email, and mobile push). It’s now available in all AWS commercial regions, at no extra charge.</p> 
<p>Here’s a few ideas for next steps to get you started:</p> 
<li>Add filter policies to your subscriptions on the <a href="https://console.aws.amazon.com/sns/">SNS console</a>,</li> 
<li>Try the 10-minute tutorial, <a href="https://aws.amazon.com/getting-started/tutorials/filter-messages-published-to-topics/">Filter Messages Published to Topics</a></li> 
<li>Read the <a href="http://docs.aws.amazon.com/sns/latest/dg/message-filtering.html">message filtering section of the SNS documentation</a></li> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/event-driven-computing/" rel="tag">event-driven computing</a>, <a href="https://aws.amazon.com/blogs/compute/tag/messaging/" rel="tag">messaging</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3362');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/budget_lambda_iam_ver7.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Serverless Automated Cost Controls, Part1</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Shankar Ramachandran</span></span> | on 
<time property="datePublished" datetime="2017-11-22T09:34:50+00:00">22 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/" title="View all posts in Application Services*"><span property="articleSection">Application Services*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/aws-cost-management/aws-budgets/" title="View all posts in AWS Budgets*"><span property="articleSection">AWS Budgets*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/aws-cost-management/" title="View all posts in AWS Cost Management*"><span property="articleSection">AWS Cost Management*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/aws-identity-and-access-management-iam/" title="View all posts in AWS Identity and Access Management (IAM)*"><span property="articleSection">AWS Identity and Access Management (IAM)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/" title="View all posts in Security, Identity, &amp; Compliance*"><span property="articleSection">Security, Identity, &amp; Compliance*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3299" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3299&amp;disqus_title=Serverless+Automated+Cost+Controls%2C+Part1&amp;disqus_url=https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3299');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Shankar Ramachandran, Pubali Sen, and George Mao</em></p> 
<p>In line with AWS’s continual efforts to reduce costs for customers, this series focuses on how customers can build serverless automated cost controls. This post provides an architecture blueprint and a sample implementation to prevent budget overruns.</p> 
<p>This solution uses the following AWS products:</p> 
<li><a href="https://aws.amazon.com/aws-cost-management/aws-budgets/">AWS Budgets</a> – An AWS Cost Management tool that helps customers define and track budgets for AWS costs, and forecast for up to three months.</li> 
<li><a href="https://aws.amazon.com/sns/">Amazon SNS</a> – An AWS service that makes it easy to set up, operate, and send notifications from&nbsp;the cloud.</li> 
<li><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> – An AWS service that lets you run code without provisioning or managing servers.</li> 
<p>You can fine-tune a budget for various parameters, for example filtering by service or tag. The Budgets tool lets you post notifications on an SNS topic. A Lambda function that subscribes to the SNS topic can act on the notification. Any programmatically implementable action can be taken.</p> 
<p>The diagram below describes the architecture blueprint.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/using_lambda_for_cost_control_ver4.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/using_lambda_for_cost_control_ver4.jpg" /></a></p> 
<p>In this post, we describe how to use this blueprint with AWS Step Functions and IAM to effectively revoke the ability of a user to start new Amazon EC2 instances, after a budget amount is exceeded.</p> 
<b>Freedom with guardrails</b> 
<p>AWS lets you quickly spin up resources as you need them, deploying hundreds or even thousands of servers in minutes.&nbsp;This means you can quickly develop and roll out new applications. Teams can experiment and innovate more quickly and frequently. If an experiment fails, you can always de-provision those servers without risk.</p> 
<p>This improved agility also brings in the need for effective cost controls. Your Finance and Accounting department must budget, monitor, and control the AWS spend. For example, this could be a budget per project. Further, Finance and Accounting must take appropriate actions if the budget for the project has been exceeded, for example. Call it “freedom with guardrails” – where Finance wants to give developers freedom, but with financial constraints.</p> 
<b>Architecture</b> 
<p>This section describes how to use the blueprint introduced earlier to implement a “freedom with guardrails” solution.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/budget_lambda_iam_ver7.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/budget_lambda_iam_ver7.jpg" /></a></p> 
<ol> 
<li>The budget for “Project Beta” is set up in Budgets. In this example, we focus on EC2 usage and identify the instances that belong to this project by filtering on the tag Project with the value Beta. For more information, see <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html">Creating a Budget</a>.</li> 
<li>The budget configuration also includes settings to send a notification on an SNS topic when the usage exceeds 100% of the budgeted amount. For more information, see <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-sns-policy.html">Creating an Amazon SNS Topic for Budget Notifications</a>.</li> 
<li>The master Lambda function receives the SNS notification.</li> 
<li>It triggers execution of a Step Functions state machine with the parameters for completing the configured action.</li> 
<li>The action Lambda function is triggered as a task in the state machine. The function interacts with IAM to effectively remove the user’s permissions to create an EC2 instance.</li> 
</ol> 
<p>This decoupled modular design allows for extensibility.&nbsp; New actions (serially or in parallel) can be added by simply adding new steps.</p> 
<b>Implementing the solution</b> 
<p>All the instructions and code needed to implement the architecture have been posted on the <a href="https://github.com/aws-samples/serverless-automated-cost-controls">Serverless Automated Cost Controls</a> GitHub repo. We recommend that you try this first in a Dev/Test environment.</p> 
<p>This implementation description can be broken down into two parts:</p> 
<ol> 
<li>Create a solution stack for serverless automated cost controls.</li> 
<li>Verify the solution by testing the EC2 fleet.</li> 
</ol> 
<p>To tie this back to the “freedom with guardrails” scenario, the Finance department performs a one-time implementation of the solution stack. To simulate resources for Project Beta, the developers spin up the test EC2 fleet.</p> 
<b>Prerequisites</b> 
<p>There are two prerequisites:</p> 
<li>Make sure that you have the necessary IAM permissions. For more information, see the section titled “Required IAM permissions” in the <a href="https://github.com/aws-samples/serverless-automated-cost-controls/blob/master/README.md">README</a>.</li> 
<li>Define and activate a cost allocation tag with the key Project. For more information, see <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html">Using Cost Allocation Tags</a>. It can take up to 12 hours for the tags to propagate to Budgets.</li> 
<b>Create resources</b> 
<p>The solution stack includes creating the following resources:</p> 
<li>Three Lambda functions</li> 
<li>One Step Functions state machine</li> 
<li>One SNS topic</li> 
<li>One IAM group</li> 
<li>One IAM user</li> 
<li>IAM policies as needed</li> 
<li>One budget</li> 
<p>Two of the Lambda functions were described in the previous section, to a) receive the SNS notification and b) trigger the Step Functions state machine. Another Lambda function is used to create the budget, as a <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">custom AWS CloudFormation resource</a>. The SNS topic connects Budgets with Lambda function A. Lambda function B is configured as a task in Step Functions. A budget for $2 is created which is filtered by Service: EC2 and Tag: Project, Beta. A test IAM group and user is created to enable you to validate this Cost Control Solution.</p> 
<p>To create the serverless automated cost control solution stack, choose the button below. It takes few minutes to spin up the stack. You can monitor the progress in the CloudFormation console.</p> 
<p><a href="https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=costControlStack&amp;templateURL=https://s3-us-west-2.amazonaws.com/computeblog-us-west-2/serverless-automated-cost-controls/cfn_budget_lambda_blog_post.json"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/cloudformation-launch-stack-button.png" /></a></p> 
<p>When you see the CREATE_COMPLETE status for the stack you had created, choose Outputs. Copy the following four values that you need later:</p> 
<li>TemplateURL</li> 
<li>UserName</li> 
<li>SignInURL</li> 
<li>Password</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_1.png" /></a></p> 
<b>Verify the stack</b> 
<p>The next step is to verify the serverless automated cost controls solution stack that you just created. To do this, spin up an EC2 fleet of t2.micro instances, representative of the resources needed for Project Beta, and tag them with <strong>Project, Beta</strong>.</p> 
<ol> 
<li>Browse to the <strong>SignInURL</strong>, and log in using the <strong>UserName</strong> and <strong>Password</strong> values copied on from the stack output.</li> 
<li>In the CloudFormation console, choose <strong>Create Stack</strong>.</li> 
<li>For <strong>Choose a template</strong>, select <strong>Choose an Amazon S3 template URL</strong> and paste the <strong>TemplateURL</strong> value from the preceding section. Choose <strong>Next</strong>.</li> 
<li>Give this stack a name, such as “testEc2FleetForProjectBeta”. Choose <strong>Next</strong>.</li> 
<li>On the Specify Details page, enter parameters such as the <strong>UserName</strong> and <strong>Password</strong> copied in the previous section. Choose Next.</li> 
<li>Ignore any errors related to listing IAM roles. The test user has a minimal set of permissions that is just sufficient to spin up this test stack (in line with security best practices).</li> 
<li>On the <strong>Options</strong> page, choose <strong>Next</strong>.</li> 
<li>On the <strong>Review</strong> page, choose <strong>Create</strong>. It takes a few minutes to spin up the stack, and you can monitor the progress in the CloudFormation console.&nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_2-1.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_2-1.jpg" /></a></li> 
<li>When you see the status “CREATE_COMPLETE”, open the EC2 console to verify that four t2.micro instances have been spun up, with the tag of Project, Beta.</li> 
</ol> 
<p>The hourly cost for these instances depends on the region in which they are running. On the average (irrespective of the region), you can expect the aggregate cost for this EC2 fleet to exceed the set $2 budget in 48 hours.</p> 
<b>Verify the solution</b> 
<p>The first step is to identify the test IAM group that was created in the previous section. The group should have “projectBeta” in the name, prepended with the CloudFormation stack name and appended with an alphanumeric string. Verify that the managed policy associated is: “EC2FullAccess”, which indicates that the users in this group have unrestricted access to EC2.</p> 
<p>There are two stages of verification for this serverless automated cost controls solution: simulating a notification and waiting for a breach.</p> 
<h3>Simulated notification</h3> 
<p>Because it takes at least a few hours for the aggregate cost of the EC2 fleet to breach the set budget, you can verify the solution by simulating the notification from Budgets.</p> 
<ol> 
<li>Log in to the SNS console (using your regular AWS credentials).</li> 
<li>Publish a message on the SNS topic that has “budgetNotificationTopic” in the name. The complete name is appended by the CloudFormation stack identifier. &nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_3.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_3-1024x488.jpg" /></a></li> 
<li>Copy the following text as the body of the notification: “This is a mock notification”.</li> 
<li>Choose Publish.</li> 
<li>Open the IAM console to verify that the policy for the test group has been switched to “EC2ReadOnly”. This prevents users in this group from creating new instances.</li> 
<li>Verify that the test user created in the previous section cannot spin up new EC2 instances. &nbsp;You can log in as the test user and try creating a new EC2 instance (via the same CloudFormation stack or the EC2 console). You should get an error message indicating that you do not have the necessary permissions.</li> 
<li>If you are proceeding to stage 2 of the verification, then you must switch the permissions back to “EC2FullAccess” for the test group, which can be done in the IAM console.</li> 
</ol> 
<h3>Automatic notification</h3> 
<p>Within 48 hours, the aggregate cost of the EC2 fleet spun up in the earlier section breaches the budget rule and triggers an automatic notification. This results in the permissions getting switched out, just as in the simulated notification.</p> 
<b>Clean up</b> 
<p>Use the following steps to delete your resources and stop incurring costs.</p> 
<ol> 
<li>Open the CloudFormation console.</li> 
<li>Delete the EC2 fleet by deleting the appropriate stack (for example, delete the stack named “testEc2FleetForProjectBeta”). &nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_4-1.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_4-1.jpg" /></a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<b>&nbsp;</b></li> 
<li>Next, delete the “costControlStack” stack. &nbsp; <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_5.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_5-1024x465.jpg" /></a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<strong>&nbsp;</strong></li> 
</ol> 
<b>Conclusion</b> 
<p>Using Lambda in tandem with Budgets, you can build Serverless automated cost controls on AWS. Find all the resources (instructions, code) for implementing the solution discussed in this post on the <a href="https://github.com/aws-samples/serverless-automated-cost-controls">Serverless Automated Cost Controls</a> GitHub repo.</p> 
<p>Stay tuned to this series for more tips about building serverless automated cost controls. In the next post, we discuss using smart lighting to influence developer behavior and describe a solution to encourage cost-aware development practices.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-sns/" rel="tag">Amazon SNS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-budgets/" rel="tag">AWS Budgets</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-cost-management/" rel="tag">AWS Cost Management</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3299');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/social_ECR.png" /> 
<b class="lb-b blog-post-title" property="name headline">AWS re:Invent 2017 Guide to All Things Containers</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tiffany Jernigan</span></span> | on 
<time property="datePublished" datetime="2017-11-17T13:05:07+00:00">17 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/aws-reinvent-2017-guide-to-all-things-containers/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3321" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3321&amp;disqus_title=AWS+re%3AInvent+2017+Guide+to+All+Things+Containers&amp;disqus_url=https://aws.amazon.com/blogs/compute/aws-reinvent-2017-guide-to-all-things-containers/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3321');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><i>Contributed</i><em>&nbsp;by <a href="https://twitter.com/tiffanyfayj">Tiffany Jernigan</a>, Developer Advocate for Amazon ECS</em></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/ecs-ship-orange_planet-1024x318.png" /></p> 
<b>Get ready for takeoff!</b> 
<p>We made sure that this year’s <a href="https://reinvent.awsevents.com/">re:Invent</a> is chock-full of containers: there are over 40 sessions! New to containers? No problem, we have several introductory sessions for you to dip your toes. Been using containers for years and know the ins and outs? Don’t miss our technical deep-dives and interactive chalk talks led by container experts.</p> 
<p>If you’re already registered for re:Invent, you can browse the session catalog <a href="https://www.portal.reinvent.awsevents.com/connect/search.ww#loadSearch-searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=abbreviationSort&amp;p=&amp;i(10042)=16546">here</a>. If you can’t make it to Las Vegas, you can catch the keynotes and session recaps from our <a href="https://reinvent.awsevents.com/live-stream/">livestream</a> and on <a href="http://www.twitch.tv/aws">Twitch</a>.</p> 
<h3>Session types</h3> 
<p>Not everyone learns the same way, so we have multiple types of breakout content:</p> 
<li><strong>Birds of a Feather</strong><br /> An interactive discussion with industry leaders about containers on AWS.</li> 
<li><strong>Breakout sessions</strong><br /> 60-minute presentations about building on AWS. Sessions are delivered by both AWS experts and customers and span all content levels.</li> 
<li><strong>Workshops</strong><br /> 2.5-hour, hands-on sessions that teach how to build on AWS. AWS credits are provided. Bring a laptop, and have an active AWS account.</li> 
<li><strong>Chalk Talks</strong><br /> 1-hour, highly interactive sessions with a smaller audience. They begin with a short lecture delivered by an AWS expert, followed by a discussion with the audience.</li> 
<h3>Session levels</h3> 
<p>Whether you’re new to containers or you’ve been using them for years, you’ll find useful information at every level.</p> 
<li><strong>Introductory</strong><br /> Sessions are focused on providing an overview of AWS services and features, with the assumption that attendees are new to the topic.</li> 
<li><strong>Advanced</strong><br /> Sessions dive deeper into the selected topic. Presenters assume that the audience has some familiarity with the topic, but may or may not have direct experience implementing a similar solution.</li> 
<li><strong>Expert</strong><br /> Sessions are for attendees who are deeply familiar with the topic, have implemented a solution on their own already, and are comfortable with how the technology works across multiple services, architectures, and implementations.</li> 
<h3>Session locations</h3> 
<p>All container sessions are located in the Aria Resort.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/reInvent_locations.png" /></p> 
<b>MONDAY 11/27</b> 
<h3>Breakout sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON202 –&nbsp;Getting Started with Docker and Amazon ECS<br /> </strong>By packaging software into standardized units, Docker gives code everything it needs to run, ensuring consistency from your laptop all the way into production. But once you have your code ready to ship, how do you run and scale it in the cloud? In this session, you become comfortable running containerized services in production using Amazon ECS. We cover container deployment, cluster management, service auto-scaling, service discovery, secrets management, logging, monitoring, security, and other core concepts. We also cover integrated AWS services and supplementary services that you can take advantage of to run and scale container-based services in the cloud.</p> 
<h3>Chalk talks</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON211 –&nbsp;Reducing your Compute Footprint with Containers and Amazon ECS<br /> </strong>Tomas Riha, platform architect for Volvo, shows how Volvo transitioned its WirelessCar platform from using Amazon EC2 virtual machines to containers running on Amazon ECS, significantly reducing cost. Tomas dives deep into the architecture that Volvo used to achieve the migration in under four months, including Amazon ECS, Amazon ECR, Elastic Load Balancing, and AWS CloudFormation.</p> 
<p><strong>CON212 –&nbsp;Anomaly Detection Using Amazon ECS, AWS Lambda, and Amazon EMR</strong><br /> Learn about the architecture that Cisco CloudLock uses to enable automated security and compliance checks throughout the entire development lifecycle, from the first line of code through runtime. It includes integration with IAM roles, Amazon VPC, and AWS KMS.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON410 –&nbsp;Advanced CICD with Amazon ECS Control Plane<br /> </strong>Mohit Gupta, product and engineering lead for Clever, demonstrates how to extend the Amazon ECS control plane to optimize management of container deployments and how the control plane can be broadly applied to take advantage of new AWS services. This includes ark—an AWS CLI-based deployment to Amazon ECS, Dapple—a slack-based automation system for deployments and notifications, and Kayvee—log and event routing libraries based on Amazon Kinesis.</p> 
<h3>Workshops</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON209 –&nbsp;Interstella 8888: Learn How to Use Docker on AWS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. &nbsp;Join this workshop to get hands-on experience with Docker as you containerize Interstella 8888’s aging monolithic application and deploy it using Amazon ECS.</p> 
<p><strong>CON213 –&nbsp;Hands-on Deployment of Kubernetes on AWS<br /> </strong>In this workshop, attendees get hands-on experience using Kubernetes and Kops (Kubernetes Operations), as described in our recent blog post. Attendees learn how to provision a cluster, assign role-based permissions and security, and launch a container. If you’re interested in learning best practices for running Kubernetes on AWS, don’t miss this workshop.</p> 
<b>TUESDAY 11/28</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON206 –&nbsp;Docker on AWS</strong><br /> In this session, Docker Technical Staff Member Patrick Chanezon discusses how Finnish Rail, the national train system for Finland, is using Docker on Amazon Web Services to modernize their customer facing applications, from ticket sales to reservations. Patrick also shares the state of Docker development and adoption on AWS, including explaining the opportunities and implications of efforts such as Project Moby, Docker EE, and how developers can use and contribute to Docker projects.</p> 
<p><strong>CON208 –&nbsp;Building Microservices on AWS</strong><br /> Increasingly, organizations are turning to microservices to help them empower autonomous teams, letting them innovate and ship software faster than ever before. But implementing a microservices architecture comes with a number of new challenges that need to be dealt with. Chief among these finding an appropriate platform to help manage a growing number of independently deployable services. In this session, Sam Newman, author of Building Microservices and a renowned expert in microservices strategy, discusses strategies for building scalable and robust microservices architectures. He also tells you how to choose the right platform for building microservices, and about common challenges and mistakes organizations make when they move to microservices architectures.</p> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON302 –&nbsp;Building a CICD Pipeline for Containers on AWS</strong><br /> Containers can make it easier to scale applications in the cloud, but how do you set up your CICD workflow to automatically test and deploy code to containerized apps? In this session, we explore how developers can build effective CICD workflows to manage their containerized code deployments on AWS.</p> 
<p>Ajit Zadgaonkar, Director of Engineering and Operations at Edmunds walks through best practices for CICD architectures used by his team to deploy containers. We also deep dive into topics such as how to create an accessible CICD platform and architect for safe blue/green deployments.</p> 
<p><strong>CON307 –&nbsp;Building Effective Container Images</strong><br /> Sick of getting paged at 2am and wondering “where did all my disk space go?” New Docker users often start with a stock image in order to get up and running quickly, but this can cause problems as your application matures and scales. Creating efficient container images is important to maximize resources, and deliver critical security benefits.</p> 
<p>In this session, AWS Sr. Technical Evangelist Abby Fuller covers how to create effective images to run containers in production. This includes an in-depth discussion of how Docker image layers work, things you should think about when creating your images, working with Amazon ECR, and mise-en-place for install dependencies. Prakash Janakiraman, Co-Founder and Chief Architect at Nextdoor discuss high-level and language-specific best practices for with building images and how Nextdoor uses these practices to successfully scale their containerized services with a small team.</p> 
<p><strong>CON309 –&nbsp;Containerized Machine Learning on AWS</strong><br /> Image recognition is a field of deep learning that uses neural networks to recognize the subject and traits for a given image. In Japan, Cookpad uses Amazon ECS to run an image recognition platform on clusters of GPU-enabled EC2 instances. In this session, hear from Cookpad about the challenges they faced building and scaling this advanced, user-friendly service to ensure high-availability and low-latency for tens of millions of users.</p> 
<p><strong>CON320 –&nbsp;Monitoring, Logging, and Debugging for Containerized Services</strong><br /> As containers become more embedded in the platform tools, debug tools, traces, and logs become increasingly important. Nare Hayrapetyan, Senior Software Engineer and Calvin French-Owen, Senior Technical Officer for Segment discuss the principals of monitoring and debugging containers and the tools Segment has implemented and built for logging, alerting, metric collection, and debugging of containerized services running on Amazon ECS.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON314 –&nbsp;Automating Zero-Downtime Production Cluster Upgrades for Amazon ECS</strong><br /> Containers make it easy to deploy new code into production to update the functionality of a service, but what happens when you need to update the Amazon EC2 compute instances that your containers are running on? In this talk, we’ll deep dive into how to upgrade the Amazon EC2 infrastructure underlying a live production Amazon ECS cluster without affecting service availability. Matt Callanan, Engineering Manager at Expedia walk through Expedia’s “PRISM” project that safely relocates hundreds of tasks onto new Amazon EC2 instances with zero-downtime to applications.</p> 
<p><strong>CON322 –&nbsp;Maximizing Amazon ECS for Large-Scale Workloads</strong><br /> Head of Mobfox DevOps, David Spitzer, shows how Mobfox used Docker and Amazon ECS to scale the Mobfox services and development teams to achieve low-latency networking and automatic scaling. This session covers Mobfox’s ecosystem architecture. It compares 2015 and today, the challenges Mobfox faced in growing their platform, and how they overcame them.</p> 
<p><strong>CON323 –&nbsp;Microservices Architectures for the Enterprise</strong><br /> Salva Jung, Principle Engineer for Samsung Mobile shares how Samsung Connect is architected as microservices running on Amazon ECS to securely, stably, and efficiently handle requests from millions of mobile and IoT devices around the world.</p> 
<p><strong>CON324 –&nbsp;Windows Containers on Amazon ECS</strong><br /> Docker containers are commonly regarded as powerful and portable runtime environments for Linux code, but Docker also offers API and toolchain support for running Windows Servers in containers. In this talk, we discuss the various options for running windows-based applications in containers on AWS.</p> 
<p><strong>CON326 –&nbsp;Remote Sensing and Image Processing on AWS</strong><br /> Learn how Encirca services by DuPont Pioneer uses Amazon ECS powered by GPU-instances and Amazon EC2 Spot Instances to run proprietary image-processing algorithms against satellite imagery. Mark Lanning and Ethan Harstad, engineers at DuPont Pioneer show how this architecture has allowed them to process satellite imagery multiple times a day for each agricultural field in the United States in order to identify crop health changes.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON317 –&nbsp;Advanced Container Management at Catsndogs.lol</strong><br /> Catsndogs.lol is a (fictional) company that needs help deploying and scaling its container-based application. During this workshop, attendees join the new DevOps team at CatsnDogs.lol, and help the company to manage their applications using Amazon ECS, and help release new features to make our customers happier than ever.Attendees get hands-on with service and container-instance auto-scaling, spot-fleet integration, container placement strategies, service discovery, secrets management with AWS Systems Manager Parameter Store, time-based and event-based scheduling, and automated deployment pipelines. If you are a developer interested in learning more about how Amazon ECS can accelerate your application development and deployment workflows, or if you are a systems administrator or DevOps person interested in understanding how Amazon ECS can simplify the operational model associated with running containers at scale, then this workshop is for you. You should have basic familiarity with Amazon ECS, Amazon EC2, and IAM.</p> 
<p>Additional requirements:</p> 
<li>The AWS CLI or AWS Tools for PowerShell installed</li> 
<li>An AWS account with administrative permissions (including the ability to create IAM roles and policies) created at least 24 hours in advance.</li> 
<b>WEDNESDAY 11/29</b> 
<h3>Birds of a Feather (BoF)</h3> 
<p><strong>CON205 –&nbsp;Birds of a Feather: Containers and Open Source at AWS</strong><br /> Cloud native architectures take advantage of on-demand delivery, global deployment, elasticity, and higher-level services to enable developer productivity and business agility. Open source is a core part of making cloud native possible for everyone. In this session, we welcome thought leaders from the CNCF, Docker, and AWS to discuss the cloud’s direction for growth and enablement of the open source community. We also discuss how AWS is integrating open source code into its container services and its contributions to open source projects.</p> 
<h3>Breakout Sessions</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON308 –&nbsp;Mastering Kubernetes on AWS</strong><br /> Much progress has been made on how to bootstrap a cluster since Kubernetes’ first commit and is now only a matter of minutes to go from zero to a running cluster on Amazon Web Services. However, evolving a simple Kubernetes architecture to be ready for production in a large enterprise can quickly become overwhelming with options for configuration and customization.</p> 
<p>In this session, Arun Gupta, Open Source Strategist for AWS and Raffaele Di Fazio, software engineer at leading European fashion platform Zalando, show the common practices for running Kubernetes on AWS and share insights from experience in operating tens of Kubernetes clusters in production on AWS. We cover options and recommendations on how to install and manage clusters, configure high availability, perform rolling upgrades and handle disaster recovery, as well as continuous integration and deployment of applications, logging, and security.</p> 
<p><strong>CON310 –&nbsp;Moving to Containers: Building with Docker and Amazon ECS</strong><br /> If you’ve ever considered moving part of your application stack to containers, don’t miss this session. We cover best practices for containerizing your code, implementing automated service scaling and monitoring, and setting up automated CI/CD pipelines with fail-safe deployments. Manjeeva Silva and Thilina Gunasinghe show how McDonalds implemented their home delivery platform in four months using Docker containers and Amazon ECS to serve tens of thousands of customers.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON402 –&nbsp;Advanced Patterns in Microservices Implementation with Amazon ECS</strong><br /> Scaling a microservice-based infrastructure can be challenging in terms of both technical implementation and developer workflow. In this talk, AWS Solutions Architect Pierre Steckmeyer is joined by Will McCutchen, Architect at BuzzFeed, to discuss Amazon ECS as a platform for building a robust infrastructure for microservices. We look at the key attributes of microservice architectures and how Amazon ECS supports these requirements in production, from configuration to sophisticated workload scheduling to networking capabilities to resource optimization. We also examine what it takes to build an end-to-end platform on top of the wider AWS ecosystem, and what it’s like to migrate a large engineering organization from a monolithic approach to microservices.</p> 
<p><strong>CON404 –&nbsp;Deep Dive into Container Scheduling with Amazon ECS</strong><br /> As your application’s infrastructure grows and scales, well-managed container scheduling is critical to ensuring high availability and resource optimization. In this session, we deep dive into the challenges and opportunities around container scheduling, as well as the different tools available within Amazon ECS and AWS to carry out efficient container scheduling. We discuss patterns for container scheduling available with Amazon ECS, the Blox scheduling framework, and how you can customize and integrate third-party scheduler frameworks to manage container scheduling on Amazon ECS.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON312 –&nbsp;Building a Selenium Fleet on the Cheap with Amazon ECS with Spot Fleet</strong><br /> Roberto Rivera&nbsp;and Matthew Wedgwood, engineers at RetailMeNot, give a practical overview of setting up a fleet of Selenium nodes running on Amazon ECS with Spot Fleet. Discuss the challenges of running Selenium with high availability at minimum cost using Amazon ECS container introspection to connect the Selenium Hub with its nodes.</p> 
<p><strong>CON315 –&nbsp;Virtually There: Building a Render Farm with Amazon ECS</strong><br /> Learn how 8i Corp scales its multi-tenanted, volumetric render farm up to thousands of instances using AWS, Docker, and an API-driven infrastructure. This render farm enables them to turn the video footage from an array of synchronized cameras into a photo-realistic hologram capable of playback on a range of devices, from mobile phones to high-end head mounted displays. Join Owen Evans, VP of Engineering for 8i, as they dive deep into how 8i’s rendering infrastructure is built and maintained by just a handful of people and powered by Amazon ECS.</p> 
<p><strong>CON325 –&nbsp;Developing Microservices – from Your Laptop to the Cloud</strong><br /> Wesley Chow, Staff Engineer at Adroll, shows how his team extends Amazon ECS by enabling local development capabilities. Hologram, Adroll’s local development program, brings the capabilities of the Amazon EC2 instance metadata service to non-EC2 hosts, so that developers can run the same software on local machines with the same credentials source as in production.</p> 
<p><strong>CON327 –&nbsp;Patterns and Considerations for Service Discovery</strong><br /> Roven Drabo, head of cloud operations at Kaplan Test Prep, illustrates Kaplan’s complete container automation solution using Amazon ECS along with how his team uses NGINX and HashiCorp Consul to provide an automated approach to service discovery and container provisioning.</p> 
<p><strong>CON328 –&nbsp;Building a Development Platform on Amazon ECS</strong><br /> Quinton Anderson, Head of Engineering for Commonwealth Bank of Australia, walks through&nbsp;how they migrated their internal development and deployment platform from Mesos/Marathon to Amazon ECS. The platform uses a custom DSL to abstract a layered application architecture, in a way that makes it easy to plug or replace new implementations into each layer in the stack.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON318 –&nbsp;Interstella 8888: Monolith to Microservices with Amazon ECS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. Join this workshop to get hands-on experience deploying Docker containers as you break Interstella 8888’s aging monolithic application into containerized microservices. Using Amazon ECS and an Application Load Balancer, you create API-based microservices and deploy them leveraging integrations with other AWS services.</p> 
<p><strong>CON332 –&nbsp;Build a Java Spring Application on Amazon ECS</strong><br /> This workshop teaches you how to lift and shift existing Spring and Spring Cloud applications onto the AWS platform. Learn how to build a Spring application container, understand bootstrap secrets, push container images to Amazon ECR, and deploy the application to Amazon ECS. Then, learn how to configure the deployment for production.</p> 
<b>THURSDAY 11/30</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON201 –&nbsp;Containers on AWS – State of the Union</strong><br /> Just over four years after the first public release of Docker, and three years to the day after the launch of Amazon ECS, the use of containers has surged to run a significant percentage of production workloads at startups and enterprise organizations. Join Deepak Singh, General Manager of Amazon Container Services, as he covers the state of containerized application development and deployment trends, new container capabilities on AWS that are available now, options for running containerized applications on AWS, and how AWS customers successfully run container workloads in production.</p> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON304 –&nbsp;Batch Processing with Containers on AWS</strong><br /> Batch processing is useful to analyze large amounts of data. But configuring and scaling a cluster of virtual machines to process complex batch jobs can be difficult. In this talk, we show how to use containers on AWS for batch processing jobs that can scale quickly and cost-effectively. We also discuss AWS Batch, our fully managed batch-processing service. You also hear from GoPro and Here about how they use AWS to run batch processing jobs at scale including best practices for ensuring efficient scheduling, fine-grained monitoring, compute resource automatic scaling, and security for your batch jobs.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON406 –&nbsp;Architecting Container Infrastructure for Security and Compliance</strong><br /> While organizations gain agility and scalability when they migrate to containers and microservices, they also benefit from compliance and security, advantages that are often overlooked. In this session, Kelvin Zhu, lead software engineer at Okta, joins Mitch Beaumont, enterprise solutions architect at AWS, to discuss security best practices for containerized infrastructure. Learn how Okta built their development workflow with an emphasis on security through testing and automation. Dive deep into how containers enable automated security and compliance checks throughout the development lifecycle. Also understand best practices for implementing AWS security and secrets management services for any containerized service architecture.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON329 –&nbsp;Full Software Lifecycle Management for Containers Running on Amazon ECS</strong><br /> Learn how The Washington Post uses Amazon ECS to run Arc Publishing, a digital journalism platform that powers The Washington Post and a growing number of major media websites. Amazon ECS enabled The Washington Post to containerize their existing microservices architecture, avoiding a complete rewrite that would have delayed the platform’s launch by several years. In this session, Jason Bartz, Technical Architect at The Washington Post, discusses the platform’s architecture. He addresses the challenges of optimizing Arc Publishing’s workload, and managing the application lifecycle to support 2,000 containers running on more than 50 Amazon ECS clusters.</p> 
<p><strong>CON330 –&nbsp;Running Containerized HIPAA Workloads on AWS</strong><br /> Nihar Pasala, Engineer at Aetion, discusses the Aetion Evidence Platform, a system for generating the real-world evidence used by healthcare decision makers to implement value-based care. This session discusses the architecture Aetion uses to run HIPAA workloads using containers on Amazon ECS, best practices, and learnings.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON408 –&nbsp;Building a Machine Learning Platform Using Containers on AWS</strong><br /> DeepLearni.ng develops&nbsp;and implements machine learning models for complex enterprise applications. In this session, Thomas Rogers, Engineer for DeepLearni.ng discusses how they worked with Scotiabank to leverage&nbsp;Amazon ECS, Amazon ECR, Docker, GPU-accelerated Amazon EC2 instances, and TensorFlow to develop a retail risk model that helps manage payment collections for millions of Canadian credit card customers.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON319 –&nbsp;Interstella 8888: CICD for Containers on AWS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. &nbsp;Join this workshop to learn how to set up a CI/CD pipeline for containerized microservices. You get hands-on experience&nbsp;deploying Docker container images using Amazon ECS, AWS CloudFormation, AWS CodeBuild, and AWS CodePipeline,&nbsp;automating&nbsp;everything from code check-in to production.</p> 
<b>FRIDAY 12/1</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON405 –&nbsp;Moving to Amazon ECS – the Not-So-Obvious Benefits</strong><br /> If you ask 10 teams why they migrated to containers, you will likely get answers like ‘developer productivity’, ‘cost reduction’, and ‘faster scaling’. But teams often find there are several other ‘hidden’ benefits to using containers for their services. In this talk, Franziska Schmidt, Platform Engineer at Mapbox and Yaniv Donenfeld from&nbsp;AWS will discuss the obvious, and not so obvious benefits of moving to containerized architecture. These include using Docker and Amazon ECS to achieve shared libraries for dev teams, separating private infrastructure from shareable code, and making it easier for non-ops engineers to run services.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON331 –&nbsp;Deploying a Regulated Payments Application on Amazon ECS</strong><br /> Travelex discusses how they built an FCA-compliant international payments service using a microservices architecture on AWS. This chalk talk covers the challenges of designing and operating an Amazon ECS-based PaaS in a regulated environment using a DevOps model.</p> 
<h3>Workshops</h3> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON407 –&nbsp;Interstella 8888: Advanced Microservice Operations</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. In this workshop, you help Interstella 8888 build a modern microservices-based logistics system to save the company from financial ruin. We give you the hands-on experience you need to run microservices in the real world. This includes implementing advanced container scheduling and scaling to deal with variable service requests, implementing a service mesh, issue&nbsp;tracing with AWS X-Ray, container and instance-level logging with Amazon CloudWatch, and load testing.</p> 
<b>Know before you go</b> 
<p>Want to brush up on your container knowledge before re:Invent? Here are some helpful resources to get started:</p> 
<li><a href="https://aws.amazon.com/ecs/getting-started">Amazon ECS Getting Started</a></li> 
<li><a href="https://aws.amazon.com/ecs/resources">Amazon ECS Resources</a></li> 
<li><a href="https://github.com/nathanpeck/awesome-ecs">Nathan Peck’s AWSome ECS</a></li> 
<li>Docs 
<li><a href="https://aws.amazon.com/documentation/ec2/">Amazon EC2</a></li> 
<li><a href="https://aws.amazon.com/documentation/ecs/">Amazon ECS</a></li> 
</ul> </li> 
<li>Blogs 
<li><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/">AWS Compute Blog</a></li> 
<li><a href="https://aws.amazon.com/blogs/aws/category/ec2-container-service/">AWS Blog</a></li> 
</ul> </li> 
<p>– Tiffany<br /> <a href="https://twitter.com/tiffanyfayj">@tiffanyfayj</a></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/2017/" rel="tag">2017</a>, <a href="https://aws.amazon.com/blogs/compute/tag/reinvent/" rel="tag">re:Invent</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3321');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
