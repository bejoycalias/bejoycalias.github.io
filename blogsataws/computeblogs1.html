<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Taking Advantage of Amazon EC2 Spot Instance Interruption Notices</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chad Schmutzer</span></span> | on 
<time property="datePublished" datetime="2018-03-09T13:59:28+00:00">09 MAR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ec2-spot/" title="View all posts in Amazon EC2 Spot"><span property="articleSection">Amazon EC2 Spot</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3990" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3990&amp;disqus_title=Taking+Advantage+of+Amazon+EC2+Spot+Instance+Interruption+Notices&amp;disqus_url=https://aws.amazon.com/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3990');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot Instances</a> are spare compute capacity in the AWS Cloud available to you at steep discounts compared to On-Demand prices. The only difference between On-Demand Instances and Spot Instances is that Spot Instances can be interrupted by Amazon EC2 with two minutes of notification when EC2 needs the capacity back.</p> 
<p>Customers have been taking advantage of Spot Instance interruption notices available via the <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">instance metadata service</a> since <a href="https://aws.amazon.com/blogs/aws/new-ec2-spot-instance-termination-notices/">January 2015</a> to orchestrate their workloads seamlessly around any potential interruptions. Examples include saving the state of a job, detaching from a load balancer, or draining containers. Needless to say, the two-minute Spot Instance interruption notice is a powerful tool when using Spot Instances.</p> 
<p>In <a href="https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-ec2-spot-two-minute-warning-is-now-available-via-amazon-cloudwatch-events/">January 2018</a>, the Spot Instance interruption notice also became available as an event in <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a>. This allows targets such as <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction-function.html">AWS Lambda functions</a> or <a href="https://docs.aws.amazon.com/sns/latest/dg/welcome.html">Amazon SNS topics</a> to process Spot Instance interruption notices by creating a CloudWatch Events rule to monitor for the notice.</p> 
<p>In this post, I walk through an example use case for taking advantage of Spot Instance interruption notices in CloudWatch Events to automatically deregister Spot Instances from an <a href="https://aws.amazon.com/elasticloadbalancing">Elastic Load Balancing</a> Application Load Balancer.</p> 
<b>Architecture</b> 
<p>In this reference architecture, you use an <a href="https://aws.amazon.com/cloudformation/aws-cloudformation-templates/">AWS CloudFormation template</a> to deploy the following:</p> 
<li>An <a href="https://aws.amazon.com/vpc/">Amazon Virtual Private Cloud</a> (Amazon VPC)&nbsp;with subnets in two Availability Zones</li> 
<li>An <a href="https://aws.amazon.com/elasticloadbalancing/details/#details">Application Load Balancer</a> with a listener and target group</li> 
<li>An Amazon CloudWatch Events rule</li> 
<li>An AWS Lambda function</li> 
<li>An Amazon Simple Notification Service (SNS) topic</li> 
<li>Associated <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">IAM policies and roles</a> for all of the above</li> 
<p>After the AWS CloudFormation stack deployment is complete, you then create an Amazon EC2&nbsp;<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html">Spot Fleet</a> request diversified across both Availability Zones and use a couple of recent Spot Fleet features: <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/spot-fleet-can-now-auto-attach-instances-to-your-load-balancers-and-scale-down-to-0-target-capacity/">Elastic Load Balancing integration</a> and&nbsp;<a href="https://aws.amazon.com/about-aws/whats-new/2017/07/tag-your-spot-fleet-ec2-instances/">Tagging Spot Fleet Instances</a>.</p> 
<p>When any of the Spot Instances receives an interruption notice, Spot Fleet sends the event to CloudWatch Events. The CloudWatch Events rule then notifies both targets, the Lambda function and SNS topic. The Lambda function detaches the Spot Instance from the Application Load Balancer target group, taking advantage of nearly a full two minutes of connection draining before the instance is interrupted. The SNS topic also receives a message, and is provided as an example for the reader to use as an exercise.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/24/interruption_notices_arch_diagram.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/24/interruption_notices_arch_diagram.jpg" /></a> 
<p class="wp-caption-text">EC2 Spot Instance Interruption Notices Reference Architecture Diagram</p> 
<b>Walkthrough</b> 
<p>To complete this walkthrough, have the <a href="https://aws.amazon.com/cli/">AWS CLI</a> installed and configured, as well as the ability to launch CloudFormation stacks.</p> 
<h3>Launch the stack</h3> 
<p>Go ahead and launch the CloudFormation stack. You can check it out from GitHub, or grab the <a href="https://github.com/awslabs/ec2-spot-labs/blob/master/ec2-spot-interruption-notice-cloudwatch-events/ec2-spot-interruption-notice-cloudwatch-events.yaml">template directly</a>. In this post, I use the stack name “<em>spot-spin-cwe</em>“, but feel free to use any name you like. Just remember to change it in the instructions.</p> 
<code class="lang-bash">$ git clone https://github.com/awslabs/ec2-spot-labs.git
$ aws cloudformation create-stack --stack-name spot-spin-cwe \
--template-body file://ec2-spot-labs/ec2-spot-interruption-notice-cloudwatch-events/ec2-spot-interruption-notice-cloudwatch-events.yaml \
--capabilities CAPABILITY_IAM</code> 
<p>You should receive a StackId value in return, confirming the stack is launching.</p> 
<code class="lang-json">{
&quot;StackId&quot;: &quot;arn:aws:cloudformation:us-east-1:123456789012:stack/spot-spin-cwe/083e7ad0-0ade-11e8-9e36-500c219ab02a&quot;
}</code> 
<h3>Review the details</h3> 
<p>Here are the details of the architecture being launched by the stack.</p> 
<h4>IAM permissions</h4> 
<p>Give permissions to a few components in the architecture:</p> 
<li>The Lambda function</li> 
<li>The CloudWatch Events rule</li> 
<li>The Spot Fleet</li> 
<p>The Lambda function needs basic Lambda function execution permissions so that it can write logs to CloudWatch Logs. You can use the AWS managed policy for this. It also needs to describe EC2 tags as well as deregister targets within Elastic Load Balancing. You can create a custom policy for these.</p> 
<code class="lang-yaml">lambdaFunctionRole:
Properties:
AssumeRolePolicyDocument:
Statement:
- Action:
- sts:AssumeRole
Effect: Allow
Principal:
Service:
- lambda.amazonaws.com
Version: 2012-10-17
ManagedPolicyArns:
- arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
Path: /
Policies:
- PolicyDocument:
Statement:
- Action: elasticloadbalancing:DeregisterTargets
Effect: Allow
Resource: '*'
- Action: ec2:DescribeTags
Effect: Allow
Resource: '*'
Version: '2012-10-17'
PolicyName:
Fn::Join:
- '-'
- - Ref: AWS::StackName
- lambdaFunctionRole
Type: AWS::IAM::Role</code> 
<p>Allow CloudWatch Events to call the Lambda function and publish to the SNS topic.</p> 
<code class="lang-yaml">lambdaFunctionPermission:
Properties:
Action: lambda:InvokeFunction
FunctionName:
Fn::GetAtt:
- lambdaFunction
- Arn
Principal: events.amazonaws.com
SourceArn:
Fn::GetAtt:
- eventRule
- Arn
Type: AWS::Lambda::Permission</code> 
<code class="lang-yaml">snsTopicPolicy:
DependsOn:
- snsTopic
Properties:
PolicyDocument:
Id:
Fn::GetAtt:
- snsTopic
- TopicName
Statement:
- Action: sns:Publish
Effect: Allow
Principal:
Service:
- events.amazonaws.com
Resource:
Ref: snsTopic
Version: '2012-10-17'
Topics:
- Ref: snsTopic
Type: AWS::SNS::TopicPolicy</code> 
<p>Finally, Spot Fleet needs permissions to request Spot Instances, tag, and register targets in Elastic Load Balancing. You can tap into an AWS managed policy for this.</p> 
<code class="lang-yaml">spotFleetRole:
Properties:
AssumeRolePolicyDocument:
Statement:
- Action:
- sts:AssumeRole
Effect: Allow
Principal:
Service:
- spotfleet.amazonaws.com
Version: 2012-10-17
ManagedPolicyArns:
- arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole
Path: /
Type: AWS::IAM::Role</code> 
<h4>Elastic Load Balancing timeout delay</h4> 
<p>Because you are taking advantage of the two-minute Spot Instance notice, you can tune the Elastic Load Balancing target group deregistration timeout delay to match. When a target is deregistered from the target group, it is put into connection draining mode for the length of the timeout delay: &nbsp;120 seconds to equal the two-minute notice.</p> 
<code class="lang-yaml">loadBalancerTargetGroup:
DependsOn:
- vpc
Properties:
HealthCheckIntervalSeconds: 5
HealthCheckPath: /
HealthCheckTimeoutSeconds: 2
Port: 80
Protocol: HTTP
TargetGroupAttributes:
- Key: deregistration_delay.timeout_seconds
Value: 120
UnhealthyThresholdCount: 2
VpcId:
Ref: vpc
Type: AWS::ElasticLoadBalancingV2::TargetGroup</code> 
<h4>CloudWatch Events rule</h4> 
<p>To capture the Spot Instance interruption notice being published to CloudWatch Events, create a rule with two targets: the Lambda function and the SNS topic.</p> 
<code class="lang-yaml">eventRule:
DependsOn:
- snsTopic
Properties:
Description: Events rule for Spot Instance Interruption Notices
EventPattern:
detail-type:
- EC2 Spot Instance Interruption Warning
source:
- aws.ec2
State: ENABLED
Targets:
- Arn:
Ref: snsTopic
Id:
Fn::GetAtt:
- snsTopic
- TopicName
- Arn:
Fn::GetAtt:
- lambdaFunction
- Arn
Id:
Ref: lambdaFunction
Type: AWS::Events::Rule</code> 
<h4>Lambda function</h4> 
<p>The Lambda function does the heavy lifting for you. The details of the CloudWatch event are published to the Lambda function, which then uses boto3 to make a couple of AWS API calls. The first call is to describe the EC2 tags for the Spot Instance, filtering on a key of “TargetGroupArn”. If this tag is found, the instance is then deregistered from the target group ARN stored as the value of the tag.</p> 
<code class="lang-python">import boto3
def handler(event, context):
instanceId = event['detail']['instance-id']
instanceAction = event['detail']['instance-action']
try:
ec2client = boto3.client('ec2')
describeTags = ec2client.describe_tags(Filters=[{'Name': 'resource-id','Values':[instanceId],'Name':'key','Values':['loadBalancerTargetGroup']}])
except:
print(&quot;No action being taken. Unable to describe tags for instance id:&quot;, instanceId)
return
try:
elbv2client = boto3.client('elbv2')
deregisterTargets = elbv2client.deregister_targets(TargetGroupArn=describeTags['Tags'][0]['Value'],Targets=[{'Id':instanceId}])
except:
print(&quot;No action being taken. Unable to deregister targets for instance id:&quot;, instanceId)
return
print(&quot;Detaching instance from target:&quot;)
print(instanceId, describeTags['Tags'][0]['Value'], deregisterTargets, sep=&quot;,&quot;)
return</code> 
<h4>SNS topic</h4> 
<p>Finally, you’ve created an SNS topic as an example target. For example, you could subscribe an email address to the SNS topic in order to receive email notifications when a Spot Instance interruption notice is received.</p> 
<code class="lang-yaml">snsTopic:
Properties:
DisplayName: SNS Topic for EC2 Spot Instance Interruption Notices
Type: AWS::SNS::Topic</code> 
<h3>Create a Spot Fleet request</h3> 
<p>To proceed to creating your Spot Fleet request, use some of the resources that the CloudFormation stack created, to populate the Spot Fleet request launch configuration. You can find the values in the outputs values of the CloudFormation stack:</p> 
<code class="lang-bash">$ aws cloudformation describe-stacks --stack-name spot-spin-cwe</code> 
<p>Using the output values of the CloudFormation stack, update the following values in the Spot Fleet request configuration:</p> 
<li><strong>%spotFleetRole%</strong></li> 
<li><strong>%publicSubnet1%</strong></li> 
<li><strong>%publicSubnet2%</strong></li> 
<li><strong>%loadBalancerTargetGroup%</strong> (in two places)</li> 
<p>Be sure to also replace <strong>%amiId%</strong> with the latest <a href="https://aws.amazon.com/amazon-linux-ami/#Amazon_Linux_AMI_IDs">Amazon Linux AMI</a> for your region and <strong>%keyName%</strong> with your environment.</p> 
<code class="lang-json">{
&quot;AllocationStrategy&quot;: &quot;diversified&quot;,
&quot;IamFleetRole&quot;: &quot;%spotFleetRole%&quot;,
&quot;LaunchSpecifications&quot;: [
{
&quot;ImageId&quot;: &quot;%amiId%&quot;,
&quot;InstanceType&quot;: &quot;c4.large&quot;,
&quot;Monitoring&quot;: {
&quot;Enabled&quot;: true
},
&quot;KeyName&quot;: &quot;%keyName%&quot;,
&quot;SubnetId&quot;: &quot;%publicSubnet1%,%publicSubnet2%&quot;,
&quot;UserData&quot;: &quot;IyEvYmluL2Jhc2gKeXVtIC15IHVwZGF0ZQp5dW0gLXkgaW5zdGFsbCBodHRwZApjaGtjb25maWcgaHR0cGQgb24KaW5zdGFuY2VpZD0kKGN1cmwgaHR0cDovLzE2OS4yNTQuMTY5LjI1NC9sYXRlc3QvbWV0YS1kYXRhL2luc3RhbmNlLWlkKQplY2hvICJoZWxsbyBmcm9tICRpbnN0YW5jZWlkIiA+IC92YXIvd3d3L2h0bWwvaW5kZXguaHRtbApzZXJ2aWNlIGh0dHBkIHN0YXJ0Cg==&quot;,
&quot;TagSpecifications&quot;: [
{
&quot;ResourceType&quot;: &quot;instance&quot;,
&quot;Tags&quot;: [
{
&quot;Key&quot;: &quot;loadBalancerTargetGroup&quot;,
&quot;Value&quot;: &quot;%loadBalancerTargetGroup%&quot;
}
]
}
]
}
],
&quot;TargetCapacity&quot;: 2,
&quot;TerminateInstancesWithExpiration&quot;: true,
&quot;Type&quot;: &quot;maintain&quot;,
&quot;ReplaceUnhealthyInstances&quot;: true,
&quot;InstanceInterruptionBehavior&quot;: &quot;terminate&quot;,
&quot;LoadBalancersConfig&quot;: {
&quot;TargetGroupsConfig&quot;: {
&quot;TargetGroups&quot;: [
{
&quot;Arn&quot;: &quot;%loadBalancerTargetGroup%&quot;
}
]
}
}
}</code> 
<p>Save the configuration and place the Spot Fleet request:</p> 
<code class="lang-bash">$ aws ec2 request-spot-fleet --spot-fleet-request-config file://sfr.json</code> 
<p>You should receive a <strong>SpotFleetRequestId</strong> in return, confirming the request:</p> 
<code class="lang-json">{
&quot;SpotFleetRequestId&quot;: &quot;sfr-3cec4927-9d86-4cc5-a4f0-faa996c841b7&quot;
}</code> 
<p>You can confirm that the Spot Fleet request was fulfilled by checking that <strong>ActivityStatus</strong> is “fulfilled”, or by checking that <strong>FulfilledCapacity</strong> is greater than or equal to <strong>TargetCapacity</strong>, while describing the request:</p> 
<code class="lang-bash">$ aws ec2 describe-spot-fleet-requests --spot-fleet-request-id sfr-3cec4927-9d86-4cc5-a4f0-faa996c841b7</code> 
<code class="lang-json">{
&quot;SpotFleetRequestConfigs&quot;: [
{
&quot;ActivityStatus&quot;: &quot;fulfilled&quot;,
&quot;CreateTime&quot;: &quot;2018-02-08T01:23:16.029Z&quot;,
&quot;SpotFleetRequestConfig&quot;: {
&quot;AllocationStrategy&quot;: &quot;diversified&quot;,
&quot;ExcessCapacityTerminationPolicy&quot;: &quot;Default&quot;,
&quot;FulfilledCapacity&quot;: 2.0,
…
&quot;TargetCapacity&quot;: 2,
…
}
]
}</code> 
<p>Next, you can confirm that the Spot Instances have been registered with the Elastic Load Balancing target group and are in a healthy state:</p> 
<code class="lang-bash">$ aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/spot-loadB-1DZUVWL720VS6/26456d12cddbf23a</code> 
<code class="lang-json">{
&quot;TargetHealthDescriptions&quot;: [
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-056c95d9dd6fde892&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;healthy&quot;
}
},
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-06c4c47228fd999b8&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;healthy&quot;
}
}
]
}</code> 
<h3>Test</h3> 
<p>In order to test, you can take advantage of the fact that any interruption action that Spot Fleet takes on a Spot Instance results in a Spot Instance interruption notice being provided. Therefore, you can simply decrease the target size of your Spot Fleet from 2 to 1. The instance that is interrupted receives the interruption notice:</p> 
<code class="lang-bash">$ aws ec2 modify-spot-fleet-request --spot-fleet-request-id sfr-3cec4927-9d86-4cc5-a4f0-faa996c841b7 --target-capacity 1</code> 
<code class="lang-json">{
&quot;Return&quot;: true
}</code> 
<p>As soon as the interruption notice is published to CloudWatch Events, the Lambda function triggers and detaches the instance from the target group, effectively putting the instance in a draining state.</p> 
<code class="lang-bash">$ aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/spot-loadB-1DZUVWL720VS6/26456d12cddbf23a</code> 
<code class="lang-json">{
&quot;TargetHealthDescriptions&quot;: [
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-0c3dcd78efb9b7e53&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;draining&quot;,
&quot;Reason&quot;: &quot;Target.DeregistrationInProgress&quot;,
&quot;Description&quot;: &quot;Target deregistration is in progress&quot;
}
},
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-088c91a66078b4299&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;healthy&quot;
}
}
]
}</code> 
<b>Conclusion</b> 
<p>In conclusion, Amazon EC2 Spot Instance interruption notices are an extremely powerful tool when taking advantage of Amazon EC2 Spot Instances in your workloads, for tasks such as saving state, draining connections, and much more. I’d love to hear how you are using them in your own environment!</p> 
<table style="margin-left: 30px"> 
<tbody style="padding-left: 30px"> 
<tr style="padding-left: 30px"> 
<td style="padding-left: 30px"> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/07/internal-cdn.amazon-2.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/07/internal-cdn.amazon-2.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Chad Schmutzer</span><br /> Solutions Architect</p> 
<td style="padding-left: 30px">Chad Schmutzer is a Solutions Architect at Amazon Web Services based in Pasadena, CA. As an extension of the Amazon EC2 Spot Instances team, Chad helps customers significantly reduce the cost of running their applications, growing their compute capacity and throughput without increasing budget, and enabling new types of cloud computing applications.</td> 
</tr> 
</tbody> 
</table> 
<p>&nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3990');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/27/xray_k8s-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Application Tracing on Kubernetes with AWS X-Ray</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2018-02-28T11:08:06+00:00">28 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-x-ray/" title="View all posts in AWS X-Ray*"><span property="articleSection">AWS X-Ray*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/application-tracing-on-kubernetes-with-aws-x-ray/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4002" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4002&amp;disqus_title=Application+Tracing+on+Kubernetes+with+AWS+X-Ray&amp;disqus_url=https://aws.amazon.com/blogs/compute/application-tracing-on-kubernetes-with-aws-x-ray/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4002');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong><em>This post was contributed by Christoph Kassen, AWS Solutions Architect</em></strong></p> 
<p>With the emergence of <a href="https://aws.amazon.com/microservices/">microservices architectures</a>, the number of services that are part of a web application has increased a lot. It’s not unusual anymore to build and operate hundreds of separate microservices, all as part of the same application.</p> 
<p>Think of a typical e-commerce application that displays products, recommends related items, provides search and faceting capabilities, and maintains a shopping cart. Behind the scenes, many more services are involved, such as clickstream tracking, ad display, targeting, and logging. When handling a single user request, many of these microservices are involved in responding. Understanding, analyzing, and debugging the landscape is becoming complex.</p> 
<p><a href="https://aws.amazon.com/xray/">AWS X-Ray</a> provides application-tracing functionality, giving deep insights into all microservices deployed. With X-Ray, every request can be traced as it flows through the involved microservices. This provides your DevOps teams the insights they need to understand how your services interact with their peers and enables them to analyze and debug issues much faster.</p> 
<p>With microservices architectures, every service should be self-contained and use the technologies best suited for the problem domain. Depending on how the service is built, it is deployed and hosted differently.</p> 
<p>One of the most popular choices for packaging and deploying microservices at the moment is <a href="https://aws.amazon.com/what-are-containers/">containers</a>. The application and its dependencies are clearly defined, the container can be built on CI infrastructure, and the deployment is simplified greatly. Container schedulers, such as Kubernetes and Amazon Elastic Container Service (<a href="https://aws.amazon.com/ecs/">Amazon ECS</a>), greatly simplify deploying and running containers at scale.</p> 
<b>Running X-Ray on Kubernetes</b> 
<p>Kubernetes is an open-source container management platform that automates deployment, scaling, and management of containerized applications.</p> 
<p>This post shows you how to run X-Ray on top of Kubernetes to provide application tracing capabilities to services hosted on a Kubernetes cluster. Additionally, X-Ray also works for applications hosted on <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, <a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a>, <a href="https://aws.amazon.com/ec2">Amazon EC2</a>, and even when building services with <a href="https://aws.amazon.com/lambda">AWS Lambda</a> functions. This flexibility helps you pick the technology you need while still being able to trace requests through all of the services running within your AWS environment.</p> 
<p>The complete code, including a simple Node.js based demo application is available in the corresponding&nbsp;<a href="https://github.com/aws-samples/aws-xray-kubernetes">aws-xray-kubernetes</a> GitHub repository, so you can quickly get started with X-Ray.</p> 
<p>The sample application within the repository consists of two simple microservices, Service-A and Service-B. The following architecture diagram shows how each service is deployed with two Pods on the Kubernetes cluster:</p> 
<ol> 
<li>Requests are sent to the Service-A from clients.</li> 
<li>Service-A then contacts Service-B.</li> 
<li>The requests are serviced by Service-B.</li> 
<li>Service-B adds a random delay to each request to show different response times in X-Ray.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/architecture-diagram-1024x569.jpeg" /></p> 
<p>To test out the sample applications on your own Kubernetes cluster use the Dockerfiles provided in the GitHub repository, build the two containers, push them to a container registry and apply the yaml configuration with kubectl to your Kubernetes cluster.</p> 
<b>Prerequisites</b> 
<p>If you currently do not have a cluster running within your AWS environment, take a look at Amazon Elastic Container Service for Kubernetes (<a href="https://aws.amazon.com/eks">Amazon EKS</a>), or use the instructions from the <a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/">Manage Kubernetes Clusters on AWS Using Kops</a> blog post to spin up a self-managed Kubernetes cluster.</p> 
<b>Security Setup for X-Ray</b> 
<p>The nodes in the Kubernetes cluster hosting web application Pods need IAM permissions so that the Pods hosting the X-Ray daemon can send traces to the X-Ray service backend.</p> 
<p>The easiest way is to set up a new IAM policy allowing all worker nodes within your Kubernetes cluster to write data to X-Ray. In the IAM console or <a href="https://aws.amazon.com/cli/">AWS CLI</a>, create a new policy like the following:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;xray:PutTraceSegments&quot;,
&quot;xray:PutTelemetryRecords&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:iam::000000000000:instance-profile/nodes.k8s.cluster.local &quot;
]
}
]
}</code> 
<p>Adjust the AWS account ID within the resource. Give the policy a descriptive name, such as <code class="lang-json">k8s-nodes-XrayWriteAccess</code>.</p> 
<p>Next, attach the policy to the instance profile for the Kubernetes worker nodes. Therefore, select the IAM role assigned to your worker instances (check the EC2 console if you are unsure) and attach the IAM policy created earlier to it. You can attach the IAM permissions directly from the command line with the following command:</p> 
<p><code class="lang-bash">aws iam attach-role-policy --role-name k8s-nodes --policy-arn arn:aws:iam::000000000000:policy/k8s-nodes-XrayWriteAccess</code></p> 
<b>Build the X-Ray daemon Docker image</b> 
<p>The X-Ray daemon is available as a single, statically compiled binary that can be downloaded directly from the AWS website.</p> 
<p>The first step is to create a Docker container hosting the X-Ray daemon binary and exposing port 2000 via UDP. The daemon is either configured via command line parameters or a configuration file. The most important option is to set the listen port to the correct IP address so that tracing requests from application Pods can be accepted.</p> 
<p>To build your own Docker image containing the X-Ray daemon, use the Dockerfile shown below.</p> 
<code class="lang-bash"># Use Amazon Linux Version 1
FROM amazonlinux:1
# Download latest 2.x release of X-Ray daemon
RUN yum install -y unzip &amp;&amp; \
cd /tmp/ &amp;&amp; \
curl https://s3.dualstack.us-east-2.amazonaws.com/aws-xray-assets.us-east-2/xray-daemon/aws-xray-daemon-linux-2.x.zip &gt; aws-xray-daemon-linux-2.x.zip &amp;&amp; \
unzip aws-xray-daemon-linux-2.x.zip &amp;&amp; \
cp xray /usr/bin/xray &amp;&amp; \
rm aws-xray-daemon-linux-2.x.zip &amp;&amp; \
rm cfg.yaml
# Expose port 2000 on udp
EXPOSE 2000/udp
ENTRYPOINT [&quot;/usr/bin/xray&quot;]
# No cmd line parameters, use default configuration
CMD [''] </code> 
<p>This container image is based on Amazon Linux which results in a small container image. To build and tag the container image run <code class="lang-bash">docker build -t xray:latest</code>.</p> 
<b>Create an Amazon ECR repository</b> 
<p>Create a repository in Amazon&nbsp;Elastic Container Registry (<a href="https://aws.amazon.com/ecr">Amazon ECR</a>)&nbsp;to hold your X-Ray Docker image. Your Kubernetes cluster uses this repository to pull the image from upon deployment of the X-Ray Pods.</p> 
<p>Use the following CLI command to create your repository or alternatively the AWS Management Console:</p> 
<p><code class="lang-bash">aws ecr create-repository --repository-name xray-daemon</code></p> 
<p>This creates a repository named xray-daemon for you and prints the repository URI used when pushing the image.</p> 
<p>After the container build completes, push it to your ECR repository with the following push commands for the repository that you just created.</p> 
<p><code class="lang-bash">docker tag xray-daemon:latest 000000000000.dkr.ecr.eu-west-1.amazonaws.com/xray-daemon:latest</code><br /> <code class="lang-bash">docker push 000000000000.dkr.ecr.eu-west-1.amazonaws.com/xray-daemon:latest</code></p> 
<p>The resulting repository should look similar to the one shown in the following screenshot.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/ecr-repo-1024x524.jpeg" /></p> 
<p>You can automate this processes using <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a> and <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a>. For instructions how to build Docker containers and push them to ECR automatically, see <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html">Docker Sample for AWS CodeBuild</a>.</p> 
<b>Deploy the X-Ray daemon to Kubernetes</b> 
<p>Make sure that you configure the <code class="lang-bash">kubectl</code> tool properly for your cluster to be able to deploy the X-Ray Pod onto your Kubernetes cluster. After the X-Ray Pods are deployed to your Kubernetes cluster, applications can send tracing information to the X-Ray daemon on their host. The biggest advantage is that you do not need to provide X-Ray as a sidecar container alongside your application. This simplifies the configuration and deployment of your applications and overall saves resources on your cluster.</p> 
<p>To deploy the X-Ray daemon as Pods onto your Kubernetes cluster, run the following from the cloned GitHub repository:</p> 
<p><code class="lang-bash">kubectl apply -f xray-k8s-daemonset.yaml</code></p> 
<p>This deploys and maintains an X-Ray Pod on each worker node, which is accepting tracing data from your microservices and routing it to one of the X-Ray pods. When deploying the container using a DaemonSet, the X-Ray port is also exposed directly on the host. This way, clients can connect directly to the daemon on your node. This avoids unnecessary network traffic going across your cluster.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-diagram-1024x362.jpeg" /></p> 
<b>Connecting to the X-Ray daemon</b> 
<p>To integrate application tracing with your applications, use the X-Ray SDK for one of the supported programming languages:</p> 
<li>Java</li> 
<li>Node.js</li> 
<li>.NET (Framework and Core)</li> 
<li>Go</li> 
<li>Python</li> 
<p>The SDKs provide classes and methods for generating and sending trace data to the X-Ray daemon. Trace data includes information about incoming HTTP requests served by the application, and calls that the application makes to downstream services using the AWS SDK or HTTP clients.</p> 
<p>By default, the X-Ray SDK expects the daemon to be available on <code class="lang-bash">127.0.0.1:2000</code>. This needs to be changed in this setup, as the daemon is not part of each Pod but hosted within its own Pod.</p> 
<p>The deployed X-Ray DaemonSet exposes all Pods via the Kubernetes service discovery, so applications can use this endpoint to discover the X-Ray daemon. If you deployed to the default namespace, the endpoint is:</p> 
<p><code class="lang-bash">xray-service.default</code></p> 
<p>Applications now need to set the daemon address either with the <code class="lang-bash">AWS_XRAY_DAEMON_ADDRESS</code> environment variable (preferred) or directly within the SDK setup code:</p> 
<p><code class="lang-js">AWSXRay.setDaemonAddress('xray-service.default:2000');</code></p> 
<p>To set up the environment variable, include the following information in your Kubernetes application deployment description YAML. That exposes the X-Ray service address via the environment variable, where it is picked up automatically by the SDK.</p> 
<code class="lang-yaml">env:
- name: AWS_XRAY_DAEMON_ADDRESS 
value: xray-service.default</code> 
<p>&nbsp;</p> 
<b>Sending tracing information to AWS X-Ray</b> 
<p>Sending tracing information from your application is straightforward with the X-Ray SDKs. The example code below serves as a starting point to instrument your application with traces. Take a look at the two sample applications in the GitHub repository to see how to send traces from Service A to Service B. The diagram below visualizes the flow of requests between the services.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-architecture-1024x589.jpeg" /></p> 
<p>Because your application is running within containers, enable both the EC2Plugin and ECSPlugin, which gives you information about the Kubernetes node hosting the Pod as well as the container name. Despite the name ECSPlugin, this plugin gives you additional information about your container when running your application on Kubernetes.</p> 
<code class="lang-java">var app = express();
//...
var AWSXRay = require('aws-xray-sdk');
AWSXRay.config([XRay.plugins.EC2Plugin, XRay.plugins.ECSPlugin]);
app.use(AWSXRay.express.openSegment('defaultName'));  //required at the start of your routes
app.get('/', function (req, res) {
res.render('index');
});
app.use(AWSXRay.express.closeSegment());  //required at the end of your routes / first in error handling routes</code> 
<p>For more information about all options and possibilities to instrument your application code, see the <a href="https://aws.amazon.com/documentation/xray/">X-Ray documentation</a>&nbsp;page for the corresponding SDK information.</p> 
<p>The picture below shows the resulting service map that provides insights into the flow of requests through the microservice landscape. You can drill down here into individual traces and see which path each request has taken.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-servicemap-1024x403.jpeg" /></p> 
<p>From the service map, you can drill down into individual requests and see where they originated from and how much time was spent in each service processing the request.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-trace-1024x337.jpeg" /></p> 
<p>You can also view details about every individual segment of the trace by clicking on it. This displays more details.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-segment-info-1024x776.jpeg" /></p> 
<p>On the <strong>Resources</strong> tab, you will see the Kubernetes Pod picked up by the ECSPlugin, which handled the request, as well as the instance that Pod was running on.</p> 
<b>Summary</b> 
<p>In this post, I shared how to deploy and run X-Ray on an existing Kubernetes cluster. Using tracing gives you deep insights into your applications to ease analysis and spot potential problems early. With X-Ray, you get these insights for all your applications running on AWS, no matter if they are hosted on <a href="https://aws.amazon.com/ecs">Amazon ECS</a>, <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, or a Kubernetes cluster.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4002');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/Picture1-3.png" /> 
<b class="lb-b blog-post-title" property="name headline">Building an Immersive VR Streaming Solution on AWS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chad Schmutzer</span></span> | on 
<time property="datePublished" datetime="2018-02-23T13:41:32+00:00">23 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ec2-spot/" title="View all posts in Amazon EC2 Spot"><span property="articleSection">Amazon EC2 Spot</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/ar-vr/" title="View all posts in AR &amp; VR"><span property="articleSection">AR &amp; VR</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/" title="View all posts in Networking &amp; Content Delivery*"><span property="articleSection">Networking &amp; Content Delivery*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/open-source/" title="View all posts in Open Source*"><span property="articleSection">Open Source*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-an-immersive-vr-streaming-solution-on-aws/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3568" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3568&amp;disqus_title=Building+an+Immersive+VR+Streaming+Solution+on+AWS&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-an-immersive-vr-streaming-solution-on-aws/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3568');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post was contributed by:</p> 
<table style="height: 124px" width="275"> 
<tbody> 
<tr> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Konstantin Wilms</span><br />Solutions Architect</p> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/profile.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/profile.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Shawn Przybilla</span><br />Solutions Architect</p> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon-2.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon-2.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Chad Schmutzer</span><br />Solutions Architect</p> 
</tr> 
</tbody> 
</table> 
<p>With the explosion in virtual reality (VR) technologies over the past few years, we’ve had an increasing number of customers ask us for advice and best practices around deploying their VR-based products and service offerings on the AWS Cloud. It soon became apparent that while the VR ecosystem is large in both scope and depth of types of workloads (gaming, e-medicine, security analytics, live streaming events, etc.), many of the workloads followed repeatable patterns, with storage and delivery of live and on-demand immersive video at the top of the list.</p> 
<p>Looking at consumer trends, the desire for live and on-demand immersive video is fairly self-explanatory. VR has ushered in convenient and low-cost access for consumers and businesses to a wide variety of options for consuming content, ranging from browser playback of live and on-demand 360&ordm; video, all the way up to positional tracking systems with a high degree of immersion. All of these scenarios contain one lowest common denominator: &nbsp;video.</p> 
<p>Which brings us to the topic of this post. We set out to build a solution that could support both live and on-demand events, bring with it a high degree of scalability, be flexible enough to support transformation of video if required, run at a low cost, and use open-source software to every extent possible.</p> 
<p>In this post, we describe the reference architecture we created to solve this challenge, using&nbsp;<a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot Instances</a>,&nbsp;<a href="https://aws.amazon.com/s3/">Amazon S3</a>,&nbsp;<a href="https://aws.amazon.com/elasticloadbalancing/">Elastic Load Balancing</a>,&nbsp;<a href="https://aws.amazon.com/cloudfront/">Amazon CloudFront</a>,&nbsp;<a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a>, and&nbsp;<a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch</a>, with open-source software such as&nbsp;<a href="https://www.nginx.com/">NGINX</a>,&nbsp;<a href="https://www.ffmpeg.org/">FFMPEG</a>,&nbsp;and JavaScript-based client-side playback technologies. We step you through deployment of the solution and how the components work, as well as the capture, processing, and playback of the underlying live and on-demand immersive media streams.</p> 
<p>This&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch">GitHub repository</a>&nbsp;includes the source code necessary to follow along. We’ve also provided a&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch/tree/master/workshop">self-paced workshop</a>, from&nbsp;<a href="https://reinvent.awsevents.com/">AWS re:Invent</a> 2017&nbsp;that breaks down this architecture even further. If you experience any issues or would like to suggest an enhancement, please use the&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch/issues">GitHub issue tracker</a>.</p> 
<b>Prerequisites</b> 
<p>As a side note, you’ll also need a few additional components to take best advantage of the infrastructure:</p> 
<li>A camera/capture device capable of encoding and streaming&nbsp;<a href="http://www.adobe.com/devnet/rtmp.html">RTMP</a>&nbsp;video</li> 
<li>A browser to consume the content.</li> 
<p>You’re going to generate HTML5-compatible video (Apple HLS to be exact), but there are many other native iOS and Android options for consuming the media that you create. It’s also worth noting that your playback device should support projection of your input stream. We’ll talk more about that in the next section.</p> 
<b>How does immersive media work?</b> 
<p>At its core, any flavor of media, be that audio or video, can be viewed with some level of immersion. The ability to interact passively or actively with the content brings with it a further level of immersion. When you look at VR devices with rotational and positional tracking, you naturally need more than an ability to interact with a flat plane of video. The challenge for any creative thus becomes a tradeoff between immersion features (degrees of freedom, monoscopic 2D or stereoscopic 3D, resolution, framerate) and overall complexity.</p> 
<p>Where can you start from a simple and effective point of view, that enables you to build out a fairly modular solution and test it? There are a few areas we chose to be prescriptive with our solution.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/Picture1-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/Picture1-3.png" /></a> 
<p class="wp-caption-text">Source capture from the Ricoh Theta S</p> 
<p>First, monoscopic 360-degree video is currently one of the most commonly consumed formats on consumer devices. We explicitly chose to focus on this format, although the infrastructure is not limited to it. More on this later.</p> 
<p>Second, if you look at most consumer-level cameras that provide live streaming ability, and even many professional rigs, there are at least two lenses or cameras at a minimum. The figure above illustrates a single capture from a Ricoh Theta S in monoscopic 2D. The left image captures 180 degrees of the field of view, and the right image captures the other 180 degrees.</p> 
<p>For this post, we chose a typical midlevel camera (the&nbsp;<a href="https://theta360.com/en/about/theta/s.html">Ricoh Theta S</a>), and used a laptop with open-source software (<a href="https://obsproject.com/">Open Broadcaster Software</a>) to encode and stream the content. Again, the solution infrastructure is not limited to this particular brand of camera. Any camera or encoder that outputs 360&ordm; video and encodes to H264+AAC with an RTMP transport will work.</p> 
<p>Third, capturing and streaming multiple camera feeds brings additional requirements around stream synchronization and cost of infrastructure. There is also a requirement to stitch media in real time, which can be CPU and GPU-intensive. Many devices and platforms do this either on the device, or via outboard processing that sits close to the camera location. If you stitch and deliver a single stream, you can save the costs of infrastructure and bitrate/connectivity requirements. We chose to keep these aspects on the encoder side to save on cost and reduce infrastructure complexity.</p> 
<p>Last, the most common delivery format that requires little to no processing on the infrastructure side is equirectangular projection, as per the above figure. By stitching and unwrapping the spherical coordinates into a flat plane, you can easily deliver the video exactly as you would with any other live or on-demand stream. The only caveat is that resolution and bit rate are of utmost importance. The higher you can push these (high bit rate @ 4K resolution), the more immersive the experience is for viewers. This is due to the increase in sharpness and reduction of compression artifacts.</p> 
<p>Knowing that we would be transcoding potentially at 4K on the source camera, but in a format that could be transmuxed without an encoding penalty on the origin servers, we implemented a pass-through for the highest bit rate, and elected to only transcode lower bitrates. This requires some level of configuration on the source encoder, but saves on cost and infrastructure. Because you can conform the source stream, you may as well take advantage of that!</p> 
<p>For this post, we chose not to focus on ways to optimize projection. However, the reference architecture does support this with additional open source components compiled into the FFMPEG toolchain. A number of options are available to this end, such as open source equirectangular to cubic transformation filters. There is a tradeoff, however, in that reprojection implies that all streams must be transcoded.</p> 
<b>Processing and origination stack</b> 
<p>To get started, we’ve provided a&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/templates/template.yaml">CloudFormation template</a>&nbsp;that you can launch directly into your own AWS account. We quickly review how it works, the solution’s components, key features, processing steps, and examine the main configuration files. Following this, you launch the stack, and then proceed with camera and encoder setup.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/ref_arch_diagram.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/ref_arch_diagram.png" /></a> 
<p class="wp-caption-text">Immersive streaming reference architecture</p> 
<ol> 
<li>The event encoder publishes the RTMP source to multiple origin elastic IP addresses for packaging into the HLS adaptive bitrate.</li> 
<li>The client requests the live stream through the CloudFront CDN.</li> 
<li>The origin responds with the appropriate HLS stream.</li> 
<li>The edge fleet caches media requests from clients and elastically scales across both Availability Zones to meet peak demand.</li> 
<li>CloudFront caches media at local edge PoPs to improve performance for users and reduce the origin load.</li> 
<li>When the live event is finished, the VOD asset is published to S3. An S3 event is then published to SQS.</li> 
<li>The encoding fleet processes the read messages from the SQS queue, processes the VOD clips, and stores them in the S3 bucket.</li> 
</ol> 
<h3>How it works</h3> 
<p>A camera captures content, and with the help of a contribution encoder, publishes a live stream in equirectangular format. The stream is encoded at a high bit rate (at least 2.5 Mbps, but typically 16+ Mbps for 4K) using H264 video and AAC audio compression codecs, and delivered to a primary origin via the RTMP protocol. Streams may transit over the internet or dedicated links to the origins. Typically, for live events in the field, internet or bonded cellular are the most widely used.</p> 
<p>The encoder is typically configured to push the live stream to a primary URI, with the ability (depending on the source encoding software/hardware) to roll over to a backup publishing point origin if the primary fails. Because you run across multiple Availability Zones, this architecture could handle an entire zone outage with minor disruption to live events. The primary and backup origins handle the ingestion of the live stream as well as transcoding to H264+AAC-based adaptive bit rate sets. After transcode, they package the streams into HLS for delivery and create a master-level manifest that references all adaptive bit rates.</p> 
<p>The edge cache fleet pulls segments and manifests from the active origin on demand, and supports failover from primary to backup if the primary origin fails. By adding this caching tier, you effectively separate the encoding backend tier from the cache tier that responds to client or CDN requests. In addition to origin protection, this separation allows you to independently monitor, configure, and scale these components.</p> 
<p>Viewers can use the sample HTML5 player (or compatible desktop, iOS or Android application) to view the streams. Navigation in the 360-degree view is handled either natively via device-based gyroscope, positionally via more advanced devices such as a head mount display, or via mouse drag on the desktop. Adaptive bit rate is key here, as this allows you to target multiple device types, giving the player on each device the option of selecting an optimum stream based on network conditions or device profile.</p> 
<h3>Solution components</h3> 
<p>When you deploy the CloudFormation template, all the architecture services referenced above are created and launched. This includes:</p> 
<li>The compute tier running on Spot Instances for the corresponding components: 
<li>the primary and backup ingest origins</li> 
<li>the edge cache fleet</li> 
<li>the transcoding fleet</li> 
<li>the test source</li> 
</ul> </li> 
<li>The CloudFront distribution</li> 
<li>S3 buckets for storage of on-demand VOD assets</li> 
<li>An Application Load Balancer for load balancing the service</li> 
<li>An&nbsp;<a href="https://aws.amazon.com/ecs/">Amazon ECS</a>&nbsp;cluster and container for the test source</li> 
<li>An&nbsp;<a href="https://aws.amazon.com/sqs/">Amazon SQS</a>&nbsp;queue</li> 
<p>The template also provisions the underlying dependencies:</p> 
<li>A VPC</li> 
<li>Security groups</li> 
<li>IAM policies and roles</li> 
<li>Elastic network interfaces</li> 
<li>Elastic IP addresses</li> 
<p>The edge cache fleet instances need some way to discover the primary and backup origin locations. You use elastic network interfaces and elastic IP addresses for this purpose.</p> 
<p>As each component of the infrastructure is provisioned, software required to transcode and process the streams across the Spot Instances is automatically deployed. This includes&nbsp;<a href="https://github.com/arut/nginx-rtmp-module">NGiNX-RTMP</a>&nbsp;for ingest of live streams, FFMPEG for transcoding, NGINX for serving, and helper scripts to handle various tasks (potential Spot Instance interruptions, queueing, moving content to S3). Metrics and logs are available through CloudWatch and you can manage the deployment using the CloudFormation console or&nbsp;<a href="https://aws.amazon.com/cli/">AWS CLI</a>.</p> 
<p>Key features include:</p> 
<li><strong>Live and video-on-demand recording</strong></li> 
<p style="padding-left: 30px">You’re supporting both live and on-demand. On-demand content is created automatically when the encoder stops publishing to the origin.</p> 
<li><strong>Cost-optimization and operating at scale using Spot Instances</strong></li> 
<p style="padding-left: 30px">Spot Instances are used exclusively for infrastructure to optimize cost and scale throughput.</p> 
<li><strong>Midtier caching</strong></li> 
<p style="padding-left: 30px">To protect the origin servers, the midtier cache fleet pulls, caches, and delivers to downstream CDNs.</p> 
<li><strong>Distribution via CloudFront or multi-CDN</strong></li> 
<p style="padding-left: 30px">The Application Load Balancer endpoint allows CloudFront or any third-party CDN to source content from the edge fleet and, indirectly, the origin.</p> 
<li><strong>FFMPEG + NGINX + NGiNX-RTMP</strong></li> 
<p style="padding-left: 30px">These three components form the core of the stream ingest, transcode, packaging, and delivery infrastructure, as well as the VOD-processing component for creating transcoded VOD content on-demand.</p> 
<li><strong>Simple deployment using a CloudFormation template</strong></li> 
<p style="padding-left: 30px">All infrastructure can be easily created and modified using CloudFormation.</p> 
<li><strong>Prototype player page</strong></li> 
<p style="padding-left: 30px">To provide an end-to-end experience right away, we’ve included a test player page hosted as a static site on S3. This page uses&nbsp;<a href="https://aframe.io/">A-Frame</a>, a cross-platform, open-source framework for building VR experiences in the browser. Though A-Frame provides many features, it’s used here to render a sphere that acts as a 3D canvas for your live stream.</p> 
<h3>Spot Instance considerations</h3> 
<p>At this stage, and before we discuss processing, it is important to understand how the architecture operates with Spot Instances.</p> 
<p>Spot Instances are spare compute capacity in the AWS Cloud available to you at steep discounts compared to On-Demand prices. Spot Instances enables you to optimize your costs on the AWS Cloud and scale your application’s throughput up to 10X for the same budget. By selecting Spot Instances, you can save up-to 90% on On-Demand prices. This allows you to greatly reduce the cost of running the solution because, outside of S3 for storage and CloudFront for delivery, this solution is almost entirely dependent on Spot Instances for infrastructure requirements.</p> 
<p>We also know that customers running events look to deploy streaming infrastructure at the lowest price point, so it makes sense to take advantage of it wherever possible. A potential challenge when using Spot Instances for live streaming and on-demand processing is that you need to proactively deal with potential Spot Instance interruptions. How can you best deal with this?</p> 
<p>First, the origin is deployed in a primary/backup deployment. If a Spot Instance interruption happens on the primary origin, you can fail over to the backup with a brief interruption. Should a potential interruption not be acceptable, then either Reserved Instances or On-Demand options (or a combination) can be used at this tier.</p> 
<p>Second, the edge cache fleet runs a job (started automatically at system boot) that periodically queries the local instance metadata to detect if an interruption is scheduled to occur.&nbsp;<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html">Spot Instance Interruption Notices</a>&nbsp;provide a two-minute warning of a pending interruption. If you poll every 5 seconds, you have almost 2 full minutes to detach from the&nbsp;Load Balancer and drain or stop any traffic directed to your instance.</p> 
<p>Lastly, use an SQS queue when transcoding. If a transcode for a Spot Instance is interrupted, the stale item falls back into the SQS queue and is eventually re-surfaced into the processing pipeline. Only remove items from the queue after the transcoded files have been successfully moved to the destination S3 bucket.</p> 
<h3>Processing</h3> 
<p>As discussed in the previous sections, you pass through the video for the highest bit rate to save on having to increase the instance size to transcode the 4K or similar high bit rate or resolution content.</p> 
<p>We’ve selected a handful of bitrates for the adaptive bit rate stack. You can customize any of these to suit the requirements for your event. The default ABR stack includes:</p> 
<li>2160p (4K)</li> 
<li>1080p</li> 
<li>540p</li> 
<li>480p</li> 
<p>These can be modified by editing the&nbsp;<em>/etc/nginx/rtmp.d/rtmp.conf</em>&nbsp;NGINX configuration file on the origin or the CloudFormation template.</p> 
<p>It’s important to understand where and how streams are transcoded. When the source high bit rate stream enters the primary or backup origin at the&nbsp;<em>/live</em>&nbsp;RTMP application entry point, it is recorded on stop and start of publishing. On completion, it is moved to S3 by a cleanup script, and a message is placed in your SQS queue for workers to use. These workers transcode the media and push it to a playout location bucket.</p> 
<p>This solution uses Spot Fleet with automatic scaling to drive the fleet size. You can customize it based on CloudWatch metrics, such as simple utilization metrics to drive the size of the fleet. Why use Spot Instances for the transcode option instead of Amazon Elastic Transcoder? This allows you to implement reprojection of the input stream via FFMPEG filters in the future.</p> 
<p>The origins handle all the heavy live streaming work. Edges only store and forward the segments and manifests, and provide scaling plus reduction of burden on the origin. This lets you customize the origin to the right compute capacity without having to rely on a ‘high watermark’ for compute sizing, thus saving additional costs.</p> 
<p>Loopback is an important concept for the live origins. The incoming stream entering&nbsp;<em>/live</em>&nbsp;is transcoded by FFMPEG to multiple bit rates, which are streamed back to the same host via RTMP, on a secondary publishing point&nbsp;<em>/show</em>. The secondary publishing point is transparent to the user and encoder, but handles HLS segment generation and cleanup, and keeps a sliding window of live segments and constantly updating manifests.</p> 
<b>Configuration</b> 
<p>Our solution provides two key points of configuration that can be used to customize the solution to accommodate ingest, recording, transcoding, and delivery, all controlled via origin and edge configuration files, which are described later. In addition, a number of job scripts run on the instances to provide hooks into Spot Instance interruption events and the VOD SQS-based processing queue.</p> 
<h3>Origin instances</h3> 
<p>The&nbsp;<em>rtmp.conf</em>&nbsp;excerpt below also shows additional parameters that can be customized, such as maximum recording file size in Kbytes, HLS Fragment length, and Playlist sizes. We’ve created these in accordance with general industry best practices to ensure the reliable streaming and delivery of your content.</p> 
<code class="lang-json">rtmp {
server {
listen 1935;
chunk_size 4000;
application live {
live on;
record all;
record_path /var/lib/nginx/rec;
record_max_size 128000K;
exec_record_done /usr/local/bin/record-postprocess.sh $path $basename;
exec /usr/local/bin/ffmpeg &lt;…parameters…&gt;;
}
application show {
live on;
hls on;
...
hls_type live;
hls_fragment 10s;
hls_playlist_length 60s;
...
}
}
}</code> 
<p>This exposes a few URL endpoints for debugging and general status. In production, you would most likely turn these off:</p> 
<li><em>/stat</em>&nbsp;provides a statistics endpoint accessible via any standard web browser.</li> 
<li><em>/control</em>&nbsp;enables control of RTMP streams and publishing points.</li> 
<p>You also control the TTLs, as previously discussed. It’s important to note here that you are setting TTLs explicitly at the origin, instead of in CloudFront’s distribution configuration. While both are valid, this approach allows you to reconfigure and restart the service on the fly without having to push changes through CloudFront. This is useful for debugging any caching or playback issues.</p> 
<code class="lang-json">location /stat {
...
}
location /stat.xsl {
...
}
location /control {
...
}
# origin controlled ttls
location ~* /hls/.*\.ts$ {
...
add_header Cache-Control &quot;max-age=900&quot;;
expires 900s;
}
location ~* /hls/.*\.m3u8$ {
...
add_header Cache-Control &quot;max-age=5&quot;;
expires 5s;
}</code> 
<h3>Handler scripts</h3> 
<p>Here is a brief overview of the scripts we use to handle the events and process of the solution. We encourage you to take some time to review them.</p> 
<li><a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/user-data/edge/bin/spot-termination-handler.sh">spot-termination-handler.sh</a>&nbsp;– Provides a graceful shutdown for the transcoding and edge fleet instances.</li> 
<li><a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/user-data/origin/bin/record-postprocess.sh">record-postprocess.sh</a>&nbsp;– Ensures that recorded files on the origin are well-formed, and transfers them to S3 for processing.</li> 
<li><a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/user-data/origin/bin/record-postprocess.sh">ffmpeg.sh</a>&nbsp;– Transcodes content on the encoding fleet, pulling source media from your S3 ingress bucket, based on SQS queue entries, and pushing transcoded adaptive bit rate segments and manifests to your VOD playout egress bucket.</li> 
<p>For more details, see the Delivery and Playback section later in this post.</p> 
<b>Camera source</b> 
<p>With the processing and origination infrastructure running, you need to configure your camera and encoder.</p> 
<p>As discussed, we chose to use a Ricoh Theta S camera and Open Broadcaster Software (OBS) to stitch and deliver a stream into the infrastructure. Ricoh provides a free ‘blender’ driver, which allows you to transform, stitch, encode, and deliver both transformed equirectangular (used for this post) video as well as spherical (two camera) video. The Theta provides an easy way to get capturing for under $300, and OBS is a free and open-source software application for capturing and live streaming on a budget. It is quick, cheap, and enjoys wide use by the gaming community. OBS lowers the barrier to getting started with immersive streaming.</p> 
<p>While the resolution and bit rate of the Theta may not be 4K, it still provides us with a way to test the functionality of the entire pipeline end to end, without having to invest in a more expensive camera rig. One could also use this type of model to target smaller events, which may involve mobile devices with smaller display profiles, such as phones and potentially smaller sized tablets.</p> 
<p>Looking for a more professional solution? Nokia, GoPro, Samsung, and many others have options ranging from $500 to $50,000. This solution is based around the Theta S capabilities, but we’d encourage you to extend it to meet your specific needs.</p> 
<p>If your device can support equirectangular RTMP, then it can deliver media through the reference architecture (dependent on instance sizing for higher bit rate sources, of course). If additional features are required such as camera stitching, mixing, or device bonding, we’d recommend exploring a commercial solution such as&nbsp;<a href="https://teradek.com/collections/sphere-family">Teradek Sphere</a>.</p> 
<table class=" aligncenter" style="height: 306px" width="874"> 
<tbody> 
<tr> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture5.png" /></a> 
<p class="wp-caption-text">Teradek Rig (Teradek)</p> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture6.png" /></a> 
<p class="wp-caption-text">Ricoh Theta (CNET)</p> 
</tr> 
</tbody> 
</table> 
<p>All cameras have varied PC connectivity support. We chose the Ricoh Theta S due to the real-time video connectivity that it provides through software drivers on macOS and PC. If you plan to purchase a camera to use with a PC, confirm that it supports real-time capabilities as a peripheral device.</p> 
<b>Encoding and publishing</b> 
<p>Now that you have a camera, encoder, and AWS stack running, you can finally publish a live stream.</p> 
<p>To start streaming with OBS, configure the source camera and set a publishing point. Use the RTMP application name&nbsp;<em>/live</em>&nbsp;on port 1935 to ingest into the primary origin’s Elastic IP address provided as the CloudFormation output:&nbsp;<em>primaryOriginElasticIp</em>.</p> 
<p>You also need to choose a stream name or stream key in OBS. You can use any stream name, but keep the naming short and lowercase, and use only alphanumeric characters. This avoids any parsing issues on client-side player frameworks. There’s no publish point protection in your deployment, so any stream key works with the default NGiNX-RTMP configuration. For more information about stream keys, publishing point security, and extending the NGiNX-RTMP module, see the&nbsp;<a href="https://github.com/arut/nginx-rtmp-module/wiki">NGiNX-RTMP Wiki</a>.</p> 
<p>You should end up with a configuration similar to the following:</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/obs_stream_settings.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/obs_stream_settings.png" /></a> 
<p class="wp-caption-text">OBS Stream Settings</p> 
<p>The Output settings dialog allows us to rescale the Video canvas and encode it for delivery to our AWS infrastructure. In the dialog below, we’ve set the Theta to encode at 5 Mbps in CBR mode using a preset optimized for low CPU utilization. We chose these settings in accordance with best practices for the stream pass-through at the origin for the initial incoming bit rate. You may notice that they largely match the FFMPEG encoding settings we use on the origin – namely constant bit rate, a single audio track, and x264 encoding with the ‘veryfast’ encoding profile.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture8.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture8.png" /></a> 
<p class="wp-caption-text">OBS Output Settings</p> 
<b>Live to On-Demand</b> 
<p>As you may have noticed, an on-demand component is included in the solution architecture. When talking to customers, one frequent request that we see is that they would like to record the incoming stream with as little effort as possible.</p> 
<p>NGINX-RTMP’s recording directives provide an easy way to accomplish this. We record any newly published stream on stream start at the primary or backup origins, using the incoming source stream, which also happens to be the highest bit rate. When the encoder stops broadcasting, NGINX-RTMP executes an exec_record_done script –&nbsp;<em>record-postprocess.sh</em>&nbsp;(described in the Configuration section earlier), which ensures that the content is well-formed, and then moves it to an S3 ingest bucket for processing.</p> 
<p>Transcoding of content to make it ready for VOD as adaptive bit rate is a multi-step pipeline. First, Spot Instances in the transcoding cluster periodically poll the SQS queue for new jobs. Items on the queue are pulled off on demand by processing instances, and transcoded via FFMPEG into adaptive bit rate HLS. This allows you to also extend FFMPEG using filters for cubic and other bitrate-optimizing 360-specific transforms. Finally, transcoded content is moved from the ingest bucket to an egress bucket, making them ready for playback via your CloudFront distribution.</p> 
<p>Separate ingest and egress by bucket to provide hard security boundaries between source recordings (which are highest quality and unencrypted), and destination derivatives (which may be lower quality and potentially require encryption). Bucket separation also allows you to order and archive input and output content using different taxonomies, which is common when moving content from an asset management and archival pipeline (the ingest bucket) to a consumer-facing playback pipeline (the egress bucket, and any other attached infrastructure or services, such as CMS, Mobile applications, and so forth).</p> 
<p>Because streams are pushed over the internet, there is always the chance that an interruption could occur in the network path, or even at the origin side of the equation (primary to backup roll-over). Both of these scenarios could result in malformed or partial recordings being created. For the best level of reliability, encoding should always be recorded locally on-site as a precaution to deal with potential stream interruptions.</p> 
<b>Delivery and playback</b> 
<p>With the camera turned on and OBS streaming to AWS, the final step is to play the live stream. We’ve primarily tested the prototype player on the latest Chrome and Firefox browsers on macOS, so your mileage may vary on different browsers or operating systems. For those looking to try the livestream on Google Cardboard, or similar headsets, native apps for iOS (VRPlayer) and Android exist that can play back HLS streams.</p> 
<p>The prototype player is hosted in an S3 bucket and can be found from the CloudFormation output&nbsp;<em>clientWebsiteUrl</em>. It requires a stream URL provided as a query parameter&nbsp;<em>?url=&lt;stream_url&gt;</em>&nbsp;to begin playback. This stream URL is determined by the RTMP stream configuration in OBS. For example, if OBS is publishing to&nbsp;<em>rtmp://x.x.x.x:1935/live/foo</em>,&nbsp;the resulting playback URL would be:</p> 
<p style="padding-left: 30px"><em>https://&lt;cloudFrontDistribution&gt;/hls/foo.m3u8</em></p> 
<p>The combined player URL and playback URL results in a path like this one:</p> 
<p style="padding-left: 30px"><em>https://&lt;clientWebsiteUrl&gt;/?url=https://&lt;cloudFrontDistribution&gt;/hls/foo.m3u8</em></p> 
<p>To assist in setup/debugging, we’ve provided a test source as part of the CloudFormation template. A color bar pattern with timecode and audio is being generated by FFmpeg running as an ECS task. Much like OBS, FFmpeg is streaming the test pattern to the primary origin over the RTMP protocol. The prototype player and test HLS stream can be accessed by opening the <em>clientTestPatternUrl</em> CloudFormation output link.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture9.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture9.png" /></a> 
<p class="wp-caption-text">Test Stream Playback</p> 
<b>What’s next?</b> 
<p>In this post, we walked you through the design and implementation of a full end-to-end immersive streaming solution architecture. As you may have noticed, there are a number of areas this could expand into, and we intend to do this in follow-up posts around the topic of virtual reality media workloads in the cloud. We’ve identified a number of topics such as load testing, content protection, client-side metrics and analytics, and CI/CD infrastructure for 24/7 live streams. If you have any requests, please drop us a line.</p> 
<p><em>We would like to extend extra-special thanks to Scott Malkie and Chad Neal for their help and contributions to this post and reference architecture.</em></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3568');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/integrating-on-premises-and-cloud-environments-1260x487.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Running ActiveMQ in a Hybrid Cloud Environment with Amazon MQ</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2018-02-19T21:47:11+00:00">19 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-mq/" title="View all posts in Amazon MQ*"><span property="articleSection">Amazon MQ*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-integration/" title="View all posts in Application Integration"><span property="articleSection">Application Integration</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/enterprise-strategy/" title="View all posts in Enterprise Strategy*"><span property="articleSection">Enterprise Strategy*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/" title="View all posts in Messaging*"><span property="articleSection">Messaging*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/migration/" title="View all posts in Migration*"><span property="articleSection">Migration*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/running-activemq-in-a-hybrid-cloud-environment-with-amazon-mq/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3946" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3946&amp;disqus_title=Running+ActiveMQ+in+a+Hybrid+Cloud+Environment+with+Amazon+MQ&amp;disqus_url=https://aws.amazon.com/blogs/compute/running-activemq-in-a-hybrid-cloud-environment-with-amazon-mq/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3946');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post courtesy of Greg Share, AWS Solutions Architect</p> 
<p>Many organizations, particularly enterprises, rely on message brokers to connect and coordinate different systems. Message brokers enable distributed applications to communicate with one another, serving as the technological backbone for their IT environment, and ultimately their business services. Applications depend on messaging to work.</p> 
<p>In many cases, those organizations have started to build new or “lift and shift” applications to AWS. In some cases, there are applications, such as mainframe systems, too costly to migrate. In these scenarios, those on-premises applications still need to interact with cloud-based components.</p> 
<p><a href="https://aws.amazon.com/amazon-mq/">Amazon MQ</a> is a managed message broker service for ActiveMQ that enables organizations to send messages between applications in the cloud and on-premises to enable hybrid environments and application modernization. For example, you can <a href="https://aws.amazon.com/blogs/compute/invoking-aws-lambda-from-amazon-mq/">invoke AWS Lambda from queues and topics managed by Amazon MQ brokers</a> to integrate legacy systems with serverless architectures. ActiveMQ is an open-source message broker written in Java that is packaged with clients in multiple languages, Java Message Server (JMS) client being one example.</p> 
<p>This post shows you can use Amazon MQ to integrate on-premises and cloud environments using the network of brokers feature of ActiveMQ. It provides configuration parameters for a one-way duplex connection for the flow of messages from an on-premises ActiveMQ message broker to Amazon MQ.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/integrating-on-premises-and-cloud-environments.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/integrating-on-premises-and-cloud-environments-1024x396.jpg" /></a></p> 
<b>ActiveMQ and the network of brokers</b> 
<p>First, look at queues within ActiveMQ and then at the network of brokers as a mechanism to distribute messages.</p> 
<p>The network of brokers behaves differently from models such as physical networks. The key consideration is that the production (sending) of a message is disconnected from the consumption of that message. Think of the delivery of a parcel: The parcel is sent by the supplier (producer) to the end customer (consumer). The path it took to get there is of little concern to the customer, as long as it receives the package.</p> 
<p>The same logic can be applied to the network of brokers. Here’s how you build the flow from a simple message to a queue and build toward a network of brokers. Before you look at setting up a hybrid connection, I discuss how a broker processes messages in a simple scenario.</p> 
<p>When a message is sent from a producer to a queue on a broker, the following steps occur:</p> 
<ol> 
<li>A message is sent to a queue from the producer.</li> 
<li>The broker persists this in its store or journal.</li> 
<li>At this point, an acknowledgement (ACK) is sent to the producer from the broker.<br /> <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-sends-ack.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-sends-ack-300x295.jpg" /></a></li> 
</ol> 
<p>When a consumer looks to consume the message from that same queue, the following steps occur:</p> 
<ol> 
<li>The message listener (consumer) calls the broker, which creates a subscription to the queue.</li> 
<li>Messages are fetched from the message store and sent to the consumer.</li> 
<li>The consumer acknowledges that the message has been received before processing it.</li> 
<li>Upon receiving the ACK, the broker sets the message as having been consumed. By default, this deletes it from the queue. 
<li>You can set the consumer to ACK after processing by setting up <strong>transaction</strong> <strong>management</strong> or handle it manually using&nbsp;<strong>Session.CLIENT_ACKNOWLEDGE</strong>.</li> 
</ul> </li> 
</ol> 
<b>Static propagation</b> 
<p>I now introduce the concept of static propagation with the network of brokers as the mechanism for message transfer from on-premises brokers to Amazon MQ. &nbsp;Static propagation refers to message propagation that occurs in the&nbsp;absence&nbsp;of subscription information. In this case, the objective is to transfer messages arriving at your selected on-premises broker to the Amazon MQ broker for consumption within the cloud environment.</p> 
<p>After you configure static propagation with a network of brokers, the following occurs:</p> 
<ol> 
<li>The on-premises broker receives a message from a producer for a specific queue.</li> 
<li>The on-premises broker sends (statically propagates) the message to the Amazon MQ broker.</li> 
<li>The Amazon MQ broker sends an acknowledgement to the on-premises broker, which marks the message as having been consumed.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-sends-ack2-sm.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-sends-ack2-sm.jpg" /></a></li> 
<li>Amazon MQ holds the message in its queue ready for consumption.</li> 
<li>A consumer connects to Amazon MQ broker, subscribes to the queue in which the message resides, and receives the message.</li> 
<li>Amazon MQ broker marks the message as having been consumed.</li> 
</ol> 
<b>Getting started</b> 
<p>The first step is creating an Amazon MQ broker.</p> 
<ol> 
<li>Sign in to the <a href="https://console.aws.amazon.com/amazon-mq/">Amazon MQ console</a> and launch a new Amazon MQ broker.</li> 
<li>Name your broker and choose <strong>Next step</strong>.</li> 
<li>For <strong>Broker instance type</strong>, choose your instance size:<br /> – <strong>mq.t2.micro</strong><br /> – <strong>mq.m4.large</strong></li> 
<li>For<strong> Deployment mode</strong>, enter one of the following:<br /> – <strong>Single-instance broker</strong> for development and test implementations (recommended)<br /> – <strong>Active/standby broker for high availability</strong> in production environments</li> 
<li>Scroll down and enter your user name and password.</li> 
<li>Expand <strong>Advanced Settings</strong>.</li> 
<li>For <strong>VPC</strong>, <strong>Subnet</strong>, and <strong>Security Group</strong>, pick the values for the resources in which your broker will reside.</li> 
<li>For <strong>Public Accessibility</strong>, choose <strong>Yes</strong>, as connectivity is internet-based. Another option would be to use private connectivity between your on-premises network and the VPC, an example being an AWS Direct Connect or VPN connection. In that case, you could set <strong>Public Accessibility</strong> to <strong>No</strong>.</li> 
<li>For <strong>Maintenance</strong>, leave the default value, <strong>No preference</strong>.</li> 
<li>Choose <strong>Create Broker</strong>. Wait several minutes for the broker to be created.<br /> <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-creation.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-creation.jpg" /></a></li> 
</ol> 
<p>After creation is complete, you see your broker listed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-listed.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/broker-listed.jpg" /></a></p> 
<p>For connectivity to work, you must configure the security group where Amazon MQ resides. For this post, I focus on the OpenWire protocol.</p> 
<p>For Openwire connectivity, allow port 61617 access for Amazon MQ from your on-premises <strong>ActiveMQ</strong> broker source IP address. For alternate protocols, see the Amazon MQ broker configuration information for the ports required:</p> 
<p>OpenWire – ssl://xxxxxxx.xxx.com:61617<br /> AMQP – amqp+ssl:// xxxxxxx.xxx.com:5671<br /> STOMP – stomp+ssl:// xxxxxxx.xxx.com:61614<br /> MQTT – mqtt+ssl:// xxxxxxx.xxx.com:8883<br /> WSS – wss:// xxxxxxx.xxx.com:61619</p> 
<b>Configuring the network of brokers</b> 
<p>Configuring the network of brokers with static propagation occurs on the on-premises broker by applying changes to the following file:<br /> &lt;activemq install directory&gt;/conf activemq.xml</p> 
<h3>Network connector</h3> 
<p>This is the first configuration item required to enable a network of brokers. It is only required on the on-premises broker, which initiates and creates the connection with Amazon MQ. This connection, after it’s established, enables the flow of messages in either direction between the on-premises broker and Amazon MQ. The focus of this post is the uni-directional flow of messages from the on-premises broker to Amazon MQ.</p> 
<p>The default activemq.xml file does not include the network connector configuration. Add this with the <strong>networkConnector</strong> element. In this scenario, edit the on-premises broker activemq.xml file to include the following information between &lt;systemUsage&gt; and &lt;transportConnectors&gt;:</p> 
<code class="lang-xml">&lt;networkConnectors&gt;
&lt;networkConnector 
name=<strong>&quot;Q:source broker name-&gt;target broker name&quot;</strong>
duplex=<strong>&quot;false&quot;</strong> 
uri=<strong>&quot;</strong>static<strong>:(ssl:// aws mq endpoint:61617)&quot;</strong> 
userName=<strong>&quot;username&quot;</strong>
password=<strong>&quot;password&quot;</strong> 
networkTTL=&quot;2&quot; 
dynamicOnly=&quot;false&quot;&gt;
&lt;staticallyIncludedDestinations&gt;
&lt;queue physicalName=<strong>&quot;queuename&quot;</strong>/&gt;
&lt;/staticallyIncludedDestinations&gt; 
&lt;excludedDestinations&gt;
&lt;queue physicalName=&quot;&gt;&quot; /&gt;
&lt;/excludedDestinations&gt;
&lt;/networkConnector&gt; 
&lt;networkConnectors&gt;</code> 
<p>The highlighted components are the most important elements when configuring your on-premises broker.</p> 
<li><strong>name</strong> – Name of the network bridge. In this case, it specifies two things: 
<li>That this connection relates to an ActiveMQ queue (Q) as opposed to a topic (T), for reference purposes.</li> 
<li>The source broker and target broker.</li> 
</ul> </li> 
<li><strong>duplex</strong> –Setting this to <strong>false</strong> ensures that messages traverse uni-directionally from the on-premises broker to Amazon MQ.</li> 
<li><strong>uri</strong> –Specifies the remote endpoint to which to connect for message transfer. In this case, it is an Openwire endpoint on your Amazon MQ broker. This information could be obtained from the Amazon MQ console or via the API.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/endpoints.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/endpoints.jpg" /></a></li> 
<li><strong>username and password</strong> – The same username and password configured when creating the Amazon MQ broker, and used to access the Amazon MQ ActiveMQ console.</li> 
<li><strong>networkTTL</strong> – Number of brokers in the network through which messages and subscriptions can pass. Leave this setting at the current value, if it is already included in your broker connection.</li> 
<li><strong>staticallyIncludedDestinations &gt; queue physicalName</strong> – The destination ActiveMQ queue for which messages are destined. This is the queue that is propagated from the on-premises broker to the Amazon MQ broker for message consumption.</li> 
<p>After the network connector is configured, you must restart the ActiveMQ service on the on-premises broker for the changes to be applied.</p> 
<b>Verify the configuration</b> 
<p>There are a number of places within the ActiveMQ console of your on-premises and Amazon MQ brokers to browse to verify that the configuration is correct and the connection has been established.</p> 
<h3>On-premises broker</h3> 
<p>Launch the ActiveMQ console of your on-premises broker and navigate to <strong>Network</strong>. You should see an active network bridge similar to the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/network-bridge.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/network-bridge.jpg" /></a></p> 
<p>This identifies that the connection between your on-premises broker and your Amazon MQ broker is up and running.</p> 
<p>Now navigate to <strong>Connections</strong> and scroll to the bottom of the page. Under the <strong>Network Connectors</strong> subsection, you should see a connector labeled with the <strong>name</strong>: value that you provided within the ActiveMQ.xml configuration file. You should see an entry similar to:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/connector.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/connector.jpg" /></a></p> 
<h3>Amazon MQ broker</h3> 
<p>Launch the ActiveMQ console of your Amazon MQ broker and navigate to<strong> Connections</strong>. Scroll to the <strong>Connections openwire</strong> subsection and you should see a connection specified that references the <strong>name: value</strong> that you provided within the ActiveMQ.xml configuration file. You should see an entry similar to:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/name-value.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/name-value.jpg" /></a></p> 
<p>If you configured the <strong>uri</strong>: for AMQP, STOMP, MQTT, or WSS as opposed to Openwire, you would see this connection under the corresponding section of the <strong>Connections</strong> page.</p> 
<b>Testing your message flow</b> 
<p>The setup described outlines a way for messages produced on premises to be propagated to the cloud for consumption in the cloud. This section provides steps on verifying the message flow.</p> 
<h3>Verify that the queue has been created</h3> 
<p>After you specify this queue name as <strong>staticallyIncludedDestinations &gt; queue physicalName</strong>: and your ActiveMQ service starts, you see the following on your on-premises ActiveMQ console <strong>Queues</strong> page.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/queues.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/queues.jpg" /></a></p> 
<p>As you can see, no messages have been sent but you have one consumer listed. If you then choose Active Consumers under the Views column, you see Active Consumers for TestingQ.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/testing-Q.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/testing-Q.jpg" /></a></p> 
<p>This is telling you that your Amazon MQ broker is a consumer of your on-premises broker for the testing queue.</p> 
<h3>Produce and send a message to the on-premises broker</h3> 
<p>Now, produce a message on an on-premises producer and send it to your on-premises broker to a queue named <strong>TestingQ</strong>. If you navigate back to the <strong>queues</strong> page of your on-premises ActiveMQ console, you see that the <strong>messages enqueued</strong> and <strong>messages dequeued</strong> column count for your <strong>TestingQ</strong> queue have changed:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/messages-enqueued.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/messages-enqueued.jpg" /></a></p> 
<p>What this means is that the message originating from the on-premises producer has traversed the on-premises broker and propagated immediately to the Amazon MQ broker. At this point, the message is no longer available for consumption from the on-premises broker.</p> 
<p>If you access the ActiveMQ console of your Amazon MQ broker and navigate to the <strong>Queues</strong> page, you see the following for the <strong>TestingQ</strong> queue:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/testing-Q-2.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/testing-Q-2.jpg" /></a></p> 
<p>This means that the message originally sent to your on-premises broker has traversed the network of brokers unidirectional network bridge, and is ready to be consumed from your Amazon MQ broker. The indicator is the <strong>Number of Pending Messages</strong> column.</p> 
<h3>Consume the message from an Amazon MQ broker</h3> 
<p>Connect to the Amazon MQ <strong>TestingQ</strong> queue from a consumer within the AWS Cloud environment for message consumption. Log on to the ActiveMQ console of your Amazon MQ broker and navigate to the <strong>Queue</strong> page:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/queues-2.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/queues-2.jpg" /></a></p> 
<p>As you can see, the <strong>Number of Pending Messages</strong> column figure has changed to 0 as that message has been consumed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/message-lifecycle.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/20/message-lifecycle-1024x518.jpg" /></a></p> 
<p>This diagram outlines the message lifecycle from the on-premises producer to the on-premises broker, traversing the hybrid connection between the on-premises broker and Amazon MQ, and finally consumption within the AWS Cloud.</p> 
<b>Conclusion</b> 
<p>This post focused on an ActiveMQ-specific scenario for transferring messages within an ActiveMQ queue from an on-premises broker to Amazon MQ.</p> 
<p>For other on-premises brokers, such as IBM MQ, another approach would be to run ActiveMQ on-premises broker and use JMS bridging to IBM MQ, while using the approach in this post to forward to Amazon MQ. Yet another approach would be to use Apache Camel for more sophisticated routing.</p> 
<p>I hope that you have found this example of hybrid messaging between an on-premises environment in the AWS Cloud to be useful. Many customers are already using on-premises ActiveMQ brokers, and this is a great use case to enable hybrid cloud scenarios.</p> 
<p>To learn more, see the <a href="https://aws.amazon.com/amazon-mq/">Amazon MQ</a> website and <a href="https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/welcome.html">Developer Guide</a>. You can <a href="https://console.aws.amazon.com/amazon-mq/home">try Amazon MQ for free</a> with the AWS Free Tier, which includes up to 750 hours of a single-instance mq.t2.micro broker and up to 1 GB of storage per month for one year.</p> 
<p>&nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3946');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/7andahalf.png" /> 
<b class="lb-b blog-post-title" property="name headline">Applying the Twelve-Factor App Methodology to Serverless Applications</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2018-02-12T10:30:26+00:00">12 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-x-ray/" title="View all posts in AWS X-Ray*"><span property="articleSection">AWS X-Ray*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/" title="View all posts in Developer Tools*"><span property="articleSection">Developer Tools*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/applying-the-twelve-factor-app-methodology-to-serverless-applications/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3904" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3904&amp;disqus_title=Applying+the+Twelve-Factor+App+Methodology+to+Serverless+Applications&amp;disqus_url=https://aws.amazon.com/blogs/compute/applying-the-twelve-factor-app-methodology-to-serverless-applications/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3904');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>The <a href="https://12factor.net/">Twelve-Factor App</a> methodology is twelve best practices for building modern, cloud-native applications. With guidance on things like configuration, deployment, runtime, and multiple service communication, the Twelve-Factor model prescribes best practices that apply to a diverse number of use cases, from web applications and APIs to data processing applications. Although serverless computing and <a href="https://aws.amazon.com/lambda">AWS Lambda</a> have changed how application development is done, the Twelve-Factor best practices remain relevant and applicable in a serverless world.</p> 
<p>In this post, I directly apply and compare the Twelve-Factor methodology to serverless application development with Lambda and <a href="https://aws.amazon.com/api-gateway">Amazon API Gateway</a>.<span id="more-3904"></span></p> 
<b>The Twelve Factors</b> 
<p>As you’ll see, many of these factors are not only directly applicable to serverless applications, but in fact a default mechanism or capability of the AWS serverless platform. Other factors don’t fit, and I talk about how these factors may not apply at all in a serverless approach.</p> 
<table> 
<tbody> 
<tr> 
<td>I.</td> 
<td><a title="undefined" href="#codebase" target="null">Codebase</a></td> 
</tr> 
<tr> 
<td>II.</td> 
<td><a title="undefined" href="#dependencies" target="null">Dependencies</a></td> 
</tr> 
<tr> 
<td>III.</td> 
<td><a title="undefined" href="#config" target="null">Config</a></td> 
</tr> 
<tr> 
<td>IV.</td> 
<td><a title="undefined" href="#backingservices" target="null">Backing services</a></td> 
</tr> 
<tr> 
<td>V.</td> 
<td><a title="undefined" href="#buildreleaserun" target="null">Build, release, run</a></td> 
</tr> 
<tr> 
<td>VI.</td> 
<td><a title="undefined" href="#processes" target="null">Processes</a></td> 
</tr> 
<tr> 
<td>VII.</td> 
<td><a title="undefined" href="#portbinding" target="null">Port binding</a></td> 
</tr> 
<tr> 
<td>VIII.</td> 
<td><a title="undefined" href="#concurrency" target="null">Concurrency</a></td> 
</tr> 
<tr> 
<td>IX.</td> 
<td><a title="undefined" href="#disposability" target="null">Disposability</a></td> 
</tr> 
<tr> 
<td>X.</td> 
<td><a title="undefined" href="#devprodparity" target="null">Dev/prod parity</a></td> 
</tr> 
<tr> 
<td>XI.</td> 
<td><a title="undefined" href="#logs" target="null">Logs</a></td> 
</tr> 
<tr> 
<td>XII.</td> 
<td><a title="undefined" href="#admin" target="null">Admin processes</a></td> 
</tr> 
</tbody> 
</table> 
<h3 id="codebase">I. Codebase</h3> 
<p><a href="https://12factor.net/codebase"><em>One codebase tracked in revision control, many deploys</em></a></p> 
<p>A general software development best practice is to have all of your code in revision control. This is no different with serverless applications.</p> 
<p>For a single serverless application, your code should be stored in a single repository in which a single deployable artifact is generated and from which it is deployed. This single code base should also represent the code used in all of your application environments (development, staging, production, etc.). What might be different for serverless applications is the bounds for what constitutes a “single application.”</p> 
<p>Here are two guidelines to help you understand the scope of an application:</p> 
<ol> 
<li>If events are shared (such as a common Amazon API Gateway API), then the Lambda function code for those events should be put in the same repository.</li> 
<li>Otherwise, break functions along event sources into their own repositories.</li> 
</ol> 
<p>Following these two guidelines helps you keep your serverless applications scoped to a single purpose and help prevent complexity in your code base.</p> 
<h3 id="dependencies">II. Dependencies</h3> 
<p><a href="https://12factor.net/dependencies"><em>Explicitly declare and isolate dependencies</em></a></p> 
<p>Code that needs to be used by multiple functions should be packaged into its own library and included inside your&nbsp;<a href="https://docs.aws.amazon.com/lambda/latest/dg/deployment-package-v2.html">deployment package</a>. Going back to the previous factor on codebase, if you find that you need to often include special processing or business logic, the best solution may be to try to create a purposeful library yourself. Every language that Lambda supports has a model for dependencies/libraries, which you can use:</p> 
<li>Node.js: npm (<a href="https://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html">docs</a>)</li> 
<li>Python: pip (<a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html">docs</a>)</li> 
<li>Java: maven (<a href="https://docs.aws.amazon.com/lambda/latest/dg/java-create-jar-pkg-maven-no-ide.html">docs</a>)</li> 
<li>C#: NuGet (<a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-dotnet-coreclr-deployment-package.html">docs</a>)</li> 
<li>Go: Go get packages (<a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-go-how-to-create-deployment-package.html">docs</a>)</li> 
<p>You’ll want to automate this with a CI/CD process, which I talk about later in this post.</p> 
<h3 id="config">III. Config</h3> 
<p><a href="https://12factor.net/config"><em>Store config in the environment</em></a></p> 
<p>Both Lambda and API Gateway allow you to set configuration information, using the environment in which each service runs.</p> 
<p>In Lambda, these are called <a href="https://docs.aws.amazon.com/lambda/latest/dg/env_variables.html">environment variables</a> and are key-value pairs that can be set at deployment or when updating the function configuration. Lambda then makes these key-value pairs available to your Lambda function code using standard APIs supported by the language, like&nbsp;process.env&nbsp;for Node.js functions. For more information, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/programming-model-v2.html">Programming Model</a>, which contains examples for each supported language.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/Screen-Shot-2018-02-12-at-1.00.53-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/Screen-Shot-2018-02-12-at-1.00.53-PM.png" /></a></p> 
<p>Lambda also allows you to encrypt these key-value pairs using KMS, such that they can be used to store secrets such as API keys or passwords for databases. You can also use them to help define application environment specifics, such as differences between testing or production environments where you might have unique databases or endpoints with which your Lambda function needs to interface. You could also use these for setting A/B testing flags or to enable or disable certain function logic.</p> 
<p>For API Gateway, these configuration variables are called <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html">stage variables</a>. Like environment variables in Lambda, these are key-value pairs that are available for API Gateway to consume or pass to your API’s backend service. Stage variables can be useful to send requests to different backend environments based on the URL from which your API is accessed. For example, a single configuration could support both beta.yourapi.com vs. prod.yourapi.com. You could also use stage variables to pass information to a Lambda function that causes it to perform different logic.</p> 
<h3 id="backingservices">IV. Backing Services</h3> 
<p><a href="https://12factor.net/backing-services"><em>Treat backing services as attached resources</em></a></p> 
<p>Because Lambda doesn’t allow you to run another service as part of your function execution, this factor is basically the default model for Lambda. Typically, you reference any database or data store as an external resource via HTTP endpoint or DNS name. These connection strings are ideally passed in via the configuration information, as previously covered.</p> 
<h3 id="buildreleaserun">V.&nbsp;Build, release, run</h3> 
<p><a href="https://12factor.net/build-release-run"><em>Strictly separate build and run stages</em></a></p> 
<p>The separation of build, release, and run stages follows the development best practices of continuous integration and delivery. AWS recommends that you have a CI &amp;CD process no matter what type of application you are building. For serverless applications, this is no different. For more information, see the <a href="https://youtu.be/dCDZ7HR7dms">Building CI/CD Pipelines for Serverless Applications (SRV302)</a> re:Invent 2017 session.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/Screen-Shot-2018-02-12-at-12.41.42-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/Screen-Shot-2018-02-12-at-12.41.42-PM.png" /></a> 
<p class="wp-caption-text">An example minimal pipeline (from presentation linked above)</p> 
<h3 id="processes">VI.&nbsp;Process</h3> 
<p><a href="https://12factor.net/processes"><em>Execute the app as one or more stateless processes</em></a></p> 
<p>This is inherent in how Lambda is designed so there is nothing more to consider. Lambda functions should always be treated as being stateless, despite the ability to potentially store some information locally between execution environment re-use. This is because there is no guaranteed affinity to any execution environment, and the potential for an execution environment to go away between invocations exists. You should always store any stateful information in a database, cache, or separate data store via a backing service.</p> 
<h3 id="portbinding">VII.&nbsp;Port Binding</h3> 
<p><a href="https://12factor.net/port-binding"><em>Export services via port binding</em></a></p> 
<p>This factor also does not apply to Lambda, as execution environments do not expose any direct networking to your functions. Instead of a port, Lambda functions are invoked via one or more triggering services or AWS APIs for Lambda. There are currently three different invocation models:</p> 
<li>Synchronous</li> 
<li>Asynchronous</li> 
<li>Stream-based</li> 
<p>Each has unique characteristics. For more information, see<a href="https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-functions.html">&nbsp;Invoking Lambda Functions.</a><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/12-factor-execution-models.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/12-factor-execution-models.png" /></a></p> 
<h3 id="concurrency">VIII. Concurrency</h3> 
<p><a href="https://12factor.net/concurrency"><em>Scale out via the process model</em></a></p> 
<p>Lambda was built with massive concurrency and scale in mind. A recent post on this blog, titled <a href="https://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/">Managing AWS Lambda Function Concurrency</a> explained that for a serverless application, “the unit of scale is a&nbsp;<a href="https://docs.aws.amazon.com/lambda/latest/dg/concurrent-executions.html">concurrent execution</a>” and that these are consumed by your functions.</p> 
<p>Lambda automatically scales to meet the demands of invocations sent at your function. This is in contrast to a traditional compute model using physical hosts, virtual machines, or containers that you self-manage. With Lambda, you do not need to manage overall capacity or apply scaling policies.</p> 
<p>Each AWS account has an overall&nbsp;<a href="https://docs.aws.amazon.com/lambda/latest/dg/API_GetAccountSettings.html">AccountLimit</a>&nbsp;value that is fixed at any point in time, but can be easily increased as needed.&nbsp;<a href="https://aws.amazon.com/about-aws/whats-new/2017/05/aws-lambda-raises-default-concurrent-execution-limit/">As of May 2017</a>, the default limit is 1000 concurrent executions per AWS Region. You can also set and manage a reserved concurrency limit, which provides a limit to how much concurrency a function can have. It also reserves concurrency capacity for a given function out of the total available for an account.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/concurrency-reservation.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/concurrency-reservation.png" /></a></p> 
<h3 id="disposability">IX. Disposability</h3> 
<p><a href="https://12factor.net/disposability"><em>Maximize robustness with fast startup and graceful shutdown</em></a></p> 
<p>Shutdown doesn’t apply to Lambda because Lambda is intrinsically event-driven. Invocations are tied directly to incoming events or triggers.</p> 
<p>However, speed at startup does matter. Initial function execution latency, or what is called “cold starts”, can occur when there isn’t a “warmed” compute resource ready to execute against your application invocations. In the <a href="https://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html">AWS Lambda Execution Model</a> topic, it explains that:</p> 
<p><em>“It takes time to set up an execution context and do the necessary “bootstrapping”, which adds some latency each time the Lambda function is invoked. You typically see this latency when a Lambda function is invoked for the first time or after it has been updated because AWS Lambda tries to reuse the execution context for subsequent invocations of the Lambda function.”</em></p> 
<p>The <a href="https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html">Best Practices</a> topic covers a number of issues around how to think about performance of your functions. This includes where to place certain logic, how to re-use execution environments, and how by configuring your function for more memory you also get a proportional increase in CPU available to your function. With <a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-services-lambda.html">AWS X-Ray</a>, you can gather some insight as to what your function is doing during an execution and make adjustments accordingly.</p> 
<h3 id="devprodparity">X. Dev/prod parity</h3> 
<p><a href="https://12factor.net/dev-prod-parity"><em>Keep development, staging, and production as similar as possible</em></a></p> 
<p>Along with continuous integration and delivery, the practice of having independent application environments is a solid best practice no matter the development approach. Being able to safely test applications in a non-production environment is key to development success. Products within the AWS <a href="https://aws.amazon.com/serverless/">Serverless Platform</a> do not charge for idle time, which greatly reduces the cost of running multiple environments. You can also use the&nbsp;<a href="https://github.com/awslabs/serverless-application-model">AWS Serverless Application Model</a> (AWS SAM), to manage the configuration of your separate environments.</p> 
<p>SAM allows you to model your serverless applications in greatly simplified <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> syntax. With SAM, you can use CloudFormation’s capabilities—such as <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html">Parameters</a> and <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html">Mappings</a>—to build dynamic templates. Along with Lambda’s environment variables and API Gateway’s stage variables, those templates give you the ability to deploy multiple environments from a single template, such as testing, staging, and production. Whenever the non-production environments are not in use, your costs for Lambda and API Gateway would be zero. For more information, see the <a href="https://www.youtube.com/watch?v=1k3XqBA2hYM">AWS Lambda Applications with AWS Serverless Application Model</a> 2017 AWS online tech talk.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/Screen-Shot-2018-02-12-at-12.54.41-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/Screen-Shot-2018-02-12-at-12.54.41-PM.png" /></a></p> 
<h3 id="logs">XI.&nbsp;Logs</h3> 
<p><a href="https://12factor.net/logs"><em>Treat logs as event streams</em></a></p> 
<p>In a typical non-serverless application environment, you might be concerned with log files, logging daemons, and centralization of the data represented in them. Thankfully, this is not a concern for serverless applications, as most of the services in the platform handle this for you.</p> 
<p>With Lambda, you can just output logs to the console via the native capabilities of the language in which your function is written. For more information about Go, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/go-programming-model-logging.html">Logging (Go)</a>. Similar&nbsp;documentation pages exist for other languages. Those messages output by your code are captured and centralized in <a href="https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-logs.html">Amazon CloudWatch Logs</a>. For more information, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-logs.html">Accessing Amazon CloudWatch Logs for AWS Lambda</a>.</p> 
<p>API Gateway provides two different methods for getting log information:</p> 
<li><strong>Execution logs</strong><br /> Includes errors or execution traces (such as request or response parameter values or payloads), data used by custom authorizers, whether API keys are required, whether usage plans are enabled, and so on.</li> 
<li><strong>Access logs</strong><br /> Provide the ability to log who has accessed your API and how the caller accessed the API. You can even customize the format of these logs as desired.</li> 
<p>Both are also made available to you in CloudWatch Logs. For more information, see <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html">Setting Up API Logging in API Gateway</a>.</p> 
<p>Capturing logs and being able to search and view them is one thing, but CloudWatch Logs also gives you the ability to treat a log message as an event and take action on them via <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html">subscription filters</a> in the service. With subscription filters, you could send a log message matching a certain pattern to a Lambda function, and have it take action based on that. Say, for example, that you want to respond to certain error messages or usage patterns that violate certain rules. You could do that with CloudWatch Logs, subscription filters, and Lambda. Another important capability of CloudWatch Logs is the ability to <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html">“pivot” log information into a metric</a> in CloudWatch. With this, you could take a data point from a log entry, create a metric, and then an alarm on a metric to show a breached threshold.</p> 
<h3 id="admin">XII.&nbsp;Admin Processes</h3> 
<p><a href="https://12factor.net/admin-processes"><em>Run admin/management tasks as one-off processes</em></a></p> 
<p>This is another factor that doesn’t directly apply to Lambda due to its design. Typically, you would have your functions scoped down to single or limited use cases and have individual functions for different components of your application. Even if they share a common invoking resource, such as an API Gateway endpoint and stage, you would still separate the individual API resources and actions to their own Lambda functions.</p> 
<b>The Seven-and-a-Half–Factor model and you</b> 
<p>As we’ve seen, Twelve-Factor application design can still be applied to serverless applications, taking into account some small differences! The following diagram highlights the factors and how applicable or not they are to serverless applications:</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/7andahalf.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/12/7andahalf.png" /></a> 
<p class="wp-caption-text">NOTE: Disposability only half applies, as discussed in this post.</p> 
<b>Conclusion</b> 
<p>If you’ve been building applications for cloud infrastructure over the past few years, the Twelve-Factor methodology should seem familiar and straight-forward. If you are new to this space, however, you should know that the general best practices and default working patterns for serverless applications overlap heavily with what I’ve discussed here.</p> 
<p>It shouldn’t require much work to adhere rather closely to the Twelve-Factor model. When you’re building serverless applications, following the applicable points listed here helps you simplify development and any operational work involved (though already minimized by services such as Lambda and API Gateway). The Twelve-Factor methodology also isn’t all-or-nothing. You can apply only the practices that work best for you and your applications, and still benefit.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3904');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store3.png" /> 
<b class="lb-b blog-post-title" property="name headline">Sharing Secrets with AWS Lambda Using AWS Systems Manager Parameter Store</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2018-02-09T13:45:54+00:00">09 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/aws-identity-and-access-management-iam/" title="View all posts in AWS Identity and Access Management (IAM)*"><span property="articleSection">AWS Identity and Access Management (IAM)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-x-ray/" title="View all posts in AWS X-Ray*"><span property="articleSection">AWS X-Ray*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/security/" title="View all posts in Security"><span property="articleSection">Security</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3862" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3862&amp;disqus_title=Sharing+Secrets+with+AWS+Lambda+Using+AWS+Systems+Manager+Parameter+Store&amp;disqus_url=https://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3862');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Roberto Iturralde,&nbsp;Sr. Application Developer- AWS Professional Services</em></p> 
<p>Application architects are faced with key decisions throughout the process of designing and implementing their systems. One decision common to nearly all solutions is how to manage the storage and access rights of application configuration. Shared configuration should be stored centrally and securely with each system component having access only to the properties that it needs for functioning.</p> 
<p>With <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html">AWS Systems Manager Parameter Store</a>, developers have access to central, secure, durable, and highly available storage for application configuration and secrets. Parameter Store also integrates with <a href="https://aws.amazon.com/iam">AWS Identity and Access Management</a> (IAM), allowing fine-grained access control to individual parameters or branches of a hierarchical tree.</p> 
<p>This post demonstrates how to create and access shared configurations in Parameter Store from <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>. Both encrypted and plaintext parameter values are stored with only the Lambda function having permissions to decrypt the secrets. You also use <a href="https://aws.amazon.com/xray/">AWS X-Ray</a> to profile the function.<br /> <span id="more-3862"></span></p> 
<b>Solution overview</b> 
<p>This example is made up of the following components:</p> 
<li>An <a href="https://github.com/awslabs/serverless-application-model">AWS SAM</a> template that defines: 
<li>A Lambda function and its permissions</li> 
<li>An unencrypted Parameter Store parameter that the Lambda function loads</li> 
<li>A KMS key that only the Lambda function can access. You use this key to create an encrypted parameter later.</li> 
</ul> </li> 
<li>Lambda function code in Python 3.6 that demonstrates how to load values from Parameter Store at function initialization for reuse across invocations.</li> 
<h3>Launch the AWS SAM template</h3> 
<p>To create the resources shown in this post, you can download the <a href="https://s3.amazonaws.com/computeblog-us-east-1/lambda-parameter-store/sam-template.yaml">SAM template</a> or choose the button to launch the stack.&nbsp;The template requires one parameter, an IAM user name, which is the name of the IAM user to be the admin of the KMS key that you create.&nbsp;In order to perform the steps listed in this post, this IAM user will need permissions to execute Lambda functions, create Parameter Store parameters, administer keys in KMS, and view the X-Ray console. If you have these privileges in your IAM user account you can use your own account to complete the walkthrough. You can not use the root user to administer the KMS keys.</p> 
<a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=computeblogexample&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/lambda-parameter-store/sam-template.yaml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/cloudformation-launch-stack.png" /></a> 
<h3>SAM template resources</h3> 
<p>The following sections show the code for the resources defined in the template.<br /> <em>Lambda function</em></p> 
<code class="lang-yaml">ParameterStoreBlogFunctionDev:
Type: 'AWS::Serverless::Function'
Properties:
FunctionName: 'ParameterStoreBlogFunctionDev'
Description: 'Integrating lambda with Parameter Store'
Handler: 'lambda_function.lambda_handler'
Role: !GetAtt ParameterStoreBlogFunctionRoleDev.Arn
CodeUri: './code'
Environment:
Variables:
ENV: 'dev'
APP_CONFIG_PATH: 'parameterStoreBlog'
AWS_XRAY_TRACING_NAME: 'ParameterStoreBlogFunctionDev'
Runtime: 'python3.6'
Timeout: 5
Tracing: 'Active'
ParameterStoreBlogFunctionRoleDev:
Type: AWS::IAM::Role
Properties:
AssumeRolePolicyDocument:
Version: '2012-10-17'
Statement:
-
Effect: Allow
Principal:
Service:
- 'lambda.amazonaws.com'
Action:
- 'sts:AssumeRole'
ManagedPolicyArns:
- 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
Policies:
-
PolicyName: 'ParameterStoreBlogDevParameterAccess'
PolicyDocument:
Version: '2012-10-17'
Statement:
-
Effect: Allow
Action:
- 'ssm:GetParameter*'
Resource: !Sub 'arn:aws:ssm:${AWS::Region}:${AWS::AccountId}:parameter/dev/parameterStoreBlog*'
-
PolicyName: 'ParameterStoreBlogDevXRayAccess'
PolicyDocument:
Version: '2012-10-17'
Statement:
-
Effect: Allow
Action:
- 'xray:PutTraceSegments'
- 'xray:PutTelemetryRecords'
Resource: '*'</code> 
<p>In this YAML code, you define a Lambda function named ParameterStoreBlogFunctionDev using the SAM AWS::Serverless::Function type. The environment variables for this function include the ENV (dev) and the APP_CONFIG_PATH where you find the configuration for this app in Parameter Store. X-Ray tracing is also enabled for profiling later.</p> 
<p>The IAM role for this function extends the AWSLambdaBasicExecutionRole by adding IAM policies that grant the function permissions to write to X-Ray and get parameters from Parameter Store, limited to paths under /dev/parameterStoreBlog*.<br /> <em>Parameter Store parameter</em></p> 
<code class="lang-yaml">SimpleParameter:
Type: AWS::SSM::Parameter
Properties:
Name: '/dev/parameterStoreBlog/appConfig'
Description: 'Sample dev config values for my app'
Type: String
Value: '{&quot;key1&quot;: &quot;value1&quot;,&quot;key2&quot;: &quot;value2&quot;,&quot;key3&quot;: &quot;value3&quot;}'</code> 
<p>This YAML code creates a plaintext string parameter in Parameter Store in a path that your Lambda function can access.<br /> <em>KMS encryption key</em></p> 
<code class="lang-yaml">ParameterStoreBlogDevEncryptionKeyAlias:
Type: AWS::KMS::Alias
Properties:
AliasName: 'alias/ParameterStoreBlogKeyDev'
TargetKeyId: !Ref ParameterStoreBlogDevEncryptionKey
ParameterStoreBlogDevEncryptionKey:
Type: AWS::KMS::Key
Properties:
Description: 'Encryption key for secret config values for the Parameter Store blog post'
Enabled: True
EnableKeyRotation: False
KeyPolicy:
Version: '2012-10-17'
Id: 'key-default-1'
Statement:
-
Sid: 'Allow administration of the key &amp; encryption of new values'
Effect: Allow
Principal:
AWS:
- !Sub 'arn:aws:iam::${AWS::AccountId}:user/${IAMUsername}'
Action:
- 'kms:Create*'
- 'kms:Encrypt'
- 'kms:Describe*'
- 'kms:Enable*'
- 'kms:List*'
- 'kms:Put*'
- 'kms:Update*'
- 'kms:Revoke*'
- 'kms:Disable*'
- 'kms:Get*'
- 'kms:Delete*'
- 'kms:ScheduleKeyDeletion'
- 'kms:CancelKeyDeletion'
Resource: '*'
-
Sid: 'Allow use of the key'
Effect: Allow
Principal:
AWS: !GetAtt ParameterStoreBlogFunctionRoleDev.Arn
Action:
- 'kms:Encrypt'
- 'kms:Decrypt'
- 'kms:ReEncrypt*'
- 'kms:GenerateDataKey*'
- 'kms:DescribeKey'
Resource: '*'</code> 
<p>This YAML code creates an encryption key with a key policy with two statements.</p> 
<p>The first statement allows a given user (${IAMUsername}) to administer the key. Importantly, this includes the ability to encrypt values using this key and disable or delete this key, but does not allow the administrator to decrypt values that were encrypted with this key.</p> 
<p>The second statement grants your Lambda function permission to encrypt and decrypt values using this key. The alias for this key in KMS is ParameterStoreBlogKeyDev, which is how you reference it later.</p> 
<h3>Lambda function</h3> 
<p>Here I walk you through the Lambda function code.</p> 
<code class="lang-python">import os, traceback, json, configparser, boto3
from aws_xray_sdk.core import patch_all
patch_all()
# Initialize boto3 client at global scope for connection reuse
client = boto3.client('ssm')
env = os.environ['ENV']
app_config_path = os.environ['APP_CONFIG_PATH']
full_config_path = '/' + env + '/' + app_config_path
# Initialize app at global scope for reuse across invocations
app = None
class MyApp:
def __init__(self, config):
&quot;&quot;&quot;
Construct new MyApp with configuration
:param config: application configuration
&quot;&quot;&quot;
self.config = config
def get_config(self):
return self.config
def load_config(ssm_parameter_path):
&quot;&quot;&quot;
Load configparser from config stored in SSM Parameter Store
:param ssm_parameter_path: Path to app config in SSM Parameter Store
:return: ConfigParser holding loaded config
&quot;&quot;&quot;
configuration = configparser.ConfigParser()
try:
# Get all parameters for this app
param_details = client.get_parameters_by_path(
Path=ssm_parameter_path,
Recursive=False,
WithDecryption=True
)
# Loop through the returned parameters and populate the ConfigParser
if 'Parameters' in param_details and len(param_details.get('Parameters')) &gt; 0:
for param in param_details.get('Parameters'):
param_path_array = param.get('Name').split(&quot;/&quot;)
section_position = len(param_path_array) - 1
section_name = param_path_array[section_position]
config_values = json.loads(param.get('Value'))
config_dict = {section_name: config_values}
print(&quot;Found configuration: &quot; + str(config_dict))
configuration.read_dict(config_dict)
except:
print(&quot;Encountered an error loading config from SSM.&quot;)
traceback.print_exc()
finally:
return configuration
def lambda_handler(event, context):
global app
# Initialize app if it doesn't yet exist
if app is None:
print(&quot;Loading config and creating new MyApp...&quot;)
config = load_config(full_config_path)
app = MyApp(config)
return &quot;MyApp config is &quot; + str(app.get_config()._sections)</code> 
<p>Beneath the import statements, you import the <a href="https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-python-patching.html">patch_all</a> function from the AWS X-Ray library, which you use to patch boto3 to create X-Ray segments for all your boto3 operations.</p> 
<p>Next, you create a boto3 SSM client at the global scope for reuse across function invocations, following <a href="http://docs.aws.amazon.com/lambda/latest/dg/best-practices.html">Lambda best practices</a>. Using the function environment variables, you assemble the path where you expect to find your configuration in Parameter Store. The class MyApp is meant to serve as an example of an application that would need its configuration injected at construction. In this example, you create an instance of ConfigParser, a class in Python’s standard library for handling basic configurations, to give to MyApp.</p> 
<p>The <em>load_config</em> function loads the all the parameters from Parameter Store at the level immediately beneath the path provided in the Lambda function environment variables. Each parameter found is put into a new section in ConfigParser. The name of the section is the name of the parameter, less the base path. In this example, the full parameter name is /dev/parameterStoreBlog/appConfig, which is put in a section named appConfig.</p> 
<p>Finally, the <em>lambda_handler</em> function initializes an instance of MyApp if it doesn’t already exist, constructing it with the loaded configuration from Parameter Store. Then it simply returns the currently loaded configuration in MyApp. The impact of this design is that the configuration is only loaded from Parameter Store the first time that the Lambda function execution environment is initialized. Subsequent invocations reuse the existing instance of MyApp, resulting in improved performance. You see this in the X-Ray traces later in this post. For more advanced use cases where configuration changes need to be received immediately, you could implement an expiry policy for your configuration entries or push notifications to your function.</p> 
<p>To confirm that everything was created successfully, test the function in the Lambda console.</p> 
<ol> 
<li>Open the <a href="https://console.aws.amazon.com/lambda">Lambda console</a>.</li> 
<li>In the navigation pane, choose <strong>Functions</strong>.</li> 
<li>In the <strong>Functions</strong> pane, filter to <strong>ParameterStoreBlogFunctionDev</strong> to find the function created by the SAM template earlier. Open the function name to view its details.</li> 
<li>On the top right of the function detail page, choose <strong>Test</strong>. You may need to create a new test event. The input JSON doesn’t matter as this function ignores the input.</li> 
</ol> 
<p>After running the test, you should see output similar to the following. This demonstrates that the function successfully fetched the unencrypted configuration from Parameter Store.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store1.png" /></a></p> 
<h3>Create an encrypted parameter</h3> 
<p>You currently have a simple, unencrypted parameter and a Lambda function that can access it.</p> 
<p>Next, you create an encrypted parameter that only your Lambda function has permission to use for decryption. This limits read access for this parameter to only this Lambda function.</p> 
<p>To follow along with this section, deploy the SAM template for this post in your account and make your IAM user name the KMS key admin mentioned earlier.</p> 
<ol> 
<li>In the <a href="https://console.aws.amazon.com/systems-manager">Systems Manager console</a>, under <strong>Shared Resources</strong>, choose <strong>Parameter Store</strong>.</li> 
<li>Choose <strong>Create Parameter</strong>. 
<li>For <strong>Name</strong>, enter <em>/dev/parameterStoreBlog/appSecrets</em>.</li> 
<li>For <strong>Type</strong>, select <strong>Secure String</strong>.</li> 
<li>For <strong>KMS Key ID</strong>, choose <em>alias/ParameterStoreBlogKeyDev</em>, which is the key that your SAM template created.</li> 
<li>For <strong>Value</strong>, enter <code>{&quot;secretKey&quot;: &quot;secretValue&quot;}</code>.</li> 
<li>Choose <strong>Create Parameter</strong>.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store2.png" /></a></li> 
</ul> </li> 
<li>If you now try to view the value of this parameter by choosing the name of the parameter in the parameters list and then choosing <strong>Show</strong> next to the <strong>Value</strong> field, you won’t see the value appear. This is because, even though you have permission to encrypt values using this KMS key, you do not have permissions to decrypt values.</li> 
<li>In the Lambda console, run another test of your function. You now also see the secret parameter that you created and its decrypted value.</li> 
</ol> 
<p>If you do not see the new parameter in the Lambda output, this may be because the Lambda execution environment is still warm from the previous test. Because the parameters are loaded at Lambda startup, you need a fresh execution environment to refresh the values.</p> 
<p>Adjust the function timeout to a different value in the <strong>Advanced Setting</strong>s at the bottom of the <strong>Lambda Configuration</strong> tab. Choose <strong>Save and test</strong> to trigger the creation of a new Lambda execution environment.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store3.png" /></a></p> 
<h3>Profiling the impact of querying Parameter Store using AWS X-Ray</h3> 
<p>By using the AWS X-Ray SDK to patch boto3 in your Lambda function code, each invocation of the function creates traces in X-Ray. In this example, you can use these traces to validate the performance impact of your design decision to only load configuration from Parameter Store on the first invocation of the function in a new execution environment.</p> 
<p>From the Lambda function details page where you tested the function earlier, under the function name, choose <strong>Monitoring</strong>. Choose <strong>View traces in X-Ray</strong>.</p> 
<p>This opens the X-Ray console in a new window filtered to your function. Be aware of the time range field next to the search bar if you don’t see any search results.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store4.png" /></a><br /> In this screenshot, I’ve invoked the Lambda function twice, one time 10.3 minutes ago with a response time of 1.1 seconds and again 9.8 minutes ago with a response time of 8 milliseconds.</p> 
<p>Looking at the details of the longer running trace by clicking the trace ID, you can see that the Lambda function spent the first ~350 ms of the full 1.1 sec routing the request through Lambda and creating a new execution environment for this function, as this was the first invocation with this code. This is the portion of time before the initialization subsegment.</p> 
<p>Next, it took 725 ms to initialize the function, which includes executing the code at the global scope (including creating the boto3 client). This is also a one-time cost for a fresh execution environment.</p> 
<p>Finally, the function executed for 65 ms, of which 63.5 ms was the <strong>GetParametersByPath</strong> call to Parameter Store.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store5.png" /></a></p> 
<p>Looking at the trace for the second, much faster function invocation, you see that the majority of the 8 ms execution time was Lambda routing the request to the function and returning the response. Only 1 ms of the overall execution time was attributed to the execution of the function, which makes sense given that after the first invocation you’re simply returning the config stored in MyApp.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store6.png" /></a></p> 
<p>While the <strong>Traces</strong> screen allows you to view the details of individual traces, the <a href="http://docs.aws.amazon.com/xray/latest/devguide/xray-console.html#xray-console-servicemap">X-Ray Service Map</a> screen allows you to view aggregate performance data for all traced services over a period of time.</p> 
<p>In the X-Ray console navigation pane, choose <strong>Service map</strong>. Selecting a service node shows the metrics for node-specific requests. Selecting an edge between two nodes shows the metrics for requests that traveled that connection. Again, be aware of the time range field next to the search bar if you don’t see any search results.</p> 
<p>After invoking your Lambda function several more times by testing it from the Lambda console, you can view some aggregate performance metrics. Look at the following:</p> 
<li>From the client perspective, requests to the Lambda service for the function are taking an average of 50 ms to respond. The function is generating ~1 trace per minute.</li> 
<li>The function itself is responding in an average of 3 ms. In the following screenshot, I’ve clicked on this node, which reveals a <a href="http://docs.aws.amazon.com/xray/latest/devguide/xray-console-histograms.html">latency histogram</a> of the traced requests showing that over 95% of requests return in under 5 ms.</li> 
<li>Parameter Store is responding to requests in an average of 64 ms, but note the much lower trace rate in the node. This is because you only fetch data from Parameter Store on the initialization of the Lambda execution environment.</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store7.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/09/para-store7.png" /></a></p> 
<b>Conclusion</b> 
<p>Deduplication, encryption, and restricted access to shared configuration and secrets is a key component to any mature architecture. Serverless architectures designed using event-driven, on-demand, compute services like Lambda are no different.</p> 
<p>In this post, I walked you through a sample application accessing unencrypted and encrypted values in Parameter Store. These values were created in a hierarchy by application environment and component name, with the permissions to decrypt secret values restricted to only the function needing access. The techniques used here can become the foundation of secure, robust configuration management in your enterprise serverless applications.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3862');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/12/06/ReInvent_HA_Fargate_SOCIAL-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Migrating Your Amazon ECS Containers to AWS Fargate</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">tiffany jernigan</span></span> | on 
<time property="datePublished" datetime="2018-02-07T13:22:49+00:00">07 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-fargate/" title="View all posts in AWS Fargate"><span property="articleSection">AWS Fargate</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/migrating-your-amazon-ecs-containers-to-aws-fargate/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3817" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3817&amp;disqus_title=Migrating+Your+Amazon+ECS+Containers+to+AWS+Fargate&amp;disqus_url=https://aws.amazon.com/blogs/compute/migrating-your-amazon-ecs-containers-to-aws-fargate/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3817');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/fargate/">AWS Fargate</a>&nbsp;is a new technology that works with&nbsp;<a href="https://aws.amazon.com/ecs/">Amazon Elastic Container Service</a>&nbsp;(ECS) to run containers without having to manage servers or clusters. What does this mean? With Fargate, you no longer need to provision or manage a single virtual machine;&nbsp;you can just create tasks and run them directly!</p> 
<p>Fargate uses&nbsp;the same API actions as ECS, so you can use the ECS console, the&nbsp;<a href="https://aws.amazon.com/cli">AWS CLI</a>, or the <a href="https://github.com/aws/amazon-ecs-cli">ECS CLI</a>. I recommend running through the <a href="https://console.aws.amazon.com/ecs/home?region=us-east-1#/firstRun">first-run experience for Fargate</a> even if you’re familiar with ECS. It creates all of the one-time setup requirements, such as the necessary IAM roles. If you’re using a CLI, make sure to upgrade to the latest version<em>.&nbsp;</em></p> 
<p>In this blog, you will see how to migrate ECS containers from running on Amazon EC2 to Fargate.</p> 
<b style="font-size: 2.4rem;color: #153655;margin-bottom: -.6em">Getting started</b> 
<p><em>Note: Anything with code blocks is a change in the task definition file. Screen captures are from the console. Additionally, Fargate is currently available in the us-east-1 (N. Virginia) region.</em></p> 
<b style="color: #153655;margin-bottom: -.6em">Launch type</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/07/Screen-Shot-2018-01-05-at-1.47.23-PM.png" /></p> 
<p>When you create&nbsp;tasks (grouping of containers) and clusters (grouping of tasks), you now have two launch type options: EC2 and Fargate. The default launch type, EC2, is ECS as you knew it before the announcement of Fargate. You need to specify Fargate as the launch type when running a Fargate task.</p> 
<p>Even though Fargate abstracts away virtual machines, tasks still must be launched into a cluster. With Fargate, clusters are a logical infrastructure and permissions boundary that allow you to isolate and manage groups of tasks. ECS also supports heterogeneous clusters that are made up of tasks running on both EC2 and Fargate launch types.</p> 
<p>The optional, new <code class="lang-bash">requiresCompatibilities</code> parameter with <code>FARGATE</code> in the field ensures that your task definition only passes validation if you include Fargate-compatible parameters. Tasks can be flagged as compatible with EC2, Fargate, or both.</p> 
&quot;requiresCompatibilities&quot;: [
&quot;FARGATE&quot;
] 
<b style="color: #153655;margin-bottom: -.6em">Networking</b> 
&quot;networkMode&quot;: &quot;awsvpc&quot; 
<p>In November, we announced the addition of <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-ecs-introduces-awsvpc-networking-mode-for-containers-to-support-full-networking-capabilities/">task networking</a>&nbsp;with the network mode&nbsp;<a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">awsvpc</a>. By default, ECS uses the <code>bridge</code> network mode. Fargate requires using the&nbsp;<code>awsvpc</code>&nbsp;network mode.</p> 
<p>In <code>bridge</code> mode, all of your tasks running on the same instance share the instance’s&nbsp;<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">elastic network interface</a>, which is a virtual network interface, IP address, and security groups.</p> 
<p>The <code>awsvpc</code>&nbsp;mode provides this networking support to your tasks natively. You now get the&nbsp;same VPC networking and security controls at the task level that were previously only available with EC2 instances.&nbsp;Each task gets its own elastic networking interface and IP address so that multiple applications or copies of a single application can run on the same port number without any conflicts.</p> 
<p>The <code>awsvpc</code>&nbsp;mode also provides a separation of responsibility for tasks. You can get complete control of task placement within your own VPCs, subnets, and the security policies associated with them, even though the underlying infrastructure is managed by Fargate. Also, you can assign different security groups to each task, which gives you more fine-grained security. You can give an application only the permissions it needs.</p> 
&quot;portMappings&quot;: [
{
&quot;containerPort&quot;: &quot;3000&quot;
}
] 
<p>What else has to change? First, you only specify a&nbsp;<code>containerPort</code> value, not a&nbsp;<code>hostPort</code> value, as&nbsp;there is no host to manage. Your container port is the port that you access on your elastic network interface IP address. Therefore, your container ports in a single task definition file need to be unique.</p> 
&quot;environment&quot;: [
{
&quot;name&quot;: &quot;WORDPRESS_DB_HOST&quot;,
&quot;value&quot;: &quot;127.0.0.1:3306&quot;
}
] 
<p>Additionally, <code>links</code>&nbsp;are not allowed as they are a property of the “bridge” network mode (and are now a <a href="https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/">legacy</a>&nbsp;feature of Docker). Instead, containers share a network namespace and communicate with each other over the localhost interface. They can be referenced using the following:</p> 
localhost/127.0.0.1:<em>&lt;some_port_number&gt;</em> 
<b style="color: #153655;margin-bottom: -.6em">CPU and memory</b> 
&quot;memory&quot;:&nbsp;&quot;1024&quot;,
&quot;cpu&quot;: &quot;256&quot;
&quot;memory&quot;: &quot;1gb&quot;,
&quot;cpu&quot;: &quot;.25vcpu&quot; 
<p>When launching a task with the EC2 launch type, task performance is influenced by the instance types that you select for your cluster combined with your task definition. If you pick larger instances, your applications make use of the extra resources if there is no contention.</p> 
<p>In Fargate, you needed a way to get additional resource information so we created task-level resources. Task-level resources define the maximum amount of <code>memory</code> and&nbsp;<code>cpu</code>&nbsp;that your task can consume.</p> 
<li><code>memory</code> can be defined in MB with just the number, or in GB, for example, “1024” or “1gb”.</li> 
<li><code>cpu</code> can be defined as the number or in vCPUs, for example,&nbsp;“256” or&nbsp;“.25vcpu”. 
<li>vCPUs are virtual CPUs. You can look at the memory and vCPUs for <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a>&nbsp;to get an idea of what you may have used before.</li> 
</ul> </li> 
<p>The memory and CPU options available with Fargate are:</p> 
<table style="height: 185px" width="501"> 
<tbody> 
<tr> 
<td width="100"><strong>CPU</strong></td> 
<td width="270"><strong>Memory</strong></td> 
</tr> 
<tr> 
<td width="100">256 (.25 vCPU)</td> 
<td width="270">0.5GB, 1GB, 2GB</td> 
</tr> 
<tr> 
<td width="100">512 (.5 vCPU)</td> 
<td width="270">1GB, 2GB, 3GB, 4GB</td> 
</tr> 
<tr> 
<td width="100">1024 (1 vCPU)</td> 
<td width="270">2GB, 3GB, 4GB, 5GB, 6GB, 7GB, 8GB</td> 
</tr> 
<tr> 
<td width="100">2048 (2 vCPU)</td> 
<td width="270">Between 4GB and 16GB in 1GB increments</td> 
</tr> 
<tr> 
<td width="100">4096 (4 vCPU)</td> 
<td width="270">Between 8GB and 30GB in 1GB increments</td> 
</tr> 
</tbody> 
</table> 
<b style="color: #153655;margin-bottom: -.6em">IAM roles</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/07/Screen-Shot-2018-01-02-at-4.47.47-PM.png" /></p> 
<p>Because Fargate uses <code>awsvpc</code> mode, you need an <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using-service-linked-roles.html">Amazon ECS&nbsp;service-linked IAM role</a>&nbsp;named&nbsp;<code>AWSServiceRoleForECS</code>. It provides Fargate with the needed permissions, such as the permission to attach an elastic network interface to your task. After you create your service-linked IAM role, you can delete the remaining roles in your services.</p> 
&quot;executionRoleArn&quot;: &quot;arn:aws:iam::&lt;<em>your_</em><em>account_id&gt;</em>:role/ecsTaskExecutionRole&quot; 
<p>With the EC2 launch type, an instance role gives the agent the ability to pull, publish, talk to ECS, and so on. With Fargate, the task execution IAM role is only needed&nbsp;if you’re pulling from Amazon ECR or publishing data to <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch Logs</a>.</p> 
<p>The <a href="https://console.aws.amazon.com/ecs/home?region=us-east-1#/firstRun">Fargate first-run experience</a>&nbsp;tutorial in the console automatically creates these roles for you.</p> 
<b style="color: #153655;margin-bottom: -.6em">Volumes</b> 
<p>Fargate currently supports non-persistent, empty <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_data_volumes.html">data volumes</a> for containers. When you define your container, you no longer use the&nbsp;host&nbsp;field&nbsp;and only specify a name.</p> 
<b style="color: #153655;margin-bottom: -.6em">Load balancers</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/07/Screen-Shot-2017-12-19-at-11.22.26-PM.png" /></p> 
<p>For <code>awsvpc</code>&nbsp;mode, and therefore for Fargate, use the IP target type instead of the instance target type. You define this in the Amazon EC2 service when creating a load balancer.</p> 
<p>If you’re using a Classic Load Balancer, change it to an <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">Application Load Balancer</a> or a <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">Network Load Balancer</a>.</p> 
<p><em>Tip: If you are using an Application Load Balancer, make sure that your tasks are launched in the same VPC and Availability Zones as your load balancer.</em></p> 
<b style="font-size: 2.4rem;color: #153655;margin-bottom: -.6em">Let’s migrate a task definition!</b> 
<p>Here is an example NGINX task definition.&nbsp;This type of task definition is what you’re used to if you created one before Fargate was announced. It’s what you would run now with the EC2 launch type.</p> 
{
&quot;containerDefinitions&quot;: [
{
&quot;name&quot;: &quot;nginx&quot;,
&quot;image&quot;: &quot;nginx&quot;,
&quot;memory&quot;: &quot;512&quot;,
&quot;cpu&quot;: &quot;100&quot;,
&quot;essential&quot;: true,
&quot;portMappings&quot;: [
{
&quot;hostPort&quot;: &quot;80&quot;,
&quot;containerPort&quot;: &quot;80&quot;,
&quot;protocol&quot;: &quot;tcp&quot;
}
],
&quot;logConfiguration&quot;: {
&quot;logDriver&quot;: &quot;awslogs&quot;,
&quot;options&quot;: {
&quot;awslogs-group&quot;: &quot;/ecs/&quot;,
&quot;awslogs-region&quot;: &quot;us-east-1&quot;,
&quot;awslogs-stream-prefix&quot;: &quot;ecs&quot;
}
}
}
],
&quot;family&quot;: &quot;nginx-ec2&quot;
} 
<p>OK, so now what do you need to do to change it to run with the Fargate launch type?</p> 
<li>Add <code>FARGATE</code> for <code>requiredCompatibilities</code> (not required, but a good safety check for your task definition).</li> 
<li>Use&nbsp;<code>awsvpc</code> as the network mode.</li> 
<li>Just specify the&nbsp;<code>containerPort</code> (the <code>hostPort</code>value is the same).</li> 
<li>Add a task <code>executionRoleARN</code> value to allow logging to CloudWatch.</li> 
<li>Provide <code>cpu</code> and <code>memory</code> limits for the task.</li> 
{
&quot;requiresCompatibilities&quot;: [
&quot;FARGATE&quot;
],
&quot;containerDefinitions&quot;: [
{
&quot;name&quot;: &quot;nginx&quot;,
&quot;image&quot;: &quot;nginx&quot;,
&quot;memory&quot;: &quot;512&quot;,
&quot;cpu&quot;: &quot;100&quot;,
&quot;essential&quot;: true,
&quot;portMappings&quot;: [
{
&quot;containerPort&quot;: &quot;80&quot;,
&quot;protocol&quot;: &quot;tcp&quot;
}
],
&quot;logConfiguration&quot;: {
&quot;logDriver&quot;: &quot;awslogs&quot;,
&quot;options&quot;: {
&quot;awslogs-group&quot;: &quot;/ecs/&quot;,
&quot;awslogs-region&quot;: &quot;us-east-1&quot;,
&quot;awslogs-stream-prefix&quot;: &quot;ecs&quot;
}
}
}
],
&quot;networkMode&quot;: &quot;awsvpc&quot;,
&quot;executionRoleArn&quot;: &quot;arn:aws:iam::<em>&lt;your_account_id&gt;</em>:role/ecsTaskExecutionRole&quot;,
&quot;family&quot;: &quot;nginx-fargate&quot;,
&quot;memory&quot;: &quot;512&quot;,
&quot;cpu&quot;: &quot;256&quot;
} 
<b style="font-size: 2.4rem;color: #153655;margin-bottom: -.6em">Are there more examples?</b> 
<p>Yep! Head to the <a href="https://github.com/aws-samples/aws-containers-task-definitions">AWS Samples GitHub</a>&nbsp;repo. We have several sample task definitions you can try for both the EC2 and Fargate launch types. Contributions are very welcome too :).</p> 
<p>— <a href="https://twitter.com/tiffanyfayj">tiffany</a><br /> &nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3817');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/reference_diagram-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Invoking AWS Lambda from Amazon MQ</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2018-01-29T19:36:27+00:00">29 JAN 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-mq/" title="View all posts in Amazon MQ*"><span property="articleSection">Amazon MQ*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-integration/" title="View all posts in Application Integration"><span property="articleSection">Application Integration</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/" title="View all posts in Application Services*"><span property="articleSection">Application Services*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/enterprise-strategy/" title="View all posts in Enterprise Strategy*"><span property="articleSection">Enterprise Strategy*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/" title="View all posts in Messaging*"><span property="articleSection">Messaging*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/migration/" title="View all posts in Migration*"><span property="articleSection">Migration*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/invoking-aws-lambda-from-amazon-mq/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3765" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3765&amp;disqus_title=Invoking+AWS+Lambda+from+Amazon+MQ&amp;disqus_url=https://aws.amazon.com/blogs/compute/invoking-aws-lambda-from-amazon-mq/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3765');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h5>This post courtesy of Josh Kahn, AWS Solutions Architect</h5> 
<p>Message brokers can be used to solve a number of needs in enterprise architectures, including managing workload queues and broadcasting messages to a number of subscribers. <a href="https://aws.amazon.com/amazon-mq/">Amazon MQ</a> is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud.</p> 
<p>In this post, I discuss one approach to invoking <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> from queues and topics managed by Amazon MQ brokers. This and other similar patterns can be useful in integrating legacy systems with serverless architectures. You could also integrate systems already migrated to the cloud that use common APIs such as JMS.</p> 
<p>For example, imagine that you work for a company that produces training videos and which recently migrated its video management system to AWS. The on-premises system used to publish a message to an ActiveMQ broker when a video was ready for processing by an on-premises transcoder. However, on AWS, your company uses <a href="https://aws.amazon.com/elastictranscoder/">Amazon Elastic Transcoder</a>. Instead of modifying the management system, Lambda polls the broker for new messages and starts a new Elastic Transcoder job. This approach avoids changes to the existing application while refactoring the workload to leverage cloud-native components.</p> 
<p>This solution uses <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a> to trigger a Lambda function that polls the Amazon MQ broker for messages. Instead of starting an Elastic Transcoder job, the sample writes the received message to an <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a> table with a time stamp indicating the time received.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/30/reference_diagram_resized.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/30/reference_diagram_resized.png" /></a></p> 
<b>Getting started</b> 
<p>To start, navigate to the <a href="https://console.aws.amazon.com/amazon-mq/home">Amazon MQ console</a>. Next, launch a new Amazon MQ instance, selecting <strong>Single-instance Broker</strong> and supplying a broker name, user name, and password. Be sure to document the user name and password for later.</p> 
<p>For the purposes of this sample, choose the default options in the <strong>Advanced settings</strong> section. Your new broker is deployed to the default VPC in the selected AWS Region with the default security group. For this post, you update the security group to allow access for your sample Lambda function. In a production scenario, I recommend deploying both the Lambda function and your Amazon MQ broker in your own VPC.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture1-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture1-1.png" /></a></p> 
<p>After several minutes, your instance changes status from “Creation Pending” to “Available.” You can then visit the Details page of your broker to retrieve connection information, including a link to the ActiveMQ web console where you can monitor the status of your broker, publish test messages, and so on. In this example, use the Stomp protocol to connect to your broker. Be sure to capture the broker host name, for example:</p> 
<p>&lt;BROKER_ID&gt;.mq.us-east-1.amazonaws.com</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture2-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture2-1.png" /></a></p> 
<p>You should also modify the Security Group for the broker by clicking on its Security Group ID. Click the<strong> Edit</strong> button and then click <strong>Add Rule</strong> to allow inbound traffic on port 8162 for your IP address.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture3-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture3-1.png" /></a></p> 
<b>Deploying and scheduling the Lambda function</b> 
<p>To simplify the deployment of this example, I’ve provided an <a href="https://github.com/awslabs/serverless-application-model">AWS Serverless Application Model (SAM)</a> template that deploys the sample function and DynamoDB table, and schedules the function to be invoked every five minutes. Detailed instructions can be found with sample code on GitHub in the <a href="https://github.com/aws-samples/amazonmq-invoke-aws-lambda">amazonmq-invoke-aws-lambda repository</a>, with sample code. I discuss a few key aspects in this post.</p> 
<p>First, SAM makes it easy to deploy and schedule invocation of our function:</p> 
<code class="lang-yaml"></code> 
<code class="lang-yaml">SubscriberFunction:
Type: AWS::Serverless::Function
Properties:
CodeUri: subscriber/
Handler: index.handler
Runtime: nodejs6.10
Role: !GetAtt SubscriberFunctionRole.Arn
Timeout: 15
Environment:
Variables:
HOST: !Ref AmazonMQHost
LOGIN: !Ref AmazonMQLogin
PASSWORD: !Ref AmazonMQPassword
QUEUE_NAME: !Ref AmazonMQQueueName
WORKER_FUNCTIOn: !Ref WorkerFunction
Events:
Timer:
Type: Schedule
Properties:
Schedule: rate(5 minutes)
WorkerFunction:
Type: AWS::Serverless::Function
Properties:
CodeUri: worker/
Handler: index.handler
Runtime: nodejs6.10
Role: !GetAtt WorkerFunctionRole.Arn
Environment:
Variables:
TABLE_NAME: !Ref MessagesTable</code> 
<p>In the code, you include the URI, user name, and password for your newly created Amazon MQ broker. These allow the function to poll the broker for new messages on the sample queue.</p> 
<p>The sample Lambda function is written in Node.js, but <a href="http://activemq.apache.org/cross-language-clients.html">clients exist for a number of programming languages</a>.</p> 
<code class="lang-js">stomp.connect(options, (error, client) =&gt; {
if (error) { /* do something */ }
let headers = {
destination: ‘/queue/SAMPLE_QUEUE’,
ack: ‘auto’
}
client.subscribe(headers, (error, message) =&gt; {
if (error) { /* do something */ }
message.readString(‘utf-8’, (error, body) =&gt; {
if (error) { /* do something */ }
let params = {
FunctionName: MyWorkerFunction,
Payload: JSON.stringify({
message: body,
timestamp: Date.now()
})
}
let lambda = new AWS.Lambda()
lambda.invoke(params, (error, data) =&gt; {
if (error) { /* do something */ }
})
}
})
})</code> 
<code class="lang- node.js"></code> 
<b>Sending a sample message</b> 
<p>For the purpose of this example, use the Amazon MQ console to send a test message. Navigate to the details page for your broker.</p> 
<p>About midway down the page, choose <strong>ActiveMQ Web Console</strong>. Next, choose <strong>Manage ActiveMQ Broker</strong> to launch the admin console. When you are prompted for a user name and password, use the credentials created earlier.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture4-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture4-1.png" /></a></p> 
<p>At the top of the page, choose <strong>Send</strong>. From here, you can send a sample message from the broker to subscribers. For this example, this is how you generate traffic to test the end-to-end system. Be sure to set the <strong>Destination</strong> value to “SAMPLE_QUEUE.” The message body can contain any text. Choose <strong>Send</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture5-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/Picture5-1.png" /></a></p> 
<p>You now have a Lambda function polling for messages on the broker. To verify that your function is working, you can confirm in the <a href="https://console.aws.amazon.com/dynamodb/home">DynamoDB console</a> that the message was successfully received and processed by the sample Lambda function.</p> 
<p>First, choose <strong>Tables</strong> on the left and select the table name “amazonmq-messages” in the middle section. With the table detail in view, choose <strong>Items</strong>. If the function was successful, you’ll find a new entry similar to the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/DynamoDB_Table2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/29/DynamoDB_Table2-1024x77.png" /></a></p> 
<p>If there is no message in DynamoDB, check again in a few minutes or review the CloudWatch Logs group for Lambda functions that contain debug messages.</p> 
<b>Alternative approaches</b> 
<p>Beyond the approach described here, you may consider other approaches as well. For example, you could use an intermediary system such as Apache Flume to pass messages from the broker to Lambda or deploy <a href="http://camel.apache.org/http.html">Apache Camel to trigger Lambda via a POST to API Gateway</a>. There are trade-offs to each of these approaches. My goal in using CloudWatch Events was to introduce an easily repeatable pattern familiar to many Lambda developers.</p> 
<b>Summary</b> 
<p>I hope that you have found this example of how to integrate AWS Lambda with Amazon MQ useful. If you have expertise or legacy systems that leverage APIs such as JMS, you may find this useful as you incorporate serverless concepts in your enterprise architectures.</p> 
<p>To learn more, see the <a href="https://aws.amazon.com/amazon-mq/">Amazon MQ</a> website and <a href="https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/welcome.html">Developer Guide</a>. You can try <a href="https://console.aws.amazon.com/amazon-mq/home">Amazon MQ for free</a> with the AWS Free Tier, which includes up to 750 hours of a single-instance mq.t2.micro broker and up to 1 GB of storage per month for one year.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3765');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide6-1169x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Task Networking in AWS Fargate</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Peck</span></span> | on 
<time property="datePublished" datetime="2018-01-26T10:31:06+00:00">26 JAN 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-fargate/" title="View all posts in AWS Fargate"><span property="articleSection">AWS Fargate</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/task-networking-in-aws-fargate/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3748" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3748&amp;disqus_title=Task+Networking+in+AWS+Fargate&amp;disqus_url=https://aws.amazon.com/blogs/compute/task-networking-in-aws-fargate/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3748');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/fargate/">AWS Fargate</a> is a technology that allows you to focus on running your application without needing to provision, monitor, or manage the underlying compute infrastructure. You package your application into a <a href="https://aws.amazon.com/what-are-containers/">Docker container</a> that you can then launch using your container orchestration tool of choice.</p> 
<p>Fargate allows you to use containers without being responsible for <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances, similar to how EC2 allows you to run VMs without managing physical infrastructure. Currently, Fargate provides support for Amazon Elastic Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>). Support for Amazon Elastic Container Service for Kubernetes (<a href="https://aws.amazon.com/eks">Amazon EKS</a>) will be made available in the near future.</p> 
<p>Despite offloading the responsibility for the underlying instances, Fargate still gives you deep control over configuration of network placement and policies. This includes the ability to use many networking fundamentals such as Amazon VPC and security groups.</p> 
<p>This post covers how to take advantage of the different ways of networking your containers in Fargate when using ECS as your orchestration platform, with a focus on how to do networking securely.</p> 
<p>The first step to running any application in Fargate is defining an ECS task for Fargate to launch. A task is a logical group of one or more Docker containers that are deployed with specified settings. When running a task in Fargate, there are two different forms of networking to consider:</p> 
<li>Container (local) networking</li> 
<li>External networking</li> 
<b>Container Networking</b> 
<p>Container networking is often used for tightly coupled application components. Perhaps your application has a web tier that is responsible for serving static content as well as generating some dynamic HTML pages. To generate these dynamic pages, it has to fetch information from another application component that has an HTTP API.</p> 
<p>One potential architecture for such an application is to deploy the web tier and the API tier together as a pair and use local networking so the web tier can fetch information from the API tier.</p> 
<p>If you are running these two components as two processes on a single EC2 instance, the web tier application process could communicate with the API process on the same machine by using the local loopback interface. The local loopback interface has a special IP address of 127.0.0.1 and hostname of localhost.</p> 
<p>By making a networking request to this local interface, it bypasses the network interface hardware and instead the operating system just routes network calls from one process to the other directly. This gives the web tier a fast and efficient way to fetch information from the API tier with almost no networking latency.</p> 
<p>In Fargate, when you launch multiple containers as part of a single task, they can also communicate with each other over the local loopback interface. Fargate uses a special container networking mode called <a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">awsvpc</a>, which gives all the containers in a task a shared elastic network interface to use for communication.</p> 
<p>If you specify a port mapping for each container in the task, then the containers can communicate with each other on that port. For example the following task definition could be used to deploy the web tier and the API tier:</p> 
<code class="lang-json">{
&quot;family&quot;: &quot;myapp&quot;
&quot;containerDefinitions&quot;: [
{
&quot;name&quot;: &quot;web&quot;,
&quot;image&quot;: &quot;my web image url&quot;,
&quot;portMappings&quot;: [
{
&quot;containerPort&quot;: 80
}
],
&quot;memory&quot;: 500,
&quot;cpu&quot;: 10,
&quot;esssential&quot;: true
},
{
&quot;name&quot;: &quot;api&quot;,
&quot;image&quot;: &quot;my api image url&quot;,
&quot;portMappings&quot;: [
{
&quot;containerPort&quot;: 8080
}
],
&quot;cpu&quot;: 10,
&quot;memory&quot;: 500,
&quot;essential&quot;: true
}
]
}</code> 
<p>ECS, with Fargate, is able to take this definition and launch two containers, each of which is bound to a specific static port on the elastic network interface for the task.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide1-1024x592.png" /></a>Because each Fargate task has its own isolated networking stack, there is no need for dynamic ports to avoid port conflicts between different tasks as in other networking modes. The static ports make it easy for containers to communicate with each other. For example, the web container makes a request to the API container using its well-known static port:</p> 
<code class="lang-bash">curl 127.0.0.1:8080/my-endpoint</code> 
<p>This sends a local network request, which goes directly from one container to the other over the local loopback interface without traversing the network. This deployment strategy allows for fast and efficient communication between two tightly coupled containers. But most application architectures require more than just internal local networking.</p> 
<b>External Networking</b> 
<p>External networking is used for network communications that go outside the task to other servers that are not part of the task, or network communications that originate from other hosts on the internet and are directed to the task.</p> 
<p>Configuring external networking for a task is done by modifying the settings of the VPC in which you launch your tasks. A VPC is a fundamental tool in AWS for controlling the networking capabilities of resources that you launch on your account.</p> 
<p>When setting up a VPC, you create one or more subnets, which are logical groups that your resources can be placed into. Each subnet has an Availability Zone and its own route table, which defines rules about how network traffic operates for that subnet. There are two main types of subnets: public and private.</p> 
<h3>Public subnets</h3> 
<p>A public subnet is a subnet that has an associated internet gateway. Fargate tasks in that subnet are assigned both private and public IP addresses:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide2-1024x866.png" /></a><br /> A browser or other client on the internet can send network traffic to the task via the internet gateway using its public IP address. The tasks can also send network traffic to other servers on the internet because the route table can route traffic out via the internet gateway.</p> 
<p>If tasks want to communicate directly with each other, they can use each other’s private IP address to send traffic directly from one to the other so that it stays inside the subnet without going out to the internet gateway and back in.</p> 
<h3>Private subnets</h3> 
<p>A private subnet does not have direct internet access. The Fargate tasks inside the subnet don’t have public IP addresses, only private IP addresses. Instead of an internet gateway, a network address translation (NAT) gateway is attached to the subnet:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide4-1024x493.png" /></a></p> 
<p>There is no way for another server or client on the internet to reach your tasks directly, because they don’t even have an address or a direct route to reach them. This is a great way to add another layer of protection for internal tasks that handle sensitive data. Those tasks are protected and can’t receive any inbound traffic at all.</p> 
<p>In this configuration, the tasks can still communicate to other servers on the internet via the NAT gateway. They would appear to have the IP address of the NAT gateway to the recipient of the communication. If you run a Fargate task in a private subnet, you must add this NAT gateway. Otherwise, Fargate can’t make a network request to Amazon ECR to download the container image, or communicate with Amazon CloudWatch to store container metrics.</p> 
<b>Load balancers</b> 
<p>If you are running a container that is hosting internet content in a private subnet, you need a way for traffic from the public to reach the container. This is generally accomplished by using a load balancer such as an Application Load Balancer or a Network Load Balancer.</p> 
<p>ECS integrates tightly with AWS load balancers by automatically configuring a service-linked load balancer to send network traffic to containers that are part of the service. When each task starts, the IP address of its elastic network interface is added to the load balancer’s configuration. When the task is being shut down, network traffic is safely drained from the task before removal from the load balancer.</p> 
<p>To get internet traffic to containers using a load balancer, the load balancer is placed into a public subnet. ECS configures the load balancer to forward traffic to the container tasks in the private subnet:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide5-1024x647.png" /></a></p> 
<p>This configuration allows your tasks in Fargate to be safely isolated from the rest of the internet. They can still initiate network communication with external resources via the NAT gateway, and still receive traffic from the public via the Application Load Balancer that is in the public subnet.</p> 
<p>Another potential use case for a load balancer is for internal communication from one service to another service within the private subnet. This is typically used for a microservice deployment, in which one service such as an internet user account service needs to communicate with an internal service such as a password service. Obviously, it is undesirable for the password service to be directly accessible on the internet, so using an internet load balancer would be a major security vulnerability. Instead, this can be accomplished by hosting an internal load balancer within the private subnet:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/26/Slide6-1024x552.png" /></a></p> 
<p>With this approach, one container can distribute requests across an Auto Scaling group of other private containers via the internal load balancer, ensuring that the network traffic stays safely protected within the private subnet.</p> 
<b>Best Practices for Fargate Networking</b> 
<h4><strong>Determine whether you should use local task networking</strong></h4> 
<p>Local task networking is ideal for communicating between containers that are tightly coupled and require maximum networking performance between them. However, when you deploy one or more containers as part of the same task they are always deployed together so it removes the ability to independently scale different types of workload up and down.</p> 
<p>In the example of the application with a web tier and an API tier, it may be the case that powering the application requires only two web tier containers but 10 API tier containers. If local container networking is used between these two container types, then an extra eight unnecessary web tier containers would end up being run instead of allowing the two different services to scale independently.</p> 
<p>A better approach would be to deploy the two containers as two different services, each with its own load balancer. This allows clients to communicate with the two web containers via the web service’s load balancer. The web service could distribute requests across the eight backend API containers via the API service’s load balancer.</p> 
<h4><strong>Run internet tasks that require internet access in a public subnet</strong></h4> 
<p>If you have tasks that require internet access and a lot of bandwidth for communication with other services, it is best to run them in a public subnet. Give them public IP addresses so that each task can communicate with other services directly.</p> 
<p>If you run these tasks in a private subnet, then all their outbound traffic has to go through an NAT gateway. AWS NAT gateways support up to 10 Gbps of burst bandwidth. If your bandwidth requirements go over this, then all task networking starts to get throttled. To avoid this, you could distribute the tasks across multiple private subnets, each with their own NAT gateway. It can be easier to just place the tasks into a public subnet, if possible.</p> 
<h4><strong>Avoid using a public subnet or public IP addresses for private, internal tasks</strong></h4> 
<p>If you are running a service that handles private, internal information, you should not put it into a public subnet or use a public IP address. For example, imagine that you have one task, which is an API gateway for authentication and access control. You have another background worker task that handles sensitive information.</p> 
<p>The intended access pattern is that requests from the public go to the API gateway, which then proxies request to the background task only if the request is from an authenticated user. If the background task is in a public subnet and has a public IP address, then it could be possible for an attacker to bypass the API gateway entirely. They could communicate directly to the background task using its public IP address, without being authenticated.</p> 
<b>Conclusion</b> 
<p>Fargate gives you a way to run containerized tasks directly without managing any EC2 instances, but you still have full control over how you want networking to work. You can set up containers to talk to each other over the local network interface for maximum speed and efficiency. For running workloads that require privacy and security, use a private subnet with public internet access locked down. Or, for simplicity with an internet workload, you can just use a public subnet and give your containers a public IP address.</p> 
<p>To deploy one of these Fargate task networking approaches, check out some <a href="https://github.com/awslabs/aws-cloudformation-templates/tree/master/aws/services/ECS">sample CloudFormation templates</a>&nbsp;showing how to configure the VPC, subnets, and load balancers.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3748');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/24/ECS_social_loading.png" /> 
<b class="lb-b blog-post-title" property="name headline">Building Blocks of Amazon ECS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">tiffany jernigan</span></span> | on 
<time property="datePublished" datetime="2018-01-24T13:57:33+00:00">24 JAN 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-fargate/" title="View all posts in AWS Fargate"><span property="articleSection">AWS Fargate</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-blocks-of-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3710" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3710&amp;disqus_title=Building+Blocks+of+Amazon+ECS&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-blocks-of-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3710');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>So, what’s <a href="https://aws.amazon.com/ecs/">Amazon Elastic Container Service (ECS)</a>? ECS is a managed service for running containers on AWS, designed to make it easy to run applications in the cloud without worrying about configuring the environment for your code to run in. Using ECS, you can easily deploy containers to host a simple website or run complex distributed microservices using thousands of containers.</p> 
<p>Getting started with ECS isn’t too difficult. To fully understand how it works and how you can use it, it helps to understand the basic building blocks of ECS and how they fit together!</p> 
<b style="font-size: 2.4rem;color: #153655;margin-bottom: -.6em">Amazon EC2 building blocks</b> 
<p>We currently provide two launch types: EC2 and <a href="https://aws.amazon.com/fargate/">Fargate</a>. With Fargate, the&nbsp;<a href="https://aws.amazon.com/ec2">Amazon EC2</a>&nbsp;instances are abstracted away and managed for you. Instead of worrying about ECS container instances, you can just worry about tasks. In this post, the infrastructure components used by ECS that&nbsp;are&nbsp;handled by Fargate are marked with a *.</p> 
<b><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/23/region-az-instance2sm.png" /></b> 
<b style="color: #153655;margin-bottom: -.6em">Instance*</b> 
<p><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Instances.html">EC2 instances</a> are good ol’ virtual machines (VMs). And yes, don’t worry, you can connect to them (via SSH). Because customers have varying needs in memory, storage, and computing power, many different <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html">instance types</a> are offered. Just want to run a small application or try a <a href="https://aws.amazon.com/free/">free trial</a>? Try t2.micro. Want to run memory-optimized workloads? R3 and X1 instances are a couple options. There are many more instance types as well, which cater to various use cases.</p> 
<h3 style="color: #153655;margin-bottom: -.6em">AMI*</h3> 
<p>Sorry if you wanted to immediately march forward, but <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instances-and-amis.html">before you create your instance</a>, you need to choose an<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"> AMI</a>. An AMI stands for Amazon Machine Image. What does that mean? Basically, an AMI provides the information required to launch an instance: root volume, launch permissions, and volume-attachment specifications. You can <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html">find</a> and choose a Linux or Windows AMI provided by AWS, the user community, the <a href="https://aws.amazon.com/marketplace">AWS Marketplace</a> (for example, the <a href="https://aws.amazon.com/marketplace/pp/B06XS8WHGJ">Amazon ECS-Optimized AMI</a>), or you can create your own.</p> 
<b style="color: #153655;margin-bottom: -.6em">Region</b> 
<p>AWS is divided into<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html"> regions</a> that are geographic areas around the world (for now it’s just Earth, but maybe someday…). These regions have semi-evocative names such as us-east-1 (N. Virginia), us-west-2 (Oregon), eu-central-1 (Frankfurt), ap-northeast-1 (Tokyo), etc.</p> 
<p>Each region is designed to be completely isolated from the others, and consists of multiple, distinct data centers. This creates a “blast radius” for failure so that even if an entire region goes down, the others aren’t affected. Like many AWS services, to start using ECS, you first need to decide the region in which to operate. Typically, this is the region nearest to you or your users.</p> 
<b style="color: #153655;margin-bottom: -.6em">Availability Zone</b> 
<p>AWS regions are subdivided into <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">Availability Zones</a>. A region has at minimum two zones, and up to a handful. Zones are <a href="https://aws.amazon.com/about-aws/global-infrastructure/">physically isolated from each other</a>, spanning one or more different data centers, but are connected through low-latency, fiber-optic networking, and share some common facilities. EC2 is designed so that the most common failures only affect a single zone to prevent region-wide outages. This means you can achieve high availability in a region by spanning your services across multiple zones and distributing across hosts.</p> 
<b style="font-size: 2.4rem;color: #153655;margin-bottom: -.6em">Amazon ECS building blocks</b> 
<b style="color: #153655;margin-bottom: -.6em"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/23/ecs-ec2-instance-sm.png" /></b> 
<b style="color: #153655;margin-bottom: -.6em">Container</b> 
<p>Well, without <a href="https://aws.amazon.com/what-are-containers/">containers</a>, ECS wouldn’t exist!</p> 
<p><strong>Are containers virtual machines?</strong><br /> Nope! Virtual machines virtualize the hardware (benefits), while containers virtualize the operating system (even more benefits!). If you look inside a container, you would see that it is made by processes running on the host, and tied together by kernel constructs like namespaces, cgroups, etc. But you don’t need to bother about that level of detail, at least not in this post!</p> 
<p><strong>Why containers?</strong><br /> Containers give you the ability to build, ship, and run your code anywhere!</p> 
<p>Before the cloud, you needed to self-host and therefore had to buy machines in addition to setting up and configuring the operating system (OS), and running your code. In the cloud, with virtualization, you can just skip to setting up the OS and running your code. Containers make the process even easier—you can just run your code.</p> 
<p>Additionally, all of the dependencies travel in a package with the code, which is called an image. This allows containers to be deployed on any host machine. From the outside, it looks like a host is just holding a bunch of containers. They all look the same, in the sense that they are generic enough to be deployed on any host.</p> 
<p>With ECS, you can easily run your containerized code and applications across a managed cluster of EC2 instances.</p> 
<p><strong>Are containers a fairly new technology?</strong><br /> The concept of containerization is not new. Its origins date back to 1979 with the creation of chroot. However, it wasn’t until the early 2000s that containers became a major technology. The most significant milestone to date was the release of <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html">Docker</a> in 2013, which led to the popularization and widespread adoption of containers.</p> 
<p><strong>What does ECS use?</strong><br /> While other container technologies exist (LXC, rkt, etc.), because of its massive adoption and use by our customers, ECS was designed first to work natively with Docker containers.</p> 
<b style="color: #153655;margin-bottom: -.6em">Container instance*</b> 
<p>Yep, you are back to instances. An instance is just slightly more complex in the ECS realm though. Here, it is an<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html"> ECS container instance</a> that is an EC2 instance running the agent, has a <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html">specifically defined</a> <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/UsingIAM.html">IAM policy and role</a>, and has been registered into your cluster.</p> 
<p>And as you probably guessed, in these instances, you are running containers.<strong>&nbsp;</strong></p> 
<h3 style="color: #153655;margin-bottom: -.6em">AMI*</h3> 
<p>These container instances can use any AMI as long as it has <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/container_instance_AMIs.html">the following specifications</a>: a modern Linux distribution with the agent and the Docker Daemon with any Docker runtime dependencies running on it.</p> 
<p>Want it more simplified? Well, AWS created the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html">Amazon ECS-Optimized AMI</a> for just that. Not only does that AMI come preconfigured with all of the previously mentioned specifications, it’s tested and includes the recommended <a href="https://github.com/aws/amazon-ecs-init">ecs-init</a> upstart process to run and monitor the agent.</p> 
<b style="color: #153655;margin-bottom: -.6em">Cluster</b> 
<p>An <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_clusters.html">ECS cluster</a> is a grouping of (container) instances* (or tasks in Fargate) that lie within a single region, but can span multiple Availability Zones – it’s even a good idea for redundancy. When launching an instance (or tasks in Fargate), unless specified, it registers with the cluster named “default”. If “default” doesn’t exist, it is created. You can also scale and delete your clusters.</p> 
<b style="color: #153655;margin-bottom: -.6em">Agent*</b> 
<p>The<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_agent.html"> Amazon ECS container agent</a> is a <a href="https://golang.org/">Go</a> program that runs in its own container within each EC2 instance that you use with ECS. (It’s also available open source on <a href="https://github.com/aws/amazon-ecs-agent">GitHub</a>!) The agent is the intermediary component that takes care of the communication between the scheduler and your instances. Want to register your instance into a cluster? (Why wouldn’t you? A cluster is both a logical boundary and provider of pool of resources!) Then you need to run the agent on it.</p> 
<b style="color: #153655;margin-bottom: -.6em">Task</b> 
<p>When you want to start a container, it has to be part of a task. Therefore, you have to create a task first. Succinctly, tasks are a logical grouping of 1 to <em>N</em> containers that run together on the same instance, with <em>N</em> defined by you, up to 10. Let’s say you want to run a custom blog engine. You could put together a web server, an application server, and an in-memory cache, each in their own container. Together, they form a basic frontend unit.</p> 
<h3 style="color: #153655;margin-bottom: -.6em">Task definition</h3> 
<p>Ah, but you cannot create a task directly. You have to create a<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html"> task definition</a> that tells ECS that “task definition X is composed of this container (and maybe that other container and that other container too!).” It’s kind of like an architectural plan for a city. Some other details it can include are how the containers interact, container CPU and memory constraints, and task permissions using <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">IAM roles</a>.</p> 
<p>Then you can tell ECS, “start one task using task definition X.” It might sound like unnecessary planning at first. As soon as you start to deal with multiple tasks, scaling, upgrades, and other “real life” scenarios, you’ll be glad that you have task definitions to keep track of things!</p> 
<b style="color: #153655;margin-bottom: -.6em">Scheduler*</b> 
<p>So, the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html">scheduler</a> schedules… sorry, this should be more helpful, huh? The scheduler is part of the “hosted orchestration layer” provided by ECS. Wait a minute, what do I mean by “hosted orchestration”? Simply put, hosted means that it’s operated by ECS on your behalf, without you having to care about it. Your applications are deployed in containers running on your instances, but the managing of tasks is taken care of by ECS. One less thing to worry about!</p> 
<p>Also, the scheduler is the component that decides what (which containers) gets to run where (on which instances), according to a number of constraints. Say that you have a custom blog engine to scale for high availability. You could create a service, which by default, spreads tasks across all zones in the chosen region. And if you want each task to be on a different instance, you can use the <em>distinctInstance</em> <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement.html">task placement</a> constraint. ECS makes sure that not only this happens, but if a task fails, it starts again.</p> 
<h3 style="color: #153655;margin-bottom: -.6em">Service</h3> 
<p>To ensure that you always have your task running without managing it yourself, you can create a <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html">service</a> based on the task that you defined and ECS ensures that it stays running. A service is a special construct that says, “at any given time, I want to make sure that <em>N</em> tasks using task definition X1 are running.” If <em>N</em>=1, it just means “make sure that this task is running, and restart it if needed!” And with <em>N</em>&gt;1, you’re basically scaling your application until you hit <em>N</em>, while also ensuring each task is running.</p> 
<b style="font-size: 2.4rem;color: #153655;margin-bottom: -.6em">So, what now?</b> 
<p>Hopefully you, at the very least, learned a tiny something. All comments are very welcome!</p> 
<p>Want to discuss ECS with others? Join the <a href="https://join.slack.com/t/amazon-ecs/shared_invite/enQtMjQ3OTk4NTM3ODQyLWViNjhhMDBkM2VmMGRkYzMwOGMxNmU3ODBiYmQyZDk0OTlkMWYxNDJjNzJjMWYyNWY0ZWE4YmMzNmNlNGY0YWU">amazon-ecs</a> slack group, which members of the community created and manage.</p> 
<p>Also, if you’re interested in learning more about the core concepts of ECS and its relation to EC2, here are some resources:</p> 
<p><strong>Pages</strong><br /> <a href="https://aws.amazon.com/ecs/">Amazon ECS landing page<br /> </a><a href="https://aws.amazon.com/fargate/">AWS Fargate landing page<br /> </a><a href="https://aws.amazon.com/ecs/getting-started/">Amazon ECS Getting Started</a><br /> <a href="https://github.com/nathanpeck/awesome-ecs">Nathan Peck’s AWSome ECS</a></p> 
<p><strong>Docs</strong><br /> <a href="https://aws.amazon.com/documentation/ec2/">Amazon EC2</a><br /> <a href="https://aws.amazon.com/documentation/ecs/">Amazon ECS</a></p> 
<p><strong>Blogs</strong><br /> <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/">AWS Compute Blog</a><br /> <a href="https://aws.amazon.com/blogs/aws/category/ec2-container-service/">AWS Blog</a></p> 
<p><strong>GitHub code</strong><br /> <a href="https://github.com/aws/amazon-ecs-agent">Amazon ECS container agent</a><br /> <a href="https://github.com/aws/amazon-ecs-cli">Amazon ECS CLI</a></p> 
<p><strong>AWS videos</strong><br /> <a href="https://www.youtube.com/watch?v=eq4wL2MiNqo&amp;list=PLhr1KZpdzukef_4labkwEs9CbK-MvReKS">Learn Amazon ECS</a><br /> <a href="https://www.youtube.com/user/AmazonWebServices/">AWS videos</a><br /> <a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukeUoelFoS68SRfY6pA_Ib6A">AWS webinars</a></p> 
<p>&nbsp;</p> 
<p>— tiffany</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/24/tiffany-120x120bw.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/24/twitter-icon-8-15x15.png">@tiffanyfayj</a></p> 
<p>&nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3710');
});
</script> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
