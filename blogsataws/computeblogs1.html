<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="aws-blogs" class="layout-inner aws-blogs">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/sentiment-negative1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Using AWS Lambda and Amazon Comprehend for sentiment analysis</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2018-04-11T08:46:37+00:00">11 APR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/artificial-intelligence/amazon-comprehend/" title="View all posts in Amazon Comprehend"><span property="articleSection">Amazon Comprehend</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/contact-center/amazon-connect/" title="View all posts in Amazon Connect*"><span property="articleSection">Amazon Connect*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/artificial-intelligence/amazon-lex/" title="View all posts in Amazon Lex*"><span property="articleSection">Amazon Lex*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/using-aws-lambda-and-amazon-comprehend-for-sentiment-analysis/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4245" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4245&amp;disqus_title=Using+AWS+Lambda+and+Amazon+Comprehend+for+sentiment+analysis&amp;disqus_url=https://aws.amazon.com/blogs/compute/using-aws-lambda-and-amazon-comprehend-for-sentiment-analysis/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4245');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Giedrius Praspaliauskas, AWS Solutions Architect</em></p> 
<p>Even with best IVR systems, customers get frustrated. What if you knew that 10 callers in your Amazon Connect contact flow were likely to say “Agent!” in frustration in the next 30 seconds? Would you like to get to them before that happens? What if your bot was smart enough to admit, “I’m sorry this isn’t helping. Let me find someone for you.”?</p> 
<p>In this post, I show you how to use <a title="undefined" href="https://aws.amazon.com/lambda/" target="null">AWS Lambda</a> and <a title="undefined" href="https://aws.amazon.com/comprehend/" target="null">Amazon Comprehend</a> for sentiment analysis to make your <a title="undefined" href="https://aws.amazon.com/lex/" target="null">Amazon Lex</a> bots in <a title="undefined" href="https://aws.amazon.com/connect/" target="null">Amazon Connect</a> more sympathetic.</p> 
<b>Setting up a Lambda function for sentiment analysis</b> 
<p>There are multiple natural language and text processing frameworks or services available to use with Lambda, including but not limited to Amazon Comprehend, TextBlob, Pattern, and NLTK. Pick one based on the nature of your system: &nbsp;the type of interaction, languages supported, and so on. For this post, I picked Amazon Comprehend, which uses natural language processing (NLP) to extract insights and relationships in text.</p> 
<p>The walkthrough in this post is just an example. In a full-scale implementation, you would likely implement a more nuanced approach. For example, you could keep the overall sentiment score through the conversation and act only when it reaches a certain threshold. It is worth noting that this Lambda function is not called for missed utterances, so there may be a gap between what is being analyzed and what was actually said.</p> 
<p>The Lambda function is straightforward. It analyses the input transcript field of the Amazon Lex event. Based on the overall sentiment value, it generates a response message with next step instructions. When the sentiment is neutral, positive, or mixed, the response leaves it to Amazon Lex to decide what the next steps should be. It adds to the response overall sentiment value as an additional session attribute, along with slots’ values received as an input.</p> 
<p>When the overall sentiment is negative, the function returns the dialog action, pointing to an escalation intent (specified in the environment variable <em>ESCALATION_INTENT_NAME</em>) or returns the <em>fulfillment closure</em> action with a failure state when the intent is not specified. In addition to actions or intents, the function returns a message, or prompt, to be provided to the customer before taking the next step. Based on the returned action, Amazon Connect can select the appropriate next step in a contact flow.</p> 
<p>For this walkthrough, you create a Lambda function using the AWS Management Console:</p> 
<ol> 
<li>Open the Lambda console.</li> 
<li>Choose <strong>Create Function</strong>.</li> 
<li>Choose <strong>Author from scratch</strong> (no blueprint).</li> 
<li>For <strong>Runtime</strong>, choose Python 3.6.</li> 
<li>For <strong>Role</strong>, choose <strong>Create a custom role</strong>. The custom execution role allows the function to detect sentiments, create a log group, stream log events, and store the log events.</li> 
<li>Enter the following values: 
<li>For <strong>Role Description</strong>, enter <em>Lambda execution role permissions</em>.</li> 
<li>For <strong>IAM Role</strong>, choose <strong>Create an IAM role</strong>.</li> 
<li>For <strong>Role Name</strong>, enter <em>LexSentimentAnalysisLambdaRole</em>.</li> 
<li>For <strong>Policy</strong>, use the following policy:</li> 
</ul> </li> 
</ol> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;,
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: &quot;arn:aws:logs:*:*:*&quot;
},
{
&quot;Action&quot;: [
&quot;comprehend:DetectDominantLanguage&quot;,
&quot;comprehend:DetectSentiment&quot;
],
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Resource&quot;: &quot;*&quot;
}
]
}
</code> 
<ol start="7"> 
<li style="list-style-type: none"> 
<ol start="7"> 
<li>Choose <strong>Create function</strong>.</li> 
<li>Copy/paste the following code to the editor window</li> 
</ol> </li> 
</ol> 
<code class="lang-python">import os, boto3
ESCALATION_INTENT_MESSAGE=&quot;Seems that you are having troubles with our service. Would you like to be transferred to the associate?&quot;
FULFILMENT_CLOSURE_MESSAGE=&quot;Seems that you are having troubles with our service. Let me transfer you to the associate.&quot;
escalation_intent_name = os.getenv('ESACALATION_INTENT_NAME', None)
client = boto3.client('comprehend')
def lambda_handler(event, context):
sentiment=client.detect_sentiment(Text=event['inputTranscript'],LanguageCode='en')['Sentiment']
if sentiment=='NEGATIVE':
if escalation_intent_name:
result = {
&quot;sessionAttributes&quot;: {
&quot;sentiment&quot;: sentiment
},
&quot;dialogAction&quot;: {
&quot;type&quot;: &quot;ConfirmIntent&quot;, 
&quot;message&quot;: {
&quot;contentType&quot;: &quot;PlainText&quot;, 
&quot;content&quot;: ESCALATION_INTENT_MESSAGE
}, 
&quot;intentName&quot;: escalation_intent_name
}
}
else:
result = {
&quot;sessionAttributes&quot;: {
&quot;sentiment&quot;: sentiment
},
&quot;dialogAction&quot;: {
&quot;type&quot;: &quot;Close&quot;,
&quot;fulfillmentState&quot;: &quot;Failed&quot;,
&quot;message&quot;: {
&quot;contentType&quot;: &quot;PlainText&quot;,
&quot;content&quot;: FULFILMENT_CLOSURE_MESSAGE
}
}
}
else:
result ={
&quot;sessionAttributes&quot;: {
&quot;sentiment&quot;: sentiment
},
&quot;dialogAction&quot;: {
&quot;type&quot;: &quot;Delegate&quot;,
&quot;slots&quot; : event[&quot;currentIntent&quot;][&quot;slots&quot;]
}
}
return result</code> 
<ol start="9"> 
<li>Below the code editor specify the environment variable ESCALATION_INTENT_NAME with a value of <em>Escalate</em>.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/lambda-env-vars.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/lambda-env-vars-1024x157.png" /></a></p> 
<ol start="10"> 
<li>Click on <strong>Save</strong> in the top right of the console.</li> 
</ol> 
<p>Now you can test your function.</p> 
<ol> 
<li>Click&nbsp;<strong>Test</strong> at the top of the console.</li> 
<li>Configure a new test event using the following test event JSON:</li> 
</ol> 
<code class="lang-json">{
&quot;messageVersion&quot;: &quot;1.0&quot;,
&quot;invocationSource&quot;: &quot;DialogCodeHook&quot;,
&quot;userId&quot;: &quot;1234567890&quot;,
&quot;sessionAttributes&quot;: {},
&quot;bot&quot;: {
&quot;name&quot;: &quot;BookSomething&quot;,
&quot;alias&quot;: &quot;None&quot;,
&quot;version&quot;: &quot;$LATEST&quot;
},
&quot;outputDialogMode&quot;: &quot;Text&quot;,
&quot;currentIntent&quot;: {
&quot;name&quot;: &quot;BookSomething&quot;,
&quot;slots&quot;: {
&quot;slot1&quot;: &quot;None&quot;,
&quot;slot2&quot;: &quot;None&quot;
},
&quot;confirmationStatus&quot;: &quot;None&quot;
},
&quot;inputTranscript&quot;: &quot;I want something&quot;
}</code> 
<ol start="3"> 
<li>Click <strong>Create</strong></li> 
<li>Click <strong>Test</strong> on the console</li> 
</ol> 
<p>This message should return a response from Lambda with a sentiment session attribute of NEUTRAL.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/execution-success1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/execution-success1.png" /></a></p> 
<p>However, if you change the input to “This is garbage!”, Lambda changes the <em>dialog</em> action to the <em>escalation</em> intent specified in the environment variable <em>ESCALATION_INTENT_NAME</em>.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/sentiment-negative1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/sentiment-negative1.png" /></a></p> 
<b>Setting up Amazon Lex</b> 
<p>Now that you have your Lambda function running, it is time to create the Amazon Lex bot. Use the BookTrip sample bot and call it BookSomething. The IAM role is automatically created on your behalf. Indicate that this bot is not subject to the <a title="undefined" href="https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule" target="null">COPPA</a>, and choose <strong>Create</strong>. A few minutes later, the bot is ready.</p> 
<p>Make the following changes to the default configuration of the bot:</p> 
<ol> 
<li>Add an intent with no associated slots. Name it <em>Escalate</em>.</li> 
<li>Specify the Lambda function for initialization and validation in the existing two intents (“BookCar” and “BookHotel”), at the same time giving Amazon Lex permission to invoke it.</li> 
<li>Leave the other configuration settings as they are and save the intents.</li> 
</ol> 
<p>You are ready to build and publish this bot. Set a new alias, <em>BookSomethingWithSentimentAnalysis</em>. When the build finishes, test it.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/bot-test.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/bot-test.png" /></a></p> 
<p>As you see, sentiment analysis works!</p> 
<b>Setting up Amazon Connect</b> 
<p>Next, <a title="undefined" href="https://docs.aws.amazon.com/connect/latest/adminguide/gettingstarted.html" target="null">provision an Amazon Connect instance</a>.</p> 
<p>After the instance is created, you need to integrate the <a title="undefined" href="https://docs.aws.amazon.com/connect/latest/adminguide/amazon-connect-instance.html#contact-flows" target="null">Amazon Lex</a> bot created in the previous step. For more information, see the Amazon Lex section in the <em>Configuring Your Amazon Connect Instance</em> topic. &nbsp;You may also want to look at the excellent post by Randall Hunt, <a title="undefined" href="https://aws.amazon.com/blogs/aws/new-amazon-connect-and-amazon-lex-integration/" target="null">New – Amazon Connect and Amazon Lex Integration</a>.</p> 
<p>Create a new contact flow, “Sentiment analysis walkthrough”:</p> 
<ol> 
<li>Log in into the Amazon Connect instance.</li> 
<li>Choose <strong>Create contact flow, Create transfer to agent flow</strong>.</li> 
<li>Add a <strong>Get customer input</strong> block, open the icon in the top left corner, and specify your Amazon Lex bot and its intents.</li> 
<li>Select the <strong>Text to speec</strong>h audio prompt type and enter text for Amazon Connect to play at the beginning of the dialog.</li> 
<li>Choose <strong>Amazon Lex</strong>, enter your Amazon Lex bot name and the alias.</li> 
<li>Specify the intents to be used as dialog branches that a customer can choose: <em>BookHotel, BookTrip,</em> or <em>Escalate</em>.</li> 
<li>Add two <strong>Play prompt</strong> blocks and connect them to the customer input block. 
<li>If <em>booking hotel or car intent</em> is returned from the bot flow, play the corresponding prompt (“OK, will book it for you”) and initiate booking (in this walkthrough, just hang up after the prompt).</li> 
<li>However, if <em>escalation</em> intent is returned (caused by the sentiment analysis results in the bot), play the prompt (“OK, transferring to an agent”) and initiate the transfer.</li> 
</ul> </li> 
<li>Save and publish the contact flow.</li> 
</ol> 
<p>As a result, you have a contact flow with a single customer input step and a text-to-speech prompt that uses the Amazon Lex bot. You expect one of the three intents returned:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/contact-flow.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/11/contact-flow.png" /></a></p> 
<p>Edit the phone number to associate the contact flow that you just created. It is now ready for testing. Call the phone number and check how your contact flow works.</p> 
<b>Cleanup</b> 
<p>Don’t forget to delete all the resources created during this walkthrough to avoid incurring any more costs:</p> 
<li>Amazon Connect instance</li> 
<li>Amazon Lex bot</li> 
<li>Lambda function</li> 
<li>IAM role <em>LexSentimentAnalysisLambdaRole</em></li> 
<b>Summary</b> 
<p>In this walkthrough, you implemented sentiment analysis with a Lambda function. The function can be integrated into Amazon Lex and, as a result, into Amazon Connect. This approach gives you the flexibility to analyze user input and then act. You may find the following potential use cases of this approach to be of interest:</p> 
<li>Extend the Lambda function to identify “hot” topics in the user input even if the sentiment is not negative and take action proactively. For example, switch to an escalation intent if a user mentioned “where is my order,” which may signal potential frustration.</li> 
<li>Use <a title="undefined" href="https://github.com/aws/amazon-connect-streams" target="null">Amazon Connect Streams</a> to provide agent sentiment analysis results along with call transfer. Enable service tailored towards particular customer needs and sentiments.</li> 
<li>Route calls to agents based on both skill set and sentiment.</li> 
<li>Prioritize calls based on sentiment using multiple Amazon Connect queues instead of transferring directly to an agent.</li> 
<li>Monitor quality and flag for review contact flows that result in high overall negative sentiment.</li> 
<li>Implement sentiment and AI/ML based call analysis, such as a real-time recommendation engine. For more details, see <a title="undefined" href="https://aws.amazon.com/machine-learning/" target="null">Machine Learning on AWS</a>.</li> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4245');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Node.js 8.10 runtime now available in AWS Lambda</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2018-04-02T17:00:21+00:00">02 APR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/programing-language/node-js/" title="View all posts in Node.js*"><span property="articleSection">Node.js*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/node-js-8-10-runtime-now-available-in-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4122" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4122&amp;disqus_title=Node.js+8.10+runtime+now+available+in+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/node-js-8-10-runtime-now-available-in-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4122');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Ed Lima, AWS Solutions Architect</em></p> 
<p>We are excited to announce that you can&nbsp;now develop your <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> functions using the Node.js 8.10 runtime, which is the current Long Term Support (LTS) version of Node.js. Start using this new version today by specifying a runtime parameter value of <strong>nodejs8.10</strong> when creating or updating functions.</p> 
<b>Supporting async/await</b> 
<p>The Lambda programming model for Node.js 8.10 now supports defining a function handler using the async/await pattern.</p> 
<p>Asynchronous or non-blocking calls are an inherent and important part of applications, as user and human interfaces are asynchronous by nature. If you decide to have a coffee with a friend, you usually order the coffee then start or continue a conversation with your friend while the coffee is getting ready. You don’t wait for the coffee to be ready before you start talking. These activities are asynchronous, because you can start one and then move to the next without waiting for completion. Otherwise, you’d delay (or block) the&nbsp;start of the next activity.</p> 
<p>Asynchronous calls used to be handled in Node.js using <em>callbacks</em>. That presented problems when they were nested within other callbacks in multiple levels, making the code difficult to maintain and understand.</p> 
<p><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Using_promises">Promises</a>&nbsp;were implemented to try to solve issues caused by “callback hell.” They allow asynchronous operations to call their own methods and handle what happens when a call is successful or when it fails. As your requirements become more complicated, even promises become harder to work with and may still end up complicating your code.</p> 
<p>Async/await is the new way of handling asynchronous operations in Node.js, and makes for simpler, easier, and cleaner code for non-blocking calls. It still uses promises but a callback is returned directly from the asynchronous function, just as if it were a synchronous blocking function.</p> 
<p>Take for instance the following Lambda function to get the current account settings, using the Node.js 6.10 runtime:</p> 
<code class="lang-js">let AWS = require('aws-sdk');
let lambda = new AWS.Lambda();
exports.handler = (event, context, callback) =&gt; {
let getAccountSettingsPromise = lambda.getAccountSettings().promise();
getAccountSettingsPromise.then(
(data) =&gt; {
callback(null, data);
},
(err) =&gt; {
console.log(err);
callback(err);
}
);
};</code> 
<p>With the new&nbsp;Node.js 8.10 runtime, there are new handler types that can be declared with the “async” keyword or can return a promise directly.</p> 
<p>This is how the same function looks like using async/await with Node.js 8.10:</p> 
<code class="lang-js">let AWS = require('aws-sdk');
let lambda = new AWS.Lambda();
exports.handler = async (event) =&gt; {
return await lambda.getAccountSettings().promise() ;
};</code> 
<p>Alternatively, you could have the handler return a promise directly:</p> 
<code class="lang-js">let AWS = require('aws-sdk');
let lambda = new AWS.Lambda();
exports.handler = (event) =&gt; {
return new Promise((resolve, reject) =&gt; {
lambda.getAccountSettings(event)
.then((data) =&gt; {
resolve data;
})
.catch(reject);
});
};</code> 
<p>The new handler types are alternatives to the callback pattern, which is still fully supported.</p> 
<p>All three functions return the same results. However, in the new runtime with async/await, all callbacks in the code are gone, which makes it easier to read. This is especially true for those less familiar with promises.</p> 
<code class="lang-json">{
&quot;AccountLimit&quot;:{
&quot;TotalCodeSize&quot;:80530636800,
&quot;CodeSizeUnzipped&quot;:262144000,
&quot;CodeSizeZipped&quot;:52428800, 
&quot;ConcurrentExecutions&quot;:1000,
&quot;UnreservedConcurrentExecutions&quot;:1000
},
&quot;AccountUsage&quot;:{
&quot;TotalCodeSize&quot;:52234461,
&quot;FunctionCount&quot;:53
}
}</code> 
<p>Another great advantage of async/await is better error handling. You can use a try/catch block inside the scope of an async function. Even though the function awaits an asynchronous operation, any errors end up in the catch block.</p> 
<p>You can improve your previous Node.js 8.10 function with this trusted try/catch error handling pattern:</p> 
<code class="lang-js">let AWS = require('aws-sdk');
let lambda = new AWS.Lambda();
let data;
exports.handler = async (event) =&gt; {
try {
data = await lambda.getAccountSettings().promise();
}
catch (err) {
console.log(err);
return err;
}
return data;
};</code> 
<p>While you now have a similar number of lines in both runtimes, the code is cleaner and more readable with async/await. It makes the asynchronous calls look more synchronous. However, it is important to notice that the code is still executed the same way as if it were using a callback or promise-based API.</p> 
<b>Backward compatibility</b> 
<p>You may port your existing Node.js 4.3 and 6.10 functions over to Node.js 8.10 by updating the runtime. Node.js 8.10 does include numerous breaking changes from previous Node versions.</p> 
<p>Make sure to review the API changes between Node.js 4.3, 6.10, and Node.js 8.10 to see if there are other changes that might affect your code.&nbsp;We recommend testing that your Lambda function passes internal validation for its behavior when upgrading to the new runtime version.</p> 
<p>You can use Lambda versions/aliases to safely test that your function runs as expected on Node 8.10, before routing production traffic to it.</p> 
<b>New node features</b> 
<p>You can now get better performance when compared to the previous LTS version 6.x (up to 20%). The new V8 6.0 engine comes with Turbofan and the Ignition pipeline, which leads to lower memory consumption and faster startup time across Node.js applications.</p> 
<p>HTTP/2, which is subject to future changes, allows developers to use the new protocol to speed application development and undo many of HTTP/1.1 workarounds to make applications faster, simpler, and more powerful.</p> 
<p>For more information, see the <a href="https://docs.aws.amazon.com/lambda/latest/dg/welcome.html">AWS Lambda Developer Guide</a>.</p> 
<p>Hope you enjoy and… go build with Node.js 8.10!</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4122');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/02/EBS-Light-Blue.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Tag Amazon EBS Snapshots on Creation and Implement Stronger Security Policies</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Woo Kim</span></span> | on 
<time property="datePublished" datetime="2018-04-02T15:59:46+00:00">02 APR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-elastic-block-storage-ebs/" title="View all posts in Amazon Elastic Block Storage (EBS)*"><span property="articleSection">Amazon Elastic Block Storage (EBS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/news/launch/" title="View all posts in Launch*"><span property="articleSection">Launch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/news/" title="View all posts in News*"><span property="articleSection">News*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/tag-amazon-ebs-snapshots-on-creation-and-implement-stronger-security-policies/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4116" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4116&amp;disqus_title=Tag+Amazon+EBS+Snapshots+on+Creation+and+Implement+Stronger+Security+Policies&amp;disqus_url=https://aws.amazon.com/blogs/compute/tag-amazon-ebs-snapshots-on-creation-and-implement-stronger-security-policies/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4116');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This blog was contributed by Rucha Nene, Sr. Product Manager for Amazon EBS</em></p> 
<p>AWS customers use tags to track ownership of resources, implement compliance protocols, control access to resources via <a href="https://aws.amazon.com/iam">IAM </a>policies, and drive their cost accounting processes. Last year, we made tagging for <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances and <a href="https://aws.amazon.com/ebs">Amazon EBS</a> volumes easier by adding the ability to tag these resources upon creation. We are now extending this capability to EBS snapshots.</p> 
<p>Earlier, you could tag your EBS snapshots only after the resource had been created and sometimes, ended up with EBS snapshots in an untagged state if tagging failed. You also could not control the actions that users and groups could take over specific snapshots, or enforce tighter security policies.</p> 
<p>To address these issues, we are making tagging for EBS snapshots more flexible and giving customers more control over EBS snapshots by introducing two new capabilities:</p> 
<li><strong>Tag on creation for EBS snapshots</strong>&nbsp;– You can now specify tags for EBS snapshots as part of the API call that creates the resource or via the Amazon EC2 Console when creating an EBS snapshot.</li> 
<li><strong>Resource-level permission and enforced tag usage –</strong>&nbsp;The <code class="lang-bash">CreateSnapshot</code>, <code class="lang-bash">DeleteSnapshot</code>, and <code class="lang-bash">ModifySnapshotAttrribute</code> API actions now support IAM resource-level permissions. You can now write IAM policies that mandate the use of specific tags when taking actions on EBS snapshots.</li> 
<h3><strong>Tag on creation</strong></h3> 
<p>You can now specify tags for EBS snapshots as part of the API call that creates the resources. The resource creation and the tagging are performed atomically; both must succeed in order for the operation <code class="lang-bash">CreateSnapshot</code> to succeed. You no longer need to build tagging scripts that run after EBS snapshots have been created.</p> 
<p>Here’s how you specify tags when you create an EBS snapshot, using the console:</p> 
<ol> 
<li>Open the Amazon EC2 console at <a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>.</li> 
<li>In the navigation pane, choose <strong>Snapshots</strong>, <strong>Create Snapshot</strong>.</li> 
<li>On the <strong>Create Snapshot</strong> page, select the volume for which to create a snapshot.</li> 
<li>(Optional) Choose <strong>Add tags to your snapshot</strong>. For each tag, provide a tag key and a tag value.</li> 
<li>Choose <strong>Create Snapshot</strong>.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/02/tagoncreation_screenshot.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/04/02/tagoncreation_screenshot.png" /></a></p> 
<p>Using the AWS CLI:</p> 
<code class="lang-json">aws ec2 create-snapshot --volume-id vol-0c0e757e277111f3c --description 'Prod_Backup' --tag-specifications 
'ResourceType=snapshot,Tags=[{Key=costcenter,Value=115},{Key=IsProd,Value=Yes}]'</code> 
<p>To learn more, see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html">Using Tags</a>.</p> 
<h3 class="unlimited-height-code"><strong>Resource-level permissions and enforced tag usage</strong></h3> 
<p><code class="lang-bash">CreateSnapshot</code>, <code class="lang-bash">DeleteSnapshot</code>, and <code class="lang-bash">ModifySnapshotAttribute</code> now support resource-level permissions, which allow you to exercise more control over EBS snapshots. You can write IAM policies that give you precise control over access to resources and let you specify which users are able to create snapshots for a given set of volumes. You can also enforce the use of specific tags to help track resources and achieve more accurate cost allocation reporting.</p> 
<p>For example, here’s a statement that requires that the costcenter tag (with a value of “115”) be present on the volume from which snapshots are being created. It requires that this tag be applied to all newly created snapshots. In addition, it requires that the created snapshots are tagged with&nbsp;User:username for the customer.</p> 
<code class="lang-json">{
&quot;Version&quot;:&quot;2012-10-17&quot;,
&quot;Statement&quot;:[
{
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Action&quot;:&quot;ec2:CreateSnapshot&quot;,
&quot;Resource&quot;:&quot;arn:aws:ec2:us-east-1:123456789012:volume/*&quot;,
&quot;Condition&quot;: {
&quot;StringEquals&quot;:{
&quot;ec2:ResourceTag/costcenter&quot;:&quot;115&quot;
}
}
},
{
&quot;Sid&quot;:&quot;AllowCreateTaggedSnapshots&quot;,
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Action&quot;:&quot;ec2:CreateSnapshot&quot;,
&quot;Resource&quot;:&quot;arn:aws:ec2:us-east-1::snapshot/*&quot;,
&quot;Condition&quot;:{
&quot;StringEquals&quot;:{
&quot;aws:RequestTag/costcenter&quot;:&quot;115&quot;,
&quot;aws:RequestTag/User&quot;:&quot;${aws:username}&quot;
},
&quot;ForAllValues:StringEquals&quot;:{
&quot;aws:TagKeys&quot;:[
&quot;costcenter&quot;,
&quot;User&quot;
]
}
}
},
{
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Action&quot;:&quot;ec2:CreateTags&quot;,
&quot;Resource&quot;:&quot;arn:aws:ec2:us-east-1::snapshot/*&quot;,
&quot;Condition&quot;:{
&quot;StringEquals&quot;:{
&quot;ec2:CreateAction&quot;:&quot;CreateSnapshot&quot;
}
}
}
]
}</code> 
<p>To implement stronger compliance and security policies, you could also restrict access to <a href="https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DeleteSnapshot.html"><code class="lang-bash">DeleteSnapshot</code></a>,&nbsp;if the resource is not tagged with the user’s name. Here’s a statement that allows the deletion of a snapshot only if the snapshot is tagged with User:username&nbsp;for the customer.</p> 
<code class="lang-json">{
&quot;Version&quot;:&quot;2012-10-17&quot;,
&quot;Statement&quot;:[
{
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Action&quot;:&quot;ec2:DeleteSnapshot&quot;,
&quot;Resource&quot;:&quot;arn:aws:ec2:us-east-1::snapshot/*&quot;,
&quot;Condition&quot;:{
&quot;StringEquals&quot;:{
&quot;ec2:ResourceTag/User&quot;:&quot;${aws:username}&quot;
}
}
}
]
}</code> 
<p>To learn more and to see some sample policies, see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policies-for-amazon-ec2.html">IAM Policies for Amazon EC2</a> and <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ExamplePolicies_EC2.html#iam-example-manage-snapshots">Working with Snapshots</a>.</p> 
<h3><strong>Available Now</strong></h3> 
<p>These new features are available now in all AWS Regions. You can start using it today from the <a href="https://console.aws.amazon.com/ec2/">Amazon EC2 Console</a>, <a href="https://aws.amazon.com/cli/">AWS Command Line Interface (CLI)</a>, or the AWS APIs.</p> 
<code class="lang-bash"></code> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4116');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-7.57.21-PM-1260x509.png" /> 
<b class="lb-b blog-post-title" property="name headline">Innovation Flywheels and the AWS Serverless Application Repository</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tim Wagner</span></span> | on 
<time property="datePublished" datetime="2018-03-30T11:56:37+00:00">30 MAR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-cognito/" title="View all posts in Amazon Cognito*"><span property="articleSection">Amazon Cognito*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-dynamodb/" title="View all posts in Amazon DynamoDB*"><span property="articleSection">Amazon DynamoDB*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/innovation-flywheels-and-the-aws-serverless-application-repository/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4096" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4096&amp;disqus_title=Innovation+Flywheels+and+the+AWS+Serverless+Application+Repository&amp;disqus_url=https://aws.amazon.com/blogs/compute/innovation-flywheels-and-the-aws-serverless-application-repository/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4096');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>At AWS, our customers have always been the motivation for our innovation. In turn, we’re committed to helping them accelerate the pace of their own innovation. It was in the spirit of helping our customers achieve their objectives faster that we launched AWS Lambda in 2014, eliminating the burden of server management and enabling AWS developers to focus on business logic instead of the challenges of provisioning and managing infrastructure.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/lambda_speed.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/lambda_speed.jpg" /></a></p> 
<p>In the years since, our customers have built amazing things using Lambda and other serverless offerings, such as&nbsp;<a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a>, <a href="https://aws.amazon.com/cognito/">Amazon Cognito</a>, and <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a>. Together, these services make it easy to build entire applications without the need to provision, manage, monitor, or patch servers. By removing much of the operational drudgery of infrastructure management, we’ve helped our customers become more agile and achieve faster time-to-market for their applications and services. By eliminating cold servers and cold containers with request-based pricing, we’ve also eliminated the high cost of idle capacity and helped our customers achieve dramatically higher utilization and better economics.</p> 
<p>After we launched Lambda, though, we quickly learned an important lesson: A single Lambda function rarely exists in isolation. Rather, many functions are part of serverless applications that collectively deliver customer value. Whether it’s the combination of event sources and event handlers, as serverless web apps that combine APIs with functions for dynamic content with static content repositories, or collections of functions that together provide a microservice architecture, our customers were building and delivering serverless architectures for every conceivable problem. Despite the economic and agility benefits that hundreds of thousands of AWS customers were enjoying with Lambda, we realized there was still more we could do.</p> 
<b>How Customer Feedback Inspired Us to Innovate</b> 
<p>We heard from our customers that getting started—either from scratch or when augmenting their implementation with new techniques or technologies—remained a challenge. When we looked for serverless assets to share, we found stellar examples built by serverless pioneers that represented a multitude of solutions across industries.</p> 
<p>There were apps to facilitate monitoring and logging, to process image and audio files, to create Alexa skills, and to integrate with notification and location services. These apps ranged from “getting started” examples to complete, ready-to-run assets. What was missing, however, was a unified place for customers to discover this diversity of serverless applications and a step-by-step interface to help them configure and deploy them.</p> 
<p>We also heard from customers and partners that building their own ecosystems—ecosystems increasingly composed of functions, APIs, and serverless applications—remained a challenge. They wanted a simple way to share samples, create extensibility, and grow consumer relationships on top of serverless approaches.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-7.57.21-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-7.57.21-PM-1024x413.png" /></a></p> 
<p>We built the <a href="https://aws.amazon.com/serverless/serverlessrepo/">AWS Serverless Application Repository</a> to help solve both of these challenges by offering publishers and consumers of serverless apps a simple, fast, and effective way to share applications and grow user communities around them. Now, developers can easily learn how to apply serverless approaches to their implementation and business challenges by discovering, customizing, and deploying serverless applications directly from the Serverless Application Repository. They can also find libraries, components, patterns, and best practices that augment their existing knowledge, helping them bring services and applications to market faster than ever before.<br /> <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-7.59.53-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-7.59.53-PM-1024x347.png" /></a></p> 
<b>How the AWS Serverless Application Repository Inspires Innovation for All Customers</b> 
<p>Companies that want to create ecosystems, share samples, deliver extensibility and customization options, and complement their existing SaaS services use the Serverless Application Repository as a distribution channel, producing apps that can be easily discovered and consumed by their customers. AWS partners like HERE have introduced their location and transit services to thousands of companies and developers. Partners like Datadog, Splunk, and TensorIoT have showcased monitoring, logging, and IoT applications to the serverless community.</p> 
<p>Individual developers are also publishing serverless applications that push the boundaries of innovation—some have published applications that leverage machine learning to predict the quality of wine while others have published applications that monitor crypto-currencies, instantly build beautiful image galleries, or create fast and simple surveys. All of these publishers are using serverless apps, and the Serverless Application Repository, as the easiest way to share what they’ve built. Best of all, their customers and fellow community members can find and deploy these applications with just a few clicks in the Lambda console. Apps in the Serverless Application Repository are free of charge, making it easy to explore new solutions or learn new technologies.</p> 
<p>Finally, we at AWS continue to publish apps for the community to use. From apps that leverage Amazon Cognito to sync user data across applications to our latest collection of serverless apps that enable users to quickly <a href="https://serverlessrepo.aws.amazon.com/applications?query=financial">execute common financial calculations</a>, we’re constantly looking for opportunities to contribute to community growth and innovation.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-8.04.30-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/Screen-Shot-2018-03-29-at-8.04.30-PM-1024x216.png" /></a></p> 
<p>At AWS, we’re more excited than ever by the growing adoption of serverless architectures and the innovation that services like AWS Lambda make possible. Helping our customers create and deliver new ideas drives us to keep inventing ways to make building and sharing serverless apps even easier. As the number of applications in the Serverless Application Repository grows, so too will the innovation that it fuels for both the owners and the consumers of those apps. With the general availability of the Serverless Application Repository, our customers become more than the engine of our innovation—they become the engine of innovation for one another.</p> 
<p>To browse, discover, deploy, and publish serverless apps in minutes, visit the <a href="https://serverlessrepo.aws.amazon.com/">Serverless Application Repository</a>. Go serverless—and go innovate!</p> 
<p><em>Dr. Tim&nbsp;Wagner is the General Manager of AWS Lambda and Amazon API Gateway.</em></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4096');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/30/microservices-cross-account01.png" /> 
<b class="lb-b blog-post-title" property="name headline">Managing Cross-Account Serverless Microservices</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2018-03-29T07:38:26+00:00">29 MAR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-dynamodb/" title="View all posts in Amazon DynamoDB*"><span property="articleSection">Amazon DynamoDB*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/serverless/" title="View all posts in Serverless*"><span property="articleSection">Serverless*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/managing-cross-account-serverless-microservices/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3929" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3929&amp;disqus_title=Managing+Cross-Account+Serverless+Microservices&amp;disqus_url=https://aws.amazon.com/blogs/compute/managing-cross-account-serverless-microservices/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3929');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Michael Edge,&nbsp;Sr. Cloud Architect – AWS Professional Services</em></p> 
<p>Applications built using a microservices architecture typically result in a number of independent, loosely coupled microservices communicating with each other, synchronously via their APIs and asynchronously via events. These microservices are often owned by different product teams, and these teams may segregate their resources into different AWS accounts for reasons that include security, billing, and resource isolation. This can sometimes result in the following challenges:</p> 
<li><strong>Cross-account deployment:</strong> A single pipeline must deploy a microservice into multiple accounts; for example, a microservice must be deployed to environments such as DEV, QA, and PROD, all in separate accounts.</li> 
<li><strong>Cross-account lookup:</strong> During deployment, a resource deployed into one AWS account may need to refer to a resource deployed in another AWS account.</li> 
<li><strong>Cross-account communication:</strong> Microservices executing in one AWS account may need to communicate with microservices executing in another AWS account.</li> 
<p>In this post, I look at ways to address these challenges using a sample application composed of a web application supported by two serverless microservices. The microservices are owned by different product teams and deployed into different accounts using <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a>, <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a>, and the <a href="https://github.com/awslabs/serverless-application-model">Serverless Application Model</a> (SAM). At runtime, the microservices communicate using an event-driven architecture that requires asynchronous, cross-account communication via an <a href="https://aws.amazon.com/sns/">Amazon Simple Notification Service</a> (Amazon SNS) topic.</p> 
<h3>Sample application</h3> 
<p>First, look at the sample application I use to demonstrate these concepts. In the following overview diagram, you can see the following:</p> 
<li>The entire application consists of three main services: 
<li>A Booking microservice, owned by the Booking account.</li> 
<li>An Airmiles microservice, owned by the Airmiles account.</li> 
<li>A web application that uses the services exposed by both microservices, owned by the Web Channel account.</li> 
</ul> </li> 
<li>The Booking microservice creates flight bookings and publishes booking events to an SNS topic.</li> 
<li>The Airmiles microservice consumes booking events from the SNS topic and uses the booking event to calculate the airmiles associated with the flight booking. It also supports querying airmiles for a specific flight booking.</li> 
<li>The web application allows an end user to make flight bookings, view flights bookings, and view the airmiles associated with a flight booking.</li> 
<li>In the sample application, the Booking and Airmiles microservices are implemented using <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>. Together with <a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a>, <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a>, and SNS, the sample application is completely serverless.</li> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/13/microservices-cross-account01.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/13/microservices-cross-account01.png" /></a> 
<p class="wp-caption-text">Sample Serverless microservices application</p> 
<p>The typical booking flow would be triggered by an end user making a flight booking using the web application, which invokes the Booking microservice via its REST API. The Booking microservice persists the flight booking and publishes the booking event to an SNS topic to enable sharing of the booking with other interested consumers. In this sample application, the Airmiles microservice subscribes to the SNS topic and consumes the booking event, using the booking information to calculate the airmiles. In line with microservices best practices, both the Booking and Airmiles microservices store their information in their own DynamoDB tables, and expose an API (via API Gateway) that is used by the web application.</p> 
<h3>Setup</h3> 
<p>Before you delve into the details of the sample application, get the source code and deploy it.</p> 
<p>Cross-account deployment of Lambda functions using CodePipeline has been previously discussed by my colleague Anuj Sharma in his post, <a href="https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/">Building a Secure Cross-Account Continuous Delivery Pipeline</a>. This sample application builds upon the solution proposed by Anuj, using some of the same scripts and a similar account structure. To make it feasible for you to deploy the sample application, I’ve reduced the number of accounts needed down to three accounts by consolidating some of the services. In the following diagram, you can see the services used by the sample application require three accounts:</p> 
<li><strong>Tools:</strong> A central location for the continuous delivery/deployment services such as CodePipeline and <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a>. To reduce the number of accounts required by the sample application, also deploy the <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a> repositories here, though typically they may belong in a separate Dev account.</li> 
<li><strong>Booking:</strong> Account for the Booking microservice.</li> 
<li><strong>Airmiles:</strong> Account for the Airmiles microservice.</li> 
<p>Without consolidation, the sample application may require up to 10 accounts: one for Tools, and three accounts each for Booking, Airmiles and Web Application (to support the DEV, QA, and PROD environments).</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/13/account-structure01.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/13/account-structure01.png" /></a> 
<p class="wp-caption-text">Account structure for sample application</p> 
<p>To follow the rest of this post, clone the repository in step 1 below. To deploy the application on AWS, follow steps 2 and 3:</p> 
<li>Clone this repository. It contains the <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation templates</a> to use in this walkthrough. <code>git clone <a href="https://github.com/aws-samples/aws-cross-account-serverless-microservices.git">https://github.com/aws-samples/aws-cross-account-serverless-microservices.git</a></code> </li> 
<li>Install the <a href="https://docs.aws.amazon.com/cli/latest/userguide/installing.html">AWS CLI</a>. To prepare your access keys or a role to make calls to AWS, <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">configure the AWS CLI settings</a>.</li> 
<li>Follow the instructions in the repository README to build the CodePipeline pipelines and deploy the microservices and web application.</li> 
<b>Challenge 1: Cross-account deployment using CodePipeline</b> 
<p>Though the Booking pipeline executes in the Tools account, it deploys the Booking Lambda functions into the Booking account.</p> 
<ol> 
<li>In the sample application code repository, open the ToolsAcct/code-pipeline.yaml CloudFormation template.</li> 
<li>Scroll down to the Pipeline resource and look for the DeployToTest pipeline stage (shown below). There are two <a href="https://aws.amazon.com/iam/">AWS Identity and Access Management</a> (IAM) service roles used in this stage that allow cross-account activity. Both of these roles exist in the Booking account: 
<li>Under Actions.RoleArn, find the service role assumed by CodePipeline to execute this pipeline stage in the Booking account. The role referred to by the parameter NonProdCodePipelineActionServiceRole allows access to the CodePipeline artifacts in the S3 bucket in the Tools account, and also access to the <a href="https://aws.amazon.com/kms/">AWS KMS</a> key needed to encrypt/decrypt the artifacts.</li> 
<li>Under Actions.Configuration.RoleArn, find the service role assumed by CloudFormation when it carries out the CHANGE_SET_REPLACE action in the Booking account.</li> 
</ul> </li> 
</ol> 
<p>These roles are created in the CloudFormation template NonProdAccount/toolsacct-codepipeline-cloudformation-deployer.yaml.</p> 
<code class="lang-yaml">- Name: DeployToTest
Actions:
- Name: CreateChangeSetTest
ActionTypeId:
Category: Deploy
Owner: AWS
Version: 1
Provider: CloudFormation
Configuration:
ChangeSetName: !Join ['-',[!Ref ProjectName, 'lambda', 'CS']]
ActionMode: CHANGE_SET_REPLACE
StackName: !Join ['-',[!Ref ProjectName, 'lambda']]
Capabilities: CAPABILITY_NAMED_IAM
TemplatePath: BuildOutput::output-sam-template.yml
TemplateConfiguration: BuildOutput::sam-config.json
RoleArn: !Ref NonProdCloudFormationServiceRole
InputArtifacts:
- Name: BuildOutput
RunOrder: 1
RoleArn: !Ref NonProdCodePipelineActionServiceRole
- Name: DeployChangeSetTest
ActionTypeId:
Category: Deploy
Owner: AWS
Version: 1
Provider: CloudFormation
Configuration:
ChangeSetName: !Join ['-',[!Ref ProjectName, 'lambda', 'CS']]
ActionMode: CHANGE_SET_EXECUTE
StackName: !Join ['-',[!Ref ProjectName, 'lambda']]
InputArtifacts:
- Name: BuildOutput
RunOrder: 2
RoleArn: !Ref NonProdCodePipelineActionServiceRole</code> 
<b>Challenge 2: Cross-account stack lookup using custom resources</b> 
<p>Asynchronous communication is a fairly common pattern in microservices architectures. A publishing microservice publishes an event that consumers may be interested in, without any concern for who those consumers may be.</p> 
<p>In the case of the sample application, the publisher is the Booking microservice, publishing a flight booking event onto an SNS topic that exists in the Booking account. The consumer is the Airmiles microservice in the Airmiles account. To enable the two microservices to communicate, the Airmiles microservice must look up the ARN of the Booking SNS topic at deployment time in order to set up a subscription to it.</p> 
<p>To enable CloudFormation templates to be reused, you are not <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-name.html">hardcoding</a> resource names in the templates. Because you allow CloudFormation to generate a resource name for the Booking SNS topic, the Airmiles microservice CloudFormation template must look up the SNS topic name at stack creation time. It’s not possible to use <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html">cross-stack references</a>, as these can’t be used across different accounts.</p> 
<p>However, you can use a CloudFormation <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">custom resource</a> to achieve the same outcome. This is discussed in the next section. Using a custom resource to look up stack exports in another stack does not create a dependency between the two stacks, unlike cross-stack references. A stack dependency would prevent one stack being deleted if another stack depended upon it. For more information, see <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html">Fn::ImportValue</a>.</p> 
<h3>Using a Lambda function as a custom resource to look up the booking SNS topic</h3> 
<p>The sample application uses a Lambda function as a custom resource. This approach is discussed in <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">AWS Lambda-backed Custom Resources</a>.<br /> Walk through the sample application and see how the custom resource is used to list the stack export variables in another account and return these values to the calling AWS CloudFormation stack. Examine each of the following aspects of the custom resource:</p> 
<ol> 
<li>Deploying the custom resource.</li> 
<li>Custom resource IAM role.</li> 
<li>A calling CloudFormation stack uses the custom resource.</li> 
<li>The custom resource assumes a role in another account.</li> 
<li>The custom resource obtains the stack exports from all stacks in the other account.</li> 
<li>The custom resource returns the stack exports to the calling CloudFormation stack.</li> 
<li>The custom resource handles all the event types sent by the calling CloudFormation stack.</li> 
</ol> 
<h3>Step1: Deploying the custom resource</h3> 
<p>The custom Lambda function is deployed using SAM, as are the Lambda functions for the Booking and Airmiles microservices. See the CustomLookupExports resource in Custom/custom-lookup-exports.yml.</p> 
<h3>Step2: Custom resource IAM role</h3> 
<p>The CustomLookupExports resource in Custom/custom-lookup-exports.yml executes using the CustomLookupLambdaRole IAM role. This role allows the custom Lambda function to assume a cross account role that is created along with the other cross account roles in the NonProdAccount/toolsacct-codepipeline-cloudformation-deployer.yml. See the resource CustomCrossAccountServiceRole.</p> 
<h3>Step3: The CloudFormation stack uses the custom resource</h3> 
<p>The Airmiles microservice is created by the Airmiles CloudFormation template Airmiles/sam-airmile.yml, a SAM template that uses the custom Lambda resource to look up the ARN of the Booking SNS topic. The custom resource is specified by the <strong>CUSTOMLOOKUP</strong> resource, and the <strong>PostAirmileFunction</strong> resource uses the custom resource to look up the ARN of the SNS topic and create a subscription to it.</p> 
<p>Because the Airmiles Lambda function is going to subscribe to an SNS topic in another account, it must grant the SNS topic in the Booking account permissions to invoke the Airmiles Lambda function in the Airmiles account whenever a new event is published. Permissions are granted by the <strong>LambdaResourcePolicy</strong> resource.</p> 
<h3>Step4: The custom resource assumes a role in another account</h3> 
<p>When the Lambda custom resource is invoked by the Airmiles CloudFormation template, it must assume a role in the Booking account (see Step 2) in order to query the stack exports in that account.</p> 
<p>This can be seen in Custom/custom-lookup-exports.py, where the <a href="http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html">AWS Simple Token Service</a> (AWS STS) is used to obtain a temporary access key to allow access to resources in the account referred to by the environment variable: ‘CUSTOM_CROSS_ACCOUNT_ROLE_ARN’. This environment variable is defined in the Custom/custom-lookup-exports.yml CloudFormation template, and refers to the role created in Step 2.</p> 
<code class="lang-python">sts = boto3.client('sts')
assumedRole = sts.assume_role(
RoleArn=os.environ['CUSTOM_CROSS_ACCOUNT_ROLE_ARN'],
RoleSessionName='LambdaCloudFormationSession'
)
credentials = assumedRole['Credentials']
accessKey = credentials['AccessKeyId']
secretAccessKey = credentials['SecretAccessKey']
sessionToken = credentials['SessionToken']
cfn = boto3.client('cloudformation',  
region_name=os.environ['AWS_DEFAULT_REGION'], 
aws_access_key_id=accessKey,
aws_secret_access_key=secretAccessKey, 
aws_session_token=sessionToken)</code> 
<h3>Step5: The custom resource obtains the stack exports from all stacks in the other account</h3> 
<p>The function get_exports() in Custom/custom-lookup-exports.py uses the AWS SDK to list all the stack exports in the Booking account.</p> 
<h3>Step 6: The custom resource returns the stack exports to the calling CloudFormation stack</h3> 
<p>The calling CloudFormation template, Airmiles/sam-airmile.yml, uses the custom resource to look up a stack export with the name of booking-lambda-BookingTopicArn. This is exported by the CloudFormation template Booking/sam-booking.yml.</p> 
<code class="lang-yaml">PostAirmileFunction:
Type: AWS::Serverless::Function
Properties:
Handler: post-airmiles-lambda.handler
Runtime: python2.7
Policies: AmazonDynamoDBFullAccess
Events:
SubmitBooking:
Type: SNS
Properties:
Topic:
!GetAtt CUSTOMLOOKUP.booking-lambda-BookingTopicArn</code> 
<h3>Step7: The custom resource handles all the event types sent by the calling CloudFormation stack</h3> 
<p>Custom resources used in a stack are called whenever the stack is created, updated, or deleted. They must therefore handle CREATE, UPDATE, and DELETE stack events. If your custom resource fails to send a SUCCESS or FAILED notification for each of these stack events, your stack may become stuck in a state such as <em>CREATE_IN_PROGRESS</em> or <em>UPDATE_ROLLBACK_IN_PROGRESS</em>. To handle this cleanly, use the <a href="https://github.com/awslabs/aws-cloudformation-templates/tree/master/community/custom_resources/python_custom_resource_helper">crhelper</a> custom resource helper. This strongly encourages you to handle the CREATE, UPDATE, and DELETE CloudFormation stack events.</p> 
<b>Challenge 3: Cross-account SNS subscription</b> 
<p>The SNS topic is specified as a resource in the Booking/sam-booking.yml CloudFormation template. To allow an event published to this topic to trigger the Airmiles Lambda function, permissions must be granted on both the Booking SNS topic and the Airmiles Lambda function. This is discussed in the next section.</p> 
<h3>Permissions on booking SNS topic</h3> 
<p>SNS topics support resource-based policies, which allow a policy to be attached directly to a resource specifying who can access the resource. This policy can specify which accounts can access a resource, and what actions those accounts are allowed to perform. This is the same approach as used by a small number of AWS services, such as <a href="https://aws.amazon.com/s3/">Amazon S3</a>, KMS, <a href="https://aws.amazon.com/sqs/">Amazon SQS</a>, and Lambda. In Booking/sam-booking.yml, the SNS topic policy allows resources in the Airmiles account (referenced by the parameter <em>NonProdAccount</em> in the following snippet) to subscribe to the Booking SNS topic:</p> 
<code class="lang-yaml">BookingTopicPolicy:
Type: AWS::SNS::TopicPolicy
Properties:
PolicyDocument:
Id: BookingTopicPolicy
Version: '2012-10-17'
Statement:
- Sid: BookingTopicPolicy-stmt1
Effect: Allow
Principal:
AWS:
- !Sub arn:aws:iam::${AWS::AccountId}:root
Action:
- sns:Publish
- sns:RemovePermission
- sns:SetTopicAttributes
- sns:DeleteTopic
- sns:ListSubscriptionsByTopic
- sns:GetTopicAttributes
- sns:Receive
- sns:AddPermission
- sns:Subscribe
Resource: !Ref BookingTopic
- Sid: BookingTopicPolicy-stmt2
Effect: Allow
Principal:
AWS:
- !Sub arn:aws:iam::${NonProdAccount}:root
Action:
- sns:Subscribe
- sns:ListSubscriptionsByTopic
Resource: !Ref BookingTopic
Topics:
- !Ref BookingTopic</code> 
<h3>Permissions on the Airmiles Lambda function</h3> 
<p>The Airmiles microservice is created by the Airmiles/sam-airmile.yml CloudFormation template. The template uses SAM to specify the Lambda functions together with their associated API Gateway configurations. SAM hides much of the complexity of deploying Lambda functions from you.</p> 
<p>For instance, by adding an ‘Events’ resource to the PostAirmileFunction in the CloudFormation template, the Airmiles Lambda function is triggered by an event published to the Booking SNS topic. SAM creates the SNS subscription, as well as the permissions necessary for the SNS subscription to trigger the Lambda function.</p> 
<code class="lang-yaml">PostAirmileFunction:
Type: AWS::Serverless::Function
Properties:
Handler: post-airmiles-lambda.handler
Runtime: python2.7
Policies: AmazonDynamoDBFullAccess
Events:
SubmitBooking:
Type: SNS
Properties:
Topic:
!GetAtt CUSTOMLOOKUP.booking-lambda-BookingTopicArn
Environment:
Variables: 
TABLE_NAME: !Ref AirmileDBTable</code> 
<p>However, the Lambda permissions generated automatically by SAM are not sufficient for cross-account SNS subscription, which means you must specify an additional Lambda permissions resource in the CloudFormation template. LambdaResourcePolicy, in the following snippet, specifies that the Booking SNS topic is allowed to invoke the Lambda function PostAirmileFunction in the Airmiles account.</p> 
<code class="lang-yaml">LambdaResourcePolicy:
Type: AWS::Lambda::Permission
Properties:
FunctionName: !GetAtt PostAirmileFunction.Arn
Principal: sns.amazonaws.com
Action: lambda:InvokeFunction
SourceArn : !GetAtt CUSTOMLOOKUP.booking-lambda-BookingTopicArn</code> 
<b>Summary</b> 
<p>In this post, I showed you how product teams can use CodePipeline to deploy microservices into different AWS accounts and different environments such as DEV, QA, and PROD. I also showed how, at runtime, microservices in different accounts can communicate securely with each other using an asynchronous, event-driven architecture. This allows product teams to maintain their own AWS accounts for billing and security purposes, while still supporting communication with microservices in other accounts owned by other product teams.</p> 
<b>Acknowledgments</b> 
<p>Thanks are due to the following people for their help in preparing this post:</p> 
<li>Kevin Yung for the great web application used in the sample application</li> 
<li>Jay McConnell for <a href="https://github.com/awslabs/aws-cloudformation-templates/tree/master/community/custom_resources/python_custom_resource_helper">crhelper</a>, the custom resource helper used with the CloudFormation custom resources</li> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3929');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/12/Pricing.png" /> 
<b class="lb-b blog-post-title" property="name headline">New Amazon EC2 Spot pricing model: Simplified purchasing without bidding and fewer interruptions</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Roshni Pary</span></span> | on 
<time property="datePublished" datetime="2018-03-13T11:50:50+00:00">13 MAR 2018</time> | 
<a href="https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4071" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4071&amp;disqus_title=New+Amazon+EC2+Spot+pricing+model%3A+Simplified+purchasing+without+bidding+and+fewer+interruptions&amp;disqus_url=https://aws.amazon.com/blogs/compute/new-amazon-ec2-spot-pricing/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4071');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by Deepthi Chelupati and Roshni Pary</em></p> 
<p>Amazon EC2 Spot Instances offer spare compute capacity in the AWS Cloud at steep discounts. Customers—including Yelp, NASA JPL, FINRA, and Autodesk—use Spot Instances to reduce costs and get faster results. Spot Instances provide acceleration, scale, and deep cost savings to big data workloads, containerized applications such as web services, test/dev, and many types of HPC and batch jobs.</p> 
<p>At re:Invent 2017, we <a href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-streamlined-access-to-spot-capacity-smooth-price-changes-instance-hibernation/">launched</a> a new pricing model that simplified the Spot purchasing experience. The new model gives you predictable prices that adjust slowly over days and weeks, with typical savings of 70-90% over On-Demand. With the previous pricing model, some of you had to invest time and effort to analyze historical prices to determine your bidding strategy and maximum bid price. Not anymore.</p> 
<h3><strong>How does the new pricing model work?</strong></h3> 
<p>You don’t have to bid for Spot Instances in the new pricing model, and you just pay the Spot price that’s in effect for the current hour for the instances that you launch. It’s that simple. Now you can request Spot capacity just like you would request On-Demand capacity, without having to spend time analyzing market prices or setting a maximum bid price.</p> 
<p>Previously, Spot Instances were terminated in ascending order of bids, and the Spot price was set to the highest unfulfilled bid. The market prices fluctuated frequently because of this. In the new model, the Spot prices are more predictable, updated less frequently, and are determined by supply and demand for Amazon EC2 spare capacity, not bid prices. You can <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances-history.html">find the price </a>that’s in effect for the current hour in the <a href="https://console.aws.amazon.com/ec2/">EC2 console</a>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/12/Pricing.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/12/Pricing.png" /></a></p> 
<p>As you can see from the above Spot Instance Pricing History graph (available in the EC2 console under <strong>Spot Requests</strong>), Spot prices were volatile before the pricing model update. However, after the pricing model update, prices are more predictable and change less frequently.</p> 
<p>In the new model, you still have the option to further control costs by submitting a “maximum price” that you are willing to pay in the console when you request Spot Instances:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/12/Pricing-2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/12/Pricing-2.png" /></a></p> 
<p>You can also set your maximum price in EC2 RunInstances or RequestSpotFleet API calls, or in command line requests:</p> 
<code class="lang-clike">$ aws ec2 run-instances --instance-market-options 
'{&quot;MarketType&quot;:&quot;Spot&quot;, &quot;SpotOptions&quot;: {&quot;SpotPrice&quot;: &quot;0.12&quot;}}' \
--image-id ami-1a2b3c4d --count 1 --instance-type c4.2xlarge</code> 
<p>The default maximum price is the On-Demand price and you can continue to set a maximum Spot price of up to 10x the On-Demand price. That means, if you have been running applications on Spot Instances and use the RequestSpotInstances or RequestSpotFleet operations, you can continue to do so. The new Spot pricing model is backward compatible and you do not need to make any changes to your existing applications.</p> 
<h3><strong>Fewer interruptions</strong></h3> 
<p>Spot Instances receive a two-minute interruption notice when these instances are about to be reclaimed by EC2, because EC2 needs the capacity back. We have significantly reduced the interruptions with the new pricing model. Now instances are not interrupted because of higher competing bids, and you can enjoy longer workload runtimes. The typical frequency of interruption for Spot Instances in the last 30 days was less than 5% on average.</p> 
<p>To reduce the impact of interruptions and optimize Spot Instances, diversify and run your application across multiple capacity pools. Each instance family, each instance size, in each Availability Zone, in every Region is a separate Spot pool. You can use the RequestSpotFleet API operation to launch thousands of Spot Instances and diversify resources automatically. To further reduce the impact of interruptions, you can also set up Spot Instances and Spot Fleets to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html#interruption-behavior">respond</a> to an interruption notice by stopping or hibernating rather than terminating instances when capacity is no longer available.</p> 
<p>Spot Instances are now available in 18 Regions and 51 Availability Zones, and offer 100s of instance options. We have eliminated bidding, simplified the pricing model, and have made it easy to get started with Amazon EC2 Spot Instances for you to take advantage of the largest pool of cost-effective compute capacity in the world. See the <a href="https://aws.amazon.com/ec2/spot/">Spot Instances</a> detail page for more information and create your Spot Instance <a href="https://console.aws.amazon.com/ec2sp/v1/spot/?#">here</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4071');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Message Filtering Operators for Numeric Matching, Prefix Matching, and Blacklisting in Amazon SNS</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2018-03-12T14:52:06+00:00">12 MAR 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/" title="View all posts in Messaging*"><span property="articleSection">Messaging*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/message-filtering-operators-for-numeric-matching-prefix-matching-and-blacklisting-in-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4060" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4060&amp;disqus_title=Message+Filtering+Operators+for+Numeric+Matching%2C+Prefix+Matching%2C+and+Blacklisting+in+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/message-filtering-operators-for-numeric-matching-prefix-matching-and-blacklisting-in-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4060');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This blog was contributed by Otavio Ferreira, Software Development Manager for Amazon SNS</em></p> 
<p>Message filtering simplifies the overall pub/sub messaging architecture by offloading message filtering logic from subscribers, as well as message routing logic from publishers. The initial launch of message filtering provided a basic operator that was based on exact string comparison. For more information, see <a href="https://aws.amazon.com/blogs/compute/simplify-pubsub-messaging-with-amazon-sns-message-filtering/">Simplify Your Pub/Sub Messaging with Amazon SNS Message Filtering</a>.</p> 
<p>Today, AWS is announcing an additional set of filtering operators that bring even more power and flexibility to your pub/sub messaging use cases.</p> 
<p><span id="more-4060"></span></p> 
<b>Message filtering operators</b> 
<p><a href="https://aws.amazon.com/sns/">Amazon SNS</a> now supports both numeric and string matching. Specifically, string matching operators allow for exact, prefix, and “anything-but” comparisons, while numeric matching operators allow for exact and range comparisons, as outlined below. Numeric matching operators work for values between -10e9 and +10e9 inclusive, with five digits of accuracy right of the decimal point.</p> 
<li><strong>Exact matching on string values (Whitelisting):</strong> Subscription filter policy&nbsp; &nbsp;<code>{&quot;sport&quot;: [&quot;rugby&quot;]}</code> matches message attribute <code>{&quot;sport&quot;: &quot;rugby&quot;}</code> only.</li> 
<li><strong>Anything-but matching on string values (Blacklisting):&nbsp;</strong>Subscription filter policy <code>{&quot;sport&quot;: [{&quot;anything-but&quot;: &quot;rugby&quot;}]}</code> matches message attributes such as <code>{&quot;sport&quot;: &quot;baseball&quot;}</code> and <code>{&quot;sport&quot;: &quot;basketball&quot;}</code> and <code>{&quot;sport&quot;: &quot;football&quot;}</code> but not <code>{&quot;sport&quot;: &quot;rugby&quot;}</code></li> 
<li><strong>Prefix matching on string values:</strong> Subscription filter policy <code>{&quot;sport&quot;: [{&quot;prefix&quot;: &quot;bas&quot;}]}</code> matches message attributes such as <code>{&quot;sport&quot;: &quot;baseball&quot;}</code> and <code>{&quot;sport&quot;: &quot;basketball&quot;}</code></li> 
<li><strong>Exact matching on numeric values:</strong> Subscription filter policy <code>{&quot;balance&quot;: [{&quot;numeric&quot;: [&quot;=&quot;, 301.5]}]}</code> matches message attributes <code>{&quot;balance&quot;: 301.500}</code> and <code>{&quot;balance&quot;: 3.015e2}</code></li> 
<li><strong>Range matching on numeric values:</strong> Subscription filter policy <code>{&quot;balance&quot;: [{&quot;numeric&quot;: [&quot;&lt;&quot;, 0]}]}</code> matches negative numbers only, and <code>{&quot;balance&quot;: [{&quot;numeric&quot;: [&quot;&gt;&quot;, 0, &quot;&lt;=&quot;, 150]}]}</code> matches any positive number up to 150.</li> 
<p>As usual, you may apply the “AND” logic by appending multiple keys in the subscription filter policy, and the “OR” logic by appending multiple values for the same key, as follows:</p> 
<li><strong>AND logic:</strong> Subscription filter policy <code>{&quot;sport&quot;: [&quot;rugby&quot;], &quot;language&quot;: [&quot;English&quot;]}</code> matches only messages that carry both attributes <code>{&quot;sport&quot;: &quot;rugby&quot;}</code> and <code>{&quot;language&quot;: &quot;English&quot;}</code></li> 
<li><strong>OR logic:</strong> Subscription filter policy <code>{&quot;sport&quot;: [&quot;rugby&quot;, &quot;football&quot;]}</code> matches messages that carry either the attribute <code>{&quot;sport&quot;: &quot;rugby&quot;}</code> or <code>{&quot;sport&quot;: &quot;football&quot;}</code></li> 
<b>Message filtering operators in action</b> 
<p>Here’s how this new set of filtering operators works. The following example is based on a pharmaceutical company that develops, produces, and markets a variety of prescription drugs, with research labs located in Asia Pacific and Europe. The company built an internal procurement system to manage the purchasing of lab supplies (for example, chemicals and utensils), office supplies (for example, paper, folders, and markers) and tech supplies (for example, laptops, monitors, and printers) from global suppliers.</p> 
<p>This distributed system is composed of the four following subsystems:</p> 
<li>A requisition system that presents the catalog of products from suppliers, and takes orders from buyers</li> 
<li>An approval system for orders targeted to Asia Pacific labs</li> 
<li>Another approval system for orders targeted to European labs</li> 
<li>A fulfillment system that integrates with shipping partners</li> 
<p>As shown in the following diagram, the company leverages AWS messaging services to integrate these distributed systems.</p> 
<li>Firstly, an SNS topic named “Orders” was created to take all orders placed by buyers on the requisition system.</li> 
<li>Secondly, two <a href="https://aws.amazon.com/sqs/">Amazon SQS</a> queues, named “Lab-Orders-AP” and “Lab-Orders-EU” (for Asia Pacific and Europe respectively), were created to backlog orders that are up for review on the approval systems.</li> 
<li>Lastly, an SQS queue named “Common-Orders” was created to backlog orders that aren’t related to lab supplies, which can already be picked up by shipping partners on the fulfillment system.</li> 
<p>The company also uses <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> functions to automatically process lab supply orders that don’t require approval or which are invalid.</p> 
<p style="text-align: center"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/09/sns-numeric-filtering-policies.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/09/sns-numeric-filtering-policies.png" /></a></p> 
<p>In this example, because different types of orders have been published to the SNS topic, the subscribing endpoints have had to set advanced filter policies on their SNS subscriptions, to have SNS automatically filter out orders they can’t deal with.</p> 
<p>As depicted in the above diagram, the following five filter policies have been created:</p> 
<li>The SNS subscription that points to the SQS queue “Lab-Orders-AP” sets a filter policy that matches lab supply orders, with a total value greater than $1,000, and that target Asia Pacific labs only. These more expensive transactions require an approver to review orders placed by buyers.</li> 
<li>The SNS subscription that points to the SQS queue “Lab-Orders-EU” sets a filter policy that matches lab supply orders, also with a total value greater than $1,000, but that target European labs instead.</li> 
<li>The SNS subscription that points to the Lambda function “Lab-Preapproved” sets a filter policy that only matches lab supply orders that aren’t as expensive, up to $1,000, regardless of their target lab location. These orders simply don’t require approval and can be automatically processed.</li> 
<li>The SNS subscription that points to the Lambda function “Lab-Cancelled” sets a filter policy that only matches lab supply orders with total value of $0 (zero), regardless of their target lab location. These orders carry no actual items, obviously need neither approval nor fulfillment, and as such can be automatically canceled.</li> 
<li>The SNS subscription that points to the SQS queue “Common-Orders” sets a filter policy that blacklists lab supply orders. Hence, this policy matches only office and tech supply orders, which have a more streamlined fulfillment process, and require no approval, regardless of price or target location.</li> 
<p>After the company finished building this advanced pub/sub architecture, they were then able to launch their internal procurement system and allow buyers to begin placing orders. The diagram above shows six example orders published to the SNS topic. Each order contains message attributes that describe the order, and cause them to be filtered in a different manner, as follows:</p> 
<li>Message #1 is a lab supply order, with a total value of $15,700 and targeting a research lab in Singapore. Because the value is greater than $1,000, and the location “Asia-Pacific-Southeast” matches the prefix “Asia-Pacific-“, this message matches the first SNS subscription and is delivered to SQS queue “Lab-Orders-AP”.</li> 
<li>Message #2 is a lab supply order, with a total value of $1,833 and targeting a research lab in Ireland. Because the value is greater than $1,000, and the location “Europe-West” matches the prefix “Europe-“, this message matches the second SNS subscription and is delivered to SQS queue “Lab-Orders-EU”.</li> 
<li>Message #3 is a lab supply order, with a total value of $415. Because the value is greater than $0 and less than $1,000, this message matches the third SNS subscription and is delivered to Lambda function “Lab-Preapproved”.</li> 
<li>Message #4 is a lab supply order, but with a total value of $0. Therefore, it only matches the fourth SNS subscription, and is delivered to Lambda function “Lab-Cancelled”.</li> 
<li>Messages #5 and #6 aren’t lab supply orders actually; one is an office supply order, and the other is a tech supply order. Therefore, they only match the fifth SNS subscription, and are both delivered to SQS queue “Common-Orders”.</li> 
<p>Although each message only matched a single subscription, each was tested against the filter policy of every subscription in the topic. Hence, depending on which attributes are set on the incoming message, the message might actually match multiple subscriptions, and multiple deliveries will take place. Also, it is important to bear in mind that subscriptions with no filter policies catch every single message published to the topic, as a blank filter policy equates to a catch-all behavior.</p> 
<b>Summary</b> 
<p>Amazon SNS allows for both string and numeric filtering operators. As explained in this post, string operators allow for exact, prefix, and “anything-but” comparisons, while numeric operators allow for exact and range comparisons. These advanced filtering operators bring even more power and flexibility to your pub/sub messaging functionality and also allow you to simplify your architecture further by removing even more logic from your subscribers.</p> 
<p>Message filtering can be implemented easily with existing <a href="https://aws.amazon.com/tools/#sdk">AWS SDKs</a> by applying message and subscription attributes across all SNS supported protocols (<a href="https://aws.amazon.com/sqs">Amazon SQS</a>, <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, HTTP, SMS, email, and mobile push). SNS filtering operators for numeric matching, prefix matching, and blacklisting are available now in all AWS Regions, for no extra charge.</p> 
<p>To experiment with these new filtering operators yourself, and continue learning, try the 10-minute Tutorial <a href="https://aws.amazon.com/getting-started/tutorials/filter-messages-published-to-topics/">Filter Messages Published to Topics</a>. For more information, see <a href="https://docs.aws.amazon.com/sns/latest/dg/message-filtering.html">Filtering Messages with Amazon SNS</a> in the SNS documentation.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4060');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Taking Advantage of Amazon EC2 Spot Instance Interruption Notices</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chad Schmutzer</span></span> | on 
<time property="datePublished" datetime="2018-03-09T13:59:28+00:00">09 MAR 2018</time> | 
<a href="https://aws.amazon.com/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3990" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3990&amp;disqus_title=Taking+Advantage+of+Amazon+EC2+Spot+Instance+Interruption+Notices&amp;disqus_url=https://aws.amazon.com/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3990');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot Instances</a> are spare compute capacity in the AWS Cloud available to you at steep discounts compared to On-Demand prices. The only difference between On-Demand Instances and Spot Instances is that Spot Instances can be interrupted by Amazon EC2 with two minutes of notification when EC2 needs the capacity back.</p> 
<p>Customers have been taking advantage of Spot Instance interruption notices available via the <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">instance metadata service</a> since <a href="https://aws.amazon.com/blogs/aws/new-ec2-spot-instance-termination-notices/">January 2015</a> to orchestrate their workloads seamlessly around any potential interruptions. Examples include saving the state of a job, detaching from a load balancer, or draining containers. Needless to say, the two-minute Spot Instance interruption notice is a powerful tool when using Spot Instances.</p> 
<p>In <a href="https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-ec2-spot-two-minute-warning-is-now-available-via-amazon-cloudwatch-events/">January 2018</a>, the Spot Instance interruption notice also became available as an event in <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a>. This allows targets such as <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction-function.html">AWS Lambda functions</a> or <a href="https://docs.aws.amazon.com/sns/latest/dg/welcome.html">Amazon SNS topics</a> to process Spot Instance interruption notices by creating a CloudWatch Events rule to monitor for the notice.</p> 
<p>In this post, I walk through an example use case for taking advantage of Spot Instance interruption notices in CloudWatch Events to automatically deregister Spot Instances from an <a href="https://aws.amazon.com/elasticloadbalancing">Elastic Load Balancing</a> Application Load Balancer.</p> 
<b>Architecture</b> 
<p>In this reference architecture, you use an <a href="https://aws.amazon.com/cloudformation/aws-cloudformation-templates/">AWS CloudFormation template</a> to deploy the following:</p> 
<li>An <a href="https://aws.amazon.com/vpc/">Amazon Virtual Private Cloud</a> (Amazon VPC)&nbsp;with subnets in two Availability Zones</li> 
<li>An <a href="https://aws.amazon.com/elasticloadbalancing/details/#details">Application Load Balancer</a> with a listener and target group</li> 
<li>An Amazon CloudWatch Events rule</li> 
<li>An AWS Lambda function</li> 
<li>An Amazon Simple Notification Service (SNS) topic</li> 
<li>Associated <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html">IAM policies and roles</a> for all of the above</li> 
<p>After the AWS CloudFormation stack deployment is complete, you then create an Amazon EC2&nbsp;<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html">Spot Fleet</a> request diversified across both Availability Zones and use a couple of recent Spot Fleet features: <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/spot-fleet-can-now-auto-attach-instances-to-your-load-balancers-and-scale-down-to-0-target-capacity/">Elastic Load Balancing integration</a> and&nbsp;<a href="https://aws.amazon.com/about-aws/whats-new/2017/07/tag-your-spot-fleet-ec2-instances/">Tagging Spot Fleet Instances</a>.</p> 
<p>When any of the Spot Instances receives an interruption notice, Spot Fleet sends the event to CloudWatch Events. The CloudWatch Events rule then notifies both targets, the Lambda function and SNS topic. The Lambda function detaches the Spot Instance from the Application Load Balancer target group, taking advantage of nearly a full two minutes of connection draining before the instance is interrupted. The SNS topic also receives a message, and is provided as an example for the reader to use as an exercise.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/24/interruption_notices_arch_diagram.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/24/interruption_notices_arch_diagram.jpg" /></a> 
<p class="wp-caption-text">EC2 Spot Instance Interruption Notices Reference Architecture Diagram</p> 
<b>Walkthrough</b> 
<p>To complete this walkthrough, have the <a href="https://aws.amazon.com/cli/">AWS CLI</a> installed and configured, as well as the ability to launch CloudFormation stacks.</p> 
<h3>Launch the stack</h3> 
<p>Go ahead and launch the CloudFormation stack. You can check it out from GitHub, or grab the <a href="https://github.com/awslabs/ec2-spot-labs/blob/master/ec2-spot-interruption-notice-cloudwatch-events/ec2-spot-interruption-notice-cloudwatch-events.yaml">template directly</a>. In this post, I use the stack name “<em>spot-spin-cwe</em>“, but feel free to use any name you like. Just remember to change it in the instructions.</p> 
<code class="lang-bash">$ git clone https://github.com/awslabs/ec2-spot-labs.git
$ aws cloudformation create-stack --stack-name spot-spin-cwe \
--template-body file://ec2-spot-labs/ec2-spot-interruption-notice-cloudwatch-events/ec2-spot-interruption-notice-cloudwatch-events.yaml \
--capabilities CAPABILITY_IAM</code> 
<p>You should receive a StackId value in return, confirming the stack is launching.</p> 
<code class="lang-json">{
&quot;StackId&quot;: &quot;arn:aws:cloudformation:us-east-1:123456789012:stack/spot-spin-cwe/083e7ad0-0ade-11e8-9e36-500c219ab02a&quot;
}</code> 
<h3>Review the details</h3> 
<p>Here are the details of the architecture being launched by the stack.</p> 
<h4>IAM permissions</h4> 
<p>Give permissions to a few components in the architecture:</p> 
<li>The Lambda function</li> 
<li>The CloudWatch Events rule</li> 
<li>The Spot Fleet</li> 
<p>The Lambda function needs basic Lambda function execution permissions so that it can write logs to CloudWatch Logs. You can use the AWS managed policy for this. It also needs to describe EC2 tags as well as deregister targets within Elastic Load Balancing. You can create a custom policy for these.</p> 
<code class="lang-yaml">lambdaFunctionRole:
Properties:
AssumeRolePolicyDocument:
Statement:
- Action:
- sts:AssumeRole
Effect: Allow
Principal:
Service:
- lambda.amazonaws.com
Version: 2012-10-17
ManagedPolicyArns:
- arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
Path: /
Policies:
- PolicyDocument:
Statement:
- Action: elasticloadbalancing:DeregisterTargets
Effect: Allow
Resource: '*'
- Action: ec2:DescribeTags
Effect: Allow
Resource: '*'
Version: '2012-10-17'
PolicyName:
Fn::Join:
- '-'
- - Ref: AWS::StackName
- lambdaFunctionRole
Type: AWS::IAM::Role</code> 
<p>Allow CloudWatch Events to call the Lambda function and publish to the SNS topic.</p> 
<code class="lang-yaml">lambdaFunctionPermission:
Properties:
Action: lambda:InvokeFunction
FunctionName:
Fn::GetAtt:
- lambdaFunction
- Arn
Principal: events.amazonaws.com
SourceArn:
Fn::GetAtt:
- eventRule
- Arn
Type: AWS::Lambda::Permission</code> 
<code class="lang-yaml">snsTopicPolicy:
DependsOn:
- snsTopic
Properties:
PolicyDocument:
Id:
Fn::GetAtt:
- snsTopic
- TopicName
Statement:
- Action: sns:Publish
Effect: Allow
Principal:
Service:
- events.amazonaws.com
Resource:
Ref: snsTopic
Version: '2012-10-17'
Topics:
- Ref: snsTopic
Type: AWS::SNS::TopicPolicy</code> 
<p>Finally, Spot Fleet needs permissions to request Spot Instances, tag, and register targets in Elastic Load Balancing. You can tap into an AWS managed policy for this.</p> 
<code class="lang-yaml">spotFleetRole:
Properties:
AssumeRolePolicyDocument:
Statement:
- Action:
- sts:AssumeRole
Effect: Allow
Principal:
Service:
- spotfleet.amazonaws.com
Version: 2012-10-17
ManagedPolicyArns:
- arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole
Path: /
Type: AWS::IAM::Role</code> 
<h4>Elastic Load Balancing timeout delay</h4> 
<p>Because you are taking advantage of the two-minute Spot Instance notice, you can tune the Elastic Load Balancing target group deregistration timeout delay to match. When a target is deregistered from the target group, it is put into connection draining mode for the length of the timeout delay: &nbsp;120 seconds to equal the two-minute notice.</p> 
<code class="lang-yaml">loadBalancerTargetGroup:
DependsOn:
- vpc
Properties:
HealthCheckIntervalSeconds: 5
HealthCheckPath: /
HealthCheckTimeoutSeconds: 2
Port: 80
Protocol: HTTP
TargetGroupAttributes:
- Key: deregistration_delay.timeout_seconds
Value: 120
UnhealthyThresholdCount: 2
VpcId:
Ref: vpc
Type: AWS::ElasticLoadBalancingV2::TargetGroup</code> 
<h4>CloudWatch Events rule</h4> 
<p>To capture the Spot Instance interruption notice being published to CloudWatch Events, create a rule with two targets: the Lambda function and the SNS topic.</p> 
<code class="lang-yaml">eventRule:
DependsOn:
- snsTopic
Properties:
Description: Events rule for Spot Instance Interruption Notices
EventPattern:
detail-type:
- EC2 Spot Instance Interruption Warning
source:
- aws.ec2
State: ENABLED
Targets:
- Arn:
Ref: snsTopic
Id:
Fn::GetAtt:
- snsTopic
- TopicName
- Arn:
Fn::GetAtt:
- lambdaFunction
- Arn
Id:
Ref: lambdaFunction
Type: AWS::Events::Rule</code> 
<h4>Lambda function</h4> 
<p>The Lambda function does the heavy lifting for you. The details of the CloudWatch event are published to the Lambda function, which then uses boto3 to make a couple of AWS API calls. The first call is to describe the EC2 tags for the Spot Instance, filtering on a key of “TargetGroupArn”. If this tag is found, the instance is then deregistered from the target group ARN stored as the value of the tag.</p> 
<code class="lang-python">import boto3
def handler(event, context):
instanceId = event['detail']['instance-id']
instanceAction = event['detail']['instance-action']
try:
ec2client = boto3.client('ec2')
describeTags = ec2client.describe_tags(Filters=[{'Name': 'resource-id','Values':[instanceId],'Name':'key','Values':['loadBalancerTargetGroup']}])
except:
print(&quot;No action being taken. Unable to describe tags for instance id:&quot;, instanceId)
return
try:
elbv2client = boto3.client('elbv2')
deregisterTargets = elbv2client.deregister_targets(TargetGroupArn=describeTags['Tags'][0]['Value'],Targets=[{'Id':instanceId}])
except:
print(&quot;No action being taken. Unable to deregister targets for instance id:&quot;, instanceId)
return
print(&quot;Detaching instance from target:&quot;)
print(instanceId, describeTags['Tags'][0]['Value'], deregisterTargets, sep=&quot;,&quot;)
return</code> 
<h4>SNS topic</h4> 
<p>Finally, you’ve created an SNS topic as an example target. For example, you could subscribe an email address to the SNS topic in order to receive email notifications when a Spot Instance interruption notice is received.</p> 
<code class="lang-yaml">snsTopic:
Properties:
DisplayName: SNS Topic for EC2 Spot Instance Interruption Notices
Type: AWS::SNS::Topic</code> 
<h3>Create a Spot Fleet request</h3> 
<p>To proceed to creating your Spot Fleet request, use some of the resources that the CloudFormation stack created, to populate the Spot Fleet request launch configuration. You can find the values in the outputs values of the CloudFormation stack:</p> 
<code class="lang-bash">$ aws cloudformation describe-stacks --stack-name spot-spin-cwe</code> 
<p>Using the output values of the CloudFormation stack, update the following values in the Spot Fleet request configuration:</p> 
<li><strong>%spotFleetRole%</strong></li> 
<li><strong>%publicSubnet1%</strong></li> 
<li><strong>%publicSubnet2%</strong></li> 
<li><strong>%loadBalancerTargetGroup%</strong> (in two places)</li> 
<p>Be sure to also replace <strong>%amiId%</strong> with the latest <a href="https://aws.amazon.com/amazon-linux-ami/#Amazon_Linux_AMI_IDs">Amazon Linux AMI</a> for your region and <strong>%keyName%</strong> with your environment.</p> 
<code class="lang-json">{
&quot;AllocationStrategy&quot;: &quot;diversified&quot;,
&quot;IamFleetRole&quot;: &quot;%spotFleetRole%&quot;,
&quot;LaunchSpecifications&quot;: [
{
&quot;ImageId&quot;: &quot;%amiId%&quot;,
&quot;InstanceType&quot;: &quot;c4.large&quot;,
&quot;Monitoring&quot;: {
&quot;Enabled&quot;: true
},
&quot;KeyName&quot;: &quot;%keyName%&quot;,
&quot;SubnetId&quot;: &quot;%publicSubnet1%,%publicSubnet2%&quot;,
&quot;UserData&quot;: &quot;IyEvYmluL2Jhc2gKeXVtIC15IHVwZGF0ZQp5dW0gLXkgaW5zdGFsbCBodHRwZApjaGtjb25maWcgaHR0cGQgb24KaW5zdGFuY2VpZD0kKGN1cmwgaHR0cDovLzE2OS4yNTQuMTY5LjI1NC9sYXRlc3QvbWV0YS1kYXRhL2luc3RhbmNlLWlkKQplY2hvICJoZWxsbyBmcm9tICRpbnN0YW5jZWlkIiA+IC92YXIvd3d3L2h0bWwvaW5kZXguaHRtbApzZXJ2aWNlIGh0dHBkIHN0YXJ0Cg==&quot;,
&quot;TagSpecifications&quot;: [
{
&quot;ResourceType&quot;: &quot;instance&quot;,
&quot;Tags&quot;: [
{
&quot;Key&quot;: &quot;loadBalancerTargetGroup&quot;,
&quot;Value&quot;: &quot;%loadBalancerTargetGroup%&quot;
}
]
}
]
}
],
&quot;TargetCapacity&quot;: 2,
&quot;TerminateInstancesWithExpiration&quot;: true,
&quot;Type&quot;: &quot;maintain&quot;,
&quot;ReplaceUnhealthyInstances&quot;: true,
&quot;InstanceInterruptionBehavior&quot;: &quot;terminate&quot;,
&quot;LoadBalancersConfig&quot;: {
&quot;TargetGroupsConfig&quot;: {
&quot;TargetGroups&quot;: [
{
&quot;Arn&quot;: &quot;%loadBalancerTargetGroup%&quot;
}
]
}
}
}</code> 
<p>Save the configuration and place the Spot Fleet request:</p> 
<code class="lang-bash">$ aws ec2 request-spot-fleet --spot-fleet-request-config file://sfr.json</code> 
<p>You should receive a <strong>SpotFleetRequestId</strong> in return, confirming the request:</p> 
<code class="lang-json">{
&quot;SpotFleetRequestId&quot;: &quot;sfr-3cec4927-9d86-4cc5-a4f0-faa996c841b7&quot;
}</code> 
<p>You can confirm that the Spot Fleet request was fulfilled by checking that <strong>ActivityStatus</strong> is “fulfilled”, or by checking that <strong>FulfilledCapacity</strong> is greater than or equal to <strong>TargetCapacity</strong>, while describing the request:</p> 
<code class="lang-bash">$ aws ec2 describe-spot-fleet-requests --spot-fleet-request-id sfr-3cec4927-9d86-4cc5-a4f0-faa996c841b7</code> 
<code class="lang-json">{
&quot;SpotFleetRequestConfigs&quot;: [
{
&quot;ActivityStatus&quot;: &quot;fulfilled&quot;,
&quot;CreateTime&quot;: &quot;2018-02-08T01:23:16.029Z&quot;,
&quot;SpotFleetRequestConfig&quot;: {
&quot;AllocationStrategy&quot;: &quot;diversified&quot;,
&quot;ExcessCapacityTerminationPolicy&quot;: &quot;Default&quot;,
&quot;FulfilledCapacity&quot;: 2.0,
…
&quot;TargetCapacity&quot;: 2,
…
}
]
}</code> 
<p>Next, you can confirm that the Spot Instances have been registered with the Elastic Load Balancing target group and are in a healthy state:</p> 
<code class="lang-bash">$ aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/spot-loadB-1DZUVWL720VS6/26456d12cddbf23a</code> 
<code class="lang-json">{
&quot;TargetHealthDescriptions&quot;: [
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-056c95d9dd6fde892&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;healthy&quot;
}
},
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-06c4c47228fd999b8&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;healthy&quot;
}
}
]
}</code> 
<h3>Test</h3> 
<p>In order to test, you can take advantage of the fact that any interruption action that Spot Fleet takes on a Spot Instance results in a Spot Instance interruption notice being provided. Therefore, you can simply decrease the target size of your Spot Fleet from 2 to 1. The instance that is interrupted receives the interruption notice:</p> 
<code class="lang-bash">$ aws ec2 modify-spot-fleet-request --spot-fleet-request-id sfr-3cec4927-9d86-4cc5-a4f0-faa996c841b7 --target-capacity 1</code> 
<code class="lang-json">{
&quot;Return&quot;: true
}</code> 
<p>As soon as the interruption notice is published to CloudWatch Events, the Lambda function triggers and detaches the instance from the target group, effectively putting the instance in a draining state.</p> 
<code class="lang-bash">$ aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/spot-loadB-1DZUVWL720VS6/26456d12cddbf23a</code> 
<code class="lang-json">{
&quot;TargetHealthDescriptions&quot;: [
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-0c3dcd78efb9b7e53&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;draining&quot;,
&quot;Reason&quot;: &quot;Target.DeregistrationInProgress&quot;,
&quot;Description&quot;: &quot;Target deregistration is in progress&quot;
}
},
{
&quot;Target&quot;: {
&quot;Id&quot;: &quot;i-088c91a66078b4299&quot;,
&quot;Port&quot;: 80
},
&quot;HealthCheckPort&quot;: &quot;80&quot;,
&quot;TargetHealth&quot;: {
&quot;State&quot;: &quot;healthy&quot;
}
}
]
}</code> 
<b>Conclusion</b> 
<p>In conclusion, Amazon EC2 Spot Instance interruption notices are an extremely powerful tool when taking advantage of Amazon EC2 Spot Instances in your workloads, for tasks such as saving state, draining connections, and much more. I’d love to hear how you are using them in your own environment!</p> 
<table style="margin-left: 30px"> 
<tbody style="padding-left: 30px"> 
<tr style="padding-left: 30px"> 
<td style="padding-left: 30px"> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/07/internal-cdn.amazon-2.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/03/07/internal-cdn.amazon-2.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Chad Schmutzer</span><br /> Solutions Architect</p> 
<td style="padding-left: 30px">Chad Schmutzer is a Solutions Architect at Amazon Web Services based in Pasadena, CA. As an extension of the Amazon EC2 Spot Instances team, Chad helps customers significantly reduce the cost of running their applications, growing their compute capacity and throughput without increasing budget, and enabling new types of cloud computing applications.</td> 
</tr> 
</tbody> 
</table> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3990');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/27/xray_k8s-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Application Tracing on Kubernetes with AWS X-Ray</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2018-02-28T11:08:06+00:00">28 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-x-ray/" title="View all posts in AWS X-Ray*"><span property="articleSection">AWS X-Ray*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/application-tracing-on-kubernetes-with-aws-x-ray/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-4002" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=4002&amp;disqus_title=Application+Tracing+on+Kubernetes+with+AWS+X-Ray&amp;disqus_url=https://aws.amazon.com/blogs/compute/application-tracing-on-kubernetes-with-aws-x-ray/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4002');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong><em>This post was contributed by Christoph Kassen, AWS Solutions Architect</em></strong></p> 
<p>With the emergence of <a href="https://aws.amazon.com/microservices/">microservices architectures</a>, the number of services that are part of a web application has increased a lot. It’s not unusual anymore to build and operate hundreds of separate microservices, all as part of the same application.</p> 
<p>Think of a typical e-commerce application that displays products, recommends related items, provides search and faceting capabilities, and maintains a shopping cart. Behind the scenes, many more services are involved, such as clickstream tracking, ad display, targeting, and logging. When handling a single user request, many of these microservices are involved in responding. Understanding, analyzing, and debugging the landscape is becoming complex.</p> 
<p><a href="https://aws.amazon.com/xray/">AWS X-Ray</a> provides application-tracing functionality, giving deep insights into all microservices deployed. With X-Ray, every request can be traced as it flows through the involved microservices. This provides your DevOps teams the insights they need to understand how your services interact with their peers and enables them to analyze and debug issues much faster.</p> 
<p>With microservices architectures, every service should be self-contained and use the technologies best suited for the problem domain. Depending on how the service is built, it is deployed and hosted differently.</p> 
<p>One of the most popular choices for packaging and deploying microservices at the moment is <a href="https://aws.amazon.com/what-are-containers/">containers</a>. The application and its dependencies are clearly defined, the container can be built on CI infrastructure, and the deployment is simplified greatly. Container schedulers, such as Kubernetes and Amazon Elastic Container Service (<a href="https://aws.amazon.com/ecs/">Amazon ECS</a>), greatly simplify deploying and running containers at scale.</p> 
<b>Running X-Ray on Kubernetes</b> 
<p>Kubernetes is an open-source container management platform that automates deployment, scaling, and management of containerized applications.</p> 
<p>This post shows you how to run X-Ray on top of Kubernetes to provide application tracing capabilities to services hosted on a Kubernetes cluster. Additionally, X-Ray also works for applications hosted on <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, <a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a>, <a href="https://aws.amazon.com/ec2">Amazon EC2</a>, and even when building services with <a href="https://aws.amazon.com/lambda">AWS Lambda</a> functions. This flexibility helps you pick the technology you need while still being able to trace requests through all of the services running within your AWS environment.</p> 
<p>The complete code, including a simple Node.js based demo application is available in the corresponding&nbsp;<a href="https://github.com/aws-samples/aws-xray-kubernetes">aws-xray-kubernetes</a> GitHub repository, so you can quickly get started with X-Ray.</p> 
<p>The sample application within the repository consists of two simple microservices, Service-A and Service-B. The following architecture diagram shows how each service is deployed with two Pods on the Kubernetes cluster:</p> 
<ol> 
<li>Requests are sent to the Service-A from clients.</li> 
<li>Service-A then contacts Service-B.</li> 
<li>The requests are serviced by Service-B.</li> 
<li>Service-B adds a random delay to each request to show different response times in X-Ray.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/architecture-diagram-1024x569.jpeg" /></p> 
<p>To test out the sample applications on your own Kubernetes cluster use the Dockerfiles provided in the GitHub repository, build the two containers, push them to a container registry and apply the yaml configuration with kubectl to your Kubernetes cluster.</p> 
<b>Prerequisites</b> 
<p>If you currently do not have a cluster running within your AWS environment, take a look at Amazon Elastic Container Service for Kubernetes (<a href="https://aws.amazon.com/eks">Amazon EKS</a>), or use the instructions from the <a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/">Manage Kubernetes Clusters on AWS Using Kops</a> blog post to spin up a self-managed Kubernetes cluster.</p> 
<b>Security Setup for X-Ray</b> 
<p>The nodes in the Kubernetes cluster hosting web application Pods need IAM permissions so that the Pods hosting the X-Ray daemon can send traces to the X-Ray service backend.</p> 
<p>The easiest way is to set up a new IAM policy allowing all worker nodes within your Kubernetes cluster to write data to X-Ray. In the IAM console or <a href="https://aws.amazon.com/cli/">AWS CLI</a>, create a new policy like the following:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;xray:PutTraceSegments&quot;,
&quot;xray:PutTelemetryRecords&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:iam::000000000000:instance-profile/nodes.k8s.cluster.local &quot;
]
}
]
}</code> 
<p>Adjust the AWS account ID within the resource. Give the policy a descriptive name, such as <code class="lang-json">k8s-nodes-XrayWriteAccess</code>.</p> 
<p>Next, attach the policy to the instance profile for the Kubernetes worker nodes. Therefore, select the IAM role assigned to your worker instances (check the EC2 console if you are unsure) and attach the IAM policy created earlier to it. You can attach the IAM permissions directly from the command line with the following command:</p> 
<p><code class="lang-bash">aws iam attach-role-policy --role-name k8s-nodes --policy-arn arn:aws:iam::000000000000:policy/k8s-nodes-XrayWriteAccess</code></p> 
<b>Build the X-Ray daemon Docker image</b> 
<p>The X-Ray daemon is available as a single, statically compiled binary that can be downloaded directly from the AWS website.</p> 
<p>The first step is to create a Docker container hosting the X-Ray daemon binary and exposing port 2000 via UDP. The daemon is either configured via command line parameters or a configuration file. The most important option is to set the listen port to the correct IP address so that tracing requests from application Pods can be accepted.</p> 
<p>To build your own Docker image containing the X-Ray daemon, use the Dockerfile shown below.</p> 
<code class="lang-bash"># Use Amazon Linux Version 1
FROM amazonlinux:1
# Download latest 2.x release of X-Ray daemon
RUN yum install -y unzip &amp;&amp; \
cd /tmp/ &amp;&amp; \
curl https://s3.dualstack.us-east-2.amazonaws.com/aws-xray-assets.us-east-2/xray-daemon/aws-xray-daemon-linux-2.x.zip &gt; aws-xray-daemon-linux-2.x.zip &amp;&amp; \
unzip aws-xray-daemon-linux-2.x.zip &amp;&amp; \
cp xray /usr/bin/xray &amp;&amp; \
rm aws-xray-daemon-linux-2.x.zip &amp;&amp; \
rm cfg.yaml
# Expose port 2000 on udp
EXPOSE 2000/udp
ENTRYPOINT [&quot;/usr/bin/xray&quot;]
# No cmd line parameters, use default configuration
CMD [''] </code> 
<p>This container image is based on Amazon Linux which results in a small container image. To build and tag the container image run <code class="lang-bash">docker build -t xray:latest</code>.</p> 
<b>Create an Amazon ECR repository</b> 
<p>Create a repository in Amazon&nbsp;Elastic Container Registry (<a href="https://aws.amazon.com/ecr">Amazon ECR</a>)&nbsp;to hold your X-Ray Docker image. Your Kubernetes cluster uses this repository to pull the image from upon deployment of the X-Ray Pods.</p> 
<p>Use the following CLI command to create your repository or alternatively the AWS Management Console:</p> 
<p><code class="lang-bash">aws ecr create-repository --repository-name xray-daemon</code></p> 
<p>This creates a repository named xray-daemon for you and prints the repository URI used when pushing the image.</p> 
<p>After the container build completes, push it to your ECR repository with the following push commands for the repository that you just created.</p> 
<p><code class="lang-bash">docker tag xray-daemon:latest 000000000000.dkr.ecr.eu-west-1.amazonaws.com/xray-daemon:latest</code><br /> <code class="lang-bash">docker push 000000000000.dkr.ecr.eu-west-1.amazonaws.com/xray-daemon:latest</code></p> 
<p>The resulting repository should look similar to the one shown in the following screenshot.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/ecr-repo-1024x524.jpeg" /></p> 
<p>You can automate this processes using <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a> and <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a>. For instructions how to build Docker containers and push them to ECR automatically, see <a href="https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.html">Docker Sample for AWS CodeBuild</a>.</p> 
<b>Deploy the X-Ray daemon to Kubernetes</b> 
<p>Make sure that you configure the <code class="lang-bash">kubectl</code> tool properly for your cluster to be able to deploy the X-Ray Pod onto your Kubernetes cluster. After the X-Ray Pods are deployed to your Kubernetes cluster, applications can send tracing information to the X-Ray daemon on their host. The biggest advantage is that you do not need to provide X-Ray as a sidecar container alongside your application. This simplifies the configuration and deployment of your applications and overall saves resources on your cluster.</p> 
<p>To deploy the X-Ray daemon as Pods onto your Kubernetes cluster, run the following from the cloned GitHub repository:</p> 
<p><code class="lang-bash">kubectl apply -f xray-k8s-daemonset.yaml</code></p> 
<p>This deploys and maintains an X-Ray Pod on each worker node, which is accepting tracing data from your microservices and routing it to one of the X-Ray pods. When deploying the container using a DaemonSet, the X-Ray port is also exposed directly on the host. This way, clients can connect directly to the daemon on your node. This avoids unnecessary network traffic going across your cluster.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-diagram-1024x362.jpeg" /></p> 
<b>Connecting to the X-Ray daemon</b> 
<p>To integrate application tracing with your applications, use the X-Ray SDK for one of the supported programming languages:</p> 
<li>Java</li> 
<li>Node.js</li> 
<li>.NET (Framework and Core)</li> 
<li>Go</li> 
<li>Python</li> 
<p>The SDKs provide classes and methods for generating and sending trace data to the X-Ray daemon. Trace data includes information about incoming HTTP requests served by the application, and calls that the application makes to downstream services using the AWS SDK or HTTP clients.</p> 
<p>By default, the X-Ray SDK expects the daemon to be available on <code class="lang-bash">127.0.0.1:2000</code>. This needs to be changed in this setup, as the daemon is not part of each Pod but hosted within its own Pod.</p> 
<p>The deployed X-Ray DaemonSet exposes all Pods via the Kubernetes service discovery, so applications can use this endpoint to discover the X-Ray daemon. If you deployed to the default namespace, the endpoint is:</p> 
<p><code class="lang-bash">xray-service.default</code></p> 
<p>Applications now need to set the daemon address either with the <code class="lang-bash">AWS_XRAY_DAEMON_ADDRESS</code> environment variable (preferred) or directly within the SDK setup code:</p> 
<p><code class="lang-js">AWSXRay.setDaemonAddress('xray-service.default:2000');</code></p> 
<p>To set up the environment variable, include the following information in your Kubernetes application deployment description YAML. That exposes the X-Ray service address via the environment variable, where it is picked up automatically by the SDK.</p> 
<code class="lang-yaml">env:
- name: AWS_XRAY_DAEMON_ADDRESS 
value: xray-service.default</code> 
<b>Sending tracing information to AWS X-Ray</b> 
<p>Sending tracing information from your application is straightforward with the X-Ray SDKs. The example code below serves as a starting point to instrument your application with traces. Take a look at the two sample applications in the GitHub repository to see how to send traces from Service A to Service B. The diagram below visualizes the flow of requests between the services.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-architecture-1024x589.jpeg" /></p> 
<p>Because your application is running within containers, enable both the EC2Plugin and ECSPlugin, which gives you information about the Kubernetes node hosting the Pod as well as the container name. Despite the name ECSPlugin, this plugin gives you additional information about your container when running your application on Kubernetes.</p> 
<code class="lang-java">var app = express();
//...
var AWSXRay = require('aws-xray-sdk');
AWSXRay.config([XRay.plugins.EC2Plugin, XRay.plugins.ECSPlugin]);
app.use(AWSXRay.express.openSegment('defaultName'));  //required at the start of your routes
app.get('/', function (req, res) {
res.render('index');
});
app.use(AWSXRay.express.closeSegment());  //required at the end of your routes / first in error handling routes</code> 
<p>For more information about all options and possibilities to instrument your application code, see the <a href="https://aws.amazon.com/documentation/xray/">X-Ray documentation</a>&nbsp;page for the corresponding SDK information.</p> 
<p>The picture below shows the resulting service map that provides insights into the flow of requests through the microservice landscape. You can drill down here into individual traces and see which path each request has taken.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-servicemap-1024x403.jpeg" /></p> 
<p>From the service map, you can drill down into individual requests and see where they originated from and how much time was spent in each service processing the request.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-trace-1024x337.jpeg" /></p> 
<p>You can also view details about every individual segment of the trace by clicking on it. This displays more details.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/28/xray-segment-info-1024x776.jpeg" /></p> 
<p>On the <strong>Resources</strong> tab, you will see the Kubernetes Pod picked up by the ECSPlugin, which handled the request, as well as the instance that Pod was running on.</p> 
<b>Summary</b> 
<p>In this post, I shared how to deploy and run X-Ray on an existing Kubernetes cluster. Using tracing gives you deep insights into your applications to ease analysis and spot potential problems early. With X-Ray, you get these insights for all your applications running on AWS, no matter if they are hosted on <a href="https://aws.amazon.com/ecs">Amazon ECS</a>, <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, or a Kubernetes cluster.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-4002');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/Picture1-3.png" /> 
<b class="lb-b blog-post-title" property="name headline">Building an Immersive VR Streaming Solution on AWS</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chad Schmutzer</span></span> | on 
<time property="datePublished" datetime="2018-02-23T13:41:32+00:00">23 FEB 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/ar-vr/" title="View all posts in AR &amp; VR"><span property="articleSection">AR &amp; VR</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/" title="View all posts in Networking &amp; Content Delivery*"><span property="articleSection">Networking &amp; Content Delivery*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/open-source/" title="View all posts in Open Source*"><span property="articleSection">Open Source*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-an-immersive-vr-streaming-solution-on-aws/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3568" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3568&amp;disqus_title=Building+an+Immersive+VR+Streaming+Solution+on+AWS&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-an-immersive-vr-streaming-solution-on-aws/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3568');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post was contributed by:</p> 
<table style="height: 124px" width="275"> 
<tbody> 
<tr> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Konstantin Wilms</span><br />Solutions Architect</p> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/profile.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/profile.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Shawn Przybilla</span><br />Solutions Architect</p> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon-2.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/internal-cdn.amazon-2.jpg" /></a> 
<p class="wp-caption-text"><span style="color: #000000">Chad Schmutzer</span><br />Solutions Architect</p> 
</tr> 
</tbody> 
</table> 
<p>With the explosion in virtual reality (VR) technologies over the past few years, we’ve had an increasing number of customers ask us for advice and best practices around deploying their VR-based products and service offerings on the AWS Cloud. It soon became apparent that while the VR ecosystem is large in both scope and depth of types of workloads (gaming, e-medicine, security analytics, live streaming events, etc.), many of the workloads followed repeatable patterns, with storage and delivery of live and on-demand immersive video at the top of the list.</p> 
<p>Looking at consumer trends, the desire for live and on-demand immersive video is fairly self-explanatory. VR has ushered in convenient and low-cost access for consumers and businesses to a wide variety of options for consuming content, ranging from browser playback of live and on-demand 360&ordm; video, all the way up to positional tracking systems with a high degree of immersion. All of these scenarios contain one lowest common denominator: &nbsp;video.</p> 
<p>Which brings us to the topic of this post. We set out to build a solution that could support both live and on-demand events, bring with it a high degree of scalability, be flexible enough to support transformation of video if required, run at a low cost, and use open-source software to every extent possible.</p> 
<p>In this post, we describe the reference architecture we created to solve this challenge, using&nbsp;<a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot Instances</a>,&nbsp;<a href="https://aws.amazon.com/s3/">Amazon S3</a>,&nbsp;<a href="https://aws.amazon.com/elasticloadbalancing/">Elastic Load Balancing</a>,&nbsp;<a href="https://aws.amazon.com/cloudfront/">Amazon CloudFront</a>,&nbsp;<a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a>, and&nbsp;<a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch</a>, with open-source software such as&nbsp;<a href="https://www.nginx.com/">NGINX</a>,&nbsp;<a href="https://www.ffmpeg.org/">FFMPEG</a>,&nbsp;and JavaScript-based client-side playback technologies. We step you through deployment of the solution and how the components work, as well as the capture, processing, and playback of the underlying live and on-demand immersive media streams.</p> 
<p>This&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch">GitHub repository</a>&nbsp;includes the source code necessary to follow along. We’ve also provided a&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch/tree/master/workshop">self-paced workshop</a>, from&nbsp;<a href="https://reinvent.awsevents.com/">AWS re:Invent</a> 2017&nbsp;that breaks down this architecture even further. If you experience any issues or would like to suggest an enhancement, please use the&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch/issues">GitHub issue tracker</a>.</p> 
<b>Prerequisites</b> 
<p>As a side note, you’ll also need a few additional components to take best advantage of the infrastructure:</p> 
<li>A camera/capture device capable of encoding and streaming&nbsp;<a href="http://www.adobe.com/devnet/rtmp.html">RTMP</a>&nbsp;video</li> 
<li>A browser to consume the content.</li> 
<p>You’re going to generate HTML5-compatible video (Apple HLS to be exact), but there are many other native iOS and Android options for consuming the media that you create. It’s also worth noting that your playback device should support projection of your input stream. We’ll talk more about that in the next section.</p> 
<b>How does immersive media work?</b> 
<p>At its core, any flavor of media, be that audio or video, can be viewed with some level of immersion. The ability to interact passively or actively with the content brings with it a further level of immersion. When you look at VR devices with rotational and positional tracking, you naturally need more than an ability to interact with a flat plane of video. The challenge for any creative thus becomes a tradeoff between immersion features (degrees of freedom, monoscopic 2D or stereoscopic 3D, resolution, framerate) and overall complexity.</p> 
<p>Where can you start from a simple and effective point of view, that enables you to build out a fairly modular solution and test it? There are a few areas we chose to be prescriptive with our solution.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/Picture1-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/10/Picture1-3.png" /></a> 
<p class="wp-caption-text">Source capture from the Ricoh Theta S</p> 
<p>First, monoscopic 360-degree video is currently one of the most commonly consumed formats on consumer devices. We explicitly chose to focus on this format, although the infrastructure is not limited to it. More on this later.</p> 
<p>Second, if you look at most consumer-level cameras that provide live streaming ability, and even many professional rigs, there are at least two lenses or cameras at a minimum. The figure above illustrates a single capture from a Ricoh Theta S in monoscopic 2D. The left image captures 180 degrees of the field of view, and the right image captures the other 180 degrees.</p> 
<p>For this post, we chose a typical midlevel camera (the&nbsp;<a href="https://theta360.com/en/about/theta/s.html">Ricoh Theta S</a>), and used a laptop with open-source software (<a href="https://obsproject.com/">Open Broadcaster Software</a>) to encode and stream the content. Again, the solution infrastructure is not limited to this particular brand of camera. Any camera or encoder that outputs 360&ordm; video and encodes to H264+AAC with an RTMP transport will work.</p> 
<p>Third, capturing and streaming multiple camera feeds brings additional requirements around stream synchronization and cost of infrastructure. There is also a requirement to stitch media in real time, which can be CPU and GPU-intensive. Many devices and platforms do this either on the device, or via outboard processing that sits close to the camera location. If you stitch and deliver a single stream, you can save the costs of infrastructure and bitrate/connectivity requirements. We chose to keep these aspects on the encoder side to save on cost and reduce infrastructure complexity.</p> 
<p>Last, the most common delivery format that requires little to no processing on the infrastructure side is equirectangular projection, as per the above figure. By stitching and unwrapping the spherical coordinates into a flat plane, you can easily deliver the video exactly as you would with any other live or on-demand stream. The only caveat is that resolution and bit rate are of utmost importance. The higher you can push these (high bit rate @ 4K resolution), the more immersive the experience is for viewers. This is due to the increase in sharpness and reduction of compression artifacts.</p> 
<p>Knowing that we would be transcoding potentially at 4K on the source camera, but in a format that could be transmuxed without an encoding penalty on the origin servers, we implemented a pass-through for the highest bit rate, and elected to only transcode lower bitrates. This requires some level of configuration on the source encoder, but saves on cost and infrastructure. Because you can conform the source stream, you may as well take advantage of that!</p> 
<p>For this post, we chose not to focus on ways to optimize projection. However, the reference architecture does support this with additional open source components compiled into the FFMPEG toolchain. A number of options are available to this end, such as open source equirectangular to cubic transformation filters. There is a tradeoff, however, in that reprojection implies that all streams must be transcoded.</p> 
<b>Processing and origination stack</b> 
<p>To get started, we’ve provided a&nbsp;<a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/templates/template.yaml">CloudFormation template</a>&nbsp;that you can launch directly into your own AWS account. We quickly review how it works, the solution’s components, key features, processing steps, and examine the main configuration files. Following this, you launch the stack, and then proceed with camera and encoder setup.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/ref_arch_diagram.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/ref_arch_diagram.png" /></a> 
<p class="wp-caption-text">Immersive streaming reference architecture</p> 
<ol> 
<li>The event encoder publishes the RTMP source to multiple origin elastic IP addresses for packaging into the HLS adaptive bitrate.</li> 
<li>The client requests the live stream through the CloudFront CDN.</li> 
<li>The origin responds with the appropriate HLS stream.</li> 
<li>The edge fleet caches media requests from clients and elastically scales across both Availability Zones to meet peak demand.</li> 
<li>CloudFront caches media at local edge PoPs to improve performance for users and reduce the origin load.</li> 
<li>When the live event is finished, the VOD asset is published to S3. An S3 event is then published to SQS.</li> 
<li>The encoding fleet processes the read messages from the SQS queue, processes the VOD clips, and stores them in the S3 bucket.</li> 
</ol> 
<h3>How it works</h3> 
<p>A camera captures content, and with the help of a contribution encoder, publishes a live stream in equirectangular format. The stream is encoded at a high bit rate (at least 2.5 Mbps, but typically 16+ Mbps for 4K) using H264 video and AAC audio compression codecs, and delivered to a primary origin via the RTMP protocol. Streams may transit over the internet or dedicated links to the origins. Typically, for live events in the field, internet or bonded cellular are the most widely used.</p> 
<p>The encoder is typically configured to push the live stream to a primary URI, with the ability (depending on the source encoding software/hardware) to roll over to a backup publishing point origin if the primary fails. Because you run across multiple Availability Zones, this architecture could handle an entire zone outage with minor disruption to live events. The primary and backup origins handle the ingestion of the live stream as well as transcoding to H264+AAC-based adaptive bit rate sets. After transcode, they package the streams into HLS for delivery and create a master-level manifest that references all adaptive bit rates.</p> 
<p>The edge cache fleet pulls segments and manifests from the active origin on demand, and supports failover from primary to backup if the primary origin fails. By adding this caching tier, you effectively separate the encoding backend tier from the cache tier that responds to client or CDN requests. In addition to origin protection, this separation allows you to independently monitor, configure, and scale these components.</p> 
<p>Viewers can use the sample HTML5 player (or compatible desktop, iOS or Android application) to view the streams. Navigation in the 360-degree view is handled either natively via device-based gyroscope, positionally via more advanced devices such as a head mount display, or via mouse drag on the desktop. Adaptive bit rate is key here, as this allows you to target multiple device types, giving the player on each device the option of selecting an optimum stream based on network conditions or device profile.</p> 
<h3>Solution components</h3> 
<p>When you deploy the CloudFormation template, all the architecture services referenced above are created and launched. This includes:</p> 
<li>The compute tier running on Spot Instances for the corresponding components: 
<li>the primary and backup ingest origins</li> 
<li>the edge cache fleet</li> 
<li>the transcoding fleet</li> 
<li>the test source</li> 
</ul> </li> 
<li>The CloudFront distribution</li> 
<li>S3 buckets for storage of on-demand VOD assets</li> 
<li>An Application Load Balancer for load balancing the service</li> 
<li>An&nbsp;<a href="https://aws.amazon.com/ecs/">Amazon ECS</a>&nbsp;cluster and container for the test source</li> 
<li>An&nbsp;<a href="https://aws.amazon.com/sqs/">Amazon SQS</a>&nbsp;queue</li> 
<p>The template also provisions the underlying dependencies:</p> 
<li>A VPC</li> 
<li>Security groups</li> 
<li>IAM policies and roles</li> 
<li>Elastic network interfaces</li> 
<li>Elastic IP addresses</li> 
<p>The edge cache fleet instances need some way to discover the primary and backup origin locations. You use elastic network interfaces and elastic IP addresses for this purpose.</p> 
<p>As each component of the infrastructure is provisioned, software required to transcode and process the streams across the Spot Instances is automatically deployed. This includes&nbsp;<a href="https://github.com/arut/nginx-rtmp-module">NGiNX-RTMP</a>&nbsp;for ingest of live streams, FFMPEG for transcoding, NGINX for serving, and helper scripts to handle various tasks (potential Spot Instance interruptions, queueing, moving content to S3). Metrics and logs are available through CloudWatch and you can manage the deployment using the CloudFormation console or&nbsp;<a href="https://aws.amazon.com/cli/">AWS CLI</a>.</p> 
<p>Key features include:</p> 
<li><strong>Live and video-on-demand recording</strong></li> 
<p style="padding-left: 30px">You’re supporting both live and on-demand. On-demand content is created automatically when the encoder stops publishing to the origin.</p> 
<li><strong>Cost-optimization and operating at scale using Spot Instances</strong></li> 
<p style="padding-left: 30px">Spot Instances are used exclusively for infrastructure to optimize cost and scale throughput.</p> 
<li><strong>Midtier caching</strong></li> 
<p style="padding-left: 30px">To protect the origin servers, the midtier cache fleet pulls, caches, and delivers to downstream CDNs.</p> 
<li><strong>Distribution via CloudFront or multi-CDN</strong></li> 
<p style="padding-left: 30px">The Application Load Balancer endpoint allows CloudFront or any third-party CDN to source content from the edge fleet and, indirectly, the origin.</p> 
<li><strong>FFMPEG + NGINX + NGiNX-RTMP</strong></li> 
<p style="padding-left: 30px">These three components form the core of the stream ingest, transcode, packaging, and delivery infrastructure, as well as the VOD-processing component for creating transcoded VOD content on-demand.</p> 
<li><strong>Simple deployment using a CloudFormation template</strong></li> 
<p style="padding-left: 30px">All infrastructure can be easily created and modified using CloudFormation.</p> 
<li><strong>Prototype player page</strong></li> 
<p style="padding-left: 30px">To provide an end-to-end experience right away, we’ve included a test player page hosted as a static site on S3. This page uses&nbsp;<a href="https://aframe.io/">A-Frame</a>, a cross-platform, open-source framework for building VR experiences in the browser. Though A-Frame provides many features, it’s used here to render a sphere that acts as a 3D canvas for your live stream.</p> 
<h3>Spot Instance considerations</h3> 
<p>At this stage, and before we discuss processing, it is important to understand how the architecture operates with Spot Instances.</p> 
<p>Spot Instances are spare compute capacity in the AWS Cloud available to you at steep discounts compared to On-Demand prices. Spot Instances enables you to optimize your costs on the AWS Cloud and scale your application’s throughput up to 10X for the same budget. By selecting Spot Instances, you can save up-to 90% on On-Demand prices. This allows you to greatly reduce the cost of running the solution because, outside of S3 for storage and CloudFront for delivery, this solution is almost entirely dependent on Spot Instances for infrastructure requirements.</p> 
<p>We also know that customers running events look to deploy streaming infrastructure at the lowest price point, so it makes sense to take advantage of it wherever possible. A potential challenge when using Spot Instances for live streaming and on-demand processing is that you need to proactively deal with potential Spot Instance interruptions. How can you best deal with this?</p> 
<p>First, the origin is deployed in a primary/backup deployment. If a Spot Instance interruption happens on the primary origin, you can fail over to the backup with a brief interruption. Should a potential interruption not be acceptable, then either Reserved Instances or On-Demand options (or a combination) can be used at this tier.</p> 
<p>Second, the edge cache fleet runs a job (started automatically at system boot) that periodically queries the local instance metadata to detect if an interruption is scheduled to occur.&nbsp;<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html">Spot Instance Interruption Notices</a>&nbsp;provide a two-minute warning of a pending interruption. If you poll every 5 seconds, you have almost 2 full minutes to detach from the&nbsp;Load Balancer and drain or stop any traffic directed to your instance.</p> 
<p>Lastly, use an SQS queue when transcoding. If a transcode for a Spot Instance is interrupted, the stale item falls back into the SQS queue and is eventually re-surfaced into the processing pipeline. Only remove items from the queue after the transcoded files have been successfully moved to the destination S3 bucket.</p> 
<h3>Processing</h3> 
<p>As discussed in the previous sections, you pass through the video for the highest bit rate to save on having to increase the instance size to transcode the 4K or similar high bit rate or resolution content.</p> 
<p>We’ve selected a handful of bitrates for the adaptive bit rate stack. You can customize any of these to suit the requirements for your event. The default ABR stack includes:</p> 
<li>2160p (4K)</li> 
<li>1080p</li> 
<li>540p</li> 
<li>480p</li> 
<p>These can be modified by editing the&nbsp;<em>/etc/nginx/rtmp.d/rtmp.conf</em>&nbsp;NGINX configuration file on the origin or the CloudFormation template.</p> 
<p>It’s important to understand where and how streams are transcoded. When the source high bit rate stream enters the primary or backup origin at the&nbsp;<em>/live</em>&nbsp;RTMP application entry point, it is recorded on stop and start of publishing. On completion, it is moved to S3 by a cleanup script, and a message is placed in your SQS queue for workers to use. These workers transcode the media and push it to a playout location bucket.</p> 
<p>This solution uses Spot Fleet with automatic scaling to drive the fleet size. You can customize it based on CloudWatch metrics, such as simple utilization metrics to drive the size of the fleet. Why use Spot Instances for the transcode option instead of Amazon Elastic Transcoder? This allows you to implement reprojection of the input stream via FFMPEG filters in the future.</p> 
<p>The origins handle all the heavy live streaming work. Edges only store and forward the segments and manifests, and provide scaling plus reduction of burden on the origin. This lets you customize the origin to the right compute capacity without having to rely on a ‘high watermark’ for compute sizing, thus saving additional costs.</p> 
<p>Loopback is an important concept for the live origins. The incoming stream entering&nbsp;<em>/live</em>&nbsp;is transcoded by FFMPEG to multiple bit rates, which are streamed back to the same host via RTMP, on a secondary publishing point&nbsp;<em>/show</em>. The secondary publishing point is transparent to the user and encoder, but handles HLS segment generation and cleanup, and keeps a sliding window of live segments and constantly updating manifests.</p> 
<b>Configuration</b> 
<p>Our solution provides two key points of configuration that can be used to customize the solution to accommodate ingest, recording, transcoding, and delivery, all controlled via origin and edge configuration files, which are described later. In addition, a number of job scripts run on the instances to provide hooks into Spot Instance interruption events and the VOD SQS-based processing queue.</p> 
<h3>Origin instances</h3> 
<p>The&nbsp;<em>rtmp.conf</em>&nbsp;excerpt below also shows additional parameters that can be customized, such as maximum recording file size in Kbytes, HLS Fragment length, and Playlist sizes. We’ve created these in accordance with general industry best practices to ensure the reliable streaming and delivery of your content.</p> 
<code class="lang-json">rtmp {
server {
listen 1935;
chunk_size 4000;
application live {
live on;
record all;
record_path /var/lib/nginx/rec;
record_max_size 128000K;
exec_record_done /usr/local/bin/record-postprocess.sh $path $basename;
exec /usr/local/bin/ffmpeg &lt;…parameters…&gt;;
}
application show {
live on;
hls on;
...
hls_type live;
hls_fragment 10s;
hls_playlist_length 60s;
...
}
}
}</code> 
<p>This exposes a few URL endpoints for debugging and general status. In production, you would most likely turn these off:</p> 
<li><em>/stat</em>&nbsp;provides a statistics endpoint accessible via any standard web browser.</li> 
<li><em>/control</em>&nbsp;enables control of RTMP streams and publishing points.</li> 
<p>You also control the TTLs, as previously discussed. It’s important to note here that you are setting TTLs explicitly at the origin, instead of in CloudFront’s distribution configuration. While both are valid, this approach allows you to reconfigure and restart the service on the fly without having to push changes through CloudFront. This is useful for debugging any caching or playback issues.</p> 
<code class="lang-json">location /stat {
...
}
location /stat.xsl {
...
}
location /control {
...
}
# origin controlled ttls
location ~* /hls/.*\.ts$ {
...
add_header Cache-Control &quot;max-age=900&quot;;
expires 900s;
}
location ~* /hls/.*\.m3u8$ {
...
add_header Cache-Control &quot;max-age=5&quot;;
expires 5s;
}</code> 
<h3>Handler scripts</h3> 
<p>Here is a brief overview of the scripts we use to handle the events and process of the solution. We encourage you to take some time to review them.</p> 
<li><a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/user-data/edge/bin/spot-termination-handler.sh">spot-termination-handler.sh</a>&nbsp;– Provides a graceful shutdown for the transcoding and edge fleet instances.</li> 
<li><a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/user-data/origin/bin/record-postprocess.sh">record-postprocess.sh</a>&nbsp;– Ensures that recorded files on the origin are well-formed, and transfers them to S3 for processing.</li> 
<li><a href="https://github.com/aws-samples/immersive-media-refarch/blob/master/user-data/origin/bin/record-postprocess.sh">ffmpeg.sh</a>&nbsp;– Transcodes content on the encoding fleet, pulling source media from your S3 ingress bucket, based on SQS queue entries, and pushing transcoded adaptive bit rate segments and manifests to your VOD playout egress bucket.</li> 
<p>For more details, see the Delivery and Playback section later in this post.</p> 
<b>Camera source</b> 
<p>With the processing and origination infrastructure running, you need to configure your camera and encoder.</p> 
<p>As discussed, we chose to use a Ricoh Theta S camera and Open Broadcaster Software (OBS) to stitch and deliver a stream into the infrastructure. Ricoh provides a free ‘blender’ driver, which allows you to transform, stitch, encode, and deliver both transformed equirectangular (used for this post) video as well as spherical (two camera) video. The Theta provides an easy way to get capturing for under $300, and OBS is a free and open-source software application for capturing and live streaming on a budget. It is quick, cheap, and enjoys wide use by the gaming community. OBS lowers the barrier to getting started with immersive streaming.</p> 
<p>While the resolution and bit rate of the Theta may not be 4K, it still provides us with a way to test the functionality of the entire pipeline end to end, without having to invest in a more expensive camera rig. One could also use this type of model to target smaller events, which may involve mobile devices with smaller display profiles, such as phones and potentially smaller sized tablets.</p> 
<p>Looking for a more professional solution? Nokia, GoPro, Samsung, and many others have options ranging from $500 to $50,000. This solution is based around the Theta S capabilities, but we’d encourage you to extend it to meet your specific needs.</p> 
<p>If your device can support equirectangular RTMP, then it can deliver media through the reference architecture (dependent on instance sizing for higher bit rate sources, of course). If additional features are required such as camera stitching, mixing, or device bonding, we’d recommend exploring a commercial solution such as&nbsp;<a href="https://teradek.com/collections/sphere-family">Teradek Sphere</a>.</p> 
<table class=" aligncenter" style="height: 306px" width="874"> 
<tbody> 
<tr> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture5.png" /></a> 
<p class="wp-caption-text">Teradek Rig (Teradek)</p> 
<td> <p></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture6.png" /></a> 
<p class="wp-caption-text">Ricoh Theta (CNET)</p> 
</tr> 
</tbody> 
</table> 
<p>All cameras have varied PC connectivity support. We chose the Ricoh Theta S due to the real-time video connectivity that it provides through software drivers on macOS and PC. If you plan to purchase a camera to use with a PC, confirm that it supports real-time capabilities as a peripheral device.</p> 
<b>Encoding and publishing</b> 
<p>Now that you have a camera, encoder, and AWS stack running, you can finally publish a live stream.</p> 
<p>To start streaming with OBS, configure the source camera and set a publishing point. Use the RTMP application name&nbsp;<em>/live</em>&nbsp;on port 1935 to ingest into the primary origin’s Elastic IP address provided as the CloudFormation output:&nbsp;<em>primaryOriginElasticIp</em>.</p> 
<p>You also need to choose a stream name or stream key in OBS. You can use any stream name, but keep the naming short and lowercase, and use only alphanumeric characters. This avoids any parsing issues on client-side player frameworks. There’s no publish point protection in your deployment, so any stream key works with the default NGiNX-RTMP configuration. For more information about stream keys, publishing point security, and extending the NGiNX-RTMP module, see the&nbsp;<a href="https://github.com/arut/nginx-rtmp-module/wiki">NGiNX-RTMP Wiki</a>.</p> 
<p>You should end up with a configuration similar to the following:</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/obs_stream_settings.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/02/22/obs_stream_settings.png" /></a> 
<p class="wp-caption-text">OBS Stream Settings</p> 
<p>The Output settings dialog allows us to rescale the Video canvas and encode it for delivery to our AWS infrastructure. In the dialog below, we’ve set the Theta to encode at 5 Mbps in CBR mode using a preset optimized for low CPU utilization. We chose these settings in accordance with best practices for the stream pass-through at the origin for the initial incoming bit rate. You may notice that they largely match the FFMPEG encoding settings we use on the origin – namely constant bit rate, a single audio track, and x264 encoding with the ‘veryfast’ encoding profile.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture8.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture8.png" /></a> 
<p class="wp-caption-text">OBS Output Settings</p> 
<b>Live to On-Demand</b> 
<p>As you may have noticed, an on-demand component is included in the solution architecture. When talking to customers, one frequent request that we see is that they would like to record the incoming stream with as little effort as possible.</p> 
<p>NGINX-RTMP’s recording directives provide an easy way to accomplish this. We record any newly published stream on stream start at the primary or backup origins, using the incoming source stream, which also happens to be the highest bit rate. When the encoder stops broadcasting, NGINX-RTMP executes an exec_record_done script –&nbsp;<em>record-postprocess.sh</em>&nbsp;(described in the Configuration section earlier), which ensures that the content is well-formed, and then moves it to an S3 ingest bucket for processing.</p> 
<p>Transcoding of content to make it ready for VOD as adaptive bit rate is a multi-step pipeline. First, Spot Instances in the transcoding cluster periodically poll the SQS queue for new jobs. Items on the queue are pulled off on demand by processing instances, and transcoded via FFMPEG into adaptive bit rate HLS. This allows you to also extend FFMPEG using filters for cubic and other bitrate-optimizing 360-specific transforms. Finally, transcoded content is moved from the ingest bucket to an egress bucket, making them ready for playback via your CloudFront distribution.</p> 
<p>Separate ingest and egress by bucket to provide hard security boundaries between source recordings (which are highest quality and unencrypted), and destination derivatives (which may be lower quality and potentially require encryption). Bucket separation also allows you to order and archive input and output content using different taxonomies, which is common when moving content from an asset management and archival pipeline (the ingest bucket) to a consumer-facing playback pipeline (the egress bucket, and any other attached infrastructure or services, such as CMS, Mobile applications, and so forth).</p> 
<p>Because streams are pushed over the internet, there is always the chance that an interruption could occur in the network path, or even at the origin side of the equation (primary to backup roll-over). Both of these scenarios could result in malformed or partial recordings being created. For the best level of reliability, encoding should always be recorded locally on-site as a precaution to deal with potential stream interruptions.</p> 
<b>Delivery and playback</b> 
<p>With the camera turned on and OBS streaming to AWS, the final step is to play the live stream. We’ve primarily tested the prototype player on the latest Chrome and Firefox browsers on macOS, so your mileage may vary on different browsers or operating systems. For those looking to try the livestream on Google Cardboard, or similar headsets, native apps for iOS (VRPlayer) and Android exist that can play back HLS streams.</p> 
<p>The prototype player is hosted in an S3 bucket and can be found from the CloudFormation output&nbsp;<em>clientWebsiteUrl</em>. It requires a stream URL provided as a query parameter&nbsp;<em>?url=&lt;stream_url&gt;</em>&nbsp;to begin playback. This stream URL is determined by the RTMP stream configuration in OBS. For example, if OBS is publishing to&nbsp;<em>rtmp://x.x.x.x:1935/live/foo</em>,&nbsp;the resulting playback URL would be:</p> 
<p style="padding-left: 30px"><em>https://&lt;cloudFrontDistribution&gt;/hls/foo.m3u8</em></p> 
<p>The combined player URL and playback URL results in a path like this one:</p> 
<p style="padding-left: 30px"><em>https://&lt;clientWebsiteUrl&gt;/?url=https://&lt;cloudFrontDistribution&gt;/hls/foo.m3u8</em></p> 
<p>To assist in setup/debugging, we’ve provided a test source as part of the CloudFormation template. A color bar pattern with timecode and audio is being generated by FFmpeg running as an ECS task. Much like OBS, FFmpeg is streaming the test pattern to the primary origin over the RTMP protocol. The prototype player and test HLS stream can be accessed by opening the <em>clientTestPatternUrl</em> CloudFormation output link.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture9.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2018/01/08/Picture9.png" /></a> 
<p class="wp-caption-text">Test Stream Playback</p> 
<b>What’s next?</b> 
<p>In this post, we walked you through the design and implementation of a full end-to-end immersive streaming solution architecture. As you may have noticed, there are a number of areas this could expand into, and we intend to do this in follow-up posts around the topic of virtual reality media workloads in the cloud. We’ve identified a number of topics such as load testing, content protection, client-side metrics and analytics, and CI/CD infrastructure for 24/7 live streams. If you have any requests, please drop us a line.</p> 
<p><em>We would like to extend extra-special thanks to Scott Malkie and Chad Neal for their help and contributions to this post and reference architecture.</em></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3568');
});
</script> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
