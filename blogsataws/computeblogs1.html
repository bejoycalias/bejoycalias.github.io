<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/social_ECR.png" /> 
<b class="lb-b blog-post-title" property="name headline">AWS re:Invent 2017 Guide to All Things Containers</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tiffany Jernigan</span></span> | on 
<time property="datePublished" datetime="2017-11-17T13:05:07+00:00">17 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/aws-reinvent-2017-guide-to-all-things-containers/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3321" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3321&amp;disqus_title=AWS+re%3AInvent+2017+Guide+to+All+Things+Containers&amp;disqus_url=https://aws.amazon.com/blogs/compute/aws-reinvent-2017-guide-to-all-things-containers/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3321');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><i>Contributed</i><em>&nbsp;by <a href="https://twitter.com/tiffanyfayj">Tiffany Jernigan</a>, Developer Advocate for Amazon ECS</em></p> 
<p><img class="aligncenter size-large wp-image-3332" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/ecs-ship-orange_planet-1024x318.png" alt="" width="1024" height="318" /></p> 
<b>Get ready for takeoff!</b> 
<p>We made sure that this year’s <a href="https://reinvent.awsevents.com/">re:Invent</a> is chock-full of containers: there are over 40 sessions! New to containers? No problem, we have several introductory sessions for you to dip your toes. Been using containers for years and know the ins and outs? Don’t miss our technical deep-dives and interactive chalk talks led by container experts.</p> 
<p>If you’re already registered for re:Invent, you can browse the session catalog <a href="https://www.portal.reinvent.awsevents.com/connect/search.ww#loadSearch-searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=abbreviationSort&amp;p=&amp;i(10042)=16546">here</a>. If you can’t make it to Las Vegas, you can catch the keynotes and session recaps from our <a href="https://reinvent.awsevents.com/live-stream/">livestream</a> and on <a href="http://www.twitch.tv/aws">Twitch</a>.</p> 
<h3>Session types</h3> 
<p>Not everyone learns the same way, so we have multiple types of breakout content:</p> 
<li><strong>Birds of a Feather</strong><br /> An interactive discussion with industry leaders about containers on AWS.</li> 
<li><strong>Breakout sessions</strong><br /> 60-minute presentations about building on AWS. Sessions are delivered by both AWS experts and customers and span all content levels.</li> 
<li><strong>Workshops</strong><br /> 2.5-hour, hands-on sessions that teach how to build on AWS. AWS credits are provided. Bring a laptop, and have an active AWS account.</li> 
<li><strong>Chalk Talks</strong><br /> 1-hour, highly interactive sessions with a smaller audience. They begin with a short lecture delivered by an AWS expert, followed by a discussion with the audience.</li> 
<h3>Session levels</h3> 
<p>Whether you’re new to containers or you’ve been using them for years, you’ll find useful information at every level.</p> 
<li><strong>Introductory</strong><br /> Sessions are focused on providing an overview of AWS services and features, with the assumption that attendees are new to the topic.</li> 
<li><strong>Advanced</strong><br /> Sessions dive deeper into the selected topic. Presenters assume that the audience has some familiarity with the topic, but may or may not have direct experience implementing a similar solution.</li> 
<li><strong>Expert</strong><br /> Sessions are for attendees who are deeply familiar with the topic, have implemented a solution on their own already, and are comfortable with how the technology works across multiple services, architectures, and implementations.</li> 
<h3>Session locations</h3> 
<p>All container sessions are located in the Aria Resort.</p> 
<p><img class="aligncenter size-full wp-image-3322" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/reInvent_locations.png" alt="" width="975" height="554" /></p> 
<b>MONDAY 11/27</b> 
<h3>Breakout sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON202 –&nbsp;Getting Started with Docker and Amazon ECS<br /> </strong>By packaging software into standardized units, Docker gives code everything it needs to run, ensuring consistency from your laptop all the way into production. But once you have your code ready to ship, how do you run and scale it in the cloud? In this session, you become comfortable running containerized services in production using Amazon ECS. We cover container deployment, cluster management, service auto-scaling, service discovery, secrets management, logging, monitoring, security, and other core concepts. We also cover integrated AWS services and supplementary services that you can take advantage of to run and scale container-based services in the cloud.</p> 
<h3>Chalk talks</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON211 –&nbsp;Reducing your Compute Footprint with Containers and Amazon ECS<br /> </strong>Tomas Riha, platform architect for Volvo, shows how Volvo transitioned its WirelessCar platform from using Amazon EC2 virtual machines to containers running on Amazon ECS, significantly reducing cost. Tomas dives deep into the architecture that Volvo used to achieve the migration in under four months, including Amazon ECS, Amazon ECR, Elastic Load Balancing, and AWS CloudFormation.</p> 
<p><strong>CON212 –&nbsp;Anomaly Detection Using Amazon ECS, AWS Lambda, and Amazon EMR</strong><br /> Learn about the architecture that Cisco CloudLock uses to enable automated security and compliance checks throughout the entire development lifecycle, from the first line of code through runtime. It includes integration with IAM roles, Amazon VPC, and AWS KMS.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON410 –&nbsp;Advanced CICD with Amazon ECS Control Plane<br /> </strong>Mohit Gupta, product and engineering lead for Clever, demonstrates how to extend the Amazon ECS control plane to optimize management of container deployments and how the control plane can be broadly applied to take advantage of new AWS services. This includes ark—an AWS CLI-based deployment to Amazon ECS, Dapple—a slack-based automation system for deployments and notifications, and Kayvee—log and event routing libraries based on Amazon Kinesis.</p> 
<h3>Workshops</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON209 –&nbsp;Interstella 8888: Learn How to Use Docker on AWS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. &nbsp;Join this workshop to get hands-on experience with Docker as you containerize Interstella 8888’s aging monolithic application and deploy it using Amazon ECS.</p> 
<p><strong>CON213 –&nbsp;Hands-on Deployment of Kubernetes on AWS<br /> </strong>In this workshop, attendees get hands-on experience using Kubernetes and Kops (Kubernetes Operations), as described in our recent blog post. Attendees learn how to provision a cluster, assign role-based permissions and security, and launch a container. If you’re interested in learning best practices for running Kubernetes on AWS, don’t miss this workshop.</p> 
<b>TUESDAY 11/28</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON206 –&nbsp;Docker on AWS</strong><br /> In this session, Docker Technical Staff Member Patrick Chanezon discusses how Finnish Rail, the national train system for Finland, is using Docker on Amazon Web Services to modernize their customer facing applications, from ticket sales to reservations. Patrick also shares the state of Docker development and adoption on AWS, including explaining the opportunities and implications of efforts such as Project Moby, Docker EE, and how developers can use and contribute to Docker projects.</p> 
<p><strong>CON208 –&nbsp;Building Microservices on AWS</strong><br /> Increasingly, organizations are turning to microservices to help them empower autonomous teams, letting them innovate and ship software faster than ever before. But implementing a microservices architecture comes with a number of new challenges that need to be dealt with. Chief among these finding an appropriate platform to help manage a growing number of independently deployable services. In this session, Sam Newman, author of Building Microservices and a renowned expert in microservices strategy, discusses strategies for building scalable and robust microservices architectures. He also tells you how to choose the right platform for building microservices, and about common challenges and mistakes organizations make when they move to microservices architectures.</p> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON302 –&nbsp;Building a CICD Pipeline for Containers on AWS</strong><br /> Containers can make it easier to scale applications in the cloud, but how do you set up your CICD workflow to automatically test and deploy code to containerized apps? In this session, we explore how developers can build effective CICD workflows to manage their containerized code deployments on AWS.</p> 
<p>Ajit Zadgaonkar, Director of Engineering and Operations at Edmunds walks through best practices for CICD architectures used by his team to deploy containers. We also deep dive into topics such as how to create an accessible CICD platform and architect for safe blue/green deployments.</p> 
<p><strong>CON307 –&nbsp;Building Effective Container Images</strong><br /> Sick of getting paged at 2am and wondering “where did all my disk space go?” New Docker users often start with a stock image in order to get up and running quickly, but this can cause problems as your application matures and scales. Creating efficient container images is important to maximize resources, and deliver critical security benefits.</p> 
<p>In this session, AWS Sr. Technical Evangelist Abby Fuller covers how to create effective images to run containers in production. This includes an in-depth discussion of how Docker image layers work, things you should think about when creating your images, working with Amazon ECR, and mise-en-place for install dependencies. Prakash Janakiraman, Co-Founder and Chief Architect at Nextdoor discuss high-level and language-specific best practices for with building images and how Nextdoor uses these practices to successfully scale their containerized services with a small team.</p> 
<p><strong>CON309 –&nbsp;Containerized Machine Learning on AWS</strong><br /> Image recognition is a field of deep learning that uses neural networks to recognize the subject and traits for a given image. In Japan, Cookpad uses Amazon ECS to run an image recognition platform on clusters of GPU-enabled EC2 instances. In this session, hear from Cookpad about the challenges they faced building and scaling this advanced, user-friendly service to ensure high-availability and low-latency for tens of millions of users.</p> 
<p><strong>CON320 –&nbsp;Monitoring, Logging, and Debugging for Containerized Services</strong><br /> As containers become more embedded in the platform tools, debug tools, traces, and logs become increasingly important. Nare Hayrapetyan, Senior Software Engineer and Calvin French-Owen, Senior Technical Officer for Segment discuss the principals of monitoring and debugging containers and the tools Segment has implemented and built for logging, alerting, metric collection, and debugging of containerized services running on Amazon ECS.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON314 –&nbsp;Automating Zero-Downtime Production Cluster Upgrades for Amazon ECS</strong><br /> Containers make it easy to deploy new code into production to update the functionality of a service, but what happens when you need to update the Amazon EC2 compute instances that your containers are running on? In this talk, we’ll deep dive into how to upgrade the Amazon EC2 infrastructure underlying a live production Amazon ECS cluster without affecting service availability. Matt Callanan, Engineering Manager at Expedia walk through Expedia’s “PRISM” project that safely relocates hundreds of tasks onto new Amazon EC2 instances with zero-downtime to applications.</p> 
<p><strong>CON322 –&nbsp;Maximizing Amazon ECS for Large-Scale Workloads</strong><br /> Head of Mobfox DevOps, David Spitzer, shows how Mobfox used Docker and Amazon ECS to scale the Mobfox services and development teams to achieve low-latency networking and automatic scaling. This session covers Mobfox’s ecosystem architecture. It compares 2015 and today, the challenges Mobfox faced in growing their platform, and how they overcame them.</p> 
<p><strong>CON323 –&nbsp;Microservices Architectures for the Enterprise</strong><br /> Salva Jung, Principle Engineer for Samsung Mobile shares how Samsung Connect is architected as microservices running on Amazon ECS to securely, stably, and efficiently handle requests from millions of mobile and IoT devices around the world.</p> 
<p><strong>CON324 –&nbsp;Windows Containers on Amazon ECS</strong><br /> Docker containers are commonly regarded as powerful and portable runtime environments for Linux code, but Docker also offers API and toolchain support for running Windows Servers in containers. In this talk, we discuss the various options for running windows-based applications in containers on AWS.</p> 
<p><strong>CON326 –&nbsp;Remote Sensing and Image Processing on AWS</strong><br /> Learn how Encirca services by DuPont Pioneer uses Amazon ECS powered by GPU-instances and Amazon EC2 Spot Instances to run proprietary image-processing algorithms against satellite imagery. Mark Lanning and Ethan Harstad, engineers at DuPont Pioneer show how this architecture has allowed them to process satellite imagery multiple times a day for each agricultural field in the United States in order to identify crop health changes.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON317 –&nbsp;Advanced Container Management at Catsndogs.lol</strong><br /> Catsndogs.lol is a (fictional) company that needs help deploying and scaling its container-based application. During this workshop, attendees join the new DevOps team at CatsnDogs.lol, and help the company to manage their applications using Amazon ECS, and help release new features to make our customers happier than ever.Attendees get hands-on with service and container-instance auto-scaling, spot-fleet integration, container placement strategies, service discovery, secrets management with AWS Systems Manager Parameter Store, time-based and event-based scheduling, and automated deployment pipelines. If you are a developer interested in learning more about how Amazon ECS can accelerate your application development and deployment workflows, or if you are a systems administrator or DevOps person interested in understanding how Amazon ECS can simplify the operational model associated with running containers at scale, then this workshop is for you. You should have basic familiarity with Amazon ECS, Amazon EC2, and IAM.</p> 
<p>Additional requirements:</p> 
<li>The AWS CLI or AWS Tools for PowerShell installed</li> 
<li>An AWS account with administrative permissions (including the ability to create IAM roles and policies) created at least 24 hours in advance.</li> 
<b>WEDNESDAY 11/29</b> 
<h3>Birds of a Feather (BoF)</h3> 
<p><strong>CON205 –&nbsp;Birds of a Feather: Containers and Open Source at AWS</strong><br /> Cloud native architectures take advantage of on-demand delivery, global deployment, elasticity, and higher-level services to enable developer productivity and business agility. Open source is a core part of making cloud native possible for everyone. In this session, we welcome thought leaders from the CNCF, Docker, and AWS to discuss the cloud’s direction for growth and enablement of the open source community. We also discuss how AWS is integrating open source code into its container services and its contributions to open source projects.</p> 
<h3>Breakout Sessions</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON308 –&nbsp;Mastering Kubernetes on AWS</strong><br /> Much progress has been made on how to bootstrap a cluster since Kubernetes’ first commit and is now only a matter of minutes to go from zero to a running cluster on Amazon Web Services. However, evolving a simple Kubernetes architecture to be ready for production in a large enterprise can quickly become overwhelming with options for configuration and customization.</p> 
<p>In this session, Arun Gupta, Open Source Strategist for AWS and Raffaele Di Fazio, software engineer at leading European fashion platform Zalando, show the common practices for running Kubernetes on AWS and share insights from experience in operating tens of Kubernetes clusters in production on AWS. We cover options and recommendations on how to install and manage clusters, configure high availability, perform rolling upgrades and handle disaster recovery, as well as continuous integration and deployment of applications, logging, and security.</p> 
<p><strong>CON310 –&nbsp;Moving to Containers: Building with Docker and Amazon ECS</strong><br /> If you’ve ever considered moving part of your application stack to containers, don’t miss this session. We cover best practices for containerizing your code, implementing automated service scaling and monitoring, and setting up automated CI/CD pipelines with fail-safe deployments. Manjeeva Silva and Thilina Gunasinghe show how McDonalds implemented their home delivery platform in four months using Docker containers and Amazon ECS to serve tens of thousands of customers.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON402 –&nbsp;Advanced Patterns in Microservices Implementation with Amazon ECS</strong><br /> Scaling a microservice-based infrastructure can be challenging in terms of both technical implementation and developer workflow. In this talk, AWS Solutions Architect Pierre Steckmeyer is joined by Will McCutchen, Architect at BuzzFeed, to discuss Amazon ECS as a platform for building a robust infrastructure for microservices. We look at the key attributes of microservice architectures and how Amazon ECS supports these requirements in production, from configuration to sophisticated workload scheduling to networking capabilities to resource optimization. We also examine what it takes to build an end-to-end platform on top of the wider AWS ecosystem, and what it’s like to migrate a large engineering organization from a monolithic approach to microservices.</p> 
<p><strong>CON404 –&nbsp;Deep Dive into Container Scheduling with Amazon ECS</strong><br /> As your application’s infrastructure grows and scales, well-managed container scheduling is critical to ensuring high availability and resource optimization. In this session, we deep dive into the challenges and opportunities around container scheduling, as well as the different tools available within Amazon ECS and AWS to carry out efficient container scheduling. We discuss patterns for container scheduling available with Amazon ECS, the Blox scheduling framework, and how you can customize and integrate third-party scheduler frameworks to manage container scheduling on Amazon ECS.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON312 –&nbsp;Building a Selenium Fleet on the Cheap with Amazon ECS with Spot Fleet</strong><br /> Roberto Rivera&nbsp;and Matthew Wedgwood, engineers at RetailMeNot, give a practical overview of setting up a fleet of Selenium nodes running on Amazon ECS with Spot Fleet. Discuss the challenges of running Selenium with high availability at minimum cost using Amazon ECS container introspection to connect the Selenium Hub with its nodes.</p> 
<p><strong>CON315 –&nbsp;Virtually There: Building a Render Farm with Amazon ECS</strong><br /> Learn how 8i Corp scales its multi-tenanted, volumetric render farm up to thousands of instances using AWS, Docker, and an API-driven infrastructure. This render farm enables them to turn the video footage from an array of synchronized cameras into a photo-realistic hologram capable of playback on a range of devices, from mobile phones to high-end head mounted displays. Join Owen Evans, VP of Engineering for 8i, as they dive deep into how 8i’s rendering infrastructure is built and maintained by just a handful of people and powered by Amazon ECS.</p> 
<p><strong>CON325 –&nbsp;Developing Microservices – from Your Laptop to the Cloud</strong><br /> Wesley Chow, Staff Engineer at Adroll, shows how his team extends Amazon ECS by enabling local development capabilities. Hologram, Adroll’s local development program, brings the capabilities of the Amazon EC2 instance metadata service to non-EC2 hosts, so that developers can run the same software on local machines with the same credentials source as in production.</p> 
<p><strong>CON327 –&nbsp;Patterns and Considerations for Service Discovery</strong><br /> Roven Drabo, head of cloud operations at Kaplan Test Prep, illustrates Kaplan’s complete container automation solution using Amazon ECS along with how his team uses NGINX and HashiCorp Consul to provide an automated approach to service discovery and container provisioning.</p> 
<p><strong>CON328 –&nbsp;Building a Development Platform on Amazon ECS</strong><br /> Quinton Anderson, Head of Engineering for Commonwealth Bank of Australia, walks through&nbsp;how they migrated their internal development and deployment platform from Mesos/Marathon to Amazon ECS. The platform uses a custom DSL to abstract a layered application architecture, in a way that makes it easy to plug or replace new implementations into each layer in the stack.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON318 –&nbsp;Interstella 8888: Monolith to Microservices with Amazon ECS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. Join this workshop to get hands-on experience deploying Docker containers as you break Interstella 8888’s aging monolithic application into containerized microservices. Using Amazon ECS and an Application Load Balancer, you create API-based microservices and deploy them leveraging integrations with other AWS services.</p> 
<p><strong>CON332 –&nbsp;Build a Java Spring Application on Amazon ECS</strong><br /> This workshop teaches you how to lift and shift existing Spring and Spring Cloud applications onto the AWS platform. Learn how to build a Spring application container, understand bootstrap secrets, push container images to Amazon ECR, and deploy the application to Amazon ECS. Then, learn how to configure the deployment for production.</p> 
<b>THURSDAY 11/30</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON201 –&nbsp;Containers on AWS – State of the Union</strong><br /> Just over four years after the first public release of Docker, and three years to the day after the launch of Amazon ECS, the use of containers has surged to run a significant percentage of production workloads at startups and enterprise organizations. Join Deepak Singh, General Manager of Amazon Container Services, as he covers the state of containerized application development and deployment trends, new container capabilities on AWS that are available now, options for running containerized applications on AWS, and how AWS customers successfully run container workloads in production.</p> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON304 –&nbsp;Batch Processing with Containers on AWS</strong><br /> Batch processing is useful to analyze large amounts of data. But configuring and scaling a cluster of virtual machines to process complex batch jobs can be difficult. In this talk, we show how to use containers on AWS for batch processing jobs that can scale quickly and cost-effectively. We also discuss AWS Batch, our fully managed batch-processing service. You also hear from GoPro and Here about how they use AWS to run batch processing jobs at scale including best practices for ensuring efficient scheduling, fine-grained monitoring, compute resource automatic scaling, and security for your batch jobs.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON406 –&nbsp;Architecting Container Infrastructure for Security and Compliance</strong><br /> While organizations gain agility and scalability when they migrate to containers and microservices, they also benefit from compliance and security, advantages that are often overlooked. In this session, Kelvin Zhu, lead software engineer at Okta, joins Mitch Beaumont, enterprise solutions architect at AWS, to discuss security best practices for containerized infrastructure. Learn how Okta built their development workflow with an emphasis on security through testing and automation. Dive deep into how containers enable automated security and compliance checks throughout the development lifecycle. Also understand best practices for implementing AWS security and secrets management services for any containerized service architecture.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON329 –&nbsp;Full Software Lifecycle Management for Containers Running on Amazon ECS</strong><br /> Learn how The Washington Post uses Amazon ECS to run Arc Publishing, a digital journalism platform that powers The Washington Post and a growing number of major media websites. Amazon ECS enabled The Washington Post to containerize their existing microservices architecture, avoiding a complete rewrite that would have delayed the platform’s launch by several years. In this session, Jason Bartz, Technical Architect at The Washington Post, discusses the platform’s architecture. He addresses the challenges of optimizing Arc Publishing’s workload, and managing the application lifecycle to support 2,000 containers running on more than 50 Amazon ECS clusters.</p> 
<p><strong>CON330 –&nbsp;Running Containerized HIPAA Workloads on AWS</strong><br /> Nihar Pasala, Engineer at Aetion, discusses the Aetion Evidence Platform, a system for generating the real-world evidence used by healthcare decision makers to implement value-based care. This session discusses the architecture Aetion uses to run HIPAA workloads using containers on Amazon ECS, best practices, and learnings.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON408 –&nbsp;Building a Machine Learning Platform Using Containers on AWS</strong><br /> DeepLearni.ng develops&nbsp;and implements machine learning models for complex enterprise applications. In this session, Thomas Rogers, Engineer for DeepLearni.ng discusses how they worked with Scotiabank to leverage&nbsp;Amazon ECS, Amazon ECR, Docker, GPU-accelerated Amazon EC2 instances, and TensorFlow to develop a retail risk model that helps manage payment collections for millions of Canadian credit card customers.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON319 –&nbsp;Interstella 8888: CICD for Containers on AWS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. &nbsp;Join this workshop to learn how to set up a CI/CD pipeline for containerized microservices. You get hands-on experience&nbsp;deploying Docker container images using Amazon ECS, AWS CloudFormation, AWS CodeBuild, and AWS CodePipeline,&nbsp;automating&nbsp;everything from code check-in to production.</p> 
<b>FRIDAY 12/1</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON405 –&nbsp;Moving to Amazon ECS – the Not-So-Obvious Benefits</strong><br /> If you ask 10 teams why they migrated to containers, you will likely get answers like ‘developer productivity’, ‘cost reduction’, and ‘faster scaling’. But teams often find there are several other ‘hidden’ benefits to using containers for their services. In this talk, Franziska Schmidt, Platform Engineer at Mapbox and Yaniv Donenfeld from&nbsp;AWS will discuss the obvious, and not so obvious benefits of moving to containerized architecture. These include using Docker and Amazon ECS to achieve shared libraries for dev teams, separating private infrastructure from shareable code, and making it easier for non-ops engineers to run services.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON331 –&nbsp;Deploying a Regulated Payments Application on Amazon ECS</strong><br /> Travelex discusses how they built an FCA-compliant international payments service using a microservices architecture on AWS. This chalk talk covers the challenges of designing and operating an Amazon ECS-based PaaS in a regulated environment using a DevOps model.</p> 
<h3>Workshops</h3> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON407 –&nbsp;Interstella 8888: Advanced Microservice Operations</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. In this workshop, you help Interstella 8888 build a modern microservices-based logistics system to save the company from financial ruin. We give you the hands-on experience you need to run microservices in the real world. This includes implementing advanced container scheduling and scaling to deal with variable service requests, implementing a service mesh, issue&nbsp;tracing with AWS X-Ray, container and instance-level logging with Amazon CloudWatch, and load testing.</p> 
<b>Know before you go</b> 
<p>Want to brush up on your container knowledge before re:Invent? Here are some helpful resources to get started:</p> 
<li><a href="https://aws.amazon.com/ecs/getting-started">Amazon ECS Getting Started</a></li> 
<li><a href="https://aws.amazon.com/ecs/resources">Amazon ECS Resources</a></li> 
<li><a href="https://github.com/nathanpeck/awesome-ecs">Nathan Peck’s AWSome ECS</a></li> 
<li>Docs 
<li><a href="https://aws.amazon.com/documentation/ec2/">Amazon EC2</a></li> 
<li><a href="https://aws.amazon.com/documentation/ecs/">Amazon ECS</a></li> 
</ul> </li> 
<li>Blogs 
<li><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/">AWS Compute Blog</a></li> 
<li><a href="https://aws.amazon.com/blogs/aws/category/ec2-container-service/">AWS Blog</a></li> 
</ul> </li> 
<p>– Tiffany<br /> <a href="https://twitter.com/tiffanyfayj">@tiffanyfayj</a></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/2017/" rel="tag">2017</a>, <a href="https://aws.amazon.com/blogs/compute/tag/reinvent/" rel="tag">re:Invent</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3321');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Resume AWS Step Functions from Any State</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-11-16T15:31:27+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/resume-aws-step-functions-from-any-state/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3166" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3166&amp;disqus_title=Resume+AWS+Step+Functions+from+Any+State&amp;disqus_url=https://aws.amazon.com/blogs/compute/resume-aws-step-functions-from-any-state/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3166');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Yash.jpeg"><img src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Yash.jpeg" alt="" width="119" height="160" class="alignnone size-full wp-image-3167" /></a><br /> <strong>Yash Pant, Solutions Architect, AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Aaron.jpeg"><img src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Aaron.jpeg" alt="" width="119" height="160" class="alignnone size-full wp-image-3168" /></a><br /> <strong>Aaron Friedman, Partner Solutions Architect, AWS</strong></p> 
<p>When we discuss how to build applications with customers, we often align to the <a href="https://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf">Well Architected Framework</a> pillars of security, reliability, performance efficiency, cost optimization, and operational excellence. Designing for failure is an essential component to developing well architected applications that are resilient to spurious errors that may occur.</p> 
<p>There are many ways you can use AWS services to achieve high availability and resiliency of your applications. For example, you can couple Elastic Load Balancing with Auto Scaling and Amazon EC2 instances to build highly available applications. Or use Amazon API Gateway and AWS Lambda to rapidly scale out a microservices-based architecture. Many AWS services have built in solutions to help with the appropriate error handling, such as <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">Dead Letter Queues (DLQ)</a> for Amazon SQS or <a href="http://docs.aws.amazon.com/batch/latest/userguide/job_retries.html">retries in AWS Batch</a>.</p> 
<p><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> is an AWS service that makes it easy for you to coordinate the components of distributed applications and microservices. Step Functions allows you to easily design for failure, by incorporating features such as <a href="https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html#amazon-states-language-retrying-after-error">error retries</a> and <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html">custom error handling</a> from AWS Lambda exceptions. These features allow you to programmatically handle many common error modes and build robust, reliable applications.</p> 
<p>In some rare cases, however, your application may fail in an unexpected manner. In these situations, you might not want to duplicate in a repeat execution those portions of your state machine that have already run. This is especially true when orchestrating long-running jobs or executing a complex state machine as part of a microservice. Here, you need to know the last successful state in your state machine from which to resume, so that you don't duplicate previous work. In this post, we present a solution to enable you to resume from any given state in your state machine in the case of an unexpected failure.</p> 
<b id="toc_1">Resuming from a given state</b> 
<p>To resume a failed state machine execution from the state at which it failed, you first run a script that dynamically creates a new state machine. When the new state machine is executed, it resumes the failed execution from the point of failure. The script contains the following two primary steps:</p> 
<ol> 
<li>Parse the execution history of the failed execution to find the name of the state at which it failed, as well as the JSON input to that state.</li> 
<li>Create a new state machine, which adds an additional state to failed state machine, called &quot;GoToState&quot;. &quot;GoToState&quot; is a <em>choice</em> state at the beginning of the state machine that branches execution directly to the failed state, allowing you to skip states that had succeeded in the previous execution.</li> 
</ol> 
<p>The full script along with a CloudFormation template that creates a demo of this is available in the <a href="https://github.com/awslabs/aws-sfn-resume-from-any-state">aws-sfn-resume-from-any-state</a> GitHub repo.</p> 
<b id="toc_2">Diving into the script</b> 
<p>In this section, we walk you through the script and highlight the core components of its functionality. The script contains a main function, which adds a command line parameter for the <em>failedExecutionArn</em> so that you can easily call the script from the command line:</p> 
<pre><code class="language-bash">python gotostate.py --failedExecutionArn '&lt;Failed_Execution_Arn&gt;'</code></pre> 
<b id="toc_3">Identifying the failed state in your execution</b> 
<p>First, the script extracts the name of the failed state along with the input to that state. It does so by using the failed state machine execution history, which is identified by the Amazon Resource Name (ARN) of the execution. The failed state is marked in the execution history, along with the input to that state (which is also the output of the preceding successful state). The script is able to parse these values from the log.</p> 
<p>The script loops through the execution history of the failed state machine, and traces it backwards until it finds the failed state. If the state machine failed in a parallel state, then it must restart from the beginning of the parallel state. The script is able to capture the name of the parallel state that failed, rather than any substate within the parallel state that may have caused the failure. The following code is the Python function that does this.</p> 
<pre><code class="language-python">
def parseFailureHistory(failedExecutionArn):
'''
Parses the execution history of a failed state machine to get the name of failed state and the input to the failed state:
Input failedExecutionArn = A string containing the execution ARN of a failed state machine y
Output = A list with two elements: [name of failed state, input to failed state]
'''
failedAtParallelState = False
try:
#Get the execution history
response = client.get\_execution\_history(
executionArn=failedExecutionArn,
reverseOrder=True
)
failedEvents = response['events']
except Exception as ex:
raise ex
#Confirm that the execution actually failed, raise exception if it didn't fail.
try:
failedEvents[0]['executionFailedEventDetails']
except:
raise('Execution did not fail')
'''
If you have a 'States.Runtime' error (for example, if a task state in your state machine attempts to execute a Lambda function in a different region than the state machine), get the ID of the failed state, and use it to determine the failed state name and input.
'''
if failedEvents[0]['executionFailedEventDetails']['error'] == 'States.Runtime':
failedId = int(filter(str.isdigit, str(failedEvents[0]['executionFailedEventDetails']['cause'].split()[13])))
failedState = failedEvents[-1 \* failedId]['stateEnteredEventDetails']['name']
failedInput = failedEvents[-1 \* failedId]['stateEnteredEventDetails']['input']
return (failedState, failedInput)
'''
You need to loop through the execution history, tracing back the executed steps.
The first state you encounter is the failed state. If you failed on a parallel state, you need the name of the parallel state rather than the name of a state within a parallel state that it failed on. This is because you can only attach goToState to the parallel state, but not a substate within the parallel state.
This loop starts with the ID of the latest event and uses the previous event IDs to trace back the execution to the beginning (id 0). However, it returns as soon it finds the name of the failed state.
'''
currentEventId = failedEvents[0]['id']
while currentEventId != 0:
#multiply event ID by -1 for indexing because you're looking at the reversed history
currentEvent = failedEvents[-1 \* currentEventId]
'''
You can determine if the failed state was a parallel state because it and an event with 'type'='ParallelStateFailed' appears in the execution history before the name of the failed state
'''
if currentEvent['type'] == 'ParallelStateFailed':
failedAtParallelState = True
'''
If the failed state is not a parallel state, then the name of failed state to return is the name of the state in the first 'TaskStateEntered' event type you run into when tracing back the execution history
'''
if currentEvent['type'] == 'TaskStateEntered' and failedAtParallelState == False:
failedState = currentEvent['stateEnteredEventDetails']['name']
failedInput = currentEvent['stateEnteredEventDetails']['input']
return (failedState, failedInput)
'''
If the failed state was a parallel state, then you need to trace execution back to the first event with 'type'='ParallelStateEntered', and return the name of the state
'''
if currentEvent['type'] == 'ParallelStateEntered' and failedAtParallelState:
failedState = failedState = currentEvent['stateEnteredEventDetails']['name']
failedInput = currentEvent['stateEnteredEventDetails']['input']
return (failedState, failedInput)
#Update the ID for the next execution of the loop
currentEventId = currentEvent['previousEventId']
</code></pre> 
<b id="toc_4">Create the new state machine</b> 
<p>The script uses the name of the failed state to create the new state machine, with &quot;GoToState&quot; branching execution directly to the failed state.</p> 
<p>To do this, the script requires the <a href="http://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html">Amazon States Language (ASL)</a> definition of the failed state machine. It modifies the definition to append &quot;GoToState&quot;, and create a new state machine from it.</p> 
<p>The script gets the ARN of the failed state machine from the execution ARN of the failed state machine. This ARN allows it to get the ASL definition of the failed state machine by calling the <a href="http://docs.aws.amazon.com/step-functions/latest/apireference/API_DescribeStateMachine.html">DesribeStateMachine</a> API action. It creates a new state machine with &quot;GoToState&quot;.</p> 
<p>When the script creates the new state machine, it also adds an additional input variable called &quot;resuming&quot;. When you execute this new state machine, you specify this resuming variable as <em>true</em> in the input JSON. This tells &quot;GoToState&quot; to branch execution to the state that had previously failed. Here's the function that does this:</p> 
<pre><code class="language-python">def attachGoToState(failedStateName, stateMachineArn):
'''
Given a state machine ARN and the name of a state in that state machine, create a new state machine that starts at a new choice state called 'GoToState'. &quot;GoToState&quot; branches to the named state, and sends the input of the state machine to that state, when a variable called &quot;resuming&quot; is set to True.
Input failedStateName = A string with the name of the failed state
stateMachineArn = A string with the ARN of the state machine
Output response from the create_state_machine call, which is the API call that creates a new state machine
'''
try:
response = client.describe\_state\_machine(
stateMachineArn=stateMachineArn
)
except:
raise('Could not get ASL definition of state machine')
roleArn = response['roleArn']
stateMachine = json.loads(response['definition'])
#Create a name for the new state machine
newName = response['name'] + '-with-GoToState'
#Get the StartAt state for the original state machine, because you point the 'GoToState' to this state
originalStartAt = stateMachine['StartAt']
'''
Create the GoToState with the variable $.resuming.
If new state machine is executed with $.resuming = True, then the state machine skips to the failed state.
Otherwise, it executes the state machine from the original start state.
'''
goToState = {'Type':'Choice', 'Choices':[{'Variable':'$.resuming', 'BooleanEquals':False, 'Next':originalStartAt}], 'Default':failedStateName}
#Add GoToState to the set of states in the new state machine
stateMachine['States']['GoToState'] = goToState
#Add StartAt
stateMachine['StartAt'] = 'GoToState'
#Create new state machine
try:
response = client.create_state_machine(
name=newName,
definition=json.dumps(stateMachine),
roleArn=roleArn
)
except:
raise('Failed to create new state machine with GoToState')
return response
</code></pre> 
<b id="toc_5">Testing the script</b> 
<p>Now that you understand how the script works, you can test it out.</p> 
<p>The following screenshot shows an example state machine that has failed, called &quot;TestMachine&quot;. This state machine successfully completed &quot;FirstState&quot; and &quot;ChoiceState&quot;, but when it branched to &quot;FirstMatchState&quot;, it failed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_1.png"><img src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_1.png" alt="" width="3332" height="1204" class="aligncenter size-full wp-image-3214" /></a></p> 
<p>Use the script to create a new state machine that allows you to rerun this state machine, but skip the &quot;FirstState&quot; and the &quot;ChoiceState&quot; steps that already succeeded. You can do this by calling the script as follows:</p> 
<pre><code class="language-bash">python gotostate.py --failedExecutionArn 'arn:aws:states:us-west-2:&lt;AWS_ACCOUNT_ID&gt;:execution:TestMachine-with-GoToState:b2578403-f41d-a2c7-e70c-7500045288595</code></pre> 
<p>This creates a new state machine called &quot;TestMachine-with-GoToState&quot;, and returns its ARN, along with the input that had been sent to &quot;FirstMatchState&quot;. You can then inspect the input to determine what caused the error. In this case, you notice that the input to &quot;FirstMachState&quot; was the following:</p> 
<pre><code class="language-json">{
&quot;foo&quot;: 1,
&quot;Message&quot;: true
}</code></pre> 
<p>However, this state machine expects the &quot;Message&quot; field of the JSON to be a string rather than a Boolean. Execute the new &quot;TestMachine-with-GoToState&quot; state machine, change the input to be a string, and add the &quot;resuming&quot; variable that &quot;GoToState&quot; requires:</p> 
<pre><code class="language-json">{
&quot;foo&quot;: 1,
&quot;Message&quot;: &quot;Hello!&quot;,
&quot;resuming&quot;:true
}</code></pre> 
<p>When you execute the new state machine, it skips &quot;FirstState&quot; and &quot;ChoiceState&quot;, and goes directly to &quot;FirstMatchState&quot;, which was the state that failed:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_2.png"><img src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_2.png" alt="" width="3214" height="1396" class="aligncenter size-full wp-image-3215" /></a></p> 
<p>Look at what happens when you have a state machine with multiple parallel steps. This example is included in the <a href="https://github.com/awslabs/aws-sfn-resume-from-any-state">GitHub repository</a> associated with this post. The repo contains a CloudFormation template that sets up this state machine and provides instructions to replicate this solution.</p> 
<p>The following state machine, &quot;ParallelStateMachine&quot;, takes an input through two subsequent parallel states before doing some final processing and exiting, along with the JSON with the ASL definition of the state machine.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_3.png"><img src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_3.png" alt="" width="1640" height="778" class="aligncenter size-full wp-image-3216" /></a></p> 
<pre><code class="language-json">{
&quot;Comment&quot;: &quot;An example of the Amazon States Language using a parallel state to execute two branches at the same time.&quot;,
&quot;StartAt&quot;: &quot;Parallel&quot;,
&quot;States&quot;: {
&quot;Parallel&quot;: {
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;ResultPath&quot;:&quot;$.output&quot;,
&quot;Next&quot;: &quot;Parallel 2&quot;,
&quot;Branches&quot;: [
{
&quot;StartAt&quot;: &quot;Parallel Step 1, Process 1&quot;,
&quot;States&quot;: {
&quot;Parallel Step 1, Process 1&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaA&quot;,
&quot;End&quot;: true
}
}
},
{
&quot;StartAt&quot;: &quot;Parallel Step 1, Process 2&quot;,
&quot;States&quot;: {
&quot;Parallel Step 1, Process 2&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaA&quot;,
&quot;End&quot;: true
}
}
}
]
},
&quot;Parallel 2&quot;: {
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;Next&quot;: &quot;Final Processing&quot;,
&quot;Branches&quot;: [
{
&quot;StartAt&quot;: &quot;Parallel Step 2, Process 1&quot;,
&quot;States&quot;: {
&quot;Parallel Step 2, Process 1&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;: true
}
}
},
{
&quot;StartAt&quot;: &quot;Parallel Step 2, Process 2&quot;,
&quot;States&quot;: {
&quot;Parallel Step 2, Process 2&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;: true
}
}
}
]
},
&quot;Final Processing&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaC&quot;,
&quot;End&quot;: true
}
}
}</code></pre> 
<p>First, use an input that initially fails:</p> 
<pre><code class="language-json">{
&quot;Message&quot;: &quot;Hello!&quot;
}</code></pre> 
<p>This fails because the state machine expects you to have a variable in the input JSON called &quot;foo&quot; in the second parallel state to run &quot;Parallel Step 2, Process 1&quot; and &quot;Parallel Step 2, Process 2&quot;. Instead, the original input gets processed by the first parallel state and produces the following output to pass to the second parallel state:</p> 
<pre><code class="language-json">{
&quot;output&quot;: [
{
&quot;Message&quot;: &quot;Hello!&quot;
},
{
&quot;Message&quot;: &quot;Hello!&quot;
}
],
}</code></pre> 
<p>Run the script on the failed state machine to create a new state machine that allows it to resume directly at the second parallel state instead of having to redo the first parallel state. This creates a new state machine called &quot;ParallelStateMachine-with-GoToState&quot;. The following JSON was created by the script to define the new state machine in ASL. It contains the &quot;GoToState&quot; value that was attached by the script.</p> 
<pre><code class="language-json">{
&quot;Comment&quot;:&quot;An example of the Amazon States Language using a parallel state to execute two branches at the same time.&quot;,
&quot;States&quot;:{
&quot;Final Processing&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaC&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
},
&quot;GoToState&quot;:{
&quot;Default&quot;:&quot;Parallel 2&quot;,
&quot;Type&quot;:&quot;Choice&quot;,
&quot;Choices&quot;:[
{
&quot;Variable&quot;:&quot;$.resuming&quot;,
&quot;BooleanEquals&quot;:false,
&quot;Next&quot;:&quot;Parallel&quot;
}
]
},
&quot;Parallel&quot;:{
&quot;Branches&quot;:[
{
&quot;States&quot;:{
&quot;Parallel Step 1, Process 1&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaA&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 1, Process 1&quot;
},
{
&quot;States&quot;:{
&quot;Parallel Step 1, Process 2&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:LambdaA&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 1, Process 2&quot;
}
],
&quot;ResultPath&quot;:&quot;$.output&quot;,
&quot;Type&quot;:&quot;Parallel&quot;,
&quot;Next&quot;:&quot;Parallel 2&quot;
},
&quot;Parallel 2&quot;:{
&quot;Branches&quot;:[
{
&quot;States&quot;:{
&quot;Parallel Step 2, Process 1&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 2, Process 1&quot;
},
{
&quot;States&quot;:{
&quot;Parallel Step 2, Process 2&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 2, Process 2&quot;
}
],
&quot;Type&quot;:&quot;Parallel&quot;,
&quot;Next&quot;:&quot;Final Processing&quot;
}
},
&quot;StartAt&quot;:&quot;GoToState&quot;
}</code></pre> 
<p>You can then execute this state machine with the correct input by adding the &quot;foo&quot; and &quot;resuming&quot; variables:</p> 
<pre><code class="language-json">{
&quot;foo&quot;: 1,
&quot;output&quot;: [
{
&quot;Message&quot;: &quot;Hello!&quot;
},
{
&quot;Message&quot;: &quot;Hello!&quot;
}
],
&quot;resuming&quot;: true
}</code></pre> 
<p>This yields the following result. Notice that this time, the state machine executed successfully to completion, and skipped the steps that had previously failed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_4.png"><img src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_4.png" alt="" width="1626" height="854" class="aligncenter size-full wp-image-3217" /></a></p> 
<hr /> 
<b id="toc_6">Conclusion</b> 
<p>When you're building out complex workflows, it's important to be prepared for failure. You can do this by taking advantage of features such as automatic error retries in Step Functions and custom error handling of Lambda exceptions.</p> 
<p>Nevertheless, state machines still have the possibility of failing. With the methodology and script presented in this post, you can resume a failed state machine from its point of failure. This allows you to skip the execution of steps in the workflow that had already succeeded, and recover the process from the point of failure.</p> 
<p>To see more examples, please visit the <a href="https://aws.amazon.com/step-functions/getting-started/" target="_blank" rel="noopener noreferrer">Step Functions Getting Started</a> page.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ec2/" rel="tag">EC2</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3166');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Event-Driven Computing with Amazon SNS and AWS Compute, Storage, Database, and Networking Services</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-16T12:51:46+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-elastic-file-system-efs/" title="View all posts in Amazon Elastic File System (EFS)*"><span property="articleSection">Amazon Elastic File System (EFS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-elasticache/" title="View all posts in Amazon ElastiCache*"><span property="articleSection">Amazon ElastiCache*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/analytics/amazon-elasticsearch-service/" title="View all posts in Amazon Elasticsearch Service*"><span property="articleSection">Amazon Elasticsearch Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-glacier/" title="View all posts in Amazon Glacier*"><span property="articleSection">Amazon Glacier*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/analytics/amazon-kinesis/" title="View all posts in Amazon Kinesis*"><span property="articleSection">Amazon Kinesis*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-rds/" title="View all posts in Amazon RDS*"><span property="articleSection">Amazon RDS*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-redshift/" title="View all posts in Amazon Redshift*"><span property="articleSection">Amazon Redshift*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-route-53/" title="View all posts in Amazon Route 53*"><span property="articleSection">Amazon Route 53*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/auto-scaling/" title="View all posts in Auto Scaling*"><span property="articleSection">Auto Scaling*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-codepipeline/" title="View all posts in AWS CodePipeline*"><span property="articleSection">AWS CodePipeline*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/aws-direct-connect/" title="View all posts in AWS Direct Connect*"><span property="articleSection">AWS Direct Connect*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-elastic-beanstalk/" title="View all posts in AWS Elastic Beanstalk*"><span property="articleSection">AWS Elastic Beanstalk*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/analytics/aws-glue/" title="View all posts in AWS Glue*"><span property="articleSection">AWS Glue*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/aws-snowball/" title="View all posts in AWS Snowball*"><span property="articleSection">AWS Snowball*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/elastic-load-balancing/" title="View all posts in Elastic Load Balancing*"><span property="articleSection">Elastic Load Balancing*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/event-driven-computing-with-amazon-sns-compute-storage-database-and-networking-services/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3282" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3282&amp;disqus_title=Event-Driven+Computing+with+Amazon+SNS+and+AWS+Compute%2C+Storage%2C+Database%2C+and+Networking+Services&amp;disqus_url=https://aws.amazon.com/blogs/compute/event-driven-computing-with-amazon-sns-compute-storage-database-and-networking-services/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3282');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by Otavio&nbsp;Ferreira,&nbsp;Manager, Software Development, AWS Messaging</em></p> 
<p>Like other developers around the world, you may be tackling increasingly complex business problems. A key success factor, in that case, is the ability to break down a large project scope into smaller, more manageable components. A service-oriented architecture guides you toward designing systems as a collection of loosely coupled, independently scaled, and highly reusable services. Microservices take this even further. To improve performance and scalability, they promote fine-grained interfaces and lightweight protocols.</p> 
<p>However, the communication among isolated microservices can be challenging. Services are often deployed onto independent servers and don’t share any compute or storage resources. Also, you should avoid hard dependencies among microservices, to preserve maintainability and reusability.</p> 
<p>If you apply the <a href="https://aws.amazon.com/pub-sub-messaging/">pub/sub design pattern</a>, you can effortlessly decouple and independently scale out your microservices and serverless architectures. A pub/sub messaging service, such as <a href="https://aws.amazon.com/sns/">Amazon SNS</a>, promotes event-driven computing that statically decouples event publishers from subscribers, while dynamically allowing for the exchange of messages between them. An event-driven architecture also introduces the responsiveness needed to deal with complex problems, which are often unpredictable and asynchronous.<span id="more-3282"></span></p> 
<b>What is event-driven computing?</b> 
<p>Given the context of microservices, event-driven computing is a model in which subscriber services automatically perform work in response to events triggered by publisher services. This paradigm can be applied to automate workflows while decoupling the services that collectively and independently work to fulfil these workflows. <a href="https://aws.amazon.com/sns/">Amazon SNS</a> is an event-driven computing hub, in the AWS Cloud, that has native integration with several AWS publisher and subscriber services.</p> 
<b>Which AWS services publish events to SNS natively?</b> 
<p>Several AWS services have been integrated as SNS publishers and, therefore, can natively trigger event-driven computing for a variety of use cases. In this post, I specifically cover AWS compute, storage, database, and networking services, as depicted below.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide01.png"><img class="aligncenter size-full wp-image-3292" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide01.png" alt="" width="720" height="480" /></a></p> 
<b>Compute services</b> 
<li><a href="https://aws.amazon.com/autoscaling/"><strong>Auto Scaling:</strong></a> Helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You can configure Auto Scaling lifecycle hooks to trigger events, as Auto Scaling resizes your EC2 cluster.As an example, you may want to warm up the local cache store on newly launched EC2 instances, and also download log files from other EC2 instances that are about to be terminated. To make this happen, set an SNS topic as your Auto Scaling group’s notification target, then subscribe two Lambda functions to this SNS topic. The first function is responsible for handling scale-out events (to warm up cache upon provisioning), whereas the second is in charge of handling scale-in events (to download logs upon termination). 
<li><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/ASGettingNotifications.html">Getting SNS Notifications When Your Auto Scaling Group Scales</a></li> 
<li><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/lifecycle-hooks.html">Auto Scaling Lifecycle Hooks</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide02.png"><img class="aligncenter size-full wp-image-3293" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide02.png" alt="" width="680" height="230" /></a></p> 
<li><strong><a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk:</a></strong> An easy-to-use service for deploying and scaling web applications and web services developed in a number of programming languages. You can configure event notifications for your Elastic Beanstalk environment so that notable events can be automatically published to an SNS topic, then pushed to topic subscribers.As an example, you may use this event-driven architecture to coordinate your continuous integration pipeline (such as Jenkins CI). That way, whenever an environment is created, Elastic Beanstalk publishes this event to an SNS topic, which triggers a subscribing Lambda function, which then kicks off a CI job against your newly created Elastic Beanstalk environment. 
<li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html">Configuring Amazon SNS Notifications with Elastic Beanstalk</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide03.png"><img class="aligncenter size-full wp-image-3294" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide03.png" alt="" width="680" height="230" /></a></p> 
<li><a href="https://aws.amazon.com/elasticloadbalancing/"><strong>Elastic Load Balancing:</strong> </a>Automatically distributes incoming application traffic across Amazon EC2 instances, containers, or other resources identified by IP addresses.You can configure CloudWatch alarms on Elastic Load Balancing metrics, to automate the handling of events derived from Classic Load Balancers. As an example, you may leverage this event-driven design to automate latency profiling in an Amazon ECS cluster behind a Classic Load Balancer. In this example, whenever your ECS cluster breaches your load balancer latency threshold, an event is posted by CloudWatch to an SNS topic, which then triggers a subscribing Lambda function. This function runs a task on your ECS cluster to trigger a latency profiling tool, hosted on the cluster itself. This can enhance your latency troubleshooting exercise by making it timely. 
<li><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html">CloudWatch Metrics and SNS Notifications for Your Classic Load Balancer</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide04.png"><img class="aligncenter size-full wp-image-3295" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide04.png" alt="" width="680" height="230" /></a></p> 
<b>Storage services</b> 
<li><strong><a href="https://aws.amazon.com/s3/">Amazon S3:</a>&nbsp;</strong>Object storage built to store and retrieve any amount of data.You can enable S3 event notifications, and automatically get them posted to SNS topics, to automate a variety of workflows. For instance, imagine that you have an S3 bucket to store incoming resumes from candidates, and a fleet of EC2 instances to encode these resumes from their original format (such as Word or text) into a portable format (such as PDF).In this example, whenever new files are uploaded to your input bucket, S3 publishes these events to an SNS topic, which in turn pushes these messages into subscribing SQS queues. Then, encoding workers running on EC2 instances poll these messages from the SQS queues; retrieve the original files from the input S3 bucket; encode them into PDF; and&nbsp;finally store them in an output S3 bucket. 
<li><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">Configuring Amazon S3 Event Notifications</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html">Configuring Amazon S3 Buckets for Amazon SNS Notifications</a> (Walkthrough)</li> 
<li><a href="https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/">Messaging Fan-out Pattern for Serverless Architectures Using Amazon SNS</a> (Multimedia Encoding Example)</li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide05.png"><img class="aligncenter size-full wp-image-3297" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide05.png" alt="" width="680" height="230" /></a></p> 
<li><a href="https://aws.amazon.com/efs/"><strong>Amazon EFS:</strong></a> Provides simple and scalable file storage, for use with Amazon EC2 instances, in the AWS Cloud.You can configure CloudWatch alarms on EFS metrics, to automate the management of your EFS systems. For example, consider a highly parallelized genomics analysis application that runs against an EFS system. By default, this file system is instantiated on the “General Purpose” performance mode. Although this performance mode allows for lower latency, it might eventually impose a scaling bottleneck. Therefore, you may leverage an event-driven design to handle it automatically.Basically, as soon as the EFS metric “Percent I/O Limit” breaches 95%, CloudWatch could post this event to an SNS topic, which in turn would push this message into a subscribing Lambda function. This function automatically creates a new file system, this time on the “Max I/O” performance mode, then switches the genomics analysis application to this new file system. As a result, your application starts experiencing higher I/O throughput rates. 
<li><a href="http://docs.aws.amazon.com/efs/latest/ug/performance.html">Amazon EFS Performance</a></li> 
<li><a href="http://docs.aws.amazon.com/efs/latest/ug/monitoring_automated_manual.html">Amazon EFS Monitoring Tools</a></li> 
<li><a href="http://docs.aws.amazon.com/efs/latest/ug/creating_alarms.html">Creating CloudWatch Alarms and SNS Topics to Monitor Amazon EFS</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide06.png"><img class="aligncenter size-full wp-image-3298" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide06.png" alt="" width="680" height="260" /></a></p> 
<li><a href="https://aws.amazon.com/glacier/"><strong>Amazon Glacier:</strong></a> A secure, durable, and low-cost cloud storage service for data archiving and long-term backup.You can set a notification configuration on an Amazon Glacier vault so that when a job completes, a message is published to an SNS topic. Retrieving an archive from Amazon Glacier is a two-step asynchronous operation, in which you first initiate a job, and then download the output after the job completes. Therefore, SNS helps you eliminate polling your Amazon Glacier vault to check whether your job has been completed, or not. As usual, you may subscribe SQS queues, Lambda functions, and HTTP endpoints to your SNS topic, to be notified when your Amazon Glacier job is done. 
<li><a href="http://docs.aws.amazon.com/amazonglacier/latest/dev/configuring-notifications.html">Configuring Vault Notifications in Amazon Glacier</a></li> 
<li><a href="http://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html">Retrieving Amazon Glacier Archives</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide07.png"><img class="aligncenter size-full wp-image-3300" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide07.png" alt="" width="680" height="230" /></a></p> 
<li><a href="https://aws.amazon.com/snowball/"><strong>AWS Snowball:</strong></a> A petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data.You can leverage Snowball notifications to automate workflows related to importing data into and exporting data from AWS. More specifically, whenever your Snowball job status changes, Snowball can publish this event to an SNS topic, which in turn can broadcast the event to all its subscribers.As an example, imagine a Geographic Information System (GIS) that distributes high-resolution satellite images to users via Web browser. In this example, the GIS vendor could capture up to 80 TB of satellite images; create a Snowball job to import these files from an on-premises system to an S3 bucket; and provide an SNS topic ARN to be notified upon job status changes in Snowball. After Snowball changes the job status from “Importing” to “Completed”, Snowball publishes this event to the specified SNS topic, which delivers this message to a subscribing Lambda function, which finally creates a CloudFront web distribution for the target S3 bucket, to serve the images to end users. 
<li><a href="http://docs.aws.amazon.com/snowball/latest/ug/notifications.html">Snowball Notifications with Amazon SNS</a> (AWS Snowball User Guide)</li> 
<li><a href="http://docs.aws.amazon.com/snowball/latest/api-reference/API_Notification.html">Snowball Notifications with Amazon SNS</a> (AWS Snowball API Reference)</li> 
<li><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.html">Getting Started with CloudFront</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide08.png"><img class="aligncenter size-full wp-image-3301" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide08.png" alt="" width="680" height="260" /></a></p> 
<b>Database services</b> 
<li><a href="https://aws.amazon.com/rds/"><strong>Amazon RDS:</strong></a> Makes it easy to set up, operate, and scale a relational database in the cloud.RDS leverages SNS to broadcast notifications when RDS events occur. As usual, these notifications can be delivered via any protocol supported by SNS, including SQS queues, Lambda functions, and HTTP endpoints.As an example, imagine that you own a social network website that has experienced organic growth, and needs to scale its compute and database resources on demand. In this case, you could provide an SNS topic to listen to RDS DB instance events. When the “Low Storage” event is published to the topic, SNS pushes this event to a subscribing Lambda function, which in turn leverages the RDS API to increase the storage capacity allocated to your DB instance. The provisioning itself takes place within the specified DB maintenance window. 
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html">Using Amazon RDS Event Notifications via Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ListEvents.html">Viewing Amazon RDS Events</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide09.png"><img class="aligncenter size-full wp-image-3302" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide09.png" alt="" width="680" height="230" /></a></p> 
<li><a href="https://aws.amazon.com/elasticache/"><strong>Amazon ElastiCache:</strong></a> A web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.ElastiCache can publish messages using Amazon SNS when significant events happen on your cache cluster. This feature can be used to refresh the list of servers on client machines connected to individual cache node endpoints of a cache cluster. For instance, an ecommerce website fetches product details from a cache cluster, with the goal of offloading a relational&nbsp;database and speeding up page load times. Ideally, you want to make sure that each web server always has an updated list of cache servers to which to connect.To automate this node discovery process, you can get your ElastiCache cluster to publish events to an SNS topic. Thus, when ElastiCache event “AddCacheNodeComplete” is published, your topic then pushes this event to all subscribing HTTP endpoints that serve your ecommerce website, so that these HTTP servers can update their list of cache nodes. 
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ElastiCacheSNS.html">ElastiCache Event Notifications via Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ECEvents.SNS.html">Managing ElastiCache Notifications with Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ECEvents.Viewing.html">Viewing ElastiCache Events</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide10.png"><img class="aligncenter size-full wp-image-3303" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide10.png" alt="" width="680" height="230" /></a></p> 
<li><a href="https://aws.amazon.com/redshift/"><strong>Amazon Redshift:</strong></a> A fully managed data warehouse that makes it simple to analyze data using standard SQL and BI (Business Intelligence) tools.Amazon Redshift uses SNS to broadcast relevant events so that data warehouse workflows can be automated. As an example, imagine a news website that sends clickstream data to a Kinesis Firehose stream, which then loads the data into Amazon Redshift, so that popular news and reading preferences might be surfaced on a BI tool. At some point though, this Amazon Redshift cluster might need to be resized, and the cluster enters a&nbsp;ready-only mode. Hence, this Amazon Redshift event is published to an SNS topic, which delivers this event to a subscribing Lambda function, which finally deletes the corresponding Kinesis Firehose delivery stream, so that clickstream data uploads can be put on hold.At a later point, after Amazon Redshift publishes the event that the maintenance window has been closed, SNS notifies a subscribing Lambda function accordingly, so that this function can re-create the Kinesis Firehose delivery stream, and resume clickstream data uploads to Amazon Redshift. 
<li><a href="http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-event-notifications.html">Amazon Redshift Event Notifications with Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/redshift/latest/mgmt/manage-event-notifications-console.html">Managing Event Notifications Using the Amazon Redshift Console</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide11.png"><img class="aligncenter size-full wp-image-3304" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide11.png" alt="" width="680" height="260" /></a></p> 
<li><a href="https://aws.amazon.com/dms/"><strong>AWS DMS:</strong></a> Helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.DMS also uses SNS to provide notifications when DMS events occur, which can automate database migration workflows. As an example, you might create data replication tasks to migrate an on-premises MS SQL database, composed of multiple tables, to MySQL. Thus, if replication tasks fail due to incompatible data encoding in the source tables, these events can be published to an SNS topic, which can push these messages into a subscribing SQS queue. Then, encoders running on EC2 can poll these messages from the SQS queue, encode the source tables into a compatible character set, and restart the corresponding replication tasks in DMS. This is an event-driven approach to a self-healing database migration process. 
<li><a href="http://docs.aws.amazon.com/dms/latest/userguide/CHAP_Events.html">Working with DMS Event Notifications via Amazon SNS</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide12.png"><img class="aligncenter size-full wp-image-3305" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide12.png" alt="" width="680" height="260" /></a></p> 
<b>Networking services</b> 
<li><a href="https://aws.amazon.com/route53/"><strong>Amazon Route 53:</strong></a> A highly available and scalable cloud-based DNS (Domain Name System). Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources.You can set CloudWatch alarms and get automated Amazon SNS notifications when the status of your Route 53 health check changes. As an example, imagine an online payment gateway that reports the health of its platform to merchants worldwide, via a status page. This page is hosted on EC2 and fetches platform health data from DynamoDB. In this case, you could configure a CloudWatch alarm for your Route 53 health check, so that when the alarm threshold is breached, and the payment gateway is no longer considered healthy, then CloudWatch publishes this event to an SNS topic, which pushes this message to a subscribing Lambda function, which finally updates the DynamoDB table that populates the status page. This event-driven approach avoids any kind of manual update to the status page visited by merchants. 
<li><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-monitor-view-status.html">Monitoring Route 53 Health Check Status and Getting Notifications via Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-creating.html">Creating and Updating Route 53 Health Checks with Notifications</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide13.png"><img class="aligncenter size-full wp-image-3306" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide13.png" alt="" width="680" height="230" /></a></p> 
<li><a href="https://aws.amazon.com/directconnect/"><strong>AWS Direct Connect (AWS DX):</strong></a> Makes it easy to establish a dedicated network connection from your premises to AWS, which can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.You can monitor physical DX connections using CloudWatch alarms, and send SNS messages when alarms change their status. As an example, when a DX connection state shifts to 0 (zero), indicating that the connection is down, this event can be published to an SNS topic, which can fan out this message to impacted servers through HTTP endpoints, so that they might reroute their traffic through a different connection instead. This is an event-driven approach to connectivity resilience. 
<li><a href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/monitoring-cloudwatch.html">Monitoring Direct Connect with Amazon CloudWatch and Amazon SNS</a></li> 
</ul> </li> 
<b><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide14.png"><img class="aligncenter size-full wp-image-3308" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide14.png" alt="" width="680" height="230" /></a>More event-driven computing on AWS</b> 
<p>In addition to SNS, event-driven computing is also addressed by <a href="https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/">Amazon CloudWatch Events</a>, which delivers a near real-time stream of system events that describe changes in AWS resources. With CloudWatch Events, you can route each event type to one or more targets, including:</p> 
<li>SNS topics</li> 
<li><a href="https://aws.amazon.com/sqs/">Amazon SQS</a> queues</li> 
<li><a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances</li> 
<li><a href="https://aws.amazon.com/ecs/">Amazon ECS</a> tasks</li> 
<li><a href="https://aws.amazon.com/kinesis/streams/">Amazon Kinesis Streams</a></li> 
<li><a href="https://aws.amazon.com/kinesis/firehose/">Amazon Kinesis Firehose</a> delivery streams</li> 
<li><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> functions</li> 
<li><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> state machines</li> 
<li><a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> pipelines</li> 
<p>Many AWS services publish events to CloudWatch. As an example, you can get CloudWatch Events to capture events on your ETL (Extract, Transform, Load) jobs running on <a href="https://aws.amazon.com/glue/">AWS Glue</a> and push failed ones to an SQS queue, so that you can retry them later.</p> 
<b>Conclusion</b> 
<p><a href="https://aws.amazon.com/sns/">Amazon SNS</a> is a pub/sub messaging service that can be used as an event-driven computing hub to AWS customers worldwide. By capturing events natively triggered by AWS services, such as EC2, S3 and RDS, you can automate and optimize all kinds of workflows, namely scaling, testing, encoding, profiling, broadcasting, discovery, failover, and much more. Business use cases presented in this post ranged from recruiting websites, to scientific research, geographic systems, social networks, retail websites, and news portals.</p> 
<p>Start now by visiting Amazon SNS in the <a href="https://console.aws.amazon.com/sns/">AWS Management Console</a>, or by trying the AWS 10-Minute Tutorial, <a href="https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/">Send Fan-out Event Notifications with Amazon SNS and Amazon SQS</a>.</p> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/event-driven-computing/" rel="tag">event-driven computing</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3282');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/dragen-arch.png" /> 
<b class="lb-b blog-post-title" property="name headline">Accelerating Precision Medicine at Scale</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2017-11-16T12:29:52+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/accelerating-precision-medicine-at-scale/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3270" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3270&amp;disqus_title=Accelerating+Precision+Medicine+at+Scale&amp;disqus_url=https://aws.amazon.com/blogs/compute/accelerating-precision-medicine-at-scale/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3270');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Aaron Friedman, Healthcare and Life Sciences Partner Solutions Architect, AWS and&nbsp;</em><em>Angel Pizarro, Genomics and Life Sciences Senior Solutions Architect, AWS</em></p> 
<p>&nbsp;</p> 
<p>Precision medicine is tailored to individuals based on quantitative signatures, including genomics, lifestyle, and environment. It is often considered to be the driving force behind the next wave of human health. Through new initiatives and technologies such as population-scale genomics sequencing and IoT-backed wearables, researchers and clinicians in both commercial and public sectors are gaining new, previously inaccessible insights.</p> 
<p>Many of these precision medicine initiatives are already happening on AWS. A few of these include:</p> 
<li><a href="https://precision.fda.gov/">PrecisionFDA</a> – This initiative is led by the US Food and Drug Administration. The goal is to define the next-generation standard of care for genomics in precision medicine.</li> 
<li><a href="https://precision.heart.org/">American Heart Association Precision Medicine Platform</a> – The platform gives researchers the ability to collaborate and analyze datasets, to better predict and intervene in cardiovascular disease and stroke.</li> 
<li><a href="https://www2.deloitte.com/us/en/pages/consulting/topics/convergehealth.html">Deloitte ConvergeHEALTH</a> – Gives healthcare and life sciences organizations the ability to analyze their disparate datasets on a singular real world evidence platform.</li> 
<p>Central to many of these initiatives is genomics, which gives healthcare organizations the ability to establish a baseline for longitudinal studies. Due to its wide applicability in precision medicine initiatives—from rare disease diagnosis to improving outcomes of clinical trials—genomics data is growing at a larger rate than Moore’s law across the globe. Many expect these datasets to grow to be in the range of tens of exabytes by 2025.</p> 
<p>Genomics data is also regularly re-analyzed by the community as researchers develop new computational methods or compare older data with newer genome references. These trends are driving innovations in data analysis methods and algorithms to address the massive increase of computational requirements.</p> 
<p><a href="http://www.edicogenome.com/">Edico Genome</a>, an AWS Partner Network (APN) Partner, has developed a novel solution that accelerates genomics analysis using field-programmable gate arrays, or FPGAs. Historically, Edico Genome deployed their FPGA appliances on-premises. When AWS announced the <a href="https://aws.amazon.com/ec2/instance-types/f1/">Amazon EC2 F1</a>&nbsp;FPGA-based instance family in December 2016, Edico Genome adopted a cloud-first strategy, became a <a href="https://aws.amazon.com/ec2/instance-types/f1/partners/">F1 launch partner</a>, and was one of the first partners to deploy FPGA-enabled applications on AWS.</p> 
<p>On October 19, 2017, Edico Genome partnered with the Children’s Hospital of Philadelphia (CHOP) to demonstrate their FPGA-accelerated genomic pipeline software, called DRAGEN. It can significantly reduce time-to-insight for patient genomes, and analyzed 1,000 genomes from the Center for Applied Genomics Biobank in the shortest time possible. This set a <a href="http://edicogenome.com/childrens-hospital-philadelphia-edico-genome-set-guinness-world-records/">Guinness World Record for the fastest analysis of 1000 whole human genomes</a>, and they did this using 1000 EC2 f1.2xlarge instances in a single AWS region. Not only were they able to analyze genomes at high throughput, they did so averaging approximately $3 per whole human genome of AWS compute for the analysis.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/minutes-dragen-run.png"><img class="aligncenter wp-image-3274" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/minutes-dragen-run.png" alt="minutes-dragon-run" width="600" height="339" /></a></p> 
<p>The version of DRAGEN that Edico Genome used for this analysis was also the same one used in the precisionFDA Hidden Treasures – Warm Up challenge, where they were one of the top performers in every assessment.</p> 
<p>In the remainder of this post, we walk through the architecture used by Edico Genome, combining EC2 F1 instances and AWS Batch to achieve this milestone.</p> 
<h3>EC2 F1 instances and Edico’s DRAGEN</h3> 
<p>EC2 F1 instances provide access to programmable hardware-acceleration using FPGAs at a cloud scale. AWS customers use F1 instances for a wide variety of applications, including big data, financial analytics and risk analysis, image and video processing, engineering simulations, AR/VR, and accelerated genomics.<br /> Edico Genome’s FPGA-backed DRAGEN Bio-IT Platform is now integrated with EC2 F1 instances. You can access the accuracy, speed, flexibility, and low compute cost of DRAGEN through a number of third-party platforms, <a href="https://aws.amazon.com/marketplace/seller-profile?id=0aad8c40-a94d-4a2a-82d1-eb5f95d165ae">AWS Marketplace</a>, and Edico Genome’s own platform. The DRAGEN platform offers a scalable, accelerated, and cost-efficient secondary analysis solution for a wide variety of genomics applications. Edico Genome also provides a highly optimized mechanism for the efficient storage of genomic data.</p> 
<h3>Scaling DRAGEN on AWS</h3> 
<p>Edico Genome used 1,000 EC2 F1 instances to help their customer, the Children’s Hospital of Philadelphia (CHOP), to process and analyze all 1,000 whole human genomes in parallel. They used AWS Batch to provision compute resources and orchestrate DRAGEN compute jobs across the 1,000 EC2 F1 instances. This solution successfully addressed the challenge of creating a scalable genomic processing pipeline that can easily scale to thousands of engines running in parallel.</p> 
<h4>Architecture</h4> 
<p>A simplified view of the architecture used for the analysis is shown in the following diagram:<br /> <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/dragen-arch.png"><img class="aligncenter wp-image-3277" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/dragen-arch.png" alt="dragen-architecture" width="600" height="372" /></a></p> 
<ol> 
<li>DRAGEN’s portal uses Elastic Load Balancing and Auto Scaling groups to scale out EC2 instances that submitted jobs to AWS Batch.</li> 
<li>Job metadata is stored in their Workflow Management (WFM) database, built on top of Amazon Aurora.</li> 
<li>The DRAGEN Workflow Manager API submits jobs to AWS Batch.</li> 
<li>These jobs are executed on the AWS Batch managed compute environment that was responsible for launching the EC2 F1 instances.</li> 
<li>These jobs run as Docker containers that have the requisite DRAGEN binaries for whole genome analysis.</li> 
<li>As each job runs, it retrieves and stores genomics data that is staged in Amazon S3.</li> 
</ol> 
<p>The steps listed previously can also be bucketed into the following higher-level layers:</p> 
<li>Workflow: &nbsp;Edico Genome used their Workflow Management API to orchestrate the submission of AWS Batch jobs. Metadata for the jobs (such as the S3 locations of the genomes, etc.) resides in the Workflow Management Database backed by Amazon Aurora.</li> 
<li>Batch execution: &nbsp;AWS Batch launches EC2 F1 instances and coordinates the execution of DRAGEN jobs on these compute resources. AWS Batch enabled Edico to quickly and easily scale up to the full number of instances they needed as jobs were submitted. They also scaled back down as each job was completed, to optimize for both cost and performance.</li> 
<li>Compute/job: &nbsp;Edico Genome stored their binaries in a Docker container that AWS Batch deployed onto each of the F1 instances, giving each instance the ability to run DRAGEN without the need to pre-install the core executables. The AWS based DRAGEN solution streams all genomics data from S3 for local computation and then writes the results to a destination bucket. They used an AWS Batch job role that specified the IAM permissions. The role ensured that DRAGEN only had access to the buckets or S3 key space it needed for the analysis. Jobs didn’t need to embed AWS credentials.</li> 
<p>We also covered these concepts previously in the <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Building High-Throughput Genomics Batch Workflows on AWS</a> series on the AWS Compute blog.</p> 
<h3>Walkthrough</h3> 
<p>In the following sections, we dive deeper into several tasks that enabled Edico Genome’s scalable FPGA genome analysis on AWS:</p> 
<ol> 
<li>Prepare your Amazon FPGA Image for AWS Batch</li> 
<li>Create a Dockerfile and build your Docker image</li> 
<li>Set up your AWS Batch FPGA compute environment</li> 
</ol> 
<h4>Prerequisites</h4> 
<p>In brief, you need a modern Linux distribution (3.10+), Amazon ECS Container Agent, awslogs driver, and Docker configured on your image. There are additional recommendations in the <a href="https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec">Compute Resource AMI specification</a>.</p> 
<h4>Preparing your Amazon FPGA Image for AWS Batch</h4> 
<p>You can use any Amazon Machine Image (AMI) or Amazon FPGA Image (AFI) with AWS Batch, provided that it meets the <a href="https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec">Compute Resource AMI specification</a>. This gives you the ability to customize any workload by increasing the size of root or data volumes, adding instance stores, and connecting with the FPGA (F) and GPU (G and P) instance families.</p> 
<p>Next, install the AWS CLI:</p> 
<p><code>pip install awscli</code></p> 
<p>Add any additional software required to interact with the FPGAs on the F1 instances.</p> 
<p>As a starting point, AWS publishes an <a href="https://aws.amazon.com/marketplace/pp/B06VVYBLZZ">FPGA Developer AMI</a> in the AWS Marketplace. It is based on a CentOS Linux image and includes pre-integrated FPGA development tools. It also includes the runtime tools required to develop and use custom FPGAs for hardware acceleration applications.</p> 
<p>For more information about how to set up custom AMIs for your AWS Batch managed compute environments, see <a href="https://docs.aws.amazon.com/batch/latest/userguide/create-batch-ami.html">Creating a Compute Resource AMI</a>.</p> 
<h4>Building your Dockerfile</h4> 
<p>There are two common methods for connecting to AWS Batch to run FPGA-enabled algorithms. The first method, which is the route Edico Genome took, involves storing your binaries in the Docker container itself and running that on top of an F1 instance with Docker installed. The following code example is what a Dockerfile to build your container might look like for this scenario.</p> 
<pre><code class="lang-bash"># DRAGEN_EXEC Docker image generator --
# Run this Dockerfile from a local directory that contains the latest release of
# - Dragen RPM and Linux DMA Driver available from Edico
# - Edico's Dragen WFMS Wrapper files
FROM centos:centos7
RUN rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# Install Basic packages needed for Dragen
RUN yum -y install \
perl \
sos \
coreutils \
gdb \
time \
systemd-libs \
bzip2-libs \
R \
ca-certificates \
ipmitool \
smartmontools \
rsync
# Install the Dragen RPM
RUN mkdir -m777 -p /var/log/dragen /var/run/dragen
ADD . /root
RUN rpm -Uvh /root/edico_driver*.rpm || true
RUN rpm -Uvh /root/dragen-aws*.rpm || true
# Auto generate the Dragen license
RUN /opt/edico/bin/dragen_lic -i auto
#########################################################
# Now install the Edico WFMS &quot;Wrapper&quot; functions
# Add development tools needed for some util
RUN yum groupinstall -y &quot;Development Tools&quot;
# Install necessary standard packages
RUN yum -y install \
dstat \
git \
python-devel \
python-pip \
time \
tree &amp;&amp; \
pip install --upgrade pip &amp;&amp; \
easy_install requests &amp;&amp; \
pip install psutil &amp;&amp; \
pip install python-dateutil &amp;&amp; \
pip install constants &amp;&amp; \
easy_install boto3
# Setup Python path used by the wrapper
RUN mkdir -p /opt/workflow/python/bin
RUN ln -s /usr/bin/python /opt/workflow/python/bin/python2.7
RUN ln -s /usr/bin/python /opt/workflow/python/bin/python
# Install d_haul and dragen_job_execute wrapper functions and associated packages
RUN mkdir -p /root/wfms/trunk/scheduler/scheduler
COPY scheduler/d_haul /root/wfms/trunk/scheduler/
COPY scheduler/dragen_job_execute /root/wfms/trunk/scheduler/
COPY scheduler/scheduler/aws_utils.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/constants.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/job_utils.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/logger.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/scheduler_utils.py&nbsp; /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/webapi.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/wfms_exception.py /root/wfms/trunk/scheduler/scheduler/
RUN touch /root/wfms/trunk/scheduler/scheduler/__init__.py
# Landing directory should be where DJX is located
WORKDIR &quot;/root/wfms/trunk/scheduler/&quot;
# Debug print of container's directories
RUN tree /root/wfms/trunk/scheduler
# Default behaviour. Over-ride with --entrypoint on docker run cmd line
ENTRYPOINT [&quot;/root/wfms/trunk/scheduler/dragen_job_execute&quot;]
CMD []</code></pre> 
<p><em>Note: Edico Genome’s custom Python wrapper functions for its Workflow Management System (WFMS) in the latter part of this Dockerfile should be replaced with functions that are specific to your workflow.</em></p> 
<p>The second method is to install binaries and then use Docker as a lightweight connector between AWS Batch and the AFI. For example, this might be a route you would choose to use if you were provisioning <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=edico+genome+dragen&amp;page=1&amp;ref_=nav_search_box">DRAGEN from the AWS Marketplace</a>.</p> 
<p>In this case, the Dockerfile would not contain the installation of the binaries to run DRAGEN, but would contain any other packages necessary for job completion. When you run your Docker container, you enable Docker to access the underlying file system.</p> 
<h3>Connecting to AWS Batch</h3> 
<p>AWS Batch provisions compute resources and runs your jobs, choosing the right instance types based on your job requirements and scaling down resources as work is completed. AWS Batch users submit a job, based on a template or “job definition” to an AWS Batch job queue.</p> 
<p>Job queues are mapped to one or more compute environments that describe the quantity and types of resources that AWS Batch can provision. In this case, Edico created a managed compute environment that was able to launch 1,000 EC2 F1 instances across multiple Availability Zones in us-east-1. As jobs are submitted to a job queue, the service launches the required quantity and types of instances that are needed. As instances become available, AWS Batch then runs each job within appropriately sized Docker containers.</p> 
<p>The Edico Genome workflow manager API submits jobs to an AWS Batch job queue. This job queue maps to an AWS Batch managed compute environment containing On-Demand F1 instances. In this section, you can set this up yourself.</p> 
<p>To create the compute environment that DRAGEN can use:</p> 
<p><code>aws batch create-compute-environment --cli-input-json file://&lt;path_to_json_file&gt;/F1OnDemand.json</code></p> 
<p>Where your JSON file contains the following code (replace with your own resource IDs):</p> 
<pre><code class="lang-json">
{
&quot;computeEnvironmentName&quot;: &quot;F1OnDemand&quot;,
&quot;type&quot;: &quot;MANAGED&quot;,
&quot;state&quot;: &quot;ENABLED&quot;,
&quot;computeResources&quot;: {
&quot;type&quot;: &quot;EC2&quot;,
&quot;minvCpus&quot;: 0,
&quot;maxvCpus&quot;: 128,
&quot;desiredvCpus&quot;: 0,
&quot;instanceTypes&quot;: [
&quot;f1.2xlarge&quot;,
&quot;f1.16xlarge&quot;
],
&quot;subnets&quot;: [
&quot;subnet-220c0e0a&quot;,
&quot;subnet-1a95556d&quot;,
&quot;subnet-978f6dce&quot;
],
&quot;securityGroupIds&quot;: [
&quot;sg-cf5093b2&quot;
],
&quot;ec2KeyPair&quot;: &quot;id_rsa&quot;,
&quot;instanceRole&quot;: &quot;ecsInstanceRole&quot;,
&quot;tags&quot;: {
&quot;Name&quot;: &quot;Batch Instance – F1OnDemand&quot;
}
},
&quot;serviceRole&quot;: &quot;arn:aws:iam::012345678910:role/service-role/AWSBatchServiceRole&quot;
}
</code></pre> 
<p>And the corresponding job queue:</p> 
<p><code>aws batch create-job-queue --cli-input-json file://&lt;path_to_json_file&gt;/dragen.json</code></p> 
<p>Where dragen.json is as follows:</p> 
<pre><code class="lang-json">
{
&quot;jobQueueName&quot;: &quot;DRAGEN-OnDemand&quot;,
&quot;state&quot;: &quot;ENABLED&quot;,
&quot;priority&quot;: 100,
&quot;computeEnvironmentOrder&quot;: 
[
{
&quot;order&quot;: 1,
&quot;computeEnvironment&quot;: &quot;F1OnDemand&quot;
}
]
}
</code></pre> 
<p>An f1.2xlarge EC2 instance contains one FPGA, eight vCPUs, and 122-GiB RAM. As DRAGEN requires an entire FPGA to run, Edico Genome needed to ensure that only one analysis per time executed on an instance. By using the f1.2xlarge vCPUs and memory as a proxy in their AWS Batch job definition, Edico Genome could ensure that only one job runs on an instance at a time. Here’s what that looks like in the AWS CLI:</p> 
<p><code>aws batch register-job-definition --job-definition-name dragen-wgs --type container --container-properties '{ &quot;image&quot;: ${DRAGEN_IMAGE}, &quot;vcpus&quot;: 8, &quot;memory&quot;: 120000}'</code></p> 
<p>Now, you can submit jobs easily to your DRAGEN environment:</p> 
<p><code>aws batch submit-job --job-name dragen-run1 --job-queue DRAGEN-OnDemand --job-definition dragen-wgs --container-overrides command=${RUN_PARAMETERS}</code></p> 
<p>You can query the status of your DRAGEN job with the following command:</p> 
<p><code>aws batch describe-jobs --jobs &lt;the job ID from the above command&gt;</code></p> 
<p>The logs for your job are written to the /aws/batch/job CloudWatch log group.</p> 
<h3>Conclusion</h3> 
<p>In this post, we demonstrated how to set up an environment with AWS Batch that can run DRAGEN on EC2 F1 instances at scale. If you followed the walkthrough, you’ve replicated much of the architecture Edico Genome used to set the Guinness World Record.</p> 
<p>There are several ways in which you can harness the computational power of DRAGEN to analyze genomes at scale. First, DRAGEN is available through several different genomics platforms, such as the <a href="https://aws.amazon.com/blogs/apn/how-dnanexus-and-edico-genome-are-powering-precision-medicine-on-amazon-web-services-aws/">DNAnexus Platform</a>. DRAGEN is also available on the <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=dragen&amp;page=1&amp;ref_=nav_search_box">AWS Marketplace</a>. You can apply the architecture presented in this post to build a scalable solution that is both performant and cost-optimized.</p> 
<p>For more information about how AWS Batch can facilitate genomics processing at scale, be sure to check out our <a href="https://github.com/awslabs/aws-batch-genomics">aws-batch-genomics</a> GitHub repo on high-throughput genomics on AWS.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-ec2/" rel="tag">Amazon EC2</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-batch/" rel="tag">AWS Batch</a>, <a href="https://aws.amazon.com/blogs/compute/tag/batch/" rel="tag">Batch</a>, <a href="https://aws.amazon.com/blogs/compute/tag/f1/" rel="tag">F1</a>, <a href="https://aws.amazon.com/blogs/compute/tag/genomics/" rel="tag">genomics</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3270');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/1_HRCaptureProcess.png" /> 
<b class="lb-b blog-post-title" property="name headline">Capturing Custom, High-Resolution Metrics from Containers Using AWS Step Functions and AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-11-16T07:25:18+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/capturing-custom-high-resolution-metrics-from-containers-using-aws-step-functions-and-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3251" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3251&amp;disqus_title=Capturing+Custom%2C+High-Resolution+Metrics+from+Containers+Using+AWS+Step+Functions+and+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/capturing-custom-high-resolution-metrics-from-containers-using-aws-step-functions-and-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3251');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by Trevor Sullivan, AWS Solutions Architect</em></p> 
<p>When&nbsp;you&nbsp;deploy containers&nbsp;with <a href="https://aws.amazon.com/ecs">Amazon ECS</a>, are you gathering all of the key metrics so that you can correctly monitor the overall health of your ECS cluster?</p> 
<p>By default, ECS writes metrics to <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch</a> in 5-minute increments. For complex or large services, this may not be sufficient to make scaling decisions quickly. You may want to respond immediately to changes in workload or to identify application performance problems. Last July, CloudWatch announced <a href="https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-cloudwatch-introduces-high-resolution-custom-metrics-and-alarms/">support for high-resolution metrics</a>, up to a per-second basis.</p> 
<p>These high-resolution metrics can be used to give you a clearer picture of the load and performance for your applications, containers, clusters, and hosts. In this post, I discuss how you can use <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a>, along with <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, to cost effectively record high-resolution metrics into CloudWatch. You implement this solution using a serverless architecture, which keeps your costs low and makes it easier to troubleshoot the solution.</p> 
<p>To show how this works, you retrieve some useful metric data from an ECS cluster running in the same AWS account and region (Oregon, us-west-2) as the Step Functions state machine and Lambda function. However, you can use this architecture to retrieve any custom application metrics from any resource in any AWS account and region.</p> 
<p><img class="aligncenter size-full wp-image-3252" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/1_HRCaptureProcess.png" alt="" width="975" height="549" /></p> 
<b>Why Step Functions?</b> 
<p>Step Functions enables you to orchestrate multi-step tasks in the AWS Cloud that run for any period of time, up to a year. Effectively, you’re building a blueprint for an end-to-end process. After it’s built, you can execute the process as many times as you want.</p> 
<p>For this architecture, you gather metrics from an ECS cluster, every five seconds, and then write the metric data to CloudWatch. After your ECS cluster metrics are stored in CloudWatch, you can create CloudWatch alarms to notify you. An alarm can also trigger an automated remediation activity such as scaling ECS services, when a metric exceeds a threshold defined by you.</p> 
<p>When you build a Step Functions state machine, you define the different states inside it as JSON objects. The bulk of the work in Step Functions is handled by the common task state, which invokes Lambda functions or Step Functions activities. There is also a built-in library of other useful states that allow you to control the execution flow of your program.</p> 
<p>One of the most useful state types in Step Functions is the parallel state. Each parallel state in your state machine can have one or more branches, each of which is executed in parallel. Another useful state type is the wait state, which waits for a period of time before moving to the next state.</p> 
<p>In this walkthrough, you combine these three states (parallel, wait, and task) to create a state machine that triggers a Lambda function, which then gathers metrics from your ECS cluster.</p> 
<h3>Step Functions pricing</h3> 
<p>This state machine is executed every minute, resulting in 60 executions per hour, and 1,440 executions per day. Step Functions is billed per state transition,&nbsp;including&nbsp;the Start and End state transitions, and giving you approximately&nbsp;37,440 state transitions per day. To reach this number, I’m using this estimated math:</p> 
<blockquote> 
<p>26 state transitions per-execution x 60 minutes x 24 hours</p> 
</blockquote> 
<p>Based on current pricing, at $0.000025 per state transition, the daily cost of this metric gathering state machine would be <strong>$0.936</strong>.</p> 
<p>Step Functions offers an&nbsp;indefinite&nbsp;4,000 free state transitions every month. This benefit is available to all customers, not just customers who are still under the 12-month AWS Free Tier. For more information and cost example scenarios, see&nbsp;<a href="https://aws.amazon.com/step-functions/pricing/">Step Functions pricing</a>.</p> 
<b>Why Lambda?</b> 
<p>The goal is to capture metrics from an ECS cluster, and write the metric data to CloudWatch. This is a straightforward, short-running process that makes Lambda the perfect place to run your code. Lambda is one of the key services that makes up “Serverless” application architectures. It enables you to consume compute capacity only when your code is actually executing.</p> 
<p>The process of gathering metric data from ECS and writing it to CloudWatch takes a short period of time. In fact, my average Lambda function execution time, while developing this post, is only about 250 milliseconds on average. For every five-second interval that occurs, I’m only using 1/20th of the compute time that I’d otherwise be paying for.</p> 
<p><img class="aligncenter size-full wp-image-3255" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/2_captureMetrics.png" alt="" width="975" height="802" /></p> 
<h3>Lambda pricing</h3> 
<p>For billing purposes, Lambda execution time is rounded up to the nearest 100-ms interval. In general, based on the metrics that I observed during development, a 250-ms runtime would be billed at 300 ms. Here, I calculate the cost of this Lambda function executing on a daily basis.</p> 
<p>Assuming 31 days in each month, there would be 535,680 five-second intervals (31 days x 24 hours x 60 minutes x 12 five-second intervals = 535,680). The Lambda function is invoked every five-second interval, by the Step Functions state machine, and runs for a 300-ms period. At current Lambda pricing, for a 128-MB function, you would be paying approximately the following:</p> 
<blockquote> 
<p><strong>Total compute</strong></p> 
<p>Total executions = 535,680<br /> Total compute = total executions x (3 x $0.000000208 per 100 ms) = <strong>$0.334 per day</strong></p> 
<p><strong>Total requests</strong></p> 
<p>Total requests = (535,680 / 1000000) * $0.20 per million requests = <strong>$0.11 per day</strong></p> 
<p><strong>Total Lambda Cost</strong></p> 
<p>$0.11 requests + $0.334 compute time = <strong>$0.444 per day</strong></p> 
</blockquote> 
<p>Similar to Step Functions, Lambda offers an indefinite free tier. For more information, see&nbsp;<a href="https://aws.amazon.com/lambda/pricing/">Lambda Pricing</a>.</p> 
<b>Walkthrough</b> 
<p>In the following sections, I step through the process of configuring the solution just discussed. If you follow along, at a high level, you will:</p> 
<li>Configure an IAM role and policy</li> 
<li>Create a Step Functions state machine to control metric gathering execution</li> 
<li>Create a metric-gathering Lambda function</li> 
<li>Configure a CloudWatch Events rule to trigger the state machine</li> 
<li>Validate the solution</li> 
<h3>Prerequisites</h3> 
<p>You should already have an AWS account with a running ECS cluster. If you don’t have one running, you can easily <a href="https://aws.amazon.com/getting-started/tutorials/deploy-docker-containers/">deploy a Docker container on an ECS cluster</a>&nbsp;using the AWS Management Console. In the example produced for this post, I use an ECS cluster running Windows Server (currently in beta), but either a Linux or Windows Server cluster works.</p> 
<h3>Create an IAM role and policy</h3> 
<p>First, create an IAM role and policy that enables Step Functions, Lambda, and CloudWatch to communicate with each other.</p> 
<li>The CloudWatch Events rule needs permissions to trigger the Step Functions state machine.</li> 
<li>The Step Functions state machine needs permissions to trigger the Lambda function.</li> 
<li>The Lambda function needs permissions to query ECS and then write to CloudWatch Logs and metrics.</li> 
<p>When you create the state machine, Lambda function, and CloudWatch Events rule, you assign this role to each of those resources. Upon execution, each of these resources assumes the specified role and executes using the role’s permissions.</p> 
<ol> 
<li>Open the IAM console.</li> 
<li>Choose <strong>Roles</strong>, create <strong>New Role</strong>.</li> 
<li>For Role Name, enter <code class="lang-bash">WriteMetricFromStepFunction</code>.</li> 
<li>Choose <strong>Save</strong>.</li> 
</ol> 
<p>Create the IAM role trust relationship<br /> The trust relationship (also known as the assume role policy document) for your IAM role looks like the following JSON document. As you can see from the document, your IAM role needs to trust the Lambda, CloudWatch Events, and Step Functions services. By configuring your role to trust these services, they can assume this role and inherit the role permissions.</p> 
<ol> 
<li>Open the IAM console.</li> 
<li>Choose <strong>Roles</strong> and select the IAM role previously created.</li> 
<li>Choose&nbsp;<strong>Trust Relationships</strong>,&nbsp;<strong>Edit Trust Relationships</strong>.</li> 
<li>Enter the following trust policy text and choose <strong>Save</strong>.</li> 
</ol> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;lambda.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;events.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;states.us-west-2.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}</code></pre> 
<h3>Create an IAM policy</h3> 
<p>After you’ve finished configuring your role’s trust relationship, grant the role access to the other AWS resources that make up the solution.</p> 
<p>The IAM policy is what gives your IAM role permissions to access various resources. You must whitelist explicitly the specific resources to which your role has access, because the default IAM behavior is to deny access to any AWS resources.</p> 
<p>I’ve tried to keep this policy document as generic as possible, without allowing permissions to be too open. If the name of your ECS cluster is different than the one in the example policy below, make sure that you update the policy document before attaching it to your IAM role. You can attach this policy as an inline policy, instead of creating the policy separately first. However, either approach is valid.</p> 
<ol> 
<li>Open the IAM console.</li> 
<li>Select the IAM role, and choose <strong>Permissions</strong>.</li> 
<li>Choose&nbsp;<strong>Add in-line policy</strong>.</li> 
<li>Choose&nbsp;<strong>Custom Policy</strong>&nbsp;and then enter the following policy. The inline policy name does not matter.</li> 
</ol> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;logs:*&quot; ],
&quot;Resource&quot;: &quot;*&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;cloudwatch:PutMetricData&quot; ],
&quot;Resource&quot;: &quot;*&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;states:StartExecution&quot; ],
&quot;Resource&quot;: [
&quot;arn:aws:states:*:*:stateMachine:WriteMetricFromStepFunction&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;lambda:InvokeFunction&quot; ],
&quot;Resource&quot;: &quot;arn:aws:lambda:*:*:function:WriteMetricFromStepFunction&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;ecs:Describe*&quot; ],
&quot;Resource&quot;: &quot;arn:aws:ecs:*:*:cluster/ECSEsgaroth&quot;
}
]
}</code></pre> 
<h3>Create a Step Functions state machine</h3> 
<p>In this section, you create a Step Functions state machine that invokes the metric-gathering Lambda function every five (5) seconds, for a one-minute period. If you divide a minute (60) seconds into equal parts of five-second intervals, you get 12. Based on this math, you create 12 branches, in a single parallel state, in the state machine. Each branch triggers the metric-gathering Lambda function at a different five-second marker, throughout the one-minute period. After all of the parallel branches finish executing, the Step Functions execution completes and another begins.</p> 
<p>Follow these steps to create your Step Functions state machine:</p> 
<ol> 
<li>Open the Step Functions console.</li> 
<li>Choose&nbsp;<strong>Dashboard</strong>,&nbsp;<strong>Create State Machine</strong>.</li> 
<li>For&nbsp;State Machine Name,&nbsp;enter <code class="lang-bash">WriteMetricFromStepFunction</code>.</li> 
<li>Enter the state machine code below into the editor. Make sure that you insert your own AWS account ID for every instance of “676655494xxx”</li> 
<li>Choose&nbsp;<strong>Create State Machine</strong>.</li> 
<li>Select the&nbsp;<strong>WriteMetricFromStepFunction</strong>&nbsp;IAM role that you previously created.</li> 
</ol> 
<pre><code class="lang-json">{
&quot;Comment&quot;: &quot;Writes ECS metrics to CloudWatch every five seconds, for a one-minute period.&quot;,
&quot;StartAt&quot;: &quot;ParallelMetric&quot;,
&quot;States&quot;: {
&quot;ParallelMetric&quot;: {
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;Branches&quot;: [
{
&quot;StartAt&quot;: &quot;WriteMetricLambda&quot;,
&quot;States&quot;: {
&quot;WriteMetricLambda&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;WaitFive&quot;,
&quot;States&quot;: {
&quot;WaitFive&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 5,
&quot;Next&quot;: &quot;WriteMetricLambdaFive&quot;
},
&quot;WriteMetricLambdaFive&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;WaitTen&quot;,
&quot;States&quot;: {
&quot;WaitTen&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 10,
&quot;Next&quot;: &quot;WriteMetricLambda10&quot;
},
&quot;WriteMetricLambda10&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;WaitFifteen&quot;,
&quot;States&quot;: {
&quot;WaitFifteen&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 15,
&quot;Next&quot;: &quot;WriteMetricLambda15&quot;
},
&quot;WriteMetricLambda15&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait20&quot;,
&quot;States&quot;: {
&quot;Wait20&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 20,
&quot;Next&quot;: &quot;WriteMetricLambda20&quot;
},
&quot;WriteMetricLambda20&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait25&quot;,
&quot;States&quot;: {
&quot;Wait25&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 25,
&quot;Next&quot;: &quot;WriteMetricLambda25&quot;
},
&quot;WriteMetricLambda25&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait30&quot;,
&quot;States&quot;: {
&quot;Wait30&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 30,
&quot;Next&quot;: &quot;WriteMetricLambda30&quot;
},
&quot;WriteMetricLambda30&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait35&quot;,
&quot;States&quot;: {
&quot;Wait35&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 35,
&quot;Next&quot;: &quot;WriteMetricLambda35&quot;
},
&quot;WriteMetricLambda35&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait40&quot;,
&quot;States&quot;: {
&quot;Wait40&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 40,
&quot;Next&quot;: &quot;WriteMetricLambda40&quot;
},
&quot;WriteMetricLambda40&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait45&quot;,
&quot;States&quot;: {
&quot;Wait45&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 45,
&quot;Next&quot;: &quot;WriteMetricLambda45&quot;
},
&quot;WriteMetricLambda45&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait50&quot;,
&quot;States&quot;: {
&quot;Wait50&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 50,
&quot;Next&quot;: &quot;WriteMetricLambda50&quot;
},
&quot;WriteMetricLambda50&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait55&quot;,
&quot;States&quot;: {
&quot;Wait55&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 55,
&quot;Next&quot;: &quot;WriteMetricLambda55&quot;
},
&quot;WriteMetricLambda55&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
}
],
&quot;End&quot;: true
}
}
}</code></pre> 
<p>Now you’ve got a shiny new Step Functions state machine! However, you might ask yourself, “After the state machine has been created, how does it get executed?” Before I answer that question, create the Lambda function that writes the custom metric, and then you get the end-to-end process moving.</p> 
<h3>Create a Lambda function</h3> 
<p>The meaty part of the solution is a Lambda function, written to consume the Python 3.6 runtime, that retrieves metric values from ECS, and then writes them to CloudWatch. This Lambda function is what the Step Functions state machine is triggering every five seconds, via the Task states. Key points to remember:</p> 
<p>The Lambda function needs permission to:</p> 
<li>Write CloudWatch metrics (PutMetricData API).</li> 
<li>Retrieve metrics from ECS clusters (DescribeCluster API).</li> 
<li>Write StdOut to CloudWatch Logs.</li> 
<p>Boto3, the AWS SDK for Python, is&nbsp;included in the Lambda execution environment&nbsp;for Python 2.x and 3.x.</p> 
<p>Because&nbsp;<a href="http://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html">Lambda includes the AWS SDK</a>, you don’t have to worry about packaging it up and uploading it to Lambda. You can focus on writing code and automatically take a dependency on boto3.</p> 
<p>As for permissions, you’ve already created the IAM role and attached a policy to it that enables your Lambda function to access the necessary API actions. When you create your Lambda function, make sure that you select the correct IAM role, to ensure it is invoked with the correct permissions.</p> 
<p>The following Lambda function code is generic. So how does the Lambda function know which ECS cluster to gather metrics for? Your Step Functions state machine automatically passes in its state to the Lambda function. When you create your CloudWatch Events rule, you specify a simple JSON object that passes the desired ECS cluster name into your Step Functions state machine, which then passes it to the Lambda function.</p> 
<p>Use the following property values as you create your Lambda function:</p> 
<blockquote> 
<p><strong>Function Name</strong>: <code class="lang-bash">WriteMetricFromStepFunction</code><br /> <strong>Description</strong>: This Lambda function retrieves metric values from an ECS cluster and writes them to Amazon CloudWatch.<br /> <strong>Runtime</strong>: Python3.6<br /> <strong>Memory</strong>: 128 MB<br /> <strong>IAM Role</strong>: WriteMetricFromStepFunction</p> 
</blockquote> 
<pre><code class="lang-python">import boto3
def handler(event, context):
cw = boto3.client('cloudwatch')
ecs = boto3.client('ecs')
print('Got boto3 client objects')
Dimension = {
'Name': 'ClusterName',
'Value': event['ECSClusterName']
}
cluster = get_ecs_cluster(ecs, Dimension['Value'])
cw_args = {
'Namespace': 'ECS',
'MetricData': [
{
'MetricName': 'RunningTask',
'Dimensions': [ Dimension ],
'Value': cluster['runningTasksCount'],
'Unit': 'Count',
'StorageResolution': 1
},
{
'MetricName': 'PendingTask',
'Dimensions': [ Dimension ],
'Value': cluster['pendingTasksCount'],
'Unit': 'Count',
'StorageResolution': 1
},
{
'MetricName': 'ActiveServices',
'Dimensions': [ Dimension ],
'Value': cluster['activeServicesCount'],
'Unit': 'Count',
'StorageResolution': 1
},
{
'MetricName': 'RegisteredContainerInstances',
'Dimensions': [ Dimension ],
'Value': cluster['registeredContainerInstancesCount'],
'Unit': 'Count',
'StorageResolution': 1
}
]
}
cw.put_metric_data(**cw_args)
print('Finished writing metric data')
def get_ecs_cluster(client, cluster_name):
cluster = client.describe_clusters(clusters = [ cluster_name ])
print('Retrieved cluster details from ECS')
return cluster['clusters'][0]</code></pre> 
<h3>Create the CloudWatch Events rule</h3> 
<p>Now you’ve created an IAM role and policy, Step Functions state machine, and Lambda function. How do these components actually start communicating with each other? The final step in this process is to set up a CloudWatch Events rule that triggers your metric-gathering Step Functions state machine every minute. You have two choices for your&nbsp;CloudWatch Events rule expression: rate or cron. In this example, use the cron expression.</p> 
<p>A couple key learning points from creating the CloudWatch Events rule:</p> 
<li>You can specify one or more targets, of different types (for example, Lambda function, Step Functions state machine, SNS topic, and so on).</li> 
<li>You’re required to specify an IAM role with permissions to trigger your target.<br /> <strong>NOTE</strong>: This applies only to certain types of targets, including Step Functions state machines.</li> 
<li>Each target that supports IAM roles can be triggered using a different IAM role, in the same CloudWatch Events rule.</li> 
<li><em>Optional:</em>&nbsp;You can provide custom JSON that is passed to your target Step Functions state machine as input.</li> 
<p>Follow these steps to create the CloudWatch Events rule:</p> 
<ol> 
<li>Open the CloudWatch console.</li> 
<li>Choose&nbsp;<strong>Events</strong>, <strong>Rules</strong>,&nbsp;<strong>Create Rule</strong>.</li> 
<li>Select Schedule, Cron Expression,&nbsp;and then enter the following rule:<br /> <code class="lang-bash">0/1 * * * ?&nbsp;*</code></li> 
<li>Choose&nbsp;<strong>Add Target</strong>, <strong>Step Functions State Machine</strong>,&nbsp;<strong>WriteMetricFromStepFunction</strong>.</li> 
<li>For&nbsp;<strong>Configure Input</strong>,&nbsp;select <strong>Constant (JSON Text)</strong>.</li> 
<li>Enter the following JSON input, which is passed to Step Functions, while changing the cluster name accordingly:<br /> <code class="lang-json">{ &quot;ECSClusterName&quot;: &quot;ECSEsgaroth&quot; }</code></li> 
<li>Choose&nbsp;Use Existing Role, WriteMetricFromStepFunction&nbsp;(the IAM role that you previously created).</li> 
</ol> 
<p>After you’ve completed with these steps, your screen should look similar to this:</p> 
<p><img class="aligncenter size-large wp-image-3257" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/3_metricTargets-822x1024.png" alt="" width="822" height="1024" /></p> 
<h3>Validate the solution</h3> 
<p>Now that you have finished implementing the solution to gather high-resolution metrics from ECS, validate that it’s working properly.</p> 
<ol> 
<li>Open the CloudWatch console.</li> 
<li>Choose&nbsp;<strong>Metrics</strong>.</li> 
<li>Choose <strong>custom</strong>&nbsp;and select the <strong>ECS</strong> namespace.</li> 
<li>Choose the&nbsp;<strong>ClusterName</strong>&nbsp;metric dimension.</li> 
</ol> 
<p>You should see your metrics listed below.</p> 
<p><img class="aligncenter size-full wp-image-3258" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/4_metricList.png" alt="" width="975" height="440" /></p> 
<h3>Troubleshoot configuration issues</h3> 
<p>If you aren’t receiving the expected ECS cluster metrics in CloudWatch, check for the following common configuration issues. Review the earlier procedures to make sure that the resources were properly configured.</p> 
<li><strong>The IAM role’s trust relationship is incorrectly configured</strong>.<br /> Make sure that the IAM role trusts Lambda, CloudWatch Events, and Step Functions in the correct region.</li> 
<li><strong>The IAM role does not have the correct policies attached to it</strong>.<br /> Make sure that you have copied the IAM policy correctly as an inline policy on the IAM role.</li> 
<li><strong>The CloudWatch Events rule is not triggering new Step Functions executions.</strong><br /> Make sure that the target configuration on the rule has the correct Step Functions state machine and IAM role selected.</li> 
<li><strong>The Step Functions state machine is being executed, but failing part way through.</strong><br /> Examine the detailed error message on the failed state within the failed Step Functions execution. It’s possible that the</li> 
<li><strong>IAM role does not have permissions to trigger the target Lambda function, that the target Lambda function may not exist, or that the Lambda function failed to complete successfully due to invalid permissions.</strong><br /> Although the above list covers several different potential configuration issues, it is not comprehensive. Make sure that you understand how each service is connected to each other, how permissions are granted through IAM policies, and how IAM trust relationships work.</li> 
<b>Conclusion</b> 
<p>In this post, you implemented a Serverless solution to gather and record high-resolution application metrics from containers running on Amazon ECS into CloudWatch. The solution consists of a Step Functions state machine, Lambda function, CloudWatch Events rule, and an IAM role and policy. The data that you gather from this solution helps you rapidly identify issues with an ECS cluster.</p> 
<p>To gather high-resolution metrics from&nbsp;any&nbsp;service, modify your Lambda function to gather the correct metrics from your target. If you prefer not to use Python, you can implement a Lambda function using one of the other supported runtimes, including Node.js, Java, or .NET Core. However, this post should give you the fundamental basics about capturing high-resolution metrics in CloudWatch.</p> 
<p>If you found this post useful,&nbsp;or have questions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/containers/" rel="tag">Containers</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/metrics/" rel="tag">Metrics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3251');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/ecs_tasknetworking.png" /> 
<b class="lb-b blog-post-title" property="name headline">Introducing Cloud Native Networking for Amazon ECS Containers</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-11-14T21:27:04+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3212" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3212&amp;disqus_title=Introducing+Cloud+Native+Networking+for+Amazon+ECS+Containers&amp;disqus_url=https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3212');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of ECS Sr. Software Dev Engineer Anirudh Aithal.</em></p> 
<p>Today, AWS announced <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-ecs-introduces-awsvpc-networking-mode-for-containers-to-support-full-networking-capabilities/">task networking</a> for Amazon ECS. This feature brings <a href="https://aws.amazon.com/ec2">Amazon EC2</a> networking capabilities to tasks using <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">elastic network interfaces</a>.</p> 
<p>An elastic network interface is a virtual network interface that you can attach to an instance in a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html">VPC</a>. When you launch an EC2 virtual machine, an elastic network interface is automatically provisioned to provide networking capabilities for the instance.</p> 
<p>A task is a logical group of running containers. Previously, tasks running on <a href="https://aws.amazon.com/ecs">Amazon ECS</a> shared the elastic network interface of their EC2 host. Now, the new <code class="lang-bash">awsvpc</code> networking mode lets you attach an elastic network interface directly to a task.</p> 
<p>This simplifies network configuration, allowing you to treat each container just like an EC2 instance with full networking features, segmentation, and security controls in the VPC.</p> 
<p>In this post, I cover how <code class="lang-bash">awsvpc</code> mode works and show you how you can start using elastic network interfaces with your tasks running on ECS.<span id="more-3212"></span></p> 
<b>Background: &nbsp;Elastic network interfaces in EC2</b> 
<p>When you launch EC2 instances within a VPC, you don’t have to configure an additional overlay network for those instances to communicate with each other. By default, routing tables in the VPC enable seamless communication between instances and other endpoints. This is made possible by virtual network interfaces in VPCs called elastic network interfaces. Every EC2 instance that launches is automatically assigned an elastic network interface (the primary network interface). All networking parameters—such as subnets, security groups, and so on—are handled as properties of this primary network interface.</p> 
<p>Furthermore, an IPv4 address is allocated to every elastic network interface by the VPC at creation (the primary IPv4 address). This primary address is unique and routable within the VPC. This effectively makes your VPC a flat network, resulting in a simple networking topology.</p> 
<p>Elastic network interfaces can be treated as fundamental building blocks for connecting various endpoints in a VPC, upon which you can build higher-level abstractions. This allows elastic network interfaces to be leveraged for:</p> 
<li>VPC-native IPv4 addressing and routing (between instances and other endpoints in the VPC)</li> 
<li>Network traffic isolation</li> 
<li>Network policy enforcement using ACLs and firewall rules (security groups)</li> 
<li>IPv4 address range enforcement (via subnet CIDRs)</li> 
<b>Why use awsvpc?</b> 
<p>Previously, ECS relied on the networking capability provided by Docker’s default networking behavior to set up the network stack for containers. With the default bridge network mode, containers on an instance are connected to each other using the docker0 bridge. Containers use this bridge to communicate with endpoints outside of the instance, using the primary elastic network interface of the instance on which they are running. Containers share and rely on the networking properties of the primary elastic network interface, including the firewall rules (security group subscription) and IP addressing.</p> 
<p>This means you cannot address these containers with the IP address allocated by Docker (it’s allocated from a pool of locally scoped addresses), nor can you enforce finely grained network ACLs and firewall rules. Instead, containers are addressable in your VPC by the combination of the IP address of the primary elastic network interface of the instance, and the host port to which they are mapped (either via static or dynamic port mapping). Also, because a single elastic network interface is shared by multiple containers, it can be difficult to create easily understandable network policies for each container.</p> 
<p>The <code class="lang-bash">awsvpc</code> networking mode addresses these issues by provisioning elastic network interfaces on a per-task basis. Hence, containers no longer share or contend use these resources. This enables you to:</p> 
<li>Run multiple copies of the container on the same instance using the same container port without needing to do any port mapping or translation, simplifying the application architecture.</li> 
<li>Extract higher network performance from your applications as they no longer contend for bandwidth on a shared bridge.</li> 
<li>Enforce finer-grained access controls for your containerized applications by associating security group rules for each Amazon ECS task, thus improving the security for your applications.</li> 
<p>Associating security group rules with a container or containers in a task allows you to restrict the ports and IP addresses from which your application accepts network traffic. For example, you can enforce a policy allowing SSH access to your instance, but blocking the same for containers. Alternatively, you could also enforce a policy where you allow HTTP traffic on port 80 for your containers, but block the same for your instances. Enforcing such security group rules greatly reduces the surface area of attack for your instances and containers.</p> 
<p>ECS manages the lifecycle and provisioning of elastic network interfaces for your tasks, creating them on-demand and cleaning them up after your tasks stop. You can specify the same properties for the task as you would when launching an EC2 instance. This means that containers in such tasks are:</p> 
<li>Addressable by IP addresses and the DNS name of the elastic network interface</li> 
<li>Attachable as ‘IP’ targets to Application Load Balancers and Network Load Balancers</li> 
<li>Observable from VPC flow logs</li> 
<li>Access controlled by security groups</li> 
<p>&shy;This also enables you to run multiple copies of the same task definition on the same instance, without needing to worry about port conflicts. You benefit from higher performance because you don’t need to perform any port translations or contend for bandwidth on the shared docker0 bridge, as you do with the bridge networking mode.</p> 
<b>Getting started</b> 
<p>If you don’t already have an ECS cluster, you can create one using the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/create_cluster.html">create cluster wizard</a>. In this post, I use&nbsp;“<code class="lang-bash">awsvpc-demo</code>” as the cluster name. Also, if you are following along with the command line instructions, make sure that you have the latest version of the <a href="https://aws.amazon.com/cli/">AWS CLI</a>&nbsp;or <a href="https://aws.amazon.com/tools/">SDK</a>.</p> 
<h3>Registering the task definition</h3> 
<p>The only change to make in your task definition for task networking is to set the <code class="lang-bash">networkMode</code> parameter to <code class="lang-bash">awsvpc</code>. In the <a href="https://console.aws.amazon.com/ecs/">ECS console</a>, enter this value for <strong>Network Mode</strong>.</p> 
<p>&nbsp;</p> 
<p><img class="aligncenter size-full wp-image-3229" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_1.png" alt="" width="708" height="383" /></p> 
<p>If you plan on registering a container in this task definition with an ECS service, also specify a container port in the task definition. This example specifies an NGINX container exposing port 80:</p> 
<p><img class="aligncenter size-full wp-image-3231" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_2.png" alt="" width="702" height="650" /></p> 
<p>This creates a task definition named “<code class="lang-bash">nginx-awsvpc&quot;</code> with networking mode set to <code class="lang-bash">awsvpc</code>. The following commands illustrate registering the task definition from the command line:</p> 
<pre><code class="lang-bash"><strong>$ cat nginx-awsvpc.json</strong>
{
&quot;family&quot;: &quot;nginx-awsvpc&quot;,
&quot;networkMode&quot;: &quot;awsvpc&quot;,
&quot;containerDefinitions&quot;: [
{
&quot;name&quot;: &quot;nginx&quot;,
&quot;image&quot;: &quot;nginx:latest&quot;,
&quot;cpu&quot;: 100,
&quot;memory&quot;: 512,
&quot;essential&quot;: true,
&quot;portMappings&quot;: [
{
&quot;containerPort&quot;: 80,
&quot;protocol&quot;: &quot;tcp&quot;
}
]
}
]
}
</code></pre> 
<pre><strong><code class="lang-bash">$ aws ecs register-task-definition --cli-input-json file://./nginx-awsvpc.json</code></strong></pre> 
<h3>Running the task</h3> 
<p>To run a task with this task definition, navigate to the cluster in the Amazon ECS console and choose <strong>Run new task</strong>. Specify the task definition as “<code class="lang-bash">nginx-awsvpc</code>“. Next, specify the set of subnets in which to run this task. You must have instances registered with ECS in at least one of these subnets. Otherwise, ECS can’t find a candidate instance to attach the elastic network interface.</p> 
<p>You can use the console to narrow down the subnets by selecting a value for <strong>Cluster VPC</strong>:</p> 
<p>&nbsp;</p> 
<p><img class="aligncenter size-full wp-image-3232" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_3.png" alt="" width="773" height="707" /></p> 
<p>Next, select a security group for the task. For the purposes of this example, create a new security group that allows ingress only on port 80. Alternatively, you can also select security groups that you’ve already created.</p> 
<p><img class="aligncenter size-full wp-image-3233" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_4.png" alt="" width="735" height="486" /></p> 
<p>Next, run the task by choosing <strong>Run Task</strong>.</p> 
<p>You should have a running task now. If you look at the details of the task, you see that it has an elastic network interface allocated to it, along with the IP address of the elastic network interface:</p> 
<p><img class="aligncenter wp-image-3234" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_5.png" alt="" width="500" height="304" /></p> 
<p>You can also use the command line to do this:</p> 
<pre><code class="lang-bash"><strong>$ aws ecs run-task --cluster awsvpc-ecs-demo --network-configuration &quot;awsvpcConfiguration={subnets=[&quot;subnet-c070009b&quot;],securityGroups=[&quot;sg-9effe8e4&quot;]}&quot; nginx-awsvpc $ aws ecs describe-tasks --cluster awsvpc-ecs-demo --task $ECS_TASK_ARN --query tasks[0]</strong>
{
&quot;taskArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:task/f5xx-...&quot;,
&quot;group&quot;: &quot;family:nginx-awsvpc&quot;,
&quot;attachments&quot;: [
{
&quot;status&quot;: &quot;ATTACHED&quot;,
&quot;type&quot;: &quot;ElasticNetworkInterface&quot;,
&quot;id&quot;: &quot;xx..&quot;,
&quot;details&quot;: [
{
&quot;name&quot;: &quot;subnetId&quot;,
&quot;value&quot;: &quot;subnet-c070009b&quot;
},
{
&quot;name&quot;: &quot;networkInterfaceId&quot;,
&quot;value&quot;: &quot;eni-b0aaa4b2&quot;
},
{
&quot;name&quot;: &quot;macAddress&quot;,
&quot;value&quot;: &quot;0a:47:e4:7a:2b:02&quot;
},
{
&quot;name&quot;: &quot;privateIPv4Address&quot;,
&quot;value&quot;: &quot;10.0.0.35&quot;
}
]
}
],
...
&quot;desiredStatus&quot;: &quot;RUNNING&quot;,
&quot;taskDefinitionArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:task-definition/nginx-awsvpc:2&quot;,
&quot;containers&quot;: [
{
&quot;containerArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:container/62xx-...&quot;,
&quot;taskArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:task/f5x-...&quot;,
&quot;name&quot;: &quot;nginx&quot;,
&quot;networkBindings&quot;: [],
&quot;lastStatus&quot;: &quot;RUNNING&quot;,
&quot;networkInterfaces&quot;: [
{
&quot;privateIpv4Address&quot;: &quot;10.0.0.35&quot;,
&quot;attachmentId&quot;: &quot;xx..&quot;
}
]
}
]
}</code></pre> 
<p>When you describe an “awsvpc” task, details of the elastic network interface are returned via the “attachments” object. You can also get this information from the “containers” object. For example:</p> 
<pre><code class="lang-bash"><strong>$ aws ecs describe-tasks --cluster awsvpc-ecs-demo --task $ECS_TASK_ARN --query tasks[0].containers[0].networkInterfaces[0].privateIpv4Address</strong>
&quot;10.0.0.35&quot;</code></pre> 
<b>Conclusion</b> 
<p>The nginx container is now addressable in your VPC via the <code>10.0.0.35</code> IPv4 address. You did not have to modify the security group on the instance to allow requests on port 80, thus improving instance security. Also, you ensured that all ports apart from port 80 were blocked for this application without modifying the application itself, which makes it easier to manage your task on the network. You did not have to interact with any of the elastic network interface API operations, as ECS handled all of that for you.</p> 
<p>You can read more about the task networking feature in the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html">ECS documentation</a>. For a detailed look at how this new networking mode is implemented on an instance, see <a href="https://aws.amazon.com/blogs/compute/under-the-hood-task-networking-for-amazon-ecs/">Under the Hood: Task Networking for Amazon ECS</a>.</p> 
<p>Please use the comments section below to send your feedback.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-ecs/" rel="tag">Amazon ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/container-networking/" rel="tag">Container Networking</a>, <a href="https://aws.amazon.com/blogs/compute/tag/containers/" rel="tag">Containers</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/task-networking/" rel="tag">Task Networking</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3212');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/ecs_tasknetworking.png" /> 
<b class="lb-b blog-post-title" property="name headline">Under the Hood: Task Networking for Amazon ECS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-11-14T21:27:01+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/under-the-hood-task-networking-for-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3199" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3199&amp;disqus_title=Under+the+Hood%3A+Task+Networking+for+Amazon+ECS&amp;disqus_url=https://aws.amazon.com/blogs/compute/under-the-hood-task-networking-for-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3199');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of ECS&nbsp;Sr. Software Dev Engineer Anirudh Aithal.</em></p> 
<p>Today, AWS&nbsp;announced <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-ecs-introduces-awsvpc-networking-mode-for-containers-to-support-full-networking-capabilities/">task networking</a> for <a href="https://aws.amazon.com/ecs">Amazon ECS</a>, which enables <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">elastic network interfaces</a> to be attached to containers.</p> 
<p>In this post, I take a closer look at how this new container-native “awsvpc” network mode is implemented using <a href="https://github.com/containernetworking/cni">container networking interface</a> plugins on ECS managed instances (referred to as container instances).</p> 
<p>This post is a deep dive into how task networking works with Amazon ECS. If you want to learn more about how you can start using task networking for your containerized applications, see <a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">Introducing Cloud Native Networking for Amazon ECS Containers</a>. Cloud Native Computing Foundation (CNCF) hosts the <a href="https://github.com/containernetworking/cni">Container Networking Interface (CNI)</a> project, which consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers. For more about cloud native computing in AWS, see Adrian Cockcroft’s post on <a href="https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf">Cloud Native Computing</a>.</p> 
<p><span id="more-3199"></span></p> 
<b>Container instance setup</b> 
<p>Before I discuss the details of enabling task networking on container instances, look at how a typical instance looks in ECS.</p> 
<p><img class="aligncenter size-full wp-image-3200" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_dd_1.png" alt="" width="408" height="341" /></p> 
<p>The diagram above shows a typical container instance. The <a href="https://github.com/aws/amazon-ecs-agent/">ECS agent</a>, which itself is running as a container, is responsible for:</p> 
<li>Registering the EC2 instance with the ECS backend</li> 
<li>Ensuring that task state changes communicated to it by the ECS backend are enacted on the container instance</li> 
<li>Interacting with the Docker daemon to create, start, stop, and monitor</li> 
<li>Relaying container state and task state transitions to the ECS backend</li> 
<p>Because the ECS agent is just acting as the supervisor for containers under its management, it offloads the problem of setting up networking for containers to either the Docker daemon (for containers configured with one of Docker’s default networking modes) or a set of CNI plugins (for containers in task with networking mode set to <code class="lang-bash">awsvpc</code>).</p> 
<p>In either case, network stacks of containers are configured via network namespaces. As per the <code class="lang-bash">ip-netns(8)</code> manual, “<em>A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices</em>.” The network namespace construct makes the partitioning of network stack between processes and containers running on a host possible.</p> 
<b>Network namespaces and CNI plugins</b> 
<p>CNI plugins are executable files that comply with the <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI specification</a> and configure the network connectivity of containers. The CNI project defines a specification for the plugins and provides a library for interacting with plugins, thus providing a consistent, reliable, and simple interface with which to interact with the plugins.</p> 
<p>You specify the container or its network namespace and invoke the plugin with the ADD command to add network interfaces to a container, and then the DEL command to tear them down. For example, the reference <a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/bridge">bridge plugin</a> adds all containers on the same host into a bridge that resides in the host network namespace.</p> 
<p>This plugin model fits in nicely with the ECS agent’s “<em>minimal intrusion in the container lifecycle</em>” model, as the agent doesn’t need to concern itself with the details of the network setup for containers. It’s also an extensible model, which allows the agent to switch to a different set of plugins if the need arises in future. Finally, the ECS agent doesn’t need to monitor the liveliness of these plugins as they are only invoked when required.</p> 
<b>Invoking CNI plugins from the ECS agent</b> 
<p>When ECS attaches an elastic network interface to the instance and sends the message to the agent to provision the elastic network interface for containers in a task, the elastic network interface (as with any network device) shows up in the global default network namespace of the host. The ECS agent invokes a chain of CNI plugins to ensure that the elastic network interface is configured appropriately in the container’s network namespace. You can review these plugins in the <a href="https://github.com/aws/amazon-ecs-cni-plugins">amazon-ecs-cni-plugins GitHub repo</a>.</p> 
<p>The first plugin invoked in this chain is the <a href="https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/eni">ecs-eni</a>&nbsp;plugin,&nbsp;which ensures that the elastic network interface is attached to container’s network namespace and configured with the VPC-allocated IP addresses and the default route to use the subnet gateway. The container also needs to make HTTP requests to the credentials endpoint (hosted by the ECS agent) for getting IAM role credentials. This is handled by the <a href="https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/ecs-bridge">ecs-bridge</a> and <a href="https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/ipam">ecs-ipam</a> plugins, which are invoked next. The CNI library provides mechanisms to interpret the results from the execution of these plugins, which results in an efficient error handling in the agent. The following diagram illustrates the different steps in this process:</p> 
<p><img class="aligncenter size-full wp-image-3202" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_dd_2.png" alt="" width="862" height="491" /></p> 
<p>To avoid the race condition between configuring the network stack and commands being invoked in application containers, the ECS agent creates an additional “pause” container for each task before starting the containers in the task definition. It then sets up the network namespace of the pause container by executing the previously mentioned CNI plugins. It also starts the rest of the containers in the task so that they share their network stack of the pause container. This means that all containers in a task are addressable by the IP addresses of the elastic network interface, and they can communicate with each other over the <code class="lang-bash">localhost</code> interface.</p> 
<p>In this example setup, you have two containers in a task behind an elastic network interface. The following commands show that they have a similar view of the network stack and can talk to each other over the localhost interface.</p> 
<h3>List the last three containers running on the host (you launched a task with two containers and the ECS agent launched the additional container to configure the network namespace):</h3> 
<pre><code class="lang-bash"><b>$ docker ps -n 3 --format &quot;{{.ID}}\t{{.Names}}\t{{.Command}}\t{{.Status}}&quot;</b>
7d7b7fbc30b9	ecs-front-envoy-5-envoy-sds-ecs-ce8bd9eca6dd81a8d101	&quot;/bin/sh -c '/usr/...&quot;	Up 3 days
dfdcb2acfc91	ecs-front-envoy-5-front-envoy-faeae686adf9c1d91000	&quot;/bin/sh -c '/usr/...&quot;	Up 3 days
f731f6dbb81c	ecs-front-envoy-5-internalecspause-a8e6e19e909fa9c9e901	&quot;./pause&quot;	Up 3 days</code></pre> 
<h3>List interfaces for these containers and make sure that they are the same:</h3> 
<pre><code class="lang-bash"><b>$ for id in `docker ps -n 3 -q`; do pid=`docker inspect $id -f '{{.State.Pid}}'`; echo container $id; sudo nsenter -t $pid -n ip link show; done</b>
container 7d7b7fbc30b9
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: ecs-eth0@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
link/ether 0a:58:a9:fe:ac:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
27: etb2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:5a:a1:1a:43:42 brd ff:ff:ff:ff:ff:ff
container dfdcb2acfc91
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: ecs-eth0@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
link/ether 0a:58:a9:fe:ac:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
27: etb2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:5a:a1:1a:43:42 brd ff:ff:ff:ff:ff:ff
container f731f6dbb81c
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: ecs-eth0@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
link/ether 0a:58:a9:fe:ac:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
27: etb2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:5a:a1:1a:43:42 brd ff:ff:ff:ff:ff:ff</code></pre> 
<b>Conclusion</b> 
<p>All of this work means that you can use the new <code class="lang-bash">awsvpc</code> networking mode and benefit from native networking support for your containers. You can learn more about using <code class="lang-bash">awsvpc</code> mode in <a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">Introducing Cloud Native Networking for Amazon ECS Containers</a> or the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html">ECS documentation</a>.</p> 
<p>I&nbsp;appreciate your feedback in the comments section. You can also reach&nbsp;me on GitHub in either the <a href="https://github.com/aws/amazon-ecs-cni-plugins">ECS CNI Plugins</a> or the <a href="https://github.com/aws/amazon-ecs-agent">ECS Agent</a> repositories.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-ecs/" rel="tag">Amazon ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/containers/" rel="tag">Containers</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/task-networking/" rel="tag">Task Networking</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3199');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/18/cross-account-integration-with-amazon-sns_img1-1260x459.png" /> 
<b class="lb-b blog-post-title" property="name headline">Cross-Account Integration with Amazon SNS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-14T12:51:03+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/" title="View all posts in Messaging*"><span property="articleSection">Messaging*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/cross-account-integration-with-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2726" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2726&amp;disqus_title=Cross-Account+Integration+with+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/cross-account-integration-with-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2726');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h4>Contributed by Zak Islam, Senior Manager, Software Development, AWS Messaging</h4> 
<p>&nbsp;</p> 
<p><a href="https://aws.amazon.com/sns/">Amazon Simple Notification Service</a>&nbsp;(Amazon SNS)&nbsp;is a fully managed AWS service that makes it easy to decouple your application components and fan-out messages. SNS provides topics (similar to topics in message brokers such as RabbitMQ or ActiveMQ) that you can use to create 1:1, 1:N, or N:N producer/consumer design patterns. For more information about how to send messages from SNS to Amazon SQS, AWS Lambda, or HTTP(S) endpoints in the same account, see <a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.html">Sending Amazon SNS Messages to Amazon SQS Queues</a>.</p> 
<p>SNS can be used to send messages within a single account or to resources in different accounts to create administrative isolation. This enables administrators to grant only the minimum level of permissions required to process a workload (for example, limiting the scope of your application account to only send messages and to deny deletes). This approach is commonly known as the “principle of least privilege.” If you are interested, read more about AWS’s <a href="https://aws.amazon.com/answers/account-management/aws-multi-account-security-strategy/">multi-account security strategy</a>.</p> 
<p>This is great from a security perspective, but why would you want to share messages between accounts? It may sound scary, but it’s a common practice to isolate application components (such as producer and consumer) to operate using different AWS accounts to lock down privileges in case credentials are exposed. In this post, I go slightly deeper and explore how to set up your SNS topic so that it can route messages to SQS queues that are owned by a separate AWS account.</p> 
<b>Potential use cases</b> 
<p style="text-align: left">First, look at a common order processing design pattern:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_1.png"><img class="aligncenter wp-image-3138 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_1.png" alt="" width="1200" height="500" /></a></p> 
<p><span id="more-2726"></span>This is a simple architecture. A web server submits an order directly to an SNS topic, which then fans out messages to two SQS queues. One SQS queue is used to track all incoming orders for audits (such as anti-entropy, comparing the data of all replicas and updating each replica to the newest version). The other is used to pass the request to the order processing systems.</p> 
<p>Imagine now that a few years have passed, and your downstream processes no longer scale, so you are kicking around the idea of a re-architecture project. To thoroughly test your system, you need a way to replay your production messages in your development system. Sure, you can build a system to replicate and replay orders from your production environment in your development environment. Wouldn’t it be easier to subscribe your development queues to the production SNS topic so you can test your new system in real time? That’s exactly what you can do here.</p> 
<p>Here’s another use case. As your business grows, you recognize the need for more metrics from your order processing pipeline. The analytics team at your company has built a metrics aggregation service and ingests data via a central SQS queue. Their architecture is as follows:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_2.png"><img class="size-full wp-image-3139 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_2.png" alt="" width="1200" height="400" /></a></p> 
<p>Again, it’s a fairly simple architecture. All data is ingested via SQS queues (master_ingest_queue, in this case). You subscribe the master_ingest_queue, running under the analytics team’s AWS account, to the topic that is in the order management team’s account.</p> 
<b>Making it work</b> 
<p>Now that you’ve seen a few scenarios, let’s dig into the details. There are a couple of ways to link an SQS queue to an SNS topic (subscribe a queue to a topic):</p> 
<ol> 
<li>The queue owner can create a subscription to the topic.</li> 
<li>The topic owner can subscribe a queue in another account to the topic.</li> 
</ol> 
<b>Queue owner subscription</b> 
<p>What happens when the queue owner subscribes to a topic? In this case, assume that the topic owner has given permission to the subscriber’s account to call the Subscribe API action using the topic ARN (Amazon Resource Name). For the examples below, also assume the following:</p> 
<li>&nbsp;<em>Topic_Owner</em> is the identifier for the account that owns the topic <em>MainTopic</em></li> 
<li><em>Queue_Owner</em> is the identifier for the account that owns the queue subscribed to the main topic</li> 
<p>To enable the subscriber to subscribe to a topic, the topic owner must add the sns:Subscribe and topic ARN to the topic policy via the AWS Management Console, as follows:</p> 
<pre><code class="lang-json">{
&quot;Version&quot;:&quot;2012-10-17&quot;,
&quot;Id&quot;:&quot;MyTopicSubscribePolicy&quot;,
&quot;Statement&quot;:[{
&quot;Sid&quot;:&quot;Allow-other-account-to-subscribe-to-topic&quot;,
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Principal&quot;:{
&quot;AWS&quot;:&quot;Topic_Owner&quot;
},
&quot;Action&quot;:&quot;sns:Subscribe&quot;,
&quot;Resource&quot;:&quot;arn:aws:sns:us-east-1:Queue_Owner:MainTopic&quot;
}
]
}</code></pre> 
<p>After this has been set up, the subscriber (using account <em>Queue_Owner</em>) can call Subscribe to link the queue to the topic. After the queue has been successfully subscribed, SNS starts to publish notifications. In this case, neither the topic owner nor the subscriber have had to process any kind of confirmation message.</p> 
<b>Topic owner subscription</b> 
<p>The second way to subscribe an SQS queue to an SNS topic is to have the <em>Topic_Owner</em> account initiate the subscription for the queue from account <em>Queue_Owner</em>. In this case, SNS first sends a confirmation message to the queue. To confirm the subscription, a user who can read messages from the queue must visit the URL specified in the&nbsp;SubscribeURL&nbsp;value in the message. Until the subscription is confirmed, no notifications published to the topic are sent to the queue. To confirm a subscription, you can use the SQS console or the&nbsp;ReceiveMessage&nbsp;API action.</p> 
<b>What’s next?</b> 
<p>In this post, I covered a few simple use cases but the principles can be extended to complex systems as well. As you architect new systems and refactor existing ones, think about where you can leverage queues (SQS) and topics (SNS) to build a loosely coupled system that can be quickly and easily extended to meet your business need.</p> 
<p>For step by step instructions, see&nbsp;<a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.cross.account.html">Sending Amazon SNS messages to an Amazon SQS queue in a different account</a>. You can also visit the following resources to get started working with message queues and topics:</p> 
<li><span style="color: #000000">10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-messages-distributed-applications/">Send Messages Between Distributed Applications with Amazon SQS</a></span></li> 
<li><span style="color: #000000">10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/">Send&nbsp;Fanout Event Notifications with Amazon SNS and Amazon SQS</a></span></li> 
<li>Blog:&nbsp;<a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</a></li> 
<li>Blog:&nbsp;<a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a></li> 
<li>Other resources:&nbsp;<a href="https://aws.amazon.com/sns/getting-started/">Getting started with Amazon SNS</a></li> 
<li>Other resources:&nbsp;<a href="https://aws.amazon.com/sqs/getting-started/">Getting started with Amazon SQS</a></li> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2726');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/route-53-healthcheck.png" /> 
<b class="lb-b blog-post-title" property="name headline">Building a Multi-region Serverless Application with Amazon API Gateway and AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Stefano Buliani</span></span> | on 
<time property="datePublished" datetime="2017-11-13T14:28:20+00:00">13 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-route-53/" title="View all posts in Amazon Route 53*"><span property="articleSection">Amazon Route 53*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3143" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3143&amp;disqus_title=Building+a+Multi-region+Serverless+Application+with+Amazon+API+Gateway+and+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3143');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
This post written by:&nbsp;Magnus Bjorkman – Solutions Architect 
<p>Many customers are looking to run their services at global scale, deploying their backend to multiple regions. In this post, we describe how to deploy a Serverless API into multiple regions and how to leverage <a title="Amazon Route 53" href="https://aws.amazon.com/route53/" target="_blank" rel="noopener noreferrer">Amazon Route 53</a> to route the traffic between regions. We use latency-based routing and health checks to achieve an active-active setup that can fail over between regions in case of an issue. We leverage the new regional API endpoint feature in <a title="Amazon API Gateway" href="https://aws.amazon.com/api-gateway" target="_blank" rel="noopener noreferrer">Amazon API Gateway</a> to make this a seamless process for the API client making the requests. This post does not cover the replication of your data, which is another aspect to consider when deploying applications across regions.</p> 
<h3>Solution overview</h3> 
<p>Currently, the default API endpoint type in API Gateway is the edge-optimized API endpoint, which enables clients to access an API through an Amazon CloudFront distribution. This typically improves connection time for geographically diverse clients. By default, a custom domain name is globally unique and the edge-optimized API endpoint would invoke a Lambda function in a single region in the case of Lambda integration. You can’t use this type of endpoint with a Route 53 active-active setup and fail-over.</p> 
<p>The new regional API endpoint in API Gateway moves the API endpoint into the region and the custom domain name is unique per region. This makes it possible to run a full copy of an API in each region and then use Route 53 to use an active-active setup and failover. The following diagram shows how you do this:</p> 
<p><img class="aligncenter wp-image-3144 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/active-active-architecture.png" alt="Active/active multi region architecture" width="975" height="548" /></p> 
<li>Deploy your Rest API stack, consisting of API Gateway and Lambda, in two regions, such as us-east-1 and us-west-2.</li> 
<li>Choose the regional API endpoint type for your API.</li> 
<li>Create a custom domain name and choose the regional API endpoint type for that one as well. In both regions, you are configuring the custom domain name to be the same, for example, helloworldapi.replacewithyourcompanyname.com</li> 
<li>Use the host name of the custom domain names from each region, for example, xxxxxx.execute-api.us-east-1.amazonaws.com and xxxxxx.execute-api.us-west-2.amazonaws.com, to configure record sets in Route 53 for your client-facing domain name, for example, helloworldapi.replacewithyourcompanyname.com</li> 
<p>The above solution provides an active-active setup for your API across the two regions, but you are not doing failover yet. For that to work, set up a health check in Route 53:</p> 
<p><img class="aligncenter wp-image-3145 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/route-53-healthcheck.png" alt="Route 53 Health Check" width="975" height="548" /></p> 
<p>A Route 53 health check must have an endpoint to call to check the health of a service. You could do a simple ping of your actual Rest API methods, but instead provide a specific method on your Rest API that does a deep ping. That is, it is a Lambda function that checks the status of all the dependencies.</p> 
<p>In the case of the Hello World API, you don’t have any other dependencies. In a real-world scenario, you could check on dependencies as databases, other APIs, and external dependencies. Route 53 health checks themselves cannot use your custom domain name endpoint’s DNS address, so you are going to directly call the API endpoints via their region unique endpoint’s DNS address.</p> 
<h3>Walkthrough</h3> 
<p>The following sections describe how to set up this solution. You can find the complete solution at the <a title="Multi region serverless app GitHub sample" href="https://github.com/aws-samples/blog-multi-region-serverless-service/" target="_blank" rel="noopener noreferrer">blog-multi-region-serverless-service GitHub repo</a>. Clone or download the repository locally to be able to do the setup as described.</p> 
<h4>Prerequisites</h4> 
<p>You need the following resources to set up the solution described in this post:</p> 
<li><a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html" target="_blank" rel="noopener noreferrer">AWS CLI</a></li> 
<li>An S3 bucket in each region in which to deploy the solution, which can be used by the AWS Serverless Application Model (SAM). You can use the following CloudFormation templates to create buckets in us-east-1 and us-west-2: 
<li>us-east-1: <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=SAMS3Bucket&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/building-a-multi-region-serverless-application/create-s3-bucket.yaml" target="_blank" rel="noopener noreferrer"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png" /></a></li> 
<li>us-west-2: <a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=SAMS3Bucket&amp;templateURL=https://s3.amazonaws.com/computeblog-us-west-2/building-a-multi-region-serverless-application/create-s3-bucket.yaml" target="_blank" rel="noopener noreferrer"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png" /></a></li> 
</ul> </li> 
<li>A hosted zone registered in Amazon Route 53. This is used for defining the domain name of your API endpoint, for example, <em>helloworldapi.replacewithyourcompanyname.com</em>. You can use a third-party domain name registrar and then <a title="Configure Route 53 DNS" href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/MigratingDNS.html" target="_blank" rel="noopener noreferrer">configure the DNS in Amazon Route 53</a>, or you can <a title="Purchase domain from Route 53" href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html" target="_blank" rel="noopener noreferrer">purchase a domain directly from Amazon Route 53.</a></li> 
<h4>Deploy API with health checks in two regions</h4> 
<p>Start by creating a small “Hello World” Lambda function that sends back a message in the region in which it has been deployed.</p> 
<pre><code class="lang-python">
&quot;&quot;&quot;Return message.&quot;&quot;&quot;
import logging
logging.basicConfig()
logger = logging.getLogger()
logger.setLevel(logging.INFO)
def lambda_handler(event, context):
&quot;&quot;&quot;Lambda handler for getting the hello world message.&quot;&quot;&quot;
region = context.invoked_function_arn.split(':')[3]
logger.info(&quot;message: &quot; + &quot;Hello from &quot; + region)
return {
&quot;message&quot;: &quot;Hello from &quot; + region
}
</code></pre> 
<p>Also create a Lambda function for doing a health check that returns a value based on another environment variable (either “ok” or “fail”) to allow for ease of testing:</p> 
<pre><code class="lang-python">
&quot;&quot;&quot;Return health.&quot;&quot;&quot;
import logging
import os
logging.basicConfig()
logger = logging.getLogger()
logger.setLevel(logging.INFO)
def lambda_handler(event, context):
&quot;&quot;&quot;Lambda handler for getting the health.&quot;&quot;&quot;
logger.info(&quot;status: &quot; + os.environ['STATUS'])
return {
&quot;status&quot;: os.environ['STATUS']
}
</code></pre> 
<p>Deploy both of these using an AWS <a title="Serverless Application Model (SAM)" href="https://github.com/awslabs/serverless-application-model" target="null">Serverless Application Model (SAM)</a> template. SAM is a CloudFormation extension that is optimized for serverless, and provides a standard way to create a complete serverless application. You can find the full <a title="Hello World SAM template" href="https://github.com/aws-samples/blog-multi-region-serverless-service/blob/master/helloworld-api/helloworld-sam.yaml" target="null">helloworld-sam.yaml</a> template in the blog-multi-region-serverless-service GitHub repo.</p> 
<p>A few things to highlight:</p> 
<li>You are using inline Swagger to define your API so you can substitute the current region in the x-amazon-apigateway-integration section.</li> 
<li>Most of the Swagger template covers CORS to allow you to test this from a browser.</li> 
<li>You are also using substitution to populate the environment variable used by the “Hello World” method with the region into which it is being deployed.</li> 
<p>The Swagger allows you to use the same SAM template in both regions.</p> 
<p>You can only use SAM from the AWS CLI, so do the following from the command prompt. First, deploy the SAM template in us-east-1 with the following commands, replacing “&lt;your bucket in us-east-1&gt;” with a bucket in your account:</p> 
<pre><code class="lang-bash">
&gt; cd helloworld-api
&gt; aws cloudformation package --template-file helloworld-sam.yaml --output-template-file /tmp/cf-helloworld-sam.yaml --s3-bucket &lt;your bucket in us-east-1&gt; --region us-east-1
&gt; aws cloudformation deploy --template-file /tmp/cf-helloworld-sam.yaml --stack-name multiregionhelloworld --capabilities CAPABILITY_IAM --region us-east-1
</code></pre> 
<p>Second, do the same in us-west-2:</p> 
<pre><code class="lang-bash">
&gt; aws cloudformation package --template-file helloworld-sam.yaml --output-template-file /tmp/cf-helloworld-sam.yaml --s3-bucket &lt;your bucket in us-west-2&gt; --region us-west-2
&gt; aws cloudformation deploy --template-file /tmp/cf-helloworld-sam.yaml --stack-name multiregionhelloworld --capabilities CAPABILITY_IAM --region us-west-2
</code></pre> 
<p>The API was created with the default endpoint type of <strong>Edge Optimized</strong>. Switch it to <strong>Regional</strong>. In the Amazon API Gateway console, select the API that you just created and choose the wheel-icon to edit it.</p> 
<p><img class="aligncenter wp-image-3147 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-edit-api.png" alt="API Gateway edit API settings" width="975" height="558" /></p> 
<p>In the edit screen, select the <strong>Regional</strong> endpoint type and save the API. Do the same in both regions.</p> 
<p>Grab the URL for the API in the console by navigating to the method in the <strong>prod</strong> stage.</p> 
<p><img class="aligncenter wp-image-3148 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-endpoint-link.png" alt="API Gateway endpoint link" width="975" height="363" /></p> 
<p>You can now test this with curl:</p> 
<pre><code class="lang-bash">
&gt; curl https://2wkt1cxxxx.execute-api.us-west-2.amazonaws.com/prod/helloworld
{&quot;message&quot;: &quot;Hello from us-west-2&quot;}
</code></pre> 
<p>Write down the domain name for the URL in each region (for example, <em>2wkt1cxxxx.execute-api.us-west-2.amazonaws.com</em>), as you need that later when you deploy the Route 53 setup.</p> 
<h4>Create the custom domain name</h4> 
<p>Next, create an Amazon API Gateway custom domain name endpoint. As part of using this feature, you must have a hosted zone and domain available to use in Route 53 as well as an SSL certificate that you use with your specific domain name.</p> 
<p>You can create the SSL certificate by using AWS Certificate Manager. In the ACM console, choose <strong>Get started</strong> (if you have no existing certificates) or <strong>Request a certificate</strong>. Fill out the form with the domain name to use for the custom domain name endpoint, which is the same across the two regions:</p> 
<p><img class="aligncenter wp-image-3149 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/acm-request-certificate.png" alt="Amazon Certificate Manager request new certificate" width="975" height="517" /></p> 
<p>Go through the remaining steps and <a title="Validate ACM certificate" href="http://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate.html" target="_blank" rel="noopener noreferrer">validate the certificate</a> for each region before moving on.</p> 
<p>You are now ready to create the endpoints. In the Amazon API Gateway console, choose <strong>Custom Domain Names</strong>, <strong>Create Custom Domain Name</strong>.</p> 
<p><img class="aligncenter wp-image-3150 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-custom-domain-name.png" alt="API Gateway create custom domain name" width="403" height="375" /></p> 
<p>A few things to highlight:</p> 
<li>The domain name is the same as what you requested earlier through ACM.</li> 
<li>The endpoint configuration should be regional.</li> 
<li>Select the ACM Certificate that you created earlier.</li> 
<li>You need to create a base path mapping that connects back to your earlier API Gateway endpoint. Set the base path to v1 so you can version your API, and then select the API and the prod stage.</li> 
<p>Choose <strong>Save</strong>. You should see your newly created custom domain name:</p> 
<p><img class="aligncenter wp-image-3151 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-custom-domain-save.png" alt="API Gateway custom domain setup" width="627" height="480" /></p> 
<p>Note the value for <strong>Target Domain Name</strong> as you need that for the next step. Do this for both regions.</p> 
<h4>Deploy Route 53 setup</h4> 
<p>Use the global Route 53 service to provide DNS lookup for the Rest API, distributing the traffic in an active-active setup based on latency. You can find the full CloudFormation template in the <a title="Multi region serverless application GitHub sample" href="https://github.com/aws-samples/blog-multi-region-serverless-service/blob/master/helloworld-dns/helloworld-dns.yaml" target="_blank" rel="noopener noreferrer">blog-multi-region-serverless-service GitHub repo</a>.</p> 
<p>The template sets up health checks, for example, for us-east-1:</p> 
<pre><code class="lang-yaml">
HealthcheckRegion1:
Type: &quot;AWS::Route53::HealthCheck&quot;
Properties:
HealthCheckConfig:
Port: &quot;443&quot;
Type: &quot;HTTPS_STR_MATCH&quot;
SearchString: &quot;ok&quot;
ResourcePath: &quot;/prod/healthcheck&quot;
FullyQualifiedDomainName: !Ref Region1HealthEndpoint
RequestInterval: &quot;30&quot;
FailureThreshold: &quot;2&quot;
</code></pre> 
<p>Use the health check when you set up the record set and the latency routing, for example, for us-east-1:</p> 
<pre><code class="lang-yaml">
Region1EndpointRecord:
Type: AWS::Route53::RecordSet
Properties:
Region: us-east-1
HealthCheckId: !Ref HealthcheckRegion1
SetIdentifier: &quot;endpoint-region1&quot;
HostedZoneId: !Ref HostedZoneId
Name: !Ref MultiregionEndpoint
Type: CNAME
TTL: 60
ResourceRecords:
- !Ref Region1Endpoint
</code></pre> 
<p>You can create the stack by using the following link, copying in the domain names from the previous section, your existing hosted zone name, and the main domain name that is created (for example, hellowordapi.replacewithyourcompanyname.com): <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=multiregiondns&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/building-a-multi-region-serverless-application/helloworld-dns.yaml" target="_blank" rel="noopener noreferrer"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png" /></a></p> 
<p>The following screenshot shows what the parameters might look like:<br /> <img class="aligncenter wp-image-3152 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/route-53-healthcheck-setup.png" alt="Serverless multi region Route 53 health check" width="975" height="573" /></p> 
<p>Specifically, the domain names that you collected earlier would map according to following:</p> 
<li>The domain names from the API Gateway “prod”-stage go into Region1HealthEndpoint and Region2HealthEndpoint.</li> 
<li>The domain names from the custom domain name’s target domain name goes into Region1Endpoint and Region2Endpoint.</li> 
<h4>Using the Rest API from server-side applications</h4> 
<p>You are now ready to use your setup. First, demonstrate the use of the API from server-side clients. You can demonstrate this by using curl from the command line:</p> 
<pre><code class="lang-bash">
&gt; curl https://hellowordapi.replacewithyourcompanyname.com/v1/helloworld/
{&quot;message&quot;: &quot;Hello from us-east-1&quot;}
</code></pre> 
<h4>Testing failover of Rest API in browser</h4> 
<p>Here’s how you can use this from the browser and test the failover. Find all of the files for this test in the <a title="Browser client" href="https://github.com/aws-samples/blog-multi-region-serverless-service/blob/master/browser-client/" target="_blank" rel="noopener noreferrer">browser-client</a> folder of the blog-multi-region-serverless-service GitHub repo.</p> 
<p>Use this html file:</p> 
<pre><code class="lang-html">
&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;/&gt;
&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;/&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;/&gt;
&lt;title&gt;Multi-Region Client&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div&gt;
&lt;b&gt;Test Client&lt;/b&gt;
&lt;p id=&quot;client_result&quot;&gt;
&lt;/p&gt;
&lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;settings.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;client.js&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre> 
<p>The html file uses this JavaScript file to repeatedly call the API and print the history of messages:</p> 
<pre><code class="lang-js">
var messageHistory = &quot;&quot;;
(function call_service() {
$.ajax({
url: helloworldMultiregionendpoint+'v1/helloworld/',
dataType: &quot;json&quot;,
cache: false,
success: function(data) {
messageHistory+=&quot;&lt;p&gt;&quot;+data['message']+&quot;&lt;/p&gt;&quot;;
$('#client_result').html(messageHistory);
},
complete: function() {
// Schedule the next request when the current one's complete
setTimeout(call_service, 10000);
},
error: function(xhr, status, error) {
$('#client_result').html('ERROR: '+status);
}
});
})();
</code></pre> 
<p>Also, make sure to update the settings in settings.js to match with the API Gateway endpoints for the DNS-proxy and the multi-regional endpoint for the Hello World API: <code>var helloworldMultiregionendpoint = &quot;https://hellowordapi.replacewithyourcompanyname.com/&quot;;</code></p> 
<p>You can now open the HTML file in the browser (you can do this directly from the file system) and you should see something like the following screenshot:</p> 
<p><img class="aligncenter wp-image-3153 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/browser-test-client-1.png" alt="Serverless multi region browser test" width="975" height="394" /></p> 
<p>You can test failover by changing the environment variable in your health check Lambda function. In the Lambda console, select your health check function and scroll down to the <strong>Environment variables</strong> section. For the <strong>STATUS</strong> key, modify the value to <strong>fail</strong>.</p> 
<p><img class="aligncenter wp-image-3154 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/lambda-change-environment-variable.png" alt="Lambda update environment variable" width="975" height="400" /></p> 
<p>You should see the region switch in the test client:</p> 
<p><img class="aligncenter wp-image-3155 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/browser-test-client-2.png" alt="Serverless multi region broker test switchover" width="975" height="658" /></p> 
<p>During an emulated failure like this, the browser might take some additional time to switch over due to connection keep-alive functionality. If you are using a browser like Chrome, you can <a title="Chrome kill connections" href="https://support.google.com/chrome/a/answer/6271171?hl=en" target="_blank" rel="noopener noreferrer">kill all the connections</a> to see a more immediate fail-over: <code>chrome://net-internals/#sockets</code></p> 
<h3>Summary</h3> 
<p>You have implemented a simple way to do multi-regional serverless applications that fail over seamlessly between regions, either being accessed from the browser or from other applications/services. You achieved this by using the capabilities of Amazon Route 53 to do latency based routing and health checks for fail-over. You unlocked the use of these features in a serverless application by leveraging the new regional endpoint feature of Amazon API Gateway.</p> 
<p>The setup was fully scripted using CloudFormation, the AWS Serverless Application Model (SAM), and the AWS CLI, and it can be integrated into deployment tools to push the code across the regions to make sure it is available in all the needed regions. For more information about cross-region deployments, see <a title="Cross-region code deployment" href="https://aws.amazon.com/blogs/devops/building-a-cross-regioncross-account-code-deployment-solution-on-aws/" target="_blank" rel="noopener noreferrer">Building a Cross-Region/Cross-Account Code Deployment Solution on AWS on the AWS DevOps blog</a>.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/api-gateway/" rel="tag">api gateway</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/route53/" rel="tag">route53</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3143');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure1-756x630.jpeg" /> 
<b class="lb-b blog-post-title" property="name headline">Bringing Datacenter-Scale Hardware-Software Co-design to the Cloud with FireSim and Amazon EC2 F1 Instances</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Mia Champion</span></span> | on 
<time property="datePublished" datetime="2017-10-25T10:09:51+00:00">25 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/bringing-datacenter-scale-hardware-software-co-design-to-the-cloud-with-firesim-and-amazon-ec2-f1-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3096" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3096&amp;disqus_title=Bringing+Datacenter-Scale+Hardware-Software+Co-design+to+the+Cloud+with+FireSim+and+Amazon+EC2+F1+Instances&amp;disqus_url=https://aws.amazon.com/blogs/compute/bringing-datacenter-scale-hardware-software-co-design-to-the-cloud-with-firesim-and-amazon-ec2-f1-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3096');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>The recent addition of Xilinx FPGAs to AWS Cloud compute offerings is one way that AWS is enabling global growth in the areas of advanced analytics, deep learning and AI. The customized F1 servers use pooled accelerators, enabling interconnectivity of up to 8 FPGAs, each one including 64 GiB DDR4 ECC protected memory, with a dedicated PCIe x16 connection. That makes this a powerful engine with the capacity to process advanced analytical applications at scale, at a significantly faster rate. For example, AWS commercial partner <a href="http://www.edicogenome.com/dragen-on-amazon-web-services/">Edico Genome </a>is able to achieve an approximately 30X speedup in analyzing whole genome sequencing datasets using their DRAGEN platform powered with F1 instances.</em></p> 
<p><em>While the availability of FPGA F1 compute on-demand provides clear accessibility and cost advantages, many mainstream users are still finding that the “threshold to entry” in developing or running FPGA-accelerated simulations is too high. Researchers at the UC Berkeley RISE Lab have developed “FireSim”, powered by Amazon FPGA F1 instances as an open-source resource, FireSim lowers that entry bar and makes it easier for everyone to leverage the power of an FPGA-accelerated compute environment. Whether you are part of a small start-up development team or working at a large datacenter scale, hardware-software co-design enables faster time-to-deployment, lower costs, and more predictable performance. We are excited to feature FireSim in this post from Sagar Karandikar and his colleagues at UC-Berkeley.</em></p> 
<p>―Mia Champion, Sr. Data Scientist, AWS</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure1.jpeg"><img class="wp-image-3123" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure1.jpeg" alt="" width="578" height="508" /></a> 
<p class="wp-caption-text">Mapping an 8-node FireSim cluster simulation to Amazon EC2 F1</p> 
<p style="text-align: left">As traditional hardware scaling nears its end, the data centers of tomorrow are trending towards heterogeneity, employing custom hardware accelerators and increasingly high-performance interconnects. Prototyping new hardware at scale has traditionally been either extremely expensive, or very slow. In this post, I introduce <a href="https://fires.im/">FireSim</a>, a new hardware simulation platform under development in the computer architecture research group at UC Berkeley that enables fast, scalable hardware simulation using <a href="https://aws.amazon.com/ec2/instance-types/f1/">Amazon EC2 F1 instances</a>.</p> 
<p>FireSim benefits both hardware and software developers working on new rack-scale systems: software developers can use the simulated nodes with new hardware features as they would use a real machine, while hardware developers have full control over the hardware being simulated and can run real software stacks while hardware is still under development. In conjunction with this post, we’re releasing the first public demo of FireSim, which lets you deploy your own 8-node simulated cluster on an F1 Instance and run benchmarks against it. This demo simulates a pre-built “vanilla” cluster, but demonstrates FireSim’s high performance and usability.</p> 
<h3>Why FireSim + F1?</h3> 
<p>FPGA-accelerated hardware simulation is by no means a new concept. However, previous attempts to use FPGAs for simulation have been fraught with usability, scalability, and cost issues. FireSim takes advantage of EC2 F1 and open-source hardware to address the traditional problems with FPGA-accelerated simulation:<br /> <em>Problem #1: FPGA-based simulations have traditionally been expensive, difficult to deploy, and difficult to reproduce.</em><br /> FireSim uses public-cloud infrastructure like F1, which means no upfront cost to purchase and deploy FPGAs. Developers and researchers can distribute pre-built AMIs and AFIs, as in this public demo (more details later in this post), to make experiments easy to reproduce. FireSim also automates most of the work involved in deploying an FPGA simulation, essentially enabling one-click conversion from new RTL to deploying on an FPGA cluster.</p> 
<p><em>Problem #2: FPGA-based simulations have traditionally been difficult (and expensive) to scale.</em><br /> Because FireSim uses F1, users can scale out experiments by spinning up additional EC2 instances, rather than spending hundreds of thousands of dollars on large FPGA clusters.</p> 
<p><em>Problem #3: Finding open hardware to simulate has traditionally been difficult.</em> <em>Finding open hardware that can run real software stacks is even harder.</em><br /> FireSim simulates <a href="https://github.com/freechipsproject/rocket-chip">RocketChip</a>, an open, silicon-proven, <a href="https://riscv.org/">RISC-V</a>-based processor platform, and adds peripherals like a NIC and disk device to build up a realistic system. Processors that implement RISC-V automatically support real operating systems (such as <a href="https://github.com/riscv/riscv-linux">Linux</a>) and even support applications like Apache and Memcached. We provide a custom Buildroot-based FireSim Linux distribution that runs on our simulated nodes and includes many popular developer tools.</p> 
<p><em>Problem #4: Writing hardware in traditional HDLs is time-consuming.</em><br /> Both FireSim and RocketChip use the <a href="https://chisel.eecs.berkeley.edu/">Chisel</a> HDL, which brings modern programming paradigms to hardware description languages. Chisel greatly simplifies the process of building large, highly parameterized hardware components.</p> 
<h3>How to use FireSim for hardware/software co-design</h3> 
<p>FireSim drastically improves the process of co-designing hardware and software by acting as a push-button interface for collaboration between hardware developers and systems software developers. The following diagram describes the workflows that hardware and software developers use when working with FireSim.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure2-1.jpeg"><img class="wp-image-3122" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure2-1.jpeg" alt="" width="625" height="369" /></a> 
<p class="wp-caption-text">Figure 2. The FireSim custom hardware development workflow.</p> 
<p>The hardware developer’s view:</p> 
<ol> 
<li>Write custom RTL for your accelerator, peripheral, or processor modification in a productive language like Chisel.</li> 
<li>Run a software simulation of your hardware design in standard gate-level simulation tools for early-stage debugging.</li> 
<li>Run FireSim build scripts, which automatically build your simulation, run it through the Vivado toolchain/AWS shell scripts, and publish an AFI.</li> 
<li>Deploy your simulation on EC2 F1 using the generated simulation driver and AFI</li> 
<li>Run real software builds released by software developers to benchmark your hardware</li> 
</ol> 
<p>The software developer’s view:</p> 
<ol> 
<li>Deploy the AMI/AFI generated by the hardware developer on an F1 instance to simulate a cluster of nodes (or scale out to many F1 nodes for larger simulated core-counts).</li> 
<li>Connect using SSH into the simulated nodes in the cluster and boot the Linux distribution included with FireSim. This distribution is easy to customize, and already supports many standard software packages.</li> 
<li>Directly prototype your software using the same exact interfaces that the software will see when deployed on the real future system you’re prototyping, with the same performance characteristics as observed from software, even at scale.</li> 
</ol> 
<p><strong>FireSim demo v1.0</strong></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure3.jpeg"><img class="wp-image-3121" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure3.jpeg" alt="" width="609" height="296" /></a> 
<p class="wp-caption-text">Figure 3. Cluster topology simulated by FireSim demo v1.0.</p> 
<p style="text-align: left">This first public demo of FireSim focuses on the aforementioned “software-developer’s view” of the custom hardware development cycle. The demo simulates a cluster of 1 to 8 RocketChip-based nodes, interconnected by a functional network simulation. The simulated nodes work just like “real” machines: &nbsp;they boot Linux, you can connect to them using SSH, and you can run real applications on top. The nodes can see each other (and the EC2 F1 instance on which they’re deployed) on the network and communicate with one another. While the demo currently simulates a pre-built “vanilla” cluster, the entire hardware configuration of these simulated nodes can be modified after FireSim is open-sourced.</p> 
<p>In this post, I walk through bringing up a single-node FireSim simulation for experienced EC2 F1 users. For more detailed instructions for new users and instructions for running a larger 8-node simulation, see <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html">FireSim Demo v1.0 on Amazon EC2 F1</a>. Both demos walk you through setting up an instance from a demo AMI/AFI and booting Linux on the simulated nodes. The full demo instructions also walk you through an example workload, running Memcached on the simulated nodes, with YCSB as a load generator to demonstrate network functionality.</p> 
<h3>Deploying the demo on F1</h3> 
<p>In this release, we provide pre-built binaries for driving simulation from the host and a pre-built AFI that contains the FPGA infrastructure necessary to simulate a RocketChip-based node.</p> 
<h3>Starting your F1 instances</h3> 
<p>First, launch an instance using the free <a href="https://aws.amazon.com/marketplace/pp/B0758SR46G">FireSim Demo v1.0 product</a> available on the AWS Marketplace on an f1.2xlarge instance. After your instance has booted, log in using the user name centos. On the first login, you should see the message “FireSim network config completed.” This sets up the necessary tap interfaces and bridge on the EC2 instance to enable communicating with the simulated nodes.</p> 
<h3>AMI contents</h3> 
<p>The AMI contains a variety of tools to help you run simulations and build software for RISC-V systems, including the riscv64 toolchain, a Buildroot-based Linux distribution that runs on the simulated nodes, and the simulation driver program. For more details, see the <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html#ami-contents">AMI Contents</a> section on the FireSim website.</p> 
<h3>Single-node demo</h3> 
<p>First, you need to flash the FPGA with the FireSim AFI. To do so, run:</p> 
<p style="text-align: left"><em>[centos@ip-IP_ADDR ~]$ sudo fpga-load-local-image -S 0 -I agfi-00a74c2d615134b21</em></p> 
<p>To start a simulation, run the following at the command line:</p> 
<p style="text-align: left"><em>[centos@ip-IP_ADDR ~]$ boot-firesim-singlenode</em></p> 
<p>This automatically calls the simulation driver, telling it to load the Linux kernel image and root filesystem for the Linux distro. This produces output similar to the following:</p> 
<p style="text-align: left"><em>Simulations Started. You can use the UART console of each simulated node by attaching to the following screens:</em></p> 
<p style="text-align: left"><em>There is a screen on:</em></p> 
<p style="text-align: left"><em>2492.fsim0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Detached)</em></p> 
<p style="text-align: left"><em>1 Socket in /var/run/screen/S-centos.</em></p> 
<p>You could connect to the simulated UART console by connecting to this screen, but instead opt to use SSH to access the node instead.</p> 
<p>First, ping the node to make sure it has come online. This is currently required because nodes may get stuck at Linux boot if the NIC does not receive any network traffic. For more information, see <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html#troubleshooting-ping">Troubleshooting/Errata</a>. The node is always assigned the IP address 192.168.1.10:</p> 
<p style="text-align: left"><em>[centos@ip-IP_ADDR ~]$ ping 192.168.1.10</em></p> 
<p>This should eventually produce the following output:</p> 
<p><em>PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.</em></p> 
<p><em>From 192.168.1.1 icmp_seq=1 Destination Host Unreachable</em></p> 
<p><em>…</em></p> 
<p><em>64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=2017 ms</em></p> 
<p><em>64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=1018 ms</em></p> 
<p><em>64 bytes from 192.168.1.10: icmp_seq=3 ttl=64 time=19.0 ms</em></p> 
<p><em>…</em></p> 
<p>At this point, you know that the simulated node is online. You can connect to it using SSH with the user name root and password firesim. It is also convenient to make sure that your TERM variable is set correctly. In this case, the simulation expects TERM=linux, so provide that:</p> 
<p><em>[centos@ip-IP_ADDR ~]$ TERM=linux ssh root@192.168.1.10</em></p> 
<p><em>The authenticity of host ‘192.168.1.10 (192.168.1.10)’ can’t be established.</em></p> 
<p><em>ECDSA key fingerprint is 63:e9:66:d0:5c:06:2c:1d:5c:95:33:c8:36:92:30:49.</em></p> 
<p><em>Are you sure you want to continue connecting (yes/no)? yes</em></p> 
<p><em>Warning: Permanently added ‘192.168.1.10’ (ECDSA) to the list of known hosts.</em></p> 
<p><em>root@192.168.1.10’s password:</em></p> 
<p><em>#</em></p> 
<p>At this point, you’re connected to the simulated node. Run uname -a as an example. You should see the following output, indicating that you’re connected to a RISC-V system:</p> 
<p><em># uname -a</em></p> 
<p><em>Linux buildroot 4.12.0-rc2 #1 Fri Aug 4 03:44:55 UTC 2017 riscv64 GNU/Linux</em></p> 
<p>Now you can run programs on the simulated node, as you would with a real machine. For an example workload (running YCSB against Memcached on the simulated node) or to run a larger 8-node simulation, see the full <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html">FireSim Demo v1.0 on Amazon EC2 F1 demo instructions</a>.</p> 
<p>Finally, when you are finished, you can shut down the simulated node by running the following command from within the simulated node:</p> 
<p style="text-align: left"><em># poweroff</em></p> 
<p>You can confirm that the simulation has ended by running screen -ls, which should now report that there are no detached screens.</p> 
<h3>Future plans</h3> 
<p>At Berkeley, we’re planning to keep improving the FireSim platform to enable our own research in future data center architectures, like <a href="https://www.usenix.org/sites/default/files/conference/protected-files/fast14_asanovic.pdf">FireBox</a>. The FireSim platform will eventually support more sophisticated processors, custom accelerators (such as <a href="http://hwacha.org/">Hwacha</a>), network models, and peripherals, in addition to scaling to larger numbers of FPGAs. In the future, we’ll open source the entire platform, including Midas, the tool used to transform RTL into FPGA simulators, allowing users to modify any part of the hardware/software stack. Follow <a href="https://twitter.com/firesimproject">@firesimproject</a> on Twitter to stay tuned to future FireSim updates.</p> 
<h3>Acknowledgements</h3> 
<p>FireSim is the joint work of many students and faculty at Berkeley: Sagar Karandikar, Donggyu Kim, Howard Mao, David Biancolin, Jack Koenig, Jonathan Bachrach, and Krste Asanović. This work is partially funded by AWS through the RISE Lab, by the Intel Science and Technology Center for Agile HW Design, and by ASPIRE Lab sponsors and affiliates Intel, Google, HPE, Huawei, NVIDIA, and SK hynix.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-ec2/" rel="tag">Amazon EC2</a>, <a href="https://aws.amazon.com/blogs/compute/tag/fpga/" rel="tag">FPGA</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3096');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
