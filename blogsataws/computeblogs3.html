<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Compute Blogs Blogs @ AWS" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Compute Blogs Blogs @ AWS</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a</p>
<br>
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Synchronizing Amazon S3 Buckets Using AWS Step Functions</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-06-25T22:26:59+00:00">25 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions"><span property="articleSection">AWS Step Functions</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/synchronizing-amazon-s3-buckets-using-aws-step-functions/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2404" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2404&amp;disqus_title=Synchronizing+Amazon+S3+Buckets+Using+AWS+Step+Functions&amp;disqus_url=https://aws.amazon.com/blogs/compute/synchronizing-amazon-s3-buckets-using-aws-step-functions/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2404');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/glez.jpg"><img class="alignnone size-full wp-image-2406" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/glez.jpg" alt="" width="120" height="160" /></a></p> 
<p><strong>Constantin Gonzalez is a Principal Solutions Architect at AWS</strong></p> 
<p>In my free time, I run a small blog that uses Amazon S3 to host static content and Amazon CloudFront to distribute it world-wide. I use a home-grown, static website generator to create and upload my blog content onto S3.</p> 
<p>My blog uses two S3 buckets: one for staging and testing, and one for production. As a website owner, I want to update the production bucket with all changes from the staging bucket in a reliable and efficient way, without having to create and populate a new bucket from scratch. Therefore, to synchronize files between these two buckets, I use <a href="https://aws.amazon.com/lambda" target="_blank" rel="noopener noreferrer">AWS Lambda</a> and <a href="https://aws.amazon.com/step-functions" target="_blank" rel="noopener noreferrer">AWS Step Functions</a>.</p> 
<p>In this post, I show how you can use Step Functions to build a scalable synchronization engine for S3 buckets and learn some common patterns for designing Step Functions state machines while you do so.<span id="more-2404"></span></p> 
<b id="toc_1">Step Functions overview</b> 
<p>Step Functions makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Building applications from individual components that each perform a discrete function lets you scale and change applications quickly.</p> 
<p>While this particular example focuses on synchronizing objects between two S3 buckets, it can be generalized to any other use case that involves coordinated processing of any number of objects in S3 buckets, or other, similar data processing patterns.</p> 
<b id="toc_2">Bucket replication options</b> 
<p>Before I dive into the details on how this particular example works, take a look at some alternatives for copying or replicating data between two Amazon S3 buckets:</p> 
<ul> 
<li>The <a href="https://aws.amazon.com/cli">AWS CLI</a> provides customers with a powerful <a href="http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html" target="_blank" rel="noopener noreferrer">aws s3 sync</a> command that can synchronize the contents of one bucket with another.</li> 
<li><a href="http://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html">S3DistCP</a> is a powerful tool for users of <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR</a> that can efficiently load, save, or copy large amounts of data between S3 buckets and HDFS.</li> 
<li>The S3 <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html" target="_blank" rel="noopener noreferrer">cross-region replication</a> functionality enables automatic, asynchronous copying of objects across buckets in different AWS regions.</li> 
</ul> 
<p>In this use case, you are looking for a slightly different bucket synchronization solution that:</p> 
<ul> 
<li>Works within the same region</li> 
<li>Is more scalable than a CLI approach running on a single machine</li> 
<li>Doesn’t require managing any servers</li> 
<li>Uses a more finely grained cost model than the hourly based Amazon EMR approach</li> 
</ul> 
<p>You need a scalable, serverless, and customizable bucket synchronization utility.</p> 
<b id="toc_3">Solution architecture</b> 
<p>Your solution needs to do three things:</p> 
<ol> 
<li>Copy all objects from a source bucket into a destination bucket, but leave out objects that are already present, for efficiency.</li> 
<li>Delete all “orphaned” objects from the destination bucket that aren’t present on the source bucket, because you don’t want obsolete objects lying around.</li> 
<li>Keep track of all objects for #1 and #2, regardless of how many objects there are.</li> 
</ol> 
<p>In the beginning, you read in the source and destination buckets as parameters and perform basic parameter validation. Then, you operate two separate, independent loops, one for copying missing objects and one for deleting obsolete objects. Each loop is a sequence of Step Functions states that read in chunks of S3 object lists and use the continuation token to decide in a choice state whether to continue the loop or not.</p> 
<p>This solution is based on the following architecture that uses Step Functions, Lambda, and two S3 buckets:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_Arch-1.png"><img class="aligncenter size-medium wp-image-2417" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_Arch-1-300x250.png" alt="" width="300" height="250" /></a></p> 
<p>As you can see, this setup involves no servers, just two main building blocks:</p> 
<ul> 
<li>Step Functions manages the overall flow of synchronizing the objects from the source bucket with the destination bucket.</li> 
<li>A set of Lambda functions carry out the individual steps necessary to perform the work, such as validating input, getting lists of objects from source and destination buckets, copying or deleting objects in batches, and so on.</li> 
</ul> 
<p>To understand the synchronization flow in more detail, look at the Step Functions state machine diagram for this example.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_State-Machine-1.png"><img class="aligncenter size-medium wp-image-2415" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_State-Machine-1-267x300.png" alt="" width="267" height="300" /></a></p> 
<b id="toc_4">Walkthrough</b> 
<p>Here’s a detailed discussion of how this works.</p> 
<p>To follow along, use the code in the <a href="https://github.com/awslabs/sync-buckets-state-machine" target="_blank" rel="noopener noreferrer">sync-buckets-state-machine</a> GitHub repo. The code comes with a ready-to-run deployment script in Python that takes care of all the IAM roles, policies, Lambda functions, and of course the Step Functions state machine deployment using <a href="https://aws.amazon.com/cloudformation" target="_blank" rel="noopener noreferrer">AWS CloudFormation</a>, as well as instructions on how to use it.</p> 
<h3 id="toc_5">Fine print: Use at your own risk</h3> 
<p>Before I start, here are some disclaimers:</p> 
<ul> 
<li><strong>Educational purposes only.</strong> <p>The following example and code are intended for educational purposes only. Make sure that you customize, test, and review it on your own before using any of this in production.</p></li> 
<li><strong>S3 object deletion.</strong> <p>In particular, using the code included below may delete objects on S3 in order to perform synchronization. Make sure that you have backups of your data. In particular, consider using the <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html" target="_blank" rel="noopener noreferrer">Amazon S3 Versioning feature</a> to protect yourself against unintended data modification or deletion.</p></li> 
</ul> 
<p>Step Functions execution starts with an initial set of parameters that contain the source and destination bucket names in JSON:</p> 
<pre><code class="language-none">{
&quot;source&quot;:       &quot;my-source-bucket-name&quot;,
&quot;destination&quot;:  &quot;my-destination-bucket-name&quot;
}</code></pre> 
<p>Armed with this data, Step Functions execution proceeds as follows.</p> 
<h3 id="toc_6">Step 1: Detect the bucket region</h3> 
<p>First, you need to know the regions where your buckets reside. In this case, take advantage of the Step Functions <em>Parallel</em> state. This allows you to use a Lambda function <code>get_bucket_location.py</code> inside two different, parallel branches of <em>task</em> states:</p> 
<ul> 
<li><code>FindRegionForSourceBucket</code></li> 
<li><code>FindRegionForDestinationBucket</code></li> 
</ul> 
<p>Each task state receives one bucket name as an input parameter, then detects the region corresponding to “their” bucket. The output of these functions is collected in a result array containing one element per parallel function.</p> 
<h3 id="toc_7">Step 2: Combine the parallel states</h3> 
<p>The output of a parallel state is a list with all the individual branches’ outputs. To combine them into a single structure, use a Lambda function called <code>combine_dicts.py</code> in its own <code>CombineRegionOutputs</code> <em>task</em> state. The function combines the two outputs from step 1 into a single JSON dict that provides you with the necessary region information for each bucket.</p> 
<h3 id="toc_8">Step 3: Validate the input</h3> 
<p>In this walkthrough, you only support buckets that reside in the same region, so you need to decide if the input is valid or if the user has given you two buckets in different regions. To find out, use a Lambda function called <code>validate_input.py</code> in the <code>ValidateInput</code> <em>task</em> state that tests if the two regions from the previous step are equal. The output is a Boolean.</p> 
<h3 id="toc_9">Step 4: Branch the workflow</h3> 
<p>Use another type of Step Functions state, a <em>Choice</em> state, which branches into a <em>Failure</em> state if the comparison in step 3 yields false, or proceeds with the remaining steps if the comparison was successful.</p> 
<h3 id="toc_10">Step 5: Execute in parallel</h3> 
<p>The actual work is happening in another <em>Parallel</em> state. Both branches of this state are very similar to each other and they re-use some of the Lambda function code.</p> 
<p>Each parallel branch implements a looping pattern across the following steps:</p> 
<ol> 
<li>Use a <em>Pass</em> state to inject either the string value <code>&quot;source&quot;</code> (<code>InjectSourceBucket</code>) or <code>&quot;destination&quot;</code> (<code>InjectDestinationBucket</code>) into the <code>listBucket</code> attribute of the state document.The next step uses either the source or the destination bucket, depending on the branch, while executing the same, generic Lambda function. You don’t need two Lambda functions that differ only slightly. This step illustrates how to use <em>Pass</em> states as a way of injecting constant parameters into your state machine and as a way of controlling step behavior while re-using common step execution code.</li> 
<li>The next step <code>UpdateSourceKeyList/UpdateDestinationKeyList</code> lists objects in the given bucket. <p>Remember that the previous step injected either <code>&quot;source&quot;</code> or <code>&quot;destination&quot;</code> into the state document’s <code>listBucket</code> attribute. This step uses the same <code>list_bucket.py</code> Lambda function to list objects in an S3 bucket. The <code>listBucket</code> attribute of its input decides which bucket to list. In the left branch of the main parallel state, use the list of source objects to work through copying missing objects. The right branch uses the list of destination objects, to check if they have a corresponding object in the source bucket and eliminate any orphaned objects. Orphans don’t have a source object of the same S3 key.</p></li> 
<li>This step performs the actual work. In the left branch, the <code>CopySourceKeys</code> step uses the <code>copy_keys.py</code> Lambda function to go through the list of source objects provided by the previous step, then copies any missing object into the destination bucket. Its sister step in the other branch, <code>DeleteOrphanedKeys</code>, uses its destination bucket key list to test whether each object from the destination bucket has a corresponding source object, then deletes any orphaned objects.</li> 
<li>The S3 <code>ListObjects</code> API action is designed to be scalable across many objects in a bucket. Therefore, it returns object lists in chunks of configurable size, along with a continuation token. If the API result has a continuation token, it means that there are more objects in this list. You can work from token to token to continue getting object list chunks, until you get no more continuation tokens.</li> 
</ol> 
<p>By breaking down large amounts of work into chunks, you can make sure each chunk is completed within the timeframe allocated for the Lambda function, and within the maximum input/output data size for a Step Functions state.</p> 
<p>This approach comes with a slight tradeoff: the more objects you process at one time in a given chunk, the faster you are done. There’s less overhead for managing individual chunks. On the other hand, if you process too many objects within the same chunk, you risk going over time and space limits of the processing Lambda function or the Step Functions state so the work cannot be completed.</p> 
<p>In this particular case, use a Lambda function that maximizes the number of objects listed from the S3 bucket that can be stored in the input/output state data. This is currently up to 32,768 bytes, assuming (based on some experimentation) that the execution of the <code>COPY/DELETE</code> requests in the processing states can always complete in time.</p> 
<p>A more sophisticated approach would use the Step Functions <em>retry/catch</em> state attributes to account for any time limits encountered and adjust the list size accordingly through some list site adjusting.</p> 
<h3 id="toc_11">Step 6: Test for completion</h3> 
<p>Because the presence of a continuation token in the S3 ListObjects output signals that you are not done processing all objects yet, use a <em>Choice</em> state to test for its presence. If a continuation token exists, it branches into the <code>UpdateSourceKeyList</code> step, which uses the token to get to the next chunk of objects. If there is no token, you’re done. The state machine then branches into the <code>FinishCopyBranch/FinishDeleteBranch</code> state.</p> 
<p>By using <em>Choice</em> states like this, you can create loops exactly like the old times, when you didn’t have for statements and used branches in assembly code instead!</p> 
<h3 id="toc_12">Step 7: Success!</h3> 
<p>Finally, you’re done, and can step into your final <em>Success</em> state.</p> 
<b id="toc_13">Lessons learned</b> 
<p>When implementing this use case with Step Functions and Lambda, I learned the following things:</p> 
<ul> 
<li>Sometimes, it is necessary to manipulate the JSON state of a Step Functions state machine with just a few lines of code that hardly seem to warrant their own Lambda function. This is ok, and the cost is actually pretty low given Lambda’s 100 millisecond billing granularity. The upside is that functions like these can be helpful to make the data more palatable for the following steps or for facilitating <em>Choice</em> states. An example here would be the <code>combine_dicts.py</code> function.</li> 
<li><em>Pass</em> states can be useful beyond debugging and tracing, they can be used to inject arbitrary values into your state JSON and guide generic Lambda functions into doing specific things.</li> 
<li><em>Choice</em> states are your friend because you can build while-loops with them. This allows you to reliably grind through large amounts of data with the patience of an engine that currently supports execution times of up to 1 year. <p>Currently, there is an execution history limit of 25,000 events. Each Lambda task state execution takes up 5 events, while each choice state takes 2 events for a total of 7 events per loop. This means you can loop about 3500 times with this state machine. For even more scalability, you can split up work across multiple Step Functions executions through object key sharding or similar approaches.</p></li> 
<li>It’s not necessary to spend a lot of time coding exception handling within your Lambda functions. You can delegate all exception handling to Step Functions and instead simplify your functions as much as possible.</li> 
<li>Step Functions are great replacements for shell scripts. This could have been a shell script, but then I would have had to worry about where to execute it reliably, how to scale it if it went beyond a few thousand objects, etc. Think of Step Functions and Lambda as tools for scripting at a cloud level, beyond the boundaries of servers or containers. “Serverless” here also means “boundary-less”.</li> 
</ul> 
<b id="toc_14">Summary</b> 
<p>This approach gives you scalability by breaking down any number of S3 objects into chunks, then using Step Functions to control logic to work through these objects in a scalable, serverless, and fully managed way.</p> 
<p>To take a look at the code or tweak it for your own needs, use the code in the <a href="https://github.com/awslabs/sync-buckets-state-machine" target="_blank" rel="noopener noreferrer">sync-buckets-state-machine</a> GitHub repo.</p> 
<p>To see more examples, please visit the <a href="https://aws.amazon.com/step-functions/getting-started/" target="_blank" rel="noopener noreferrer">Step Functions Getting Started</a> page.</p> 
<p>Enjoy!</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-storage-service/" rel="tag">Amazon Simple Storage Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/s3/" rel="tag">S3</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2404');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Kotlin and Groovy JVM Languages with AWS Lambda</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Juan Villa</span></span> | on 
<time property="datePublished" datetime="2017-06-23T07:21:46+00:00">23 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/kotlin-and-groovy-jvm-languages-with-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2430" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2430&amp;disqus_title=Kotlin+and+Groovy+JVM+Languages+with+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/kotlin-and-groovy-jvm-languages-with-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2430');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/23/jcv.jpeg"><img class="alignnone size-full wp-image-2442" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/23/jcv.jpeg" alt="" width="119" height="160" /></a><br /> Juan Villa – Partner Solutions Architect</p> 
<p>&nbsp;</p> 
<p>When most people hear “Java” they think of Java the programming language. Java is a lot more than a programming language, it also implies a larger ecosystem including the Java Virtual Machine (JVM). Java, the programming language, is just one of the many languages that can be compiled to run on the JVM. Some of the most popular JVM languages, other than Java, are Clojure, Groovy, Scala, Kotlin, JRuby, and Jython (see <a href="https://en.wikipedia.org/wiki/List_of_JVM_languages">this link</a> for a list of more JVM languages).</p> 
<p>Did you know that you can compile and subsequently run all these languages on AWS Lambda?</p> 
<p><a href="https://aws.amazon.com/lambda">AWS Lambda</a> supports the Java 8 runtime, but this does not mean you are limited to the Java language. The Java 8 runtime is capable of running JVM languages such as Kotlin and Groovy once they have been compiled and packaged as a “fat” JAR (a JAR file containing all necessary dependencies and classes bundled in).</p> 
<p>In this blog post we’ll work through building AWS Lambda functions in both <a href="https://kotlinlang.org/">Kotlin</a> and <a href="http://www.groovy-lang.org/">Groovy</a> programming languages. To compile and package our projects we will use <a href="https://gradle.org/">Gradle</a> build tool.</p> 
<p><span id="more-2430"></span></p> 
<p>To follow along, please clone the Git repository available at GitHub <a href="https://github.com/awslabs/lambda-kotlin-groovy-example">here</a>. Also, I recommend using an Integrated Development Environment (IDE) such as JetBrain’s <a href="https://www.jetbrains.com/idea/">IntelliJ IDEA</a>, this is the IDE I used while working on these projects.</p> 
<h4>Kotlin</h4> 
<p><a href="https://kotlinlang.org/">Kotlin</a> is a statically-typed JVM language designed and developed by <a href="https://www.jetbrains.com/">JetBrains</a> (one of our <a href="https://aws.amazon.com/partners/">Amazon Partner Network</a> Technology partners) and the open source community. Compared to Java the programming language, Kotlin has additional powerful language features such as: <a href="https://kotlinlang.org/docs/reference/data-classes.html">Data Classes</a>, <a href="https://kotlinlang.org/docs/reference/functions.html#default-arguments">Default Arguments</a>, <a href="https://kotlinlang.org/docs/reference/extensions.html">Extensions</a>, <a href="https://kotlinlang.org/docs/reference/null-safety.html">Elvis Operator</a>, and <a href="https://kotlinlang.org/docs/reference/multi-declarations.html">Destructuring Declarations</a>. This is a just a short list of Kotlin’s powerful language features. For a more thorough list of features, and how to use them, refer to the <a href="https://kotlinlang.org/docs/reference/">full documentation</a> of the Kotlin language.</p> 
<p>Let’s jump right into the code and see what an AWS Lambda function looks like in Kotlin.</p> 
<pre><code class="lang-kotlin">package com.aws.blog.jvmlangs.kotlin
import java.io.*
import com.fasterxml.jackson.module.kotlin.*
data class HandlerInput(val who: String)
data class HandlerOutput(val message: String)
class Main {
&nbsp;&nbsp;&nbsp; val mapper = jacksonObjectMapper()
&nbsp;&nbsp;&nbsp; fun handler(input: InputStream, output: OutputStream): Unit {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val inputObj = mapper.readValue&lt;HandlerInput&gt;(input)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mapper.writeValue(output, HandlerOutput(&quot;Hello ${inputObj.who}&quot;))
&nbsp;&nbsp;&nbsp; }
}</code></pre> 
<p>The above example is a very simple Hello World application that accepts as an input a JSON object containing a key called “who” and returns a JSON object containing a key called “message” with a value of “Hello {who}”.</p> 
<p>AWS Lambda does not support serializing JSON objects into Kotlin data classes, but don’t worry! AWS Lambda supports passing an input object as a Stream, and also supports an output Stream for returning a result (see <a href="http://docs.aws.amazon.com/lambda/latest/dg/java-handler-io-type-stream.html">this link</a> for more information). Combined with the Input/Output Stream form of the handler function, we are using the <a href="https://github.com/FasterXML/jackson">Jackson</a> library with a Kotlin extension function to support serialization and deserialization of Kotlin data class types.</p> 
<p>To get started with this example, let’s first compile and package the Kotlin project.</p> 
<pre><code class="lang-bash">git clone https://github.com/awslabs/lambda-kotlin-groovy-example
cd lambda-kotlin-groovy-example/kotlin
./gradlew shadowJar</code></pre> 
<p>Once packaged, a JAR file containing all necessary dependencies will be available at “build/libs/ jvmlangs-kotlin-1.0-SNAPSHOT-all.jar”. Now let’s deploy this package to AWS Lambda.</p> 
<p>To deploy the lambda function, we will be using the AWS Command Line Interface (CLI). You can find information on how to set up the AWS CLI <a href="https://aws.amazon.com/cli/">here</a>. This tool allows you to set up and manage AWS services via the command line.</p> 
<pre><code class="lang-bash">aws lambda create-function --region us-east-1 --function-name kotlin-hello \
--zip-file fileb://build/libs/jvmlangs-kotlin-1.0-SNAPSHOT-all.jar \
--role arn:aws:iam::&lt;account_id&gt;:role/lambda_basic_execution \
--handler com.aws.blog.jvmlangs.kotlin.Main::handler --runtime java8 \
--timeout 15 --memory-size 128</code></pre> 
<p>Once deployed, we can test the function by invoking the lambda function from the CLI.</p> 
<pre><code class="lang-bash">aws lambda invoke --function-name kotlin-hello --payload '{&quot;who&quot;: &quot;AWS Fan&quot;}' output.txt
cat output.txt</code></pre> 
<p>If successful, you’ll see an output of <code class="lang-json">“{&quot;message&quot;:&quot;Hello AWS Fan&quot;}”</code>.</p> 
<h4>Groovy</h4> 
<p><a href="http://groovy-lang.org/">Groovy</a> is an optionally typed JVM language with both dynamic and static typing capabilities. Groovy is currently being supported by the <a href="http://www.apache.org/">Apache Software Foundation</a>. Like Kotlin, Groovy also packs a lot of powerful features such as: <a href="http://docs.groovy-lang.org/latest/html/documentation/#_closures">Closures</a>, <a href="http://docs.groovy-lang.org/latest/html/documentation/#_dynamic_vs_static">Dynamic Typing</a>, <a href="http://docs.groovy-lang.org/latest/html/documentation/#_collection_literal_type_inference">Collection Literals</a>, <a href="http://docs.groovy-lang.org/latest/html/documentation/#_string_interpolation">String Interpolation</a>, and <a href="http://docs.groovy-lang.org/latest/html/documentation/#_elvis_operator">Elvis Operator</a>. This is just a short list, see the <a href="http://docs.groovy-lang.org/latest/html/documentation/">full documentation</a> for a list of features and how to use them.</p> 
<p>Once again, let’s jump right into the code.</p> 
<pre><code class="lang-groovy">package com.aws.blog.jvmlangs.groovy
class HandlerInput {
&nbsp;&nbsp;&nbsp; String who
}
class HandlerOutput {
&nbsp;&nbsp;&nbsp; String message
}
class Main {
&nbsp;&nbsp;&nbsp; def handler(HandlerInput input) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return new HandlerOutput(message: &quot;Hello ${input.who}&quot;)
&nbsp;&nbsp;&nbsp; }
}</code></pre> 
<p>Just like the Kotlin example, we have defined a function that takes a simple JSON object containing a “who” key value and build a response containing a “message” key. Note that in this case we are not using the Input/Output Stream form of the handler function, but rather we are letting AWS Lambda serialize the input JSON object into the type HandlerInput. To accomplish this, AWS Lambda uses the Jackson library and handles the serialization for us.</p> 
<p>Let’s go ahead and compile and package this Groovy example.</p> 
<pre><code class="lang-bash">git clone https://github.com/awslabs/lambda-kotlin-groovy-example
cd lambda-kotlin-groovy-example/groovy
./gradlew shadowJar</code></pre> 
<p>Once packaged, a JAR file containing all necessary dependencies will be available at “build/libs/ jvmlangs-groovy-1.0-SNAPSHOT-all.jar”. Now let’s deploy this package to AWS Lambda.</p> 
<pre><code class="lang-bash">aws lambda create-function --region us-east-1 --function-name groovy-hello \
--zip-file fileb://build/libs/jvmlangs-groovy-1.0-SNAPSHOT-all.jar \
--role arn:aws:iam::&lt;account_id&gt;:role/lambda_basic_execution \
--handler com.aws.blog.jvmlangs.groovy.Main::handler --runtime java8 \
--timeout 15 --memory-size 128</code></pre> 
<p>Once deployed, we can test the function by invoking the lambda function from the CLI.</p> 
<pre><code class="lang-bash">aws lambda invoke --function-name groovy-hello --payload '{&quot;who&quot;: &quot;AWS Fan&quot;}' output.txt
cat output.txt</code></pre> 
<p>If successful, you’ll see an output of <code class="lang-json">“{&quot;message&quot;:&quot;Hello AWS Fan&quot;}”</code>.</p> 
<h4>Gradle Build Tool</h4> 
<p>Finally, let’s touch up on how we built the JAR package from the Kotlin and Groovy sources above. To build the JARs we used the <a href="https://gradle.org/">Gradle</a> build tool. Gradle builds a project by reading instructions from a file called “build.gradle”. This is a file written in Gradle’s Groovy Domain Specific Langauge (DSL). You can find more information on the gradle build file by looking at their <a href="https://gradle.org/docs">documentation</a>. Let’s take a look at the Gradle build files we used for this post.</p> 
<p>For the Kotlin example, this is the build file we used.</p> 
<pre><code class="lang-gradle">buildscript {
&nbsp;&nbsp;&nbsp; repositories {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mavenCentral()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; jcenter()
&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; dependencies {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classpath &quot;org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classpath &quot;com.github.jengelman.gradle.plugins:shadow:1.2.3&quot;
&nbsp;&nbsp;&nbsp; }
}
group 'com.aws.blog.jvmlangs.kotlin'
version '1.0-SNAPSHOT'
apply plugin: 'kotlin'
apply plugin: 'com.github.johnrengelman.shadow'
repositories {
&nbsp;&nbsp;&nbsp; mavenCentral()
}
dependencies {
&nbsp;&nbsp;&nbsp; compile &quot;org.jetbrains.kotlin:kotlin-stdlib:$kotlin_version&quot;
&nbsp;&nbsp;&nbsp; compile &quot;com.fasterxml.jackson.module:jackson-module-kotlin:2.8.2&quot;
}</code></pre> 
<p>For the Groovy example this is the build file we used.</p> 
<pre><code class="lang-gradle">buildscript {
&nbsp;&nbsp;&nbsp; repositories {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; jcenter()
&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; dependencies {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.3'
&nbsp;&nbsp;&nbsp; }
}
group 'com.aws.blog.jvmlangs.groovy'
version '1.0-SNAPSHOT'
apply plugin: 'groovy'
apply plugin: 'com.github.johnrengelman.shadow'
repositories {
&nbsp;&nbsp;&nbsp; mavenCentral()
}
dependencies {
&nbsp;&nbsp;&nbsp; compile 'org.codehaus.groovy:groovy-all:2.3.11'
&nbsp;&nbsp;&nbsp; testCompile group: 'junit', name: 'junit', version: '4.11'
}</code></pre> 
<p>As you can see, the build files for both Kotlin and Groovy files are very similar. For the Kotlin project we define a dependency on the Jackson Kotlin module. Also, for each respective language we include the language supporting libraries (kotlin-stdlib and groovy-all respectively).</p> 
<p>In addition, you will notice that we are using a plugin called “shadow”. We use this plugin to package all the project dependencies into one JAR by using the Gradle task “shadowJar”. You can find more information on Shadow in their <a href="http://imperceptiblethoughts.com/shadow/">documentation</a>.</p> 
<h4>Final Words</h4> 
<p>Don’t stop here though! Take a look at other JVM languages and get them running on AWS Lambda with the Java 8 runtime. Maybe start with <a href="http://clojure.org/">Clojure</a>? or <a href="http://www.scala-lang.org/">Scala</a>?</p> 
<p>Also take a look <a href="https://github.com/aws/aws-lambda-java-libs">AWS Lambda Java libraries</a> provided by AWS. They provide interfaces and models to make handling events from <a href="http://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html">event sources</a> easier to handle.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2430');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2017-06-20T12:56:00+00:00">20 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2369" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2369&amp;disqus_title=Building+Loosely+Coupled%2C+Scalable%2C+C%23+Applications+with+Amazon+SQS+and+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2369');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<h5>&nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/Stephen_Liedig.png"><img class="alignnone size-full wp-image-2371" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/Stephen_Liedig.png" alt="" width="203" height="160" /></a><br /> Stephen Liedig, Solutions Architect</h5> 
<p>&nbsp;</p> 
<p>One of the many challenges professional software architects and developers face is how to make cloud-native applications scalable, fault-tolerant, and highly available.</p> 
<p>Fundamental to your project success is understanding the importance of making systems highly cohesive and loosely coupled. That means considering the multi-dimensional facets of system coupling to support the distributed nature of the applications that you are building for the cloud.</p> 
<p>By that, I mean addressing not only the application-level coupling (managing incoming and outgoing dependencies), but also considering the impacts of of platform, spatial, and temporal coupling of your systems. Platform coupling relates to the interoperability (or lack thereof) of heterogeneous systems components. Spatial coupling deals with managing components at a network topology level or protocol level. Temporal, or runtime coupling, refers to the ability of a component within your system to do any kind of meaningful work while it is performing a synchronous, blocking operation.</p> 
<p>The AWS messaging services, <a href="https://aws.amazon.com/sqs/">Amazon SQS</a> and <a href="https://aws.amazon.com/sns/">Amazon SNS</a>, help you deal with these forms of coupling by providing mechanisms for:</p> 
<ul> 
<li>Reliable, durable, and fault-tolerant delivery of messages between application components</li> 
<li>Logical decomposition of systems and increased autonomy of components</li> 
<li>Creating unidirectional, non-blocking operations, temporarily decoupling system components at runtime</li> 
<li>Decreasing the dependencies that components have on each other through standard communication and network channels</li> 
</ul> 
<p>Following on the recent topic, <a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a>, in this post, I look at some of the ways you can introduce SQS and SNS into your architectures to decouple your components, and show how you can implement them using C#.<span id="more-2369"></span></p> 
<h3>Walkthrough</h3> 
<p>To illustrate some of these concepts, consider a web application that processes customer orders. As good architects and developers, you have followed best practices and made your application scalable and highly available. Your solution included implementing load balancing, dynamic scaling across multiple Availability Zones, and persisting orders in a Multi-AZ Amazon RDS database instance, as in the following diagram.</p> 
<p><img class="aligncenter wp-image-2373 size-large" title="Order_Processing_Application" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/WebApplicationDiagram1-1024x552.png" alt="" width="640" height="345" /><br /> In this example, the application is responsible for handling and persisting the order data, as well as dealing with increases in traffic for popular items.</p> 
<p>One potential point of vulnerability in the order processing workflow is in saving the order in the database. The business expects that every order has been persisted into the database. However, any potential deadlock, race condition, or network issue could cause the persistence of the order to fail. Then, the order is lost with no recourse to restore the order.</p> 
<p>With good logging capability, you may be able to identify when an error occurred and which customer order failed. This wouldn’t allow you to “restore” the transaction, and by that stage, your customer is no longer your customer.</p> 
<p>As illustrated in the following diagram, introducing an SQS queue helps improve your ordering application. Using the queue isolates the processing logic into its own component and runs it in a separate process from the web application. This, in turn, allows the system to be more resilient to spikes in traffic, while allowing work to be performed only as fast as necessary in order to manage costs.</p> 
<p><img class="aligncenter wp-image-2379 size-large" title="Order_Processing_SQS" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/CustomOrderQueue-1024x632.png" alt="" width="640" height="395" /><br /> In addition, you now have a mechanism for persisting orders as messages (with the queue acting as a temporary database), and have moved the scope of your transaction with your database further down the stack. In the event of an application exception or transaction failure, this ensures that the order processing can be retired or redirected to the Amazon SQS Dead Letter Queue (DLQ), for re-processing at a later stage. (See the recent post, <a href="https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/">Using Amazon SQS Dead-Letter Queues to Control Message Failure,</a> for more information on dead-letter queues.)</p> 
<h3>Scaling the order processing nodes</h3> 
<p>This change allows you now to scale the web application frontend independently from the processing nodes. The frontend application can continue to scale based on metrics such as CPU usage, or the number of requests hitting the load balancer. Processing nodes can scale based on the number of orders in the queue. Here is an example of scale-in and scale-out alarms that you would associate with the scaling policy.</p> 
<p><strong class="lang-bash">Scale-out Alarm</strong></p> 
<pre><code class="lang-powershell">aws cloudwatch put-metric-alarm --alarm-name AddCapacityToCustomerOrderQueue --metric-name ApproximateNumberOfMessagesVisible --namespace &quot;AWS/SQS&quot; 
--statistic Average --period 300 --threshold 3 --comparison-operator GreaterThanOrEqualToThreshold --dimensions Name=QueueName,Value=customer-orders
--evaluation-periods 2 --alarm-actions &lt;arn of the scale-out autoscaling policy&gt;
</code></pre> 
<p><strong>Scale-in Alarm</strong></p> 
<pre><code class="lang-powershell">aws cloudwatch put-metric-alarm --alarm-name RemoveCapacityFromCustomerOrderQueue --metric-name ApproximateNumberOfMessagesVisible --namespace &quot;AWS/SQS&quot; 
--statistic Average --period 300 --threshold 1 --comparison-operator LessThanOrEqualToThreshold --dimensions Name=QueueName,Value=customer-orders
--evaluation-periods 2 --alarm-actions &lt;arn of the scale-in autoscaling policy&gt;
</code></pre> 
<p>In the above example, use the A<em>pproximateNumberOfMessagesVisible</em> metric to discover the queue length and drive the scaling policy of the Auto Scaling group. Another useful metric is <em>ApproximateAgeOfOldestMessage</em>, when applications have time-sensitive messages and developers need to ensure that messages are processed within a specific time period.</p> 
<h3>Scaling the order processing implementation</h3> 
<p>On top of scaling at an infrastructure level using Auto Scaling, make sure to take advantage of the processing power of your <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances by using as many of the available threads as possible. There are several ways to implement this. In this post, we build a Windows service that uses the <strong>BackgroundWorker</strong> class to process the messages from the queue.</p> 
<p>Here’s a closer look at the implementation. In the first section of the consuming application, use a loop to continually poll the queue for new messages, and construct a <strong>ReceiveMessageRequest</strong> variable.</p> 
<pre><code class="lang-csharp">public static void PollQueue()
{
while (_running)
{
Task&lt;ReceiveMessageResponse&gt; receiveMessageResponse;
// Pull messages off the queue
using (var sqs = new AmazonSQSClient())
{
const int maxMessages = 10;  // 1-10
//Receiving a message
var receiveMessageRequest = new ReceiveMessageRequest
{
// Get URL from Configuration
QueueUrl = _queueUrl, 
// The maximum number of messages to return. 
// Fewer messages might be returned. 
MaxNumberOfMessages = maxMessages, 
// A list of attributes that need to be returned with message.
AttributeNames = new List&lt;string&gt; { &quot;All&quot; },
// Enable long polling. 
// Time to wait for message to arrive on queue.
WaitTimeSeconds = 5 
};
receiveMessageResponse = sqs.ReceiveMessageAsync(receiveMessageRequest);
}
</code></pre> 
<p>The<strong> WaitTimeSeconds</strong> property of the <strong>ReceiveMessageRequest</strong> specifies the duration (in seconds) that the call waits for a message to arrive in the queue before returning a response to the calling application. There are a few benefits to using long polling:</p> 
<ul> 
<li>It reduces the number of empty responses by allowing SQS to wait until a message is available in the queue before sending a response.</li> 
<li>It eliminates false empty responses by querying all (rather than a limited number) of the servers.</li> 
<li>It returns messages as soon any message becomes available.</li> 
</ul> 
<p>For more information, see <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html">Amazon SQS Long Polling</a>.</p> 
<p>After you have returned messages from the queue, you can start to process them by looping through each message in the response and invoking a new <strong>BackgroundWorker</strong> thread.</p> 
<pre><code class="lang-csharp">// Process messages
if (receiveMessageResponse.Result.Messages != null)
{
foreach (var message in receiveMessageResponse.Result.Messages)
{
Console.WriteLine(&quot;Received SQS message, starting worker thread&quot;);
// Create background worker to process message
BackgroundWorker worker = new BackgroundWorker();
worker.DoWork += (obj, e) =&gt; ProcessMessage(message);
worker.RunWorkerAsync();
}
}
else
{
Console.WriteLine(&quot;No messages on queue&quot;);
}
</code></pre> 
<p>The event handler, <strong>ProcessMessage</strong>, is where you implement business logic for processing orders. It is important to have a good understanding of how long a typical transaction takes so you can set a message <strong>VisibilityTimeout</strong> that is long enough to complete your operation. If order processing takes longer than the specified timeout period, the message becomes visible on the queue. Other nodes may pick it and process the same order twice, leading to unintended consequences.</p> 
<h3>Handling Duplicate Messages</h3> 
<p>In order to manage duplicate messages, seek to make your processing application idempotent. In mathematics, idempotent describes a function that produces the same result if it is applied to itself:</p> 
<p style="text-align: center">f(x) = f(f(x))</p> 
<p>No matter how many times you process the same message, the end result is the same (definition from Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions, Hohpe and Wolf, 2004).</p> 
<p>There are several strategies you could apply to achieve this:</p> 
<ul> 
<li>Create messages that have inherent idempotent characteristics. That is, they are non-transactional in nature and are unique at a specified point in time. Rather than saying “place new order for Customer A,” which adds a duplicate order to the customer, use “place order &lt;orderid&gt; on &lt;timestamp&gt; for Customer A,” which creates a single order no matter how often it is persisted.</li> 
<li>Deliver your messages via an <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html">Amazon SQS FIFO queue</a>, which provides the benefits of message sequencing, but also mechanisms for content-based deduplication. You can deduplicate using the <strong>MessageDeduplicationId</strong> property on the <strong>SendMessage</strong> request or by enabling content-based deduplication on the queue, which generates a hash for <strong>MessageDeduplicationId</strong>, based on the content of the message, not the attributes.</li> 
</ul> 
<pre><code class="lang-csharp">var sendMessageRequest = new SendMessageRequest
{
QueueUrl = _queueUrl,
MessageBody = JsonConvert.SerializeObject(order),
MessageGroupId = Guid.NewGuid().ToString(&quot;N&quot;),
MessageDeduplicationId = Guid.NewGuid().ToString(&quot;N&quot;)
};
</code></pre> 
<ul> 
<li>If using SQS FIFO queues is not an option, keep a message log of all messages attributes processed for a specified period of time, as an alternative to message deduplication on the receiving end. Verifying the existence of the message in the log before processing the message adds additional computational overhead to your processing. This can be minimized through low latency persistence solutions such as Amazon DynamoDB. Bear in mind that this solution is dependent on the successful, distributed transaction of the message and the message log.</li> 
</ul> 
<h3>Handling exceptions</h3> 
<p>Because of the distributed nature of SQS queues, it does not automatically delete the message. Therefore, you must explicitly delete the message from the queue after processing it, using the message <strong>ReceiptHandle</strong> property (see the following code example).</p> 
<p>However, if at any stage you have an exception, avoid handling it as you normally would. The intention is to make sure that the message ends back on the queue, so that you can gracefully deal with intermittent failures. Instead, log the exception to capture diagnostic information, and swallow it.</p> 
<p>By not explicitly deleting the message from the queue, you can take advantage of the <strong>VisibilityTimeout</strong> behavior described earlier. Gracefully handle the message processing failure and make the unprocessed message available to other nodes to process.</p> 
<p><img class="aligncenter wp-image-2378 size-large" title="SQS_queue_settings" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/ConfigureCustomerOrders-1024x732.png" alt="" width="640" height="458" /></p> 
<p>In the event that subsequent retries fail, SQS automatically moves the message to the configured DLQ after the configured number of receives has been reached. You can further investigate why the order process failed. Most importantly, the order has not been lost, and your customer is still your customer.</p> 
<pre><code class="lang-csharp">private static void ProcessMessage(Message message)
{
using (var sqs = new AmazonSQSClient())
{
try
{
Console.WriteLine(&quot;Processing message id: {0}&quot;, message.MessageId);
// Implement messaging processing here
// Ensure no downstream resource contention (parallel processing)
// &lt;your order processing logic in here…&gt;
Console.WriteLine(&quot;{0} Thread {1}: {2}&quot;, DateTime.Now.ToString(&quot;s&quot;), Thread.CurrentThread.ManagedThreadId, message.MessageId);
// Delete the message off the queue. 
// Receipt handle is the identifier you must provide 
// when deleting the message.
var deleteRequest = new DeleteMessageRequest(_queueName, message.ReceiptHandle);
sqs.DeleteMessageAsync(deleteRequest);
Console.WriteLine(&quot;Processed message id: {0}&quot;, message.MessageId);
}
catch (Exception ex)
{
// Do nothing.
// Swallow exception, message will return to the queue when 
// visibility timeout has been exceeded.
Console.WriteLine(&quot;Could not process message due to error. Exception: {0}&quot;, ex.Message);
}
}
}
</code></pre> 
<h3>Using SQS to adapt to changing business requirements</h3> 
<p>One of the benefits of introducing a <a href="https://aws.amazon.com/message-queue/">message queue</a> is that you can accommodate new business requirements without dramatically affecting your application.</p> 
<p>If, for example, the business decided that all orders placed over $5000 are to be handled as a priority, you could introduce a new “priority order” queue. The way the orders are processed does not change. The only significant change to the processing application is to ensure that messages from the “priority order” queue are processed before the “standard order” queue.</p> 
<p>The following diagram shows how this logic could be isolated in an “order dispatcher,” whose only purpose is to route order messages to the appropriate queue based on whether the order exceeds $5000. Nothing on the web application or the processing nodes changes other than the target queue to which the order is sent. The rates at which orders are processed can be achieved by modifying the poll rates and scalability settings that I have already discussed.</p> 
<p><img class="aligncenter wp-image-2377 size-large" title="Order_Dispatcher" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/OrderDispatcher-1024x534.png" alt="" width="640" height="334" /></p> 
<h3>Extending the design pattern with Amazon SNS</h3> 
<p>Amazon SNS supports reliable <a href="https://aws.amazon.com/pub-sub-messaging/">publish-subscribe (pub-sub) scenarios</a> and push notifications to known endpoints across a wide variety of protocols. It eliminates the need to periodically check or poll for new information and updates. SNS supports:</p> 
<ul> 
<li>Reliable storage of messages for immediate or delayed processing</li> 
<li>Publish / subscribe – direct, broadcast, targeted “push” messaging</li> 
<li>Multiple subscriber protocols</li> 
<li>Amazon SQS, HTTP, HTTPS, email, SMS, mobile push, AWS Lambda</li> 
</ul> 
<p>With these capabilities, you can provide parallel asynchronous processing of orders in the system and extend it to support any number of different business use cases without affecting the production environment. This is commonly referred to as a “fanout” scenario.</p> 
<p>Rather than your web application pushing orders to a queue for processing, send a notification via SNS. The SNS messages are sent to a topic and then replicated and pushed to multiple SQS queues and Lambda functions for processing.</p> 
<p><img class="aligncenter wp-image-2376 size-large" title="Order_Dispatcher_Fanout" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/OrderDispatcher2-1024x718.png" alt="" width="640" height="449" /></p> 
<p>As the diagram above shows, you have the development team consuming “live” data as they work on the next version of the processing application, or potentially using the messages to troubleshoot issues in production.</p> 
<p>Marketing is consuming all order information, via a Lambda function that has subscribed to the SNS topic, inserting the records into an Amazon Redshift warehouse for analysis.</p> 
<p>All of this, of course, is happening without affecting your order processing application.</p> 
<h3>Summary</h3> 
<p>While I haven’t dived deep into the specifics of each service, I have discussed how these services can be applied at an architectural level to build loosely coupled systems that facilitate multiple business use cases. I’ve also shown you how to use infrastructure and application-level scaling techniques, so you can get the most out of your EC2 instances.</p> 
<p>One of the many benefits of using these managed services is how quickly and easily you can implement powerful messaging capabilities in your systems, and lower the capital and operational costs of managing your own messaging middleware.</p> 
<p>Using Amazon SQS and Amazon SNS together can provide you with a powerful mechanism for decoupling application components. This should be part of design considerations as you architect for the cloud.</p> 
<p>For more information, see the <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/Welcome.html">Amazon SQS Developer Guide</a> and <a href="http://docs.aws.amazon.com/sns/latest/dg/welcome.html">Amazon SNS Developer Guide</a>. You’ll find tutorials on all the concepts covered in this post, and more. To can get started using the AWS console or SDK of your choice visit:</p> 
<ul> 
<li><a href="https://aws.amazon.com/sqs/getting-started/">Getting Started with Amazon SQS</a></li> 
<li><a href="https://aws.amazon.com/sns/getting-started/">Getting Started with Amazon SNS</a></li> 
</ul> 
<p>Happy messaging!</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-notification-service/" rel="tag">Amazon Simple Notification Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-queue-service/" rel="tag">Amazon Simple Queue Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sns/" rel="tag">Amazon SNS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sqs/" rel="tag">Amazon SQS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloud-messaging/" rel="tag">cloud messaging</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-queues/" rel="tag">message queues</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-topics/" rel="tag">message topics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/notifications/" rel="tag">notifications</a>, <a href="https://aws.amazon.com/blogs/compute/tag/pubsub/" rel="tag">pub/sub</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2369');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Secure API Access with Amazon Cognito Federated Identities, Amazon Cognito User Pools, and Amazon API Gateway</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Ed Lima</span></span> | on 
<time property="datePublished" datetime="2017-06-19T06:31:15+00:00">19 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-api-gateway/" title="View all posts in Amazon API Gateway"><span property="articleSection">Amazon API Gateway</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-cognito/" title="View all posts in Amazon Cognito"><span property="articleSection">Amazon Cognito</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/secure-api-access-with-amazon-cognito-federated-identities-amazon-cognito-user-pools-and-amazon-api-gateway/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image002-1.png"><img class="alignnone size-full wp-image-2274" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image002-1.png" alt="" width="90" height="120" /></a></p> 
<p>Ed Lima, Solutions Architect</p> 
<p>&nbsp;</p> 
<p>Our identities are what define us as human beings. Philosophical discussions aside, it also applies to our day-to-day lives. For instance, I need my work badge to get access to my office building or my passport to travel overseas. My identity in this case is attached to my work badge or passport. As part of the system that checks my access, these documents or objects help define whether I have access to get into the office building or travel internationally.</p> 
<p>This exact same concept can also be applied to cloud applications and APIs. To provide secure access to your application users, you define who can access the application resources and what kind of access can be granted. <span id="more-2229"></span>Access is based on identity controls that can confirm authentication (AuthN) and authorization (AuthZ), which are different concepts. According to <a href="https://en.wikipedia.org/wiki/Authentication">Wikipedia</a>:</p> 
<p>&nbsp;</p> 
<blockquote> 
<p>“<em>The process of authorization is distinct from that of authentication. Whereas authentication is the process of verifying that “you are who you say you are,” authorization is the process of verifying that “you are permitted to do what you are trying to do.” This does not mean authorization presupposes authentication; an anonymous agent could be authorized to a limited action set.</em>”</p> 
</blockquote> 
<p><a href="https://aws.amazon.com/cognito">Amazon Cognito</a> allows building, securing, and scaling a solution to handle user management and authentication, and to sync across platforms and devices. In this post, I discuss the different ways that you can use Amazon Cognito to authenticate API calls to <a href="https://aws.amazon.com/apigateway">Amazon API Gateway</a> and secure access to your own API resources.</p> 
<p>&nbsp;</p> 
<b>Amazon Cognito Concepts</b> 
<p>&nbsp;</p> 
<p>It’s important to understand that Amazon Cognito provides three different services:</p> 
<ul> 
<li><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html">Amazon Cognito Federated Identities </a></li> 
<li><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html">Amazon Cognito User Pools</a></li> 
<li><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html">Amazon Cognito Sync</a></li> 
</ul> 
<p>Today, I discuss the use of the first two. One service doesn’t need the other to work; however, they can be configured to work together.<br /> &nbsp;</p> 
<h3>Amazon Cognito Federated Identities</h3> 
<p>&nbsp;<br /> To use Amazon Cognito Federated Identities in your application, create an identity pool. An identity pool is a store of user data specific to your account. It can be configured to require an identity provider (IdP) for user authentication, after you enter details such as app IDs or keys related to that specific provider.</p> 
<p>After the user is validated, the provider sends an identity token to Amazon Cognito Federated Identities. In turn, Amazon Cognito Federated Identities contacts the <a href="http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html">AWS Security Token Service</a> (AWS STS) to retrieve temporary AWS credentials based on a configured, authenticated IAM role linked to the identity pool. The role has appropriate IAM policies attached to it and uses these policies to provide access to other AWS services.</p> 
<p>Amazon Cognito Federated Identities currently supports the IdPs listed in the following graphic.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/18/CognitoDiagram.png"><img class="aligncenter wp-image-2309 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/18/CognitoDiagram.png" alt="" width="1275" height="714" /></a><br /> 
<!--more--><br /> &nbsp;</p> 
<p>In a nutshell, Amazon Cognito Federated Identities can be compared to a token vending machine that uses STS as a backend. The simplified user authentication flow for a given provider is:</p> 
<ol> 
<li>App sends user credentials to provider, usually user name and password.</li> 
<li>After the user is authenticated, the provider sends a valid token back to the application.</li> 
<li>The application sends the token to the identity pool associated with it.</li> 
<li>Amazon Cognito Federated Identities validates the token with the IdP.</li> 
<li>If the token is valid, Amazon Cognito Federated Identities contacts STS to retrieve temporary access credentials (access key, secret key, and session token) based on the authenticated IAM role associated with the identity pool.</li> 
<li>App contacts the specific AWS service with the temporary credentials.</li> 
</ol> 
<p>For more information about the different authentication flows, see the <a href="http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-flow.html">Authentication Flow</a> topic and the <a href="https://aws.amazon.com/blogs/mobile/understanding-amazon-cognito-authentication-part-4-enhanced-flow/">Understanding Amazon Cognito Authentication Part 4: Enhanced Flow</a> blog post.</p> 
<p>If you don’t want to use an IdP, Amazon Cognito Federated Identities can also support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an IdP. If your application allows customers to connect as a guest user without logging in, you can enable access for unauthenticated identities. In that case, STS sends temporary credentials based on a specific unauthenticated IAM role with appropriate policies. In these cases, AWS strongly recommends that you stick with the principle of the least privilege, only allowing access to perform a certain task and nothing else.<br /> &nbsp;</p> 
<h3>Amazon Cognito User Pools</h3> 
<p>&nbsp;<br /> Amazon Cognito User Pools, on other hand is a full-fledged IdP that you can use to maintain a user directory and add sign-up and sign-in support to your mobile or web application. It uses JSON Web Tokens (JWTs) to authenticate and validate users. JWT is an open standard that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed.</p> 
<p>Amazon Cognito User Pools and identity pools can be used in conjunction to provide access to your application. An Amazon Cognito User Pools user authenticated with a user name and password can send a JWT to an associated identity pool. In turn, the identity pool sends temporary AWS credentials back to the application to access other AWS services. There’s no difference with the authentication flow mentioned above for other IdPs.</p> 
<p>To use a metaphor, Amazon Cognito User Pools provided you with a passport (JWT) that you can use at the airport counter to retrieve a boarding pass (access credentials) from an identity pool. With your valid boarding pass, you are then allowed to go to the airport gate and board your flight to the AWS Cloud, which is a fitting analogy for this post. The boarding pass is only valid for a specific time. In application terms, the token (passport) authenticates the user and the issued temporary credentials (boarding pass) authorize the access to connect (board).</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image005.png"><img class="aligncenter wp-image-2276 size-medium" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image005-300x175.png" alt="" width="300" height="175" /></a></p> 
<p>&nbsp;</p> 
<b>A Practical Example – Integrating Amazon Cognito with API Gateway</b> 
<p>&nbsp;</p> 
<p>To demonstrate the different ways that Amazon Cognito User Pools and Amazon Cognito Federated Identities can be used to authorize access to your API Gateway API, use a simple AngularV4 single page web application:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image007.png"><img class="aligncenter wp-image-2277 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image007.png" alt="" width="940" height="449" /></a></p> 
<p>&nbsp;</p> 
<p>Here’s the basic concept. You have a sample application that authenticates its users with three different IdPs. You also have an API Gateway API with three different resources (paths), one for each type of user. Each provider can only access one API-specific resource/path and cannot access any other resource allocated to other providers.</p> 
<p>After authentication, the user retrieves specific user attributes such as first name, last name, and email details from their provider and sends a request to the API resource (POST) with the data. After access is granted, an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function is invoked to add the details of the specific user to an <a href="https://aws.amazon.com/dynamodb">Amazon DynamoDB</a> table.</p> 
<p>You are using a single identity pool and a single API Gateway API to demonstrate that you can secure API access using multiple providers and multiple AuthN/AuthZ options in different ways, but sharing the same resources.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image009.png"><img class="aligncenter wp-image-2278 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image009.png" alt="" width="939" height="342" /></a></p> 
<p>&nbsp;</p> 
<p>Both “/google” and “/cip” resources GET/POST methods are configured and secured with IAM authorization. The GET/POST methods in “/cup”, on the other hand, are secured with a user pool authorizer.</p> 
<p>You can find the application code and a SAM template with instructions to deploy all the backend services in the <strong><a href="https://github.com/awslabs/aws-cognito-apigw-angular-auth">aws-cognito-apigw-angular-auth</a></strong> GitHub repository.</p> 
<p>There’s yet another way to authenticate API calls with Amazon Cognito: using a Lambda custom authorizer. I’m not covering it in this post; however, the <a href="https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/">Integrating Amazon Cognito User Pools with API Gateway</a> post showcases this specific use case.</p> 
<p>For the first provider, use a public IdP, such as Google. After authenticating with Google credentials, the application collects the user details and then issue a POST call to your API.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image011.png"><img class="aligncenter wp-image-2279 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image011.png" alt="" width="939" height="489" /></a></p> 
<p>&nbsp;</p> 
<p>The DynamoDB table is updated with the latest user details and you can retrieve the data about the user making the API call. Notice the ID in blue in the user information card.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image013.png"><img class="aligncenter wp-image-2280 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image013.png" alt="" width="941" height="405" /></a></p> 
<p>&nbsp;</p> 
<p>After Amazon Cognito validates a user, it creates a unique identity ID for that user and links it with the specific IdP.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image015.png"><img class="aligncenter size-full wp-image-2281" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image015.png" alt="" width="572" height="347" /></a></p> 
<p>&nbsp;</p> 
<p>API Gateway is able to identify the identity ID based on the IAM credentials sent to the API method using the variable <em>$context.identity.cognitoIdentityId</em> in the integration request. A body mapping template adds the ID data automatically to the payload sent to the Lambda function, as follows:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">#set($inputRoot = $input.path('$'))
{
&quot;operation&quot;: &quot;create&quot;,
&quot;payload&quot;: {
&quot;Item&quot; : {
&quot;userId&quot; : &quot;$context.identity.cognitoIdentityId&quot;,
&quot;name&quot; : &quot;$inputRoot.name&quot;,
&quot;surname&quot; : &quot;$inputRoot.surname&quot;,
&quot;email&quot; : &quot;$inputRoot.email&quot;,
&quot;provider&quot;: &quot;Google&quot;
}
}
}
</code></pre> 
<p>&nbsp;</p> 
<p>Lambda processes the data and sends it in a proper format to DynamoDB, which uses the identity ID as a hash key. Due to the random and unique nature of the ID, it’s perfect to be used for querying the table.</p> 
<p>To guarantee that the Google users can only access their specific allocated API resources, attach the following authenticated role to the identity pool. The users assume this IAM role when validated and it explicitly allows only GET (read) and POST (write) access to the API resource “/google”:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;execute-api:Invoke&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/GET/google&quot;,
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/POST/google&quot;
]
}
]
}
</code></pre> 
<p>&nbsp;</p> 
<p>It’s time to sign in with the second user. This user is authenticated using Amazon Cognito User Pools as IdP. A JWT is used to retrieve temporary AWS credentials from the associated identity pool, same as is used by the Google provider.</p> 
<p>Using the Test Access card, you can issue API calls to each one of the three different resources to test the credentials. For instance, if the user tries to make an API call to “/cip” using the IAM credentials provided by Amazon Cognito, the access is granted (Status 200). However, if you try to access the “/google” resource, you get access denied (Status 403). You can confirm using the browser developer tools.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image017.png"><img class="aligncenter size-full wp-image-2282" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image017.png" alt="" width="941" height="552" /></a></p> 
<p>&nbsp;</p> 
<p>Even if you have the same identity pool in use by different providers (Google and Amazon Cognito User Pools), you can associate another IAM role to a group with Amazon Cognito User Pools. Because the user “jdoe” is part of that group, it inherits the IAM role association.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image019.png"><img class="aligncenter size-full wp-image-2283" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image019.png" alt="" width="454" height="331" /></a></p> 
<p>&nbsp;</p> 
<p>The IAM role explicitly allows only GET (read) and POST (write) access to the API resource “/cip”, nothing else.</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;execute-api:Invoke&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/GET/cip&quot;,
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/POST/cip&quot;
]
}
]
}
</code></pre> 
<p>&nbsp;</p> 
<p>The role details are passed in the JWT itself. For that reason, the identity pool allows the role to be assumed. The default authenticated role for the identity pool needs to be DENIED or else your user would also have access to the “/google” resource.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image021.png"><img class="aligncenter size-full wp-image-2284" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image021.png" alt="" width="796" height="512" /></a></p> 
<p>&nbsp;</p> 
<p>For the third and final user, skip Amazon Cognito Federated Identities altogether and authenticate the user from the Amazon Cognito User Pool directly to API Gateway using a Cognito user pool authorizer. You can find more information on my previous blog post, <a href="https://aws.amazon.com/blogs/compute/authorizing-access-through-a-proxy-resource-to-amazon-api-gateway-and-aws-lambda-using-amazon-cognito-user-pools/">Authorizing Access Through a Proxy Resource to Amazon API Gateway and AWS Lambda Using Amazon Cognito User Pools</a>.</p> 
<p>The user can only access the API resource assigned to the user pool and it’s denied for the other API paths:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image023-1.png"><img class="aligncenter size-full wp-image-2285" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image023-1.png" alt="" width="940" height="624" /></a></p> 
<p>&nbsp;</p> 
<p>The ID Token (JWT) is sent directly to API Gateway and there’s no IAM role involved in the user validation. The ID in the user information card looks different compared to the previous users. As no cognito identity ID is generated in this scenario, you need another type of unique ID for the user.</p> 
<p>Test the JWT generated in this particular user session in the API Gateway console, and retrieve user attributes and claims:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image025.png"><img class="aligncenter size-full wp-image-2286" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image025.png" alt="" width="926" height="423" /></a></p> 
<p>&nbsp;</p> 
<p>The “sub” attribute is a unique identifier generated by Amazon Cognito User Pools and can be used to track each user from the user pool. You use this attribute as a hash key for the user pool users in the DynamoDB table. Use the variable <em>$context.authorizer.claims.sub</em> in the integration request, and the following body mapping template adds the ID data automatically to the payload sent to the Lambda function:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">#set($inputRoot = $input.path('$'))
{
&quot;operation&quot;: &quot;create&quot;,
&quot;payload&quot;: {
&quot;Item&quot; : {
&quot;userId&quot; : &quot;$context.authorizer.claims.sub&quot;,
&quot;name&quot; : &quot;$context.authorizer.claims.given_name&quot;,
&quot;surname&quot; : &quot;$context.authorizer.claims.family_name&quot;,
&quot;email&quot; : &quot;$context.authorizer.claims.email&quot;,
&quot;provider&quot; : &quot;Cognito User Pools&quot;
}
}
}
</code></pre> 
<p>&nbsp;</p> 
<p>Check the DynamoDB console and confirm that all users were successfully added to the table with the API calls. As each user can only add or retrieve data from their specific API resource and only using their own UserID, the data is unavailable to other users.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image027.png"><img class="aligncenter size-full wp-image-2287" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image027.png" alt="" width="684" height="346" /></a></p> 
<p>&nbsp;</p> 
<b>Summary</b> 
<p>&nbsp;</p> 
<p>Amazon Cognito provide a rich set of features to authenticate and authorize users. You can take advantage of the built-in flexibility and different options to secure access to your API Gateway API integrating with multiple IdPs.</p> 
<p>There are distinct and specific use cases to authenticate, secure, and provide access to your API leveraging the integration between Amazon Cognito and API Gateway, depending on the work load and implementation of your application. I covered a couple of different scenarios in this post:</p> 
<ul> 
<li><strong>Your users are defined and authenticated by an external or public IdP.</strong></li> 
</ul> 
<p>You used Google for your sample application but it could be Facebook, Twitter, Amazon, OpenID Connect, your own developer IdP, or even an enterprise-level SAML provider, such as Active Directory. Use any IdP that can seamlessly integrate with Amazon Cognito Federated Identities linked with AWS Identity and Access Management roles.</p> 
<ul> 
<li><strong>Your users are defined in your own IdP powered by Amazon Cognito User Pools, leveraging aditional secure access with IAM permissions.</strong></li> 
</ul> 
<p>You want to have additional access granularity and security by integrating the authentication with IAM role-based access controls and Amazon Cognito User Pools groups.</p> 
<ul> 
<li><strong>Your users are defined in your own IdP powered by Amazon Cognito User Pools, leveraging secure JWTs for authentication.</strong></li> 
</ul> 
<p>You want to authenticate your API calls directly and seamlessly using these tokens and you are not interested in using IAM integration to authorize access with IAM roles.</p> 
<p>With Amazon Cognito User Pools, there’s no need to worry about the undifferentiated heavy lifting of maintaining your own IdP servers, allowing you to focus on application logic instead. It also gives you more control around your users, groups, applications, and attributes with no dependency on an external public provider.</p> 
<p>It’s a great balance between control and ease of use as it integrates seamlessly with identity pools for specific scenarios that require the additional use of IAM roles for granular security but can also work directly with JWTs. In addition to all these advantages, you can also integrate Lambda triggers for different sign-in, sign-up, confirmation, validation, and verification workflows to customize and adapt the user identification process even further.</p> 
<p>The integration between Amazon Cognito and API Gateway allows great flexibility such that you can implement these different authentication scenarios separately or even use all of them in conjunction (as demonstrated with the sample web application). You can provide access to the different API users and resources based on a specific IdP or multiple providers of your choice.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/apigateway/" rel="tag">apigateway</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cognito/" rel="tag">cognito</a>, <a href="https://aws.amazon.com/blogs/compute/tag/dynamodb/" rel="tag">DynamoDB</a>, <a href="https://aws.amazon.com/blogs/compute/tag/lambda/" rel="tag">lambda</a></span> 
</footer> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Using Amazon SQS Dead-Letter Queues to Control Message Failure</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2017-06-07T13:18:10+00:00">07 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2216" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2216&amp;disqus_title=Using+Amazon+SQS+Dead-Letter+Queues+to+Control+Message+Failure&amp;disqus_url=https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2216');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<h5><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/headshot.jpg"><img class="alignnone wp-image-2218 size-thumbnail" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/headshot-150x150.jpg" alt="" width="150" height="150" /></a><strong><br /> Michael G. Khmelnitsky, Senior Programmer Writer</strong></h5> 
<p>&nbsp;</p> 
<p>Sometimes, messages can’t be processed because of a variety of possible issues, such as erroneous conditions within the producer or consumer application. For example, if a user places an order within a certain number of minutes of creating an account, the producer might pass a message with an empty string instead of a customer identifier. Occasionally, producers and consumers might fail to interpret aspects of the protocol that they use to communicate, causing message corruption or loss. Also, the consumer’s hardware errors might corrupt message payload. For these reasons, messages that can’t be processed in a timely manner are delivered to a <em>dead-letter message queue</em>.<span id="more-2216"></span></p> 
<p>The recent post <a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a> gives an overview of messaging in the microservice architecture of modern applications. This post explains how and when you should use dead-letter queues to gain better control over message handling in your applications. It also offers some resources for configuring a dead-letter <a href="https://aws.amazon.com/message-queue/">message queue in Amazon Simple Queue Service (SQS)</a>.</p> 
<h3>What are the benefits of dead-letter queues?</h3> 
<p>The main task of a dead-letter queue is handling message failure. A dead-letter queue lets you set aside and isolate messages that can’t be processed correctly to determine why their processing didn’t succeed. Setting up a dead-letter queue allows you to do the following:</p> 
<ul> 
<li>Configure an alarm for any messages delivered to a dead-letter queue.</li> 
<li>Examine logs for exceptions that might have caused messages to be delivered to a dead-letter queue.</li> 
<li>Analyze the contents of messages delivered to a dead-letter queue to diagnose software or the producer’s or consumer’s hardware issues.</li> 
<li>Determine whether you have given your consumer sufficient time to process messages.</li> 
</ul> 
<h3>How do high-throughput, unordered queues handle message failure?</h3> 
<p>High-throughput, unordered queues (sometimes called <em>standard</em> or <em>storage queues</em>) keep processing messages until the expiration of the retention period. This helps ensure continuous processing of messages, which minimizes the chances of your queue being blocked by messages that can’t be processed. It also ensures fast recovery for your queue.</p> 
<p>In a system that processes thousands of messages, having a large number of messages that the consumer repeatedly fails to acknowledge and delete might increase costs and place extra load on the hardware. Instead of trying to process failing messages until they expire, it is better to move them to a dead-letter queue after a few processing attempts.</p> 
<p><strong>Note:</strong> This queue type often allows a high number of in-flight messages. If the majority of your messages can’t be consumed and aren’t sent to a dead-letter queue, your rate of processing valid messages can slow down. Thus, to maintain the efficiency of your queue, you must ensure that your application handles message processing correctly.</p> 
<h3>How do FIFO queues handle message failure?</h3> 
<p>FIFO (first-in-first-out) queues (sometimes called <em>service bus queues</em>) help ensure exactly-once processing by consuming messages in sequence from a <em>message group</em>. Thus, although the consumer can continue to retrieve ordered messages from another message group, the first message group remains unavailable until the message blocking the queue is processed successfully.</p> 
<p><strong>Note:</strong> This queue type often allows a lower number of in-flight messages. Thus, to help ensure that your FIFO queue doesn’t get blocked by a message, you must ensure that your application handles message processing correctly.</p> 
<h3>When should I use a dead-letter queue?</h3> 
<ul> 
<li><strong>Do</strong> use dead-letter queues with high-throughput, unordered queues. You should always take advantage of dead-letter queues when your applications don’t depend on ordering. Dead-letter queues can help you troubleshoot incorrect message transmission operations. <strong>Note:</strong> Even when you use dead-letter queues, you should continue to monitor your queues and retry sending messages that fail for transient reasons.</li> 
<li><strong>Do</strong> use dead-letter queues to decrease the number of messages and to reduce the possibility of exposing your system to <em>poison-pill messages</em> (messages that can be received but can’t be processed).</li> 
<li><strong>Don’t</strong> use a dead-letter queue with high-throughput, unordered queues when you want to be able to keep retrying the transmission of a message indefinitely. For example, don’t use a dead-letter queue if your program must wait for a dependent process to become active or available.</li> 
<li><strong>Don’t</strong> use a dead-letter queue with a FIFO queue if you don’t want to break the exact order of messages or operations. For example, don’t use a dead-letter queue with instructions in an Edit Decision List (EDL) for a video editing suite, where changing the order of edits changes the context of subsequent edits.</li> 
</ul> 
<h3>How do I get started with dead-letter queues in Amazon SQS?</h3> 
<p>Amazon SQS is a fully managed service that offers reliable, highly scalable hosted queues for exchanging messages between applications or microservices. Amazon SQS moves data between distributed application components and helps you decouple these components. It supports both <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html">standard queues</a> and <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html">FIFO queues</a>. To configure a queue as a dead-letter queue, you can use the <a href="http://console.aws.amazon.com/sqs/home">AWS Management Console</a> or the Amazon SQS <code class="lang-setqueueattributes">SetQueueAttributes</code> API action.</p> 
<p>To get started with dead-letter queues in Amazon SQS, see the following topics in the <em>Amazon SQS Developer Guide</em>:</p> 
<ul> 
<li><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/Welcome.html">What is Amazon SQS?</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">Using Amazon SQS Dead-Letter Queues</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/MonitorSQSwithCloudWatch.html">Monitoring Amazon SQS Using CloudWatch</a></li> 
</ul> 
<p>To start working with dead-letter queues programmatically, see the following resources:</p> 
<ul> 
<li>Java: <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue.html#configure-dead-letter-queue-java">Configure a Dead-Letter Queue with the Amazon SQS API</a></li> 
<li>Java: <a href="https://aws.amazon.com/blogs/developer/using-amazon-sqs-dead-letter-queues-2/">Using Amazon SQS Dead-Letter Queues</a></li> 
<li>C#: <a href="https://aws.amazon.com/blogs/developer/using-amazon-sqs-dead-letter-queues/">Using Amazon SQS Dead-Letter Queues</a></li> 
<li>Lambda: <a href="https://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/">Robust Serverless Application Design with AWS Lambda Dead-Letter Queues</a></li> 
</ul> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-queue-service/" rel="tag">Amazon Simple Queue Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sqs/" rel="tag">Amazon SQS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloud-messaging/" rel="tag">cloud messaging</a>, <a href="https://aws.amazon.com/blogs/compute/tag/decoupling/" rel="tag">decoupling</a>, <a href="https://aws.amazon.com/blogs/compute/tag/lambda/" rel="tag">lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-queues/" rel="tag">message queues</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a>, <a href="https://aws.amazon.com/blogs/compute/tag/sqs/" rel="tag">sqs</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2216');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Powering your Amazon ECS Cluster with Amazon EC2 Spot Instances</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> and 
<span property="author" typeof="Person"><span property="name">Sebastian Dreisch</span></span> | on 
<time property="datePublished" datetime="2017-06-06T16:21:58+00:00">06 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ec2-spot/" title="View all posts in Amazon EC2 Spot"><span property="articleSection">Amazon EC2 Spot</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2071" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2071&amp;disqus_title=Powering+your+Amazon+ECS+Cluster+with+Amazon+EC2+Spot+Instances&amp;disqus_url=https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2071');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p>This post was graciously contributed by:</p> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 15px 0px 0px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ChadS.jpeg"><img class="size-full wp-image-2197" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ChadS.jpeg" alt="Chad Schmutzer, Solutions Architect" width="120" height="160" /></a></td> 
<td style="padding: 0px 0px 0px 15px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ShawnOConner.jpeg"><img class="wp-image-2198 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ShawnOConner.jpeg" alt="Shawn O'Conner, Enterprise Solutions Architect" width="119" height="160" /></a></td> 
</tr> 
<tr> 
<td style="padding: 0px 15px 0px 0px"><b>Chad Schmutzer</b><br /> Solutions Architect</td> 
<td style="padding: 0px 0px 0px 15px"><b>Shawn O’Connor</b><br /> Solutions Architect</td> 
</tr> 
</tbody> 
</table> 
<p>Today <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-ecs-adds-console-support-for-spot-fleet-creation/">we are excited to announce</a> that Amazon EC2 Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>) now supports the ability to launch your ECS cluster on <a href="https://aws.amazon.com/ec2/spot">Amazon EC2 Spot Instances</a> directly from the ECS console.</p> 
<p>Spot Instances allow you to bid on spare Amazon EC2 compute capacity. Spot Instances typically cost 50-90% less than On-Demand Instances. Powering your ECS cluster with Spot Instances lets you reduce the cost of running your existing containerized workloads, or increase your compute capacity by two to ten times while keeping the same budget. Or you could do a combination of both!</p> 
<p><span id="more-2071"></span></p> 
<p>Using Spot Instances, you specify the price you are willing to pay per instance-hour. Your Spot Instance runs whenever your bid exceeds the current Spot&nbsp;price. If your instance is reclaimed due to an increase in the Spot price, you are not charged for the partial hour that your instance has run.</p> 
<p>The ECS console uses <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html">Spot Fleet</a> to deploy your Spot Instances. Spot Fleet attempts to deploy the target capacity you request (expressed in terms of instances or a vCPU count) for your containerized application by launching Spot Instances that result in the best prices for you. If your Spot Instances are reclaimed due to a change in Spot prices or available capacity, Spot Fleet also attempts to maintain its target capacity.</p> 
<p>Containers are a natural fit for the diverse pool of resources that Spot Fleet thrives on. Spot Fleet enable you to provision capacity across multiple Spot Instance pools (combinations of instance types and Availability Zones), which helps improve your application’s availability and reduce operating costs of the fleet over time. Combining the extensible and flexible container placement system provided by ECS with Spot Fleet allows you to efficiently deploy containerized workloads and easily manage clusters at any scale for a fraction of the cost.</p> 
<p>Previously, deploying your ECS cluster on Spot Instances was a manual process. In this post, we show you how to achieve high availability, scalability, and cost savings for your container workloads by using the new Spot Fleet integration in the ECS console. We also show you how to build your own ECS cluster on Spot Instances using AWS CloudFormation.</p> 
<b>Creating an ECS cluster running on Spot Instances</b> 
<p>You can create an ECS cluster using the AWS Management Console.</p> 
<ol> 
<li>Open the Amazon ECS console at <a href="https://console.aws.amazon.com/ecs/">https://console.aws.amazon.com/ecs/</a>.</li> 
<li>In the navigation pane, choose&nbsp;<strong>Clusters</strong>.</li> 
<li>On the&nbsp;<strong>Clusters</strong>&nbsp;page, choose&nbsp;<strong>Create Cluster</strong>.</li> 
<li>For&nbsp;<strong>Cluster name</strong>, enter a name.</li> 
<li>In <strong>Instance configuration</strong>, for <strong>Provisioning model</strong>, choose <strong>Spot</strong>.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-1.png"><img class="aligncenter wp-image-2186" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-1.png" alt="ECS Create Cluster - Spot Fleet" width="700" height="345" /></a></p> 
<b>Choosing an allocation strategy</b> 
<p>The two available Spot Fleet allocation strategies are&nbsp;Diversified and Lowest price.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-2.png"><img class="aligncenter size-full wp-image-2187" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-2.png" alt="ECS Spot Allocation Strategies" width="572" height="122" /></a></p> 
<p>The allocation strategy you choose for your Spot Fleet determines how it fulfills your Spot Fleet request from the possible Spot Instance pools. When you use the diversified strategy, the Spot Instances are distributed across all pools. When you use the lowest price strategy, the Spot Instances come from the pool with the lowest price specified in your request.</p> 
<p>Remember that each instance type (the instance size within each instance family, e.g., c4.4xlarge), in each Availability Zone, in every region, is a separate pool of capacity, and therefore a separate Spot market. By diversifying across as many different instance types and Availability Zones as possible, you can improve the availability of your fleet. You also make your fleet less sensitive to increases in the Spot price in any one pool over time.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-3.png"><img class="aligncenter size-full wp-image-2188" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-3.png" alt="Spot Fleet Market" width="366" height="448" /></a></p> 
<p>You can select up to six EC2 instance types to use for your Spot Fleet. In this example, we’ve selected m3, m4, c3, c4, r3, and r4 instance types of size xlarge.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-4.png"><img class="aligncenter wp-image-2189" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-4.png" alt="Spot Instance Selection" width="400" height="302" /></a></p> 
<p>You need to enter a bid for your instances. Typically, bidding at or near the On-Demand Instance price is a good starting point. Your bid is the maximum price that you are willing to pay for instance types in that Spot pool. While the Spot&nbsp;price is at or below your bid, you pay the Spot&nbsp;price. Bidding lower ensures that you have lower costs, while bidding higher reduces the probability of interruption.</p> 
<p>Configure the number of instances to have in your cluster. Spot Fleet attempts to launch the number of Spot Instances that are required to meet the target capacity specified in your request. The Spot Fleet also attempts to maintain its target capacity if your Spot Instances are reclaimed due to a change in Spot prices or available capacity.</p> 
<p>The latest <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html">ECS–optimized AMI</a> is used for the instances when they are launched.</p> 
<p>Configure your storage and network settings. To enable diversification and high availability, be sure to select subnets in multiple Availability Zones. You can’t select multiple subnets in the same Availability Zone in a single Spot Fleet.</p> 
<p>The ECS container agent makes calls to the ECS API actions on your behalf. Container instances that run the agent require the ecsInstanceRole IAM policy and role for the service to know that the agent belongs to you. If you don’t have the ecsInstanceRole already, you can create one using the ECS console.</p> 
<p>If you create a managed compute environment that uses Spot Fleet, you must create a role that grants the Spot Fleet permission to bid on, launch, and terminate instances on your behalf. You can also create this role using the ECS console.</p> 
<p>That’s it! In the ECS console, choose <strong>Create</strong> to spin up your new ECS cluster running on Spot Instances.</p> 
<b>Using AWS CloudFormation to deploy your ECS cluster on Spot Instances</b> 
<p>We have also published a reference architecture AWS CloudFormation template that demonstrates how to easily launch a CloudFormation stack and deploy your ECS cluster on Spot Instances.</p> 
<p>The CloudFormation template includes the Spot Instance termination notice script mentioned earlier, as well as some additional logging and other example features to get you started quickly. You can find the CloudFormation template in the <a href="https://github.com/awslabs/ec2-spot-labs/tree/master/ecs-ec2-spot-fleet">Amazon EC2 Spot Instances GitHub repo</a>.</p> 
<p>Give it a try and customize it as needed for your environment!</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/0606-Spot-5.jpg"><img class="size-large wp-image-2194 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/0606-Spot-5-1024x709.jpg" alt="Spot Fleet Architecture" width="640" height="443" /></a></p> 
<b>Handling termination</b> 
<p>With Spot Instances, you never pay more than the price you specified. If the Spot price exceeds your bid price for a given instance, it is terminated automatically for you.</p> 
<p>The best way to protect against Spot Instance interruption is to architect your containerized application to be fault-tolerant. In addition, you can take advantage of a feature called Spot Instance termination notices, which provides a two-minute warning before EC2 must terminate your Spot Instance.</p> 
<p>This warning is made available to the applications on your Spot Instance using an item in the instance metadata. When you deploy your ECS cluster on Spot Instances using the console, AWS installs a script that checks every 5 seconds for the Spot Instance termination notice. If the notice is detected, the script immediately updates the container instance state to <strong>DRAINING</strong>.</p> 
<p>A simplified version of the Spot Instance termination notice script is as follows:</p> 
<pre><code class="lang-bash">#!/bin/bash
while sleep 5; do
if [ -z $(curl -Isf http://169.254.169.254/latest/meta-data/spot/termination-time) ]; then
/bin/false
else
ECS_CLUSTER=$(curl -s http://localhost:51678/v1/metadata | jq .Cluster | tr -d \&quot;)
CONTAINER_INSTANCE=$(curl -s http://localhost:51678/v1/metadata \
| jq .ContainerInstanceArn | tr -d \&quot;)
aws ecs update-container-instances-state --cluster $ECS_CLUSTER \
--container-instances $CONTAINER_INSTANCE --status DRAINING
fi
done</code></pre> 
<p>When you set a container instance to <strong>DRAINING</strong>, ECS prevents new tasks from being scheduled for placement on the container instance. If the resources are available, replacement service tasks are started on other container instances in the cluster. Container instance draining enables you to remove a container instance from a cluster without impacting tasks in your cluster. Service tasks on the container instance that are in the <strong>PENDING</strong> state are stopped immediately.</p> 
<p>Service tasks on the container instance that are in the <strong>RUNNING</strong> state are stopped and replaced according to the service’s deployment configuration parameters, minimumHealthyPercent and maximumPercent.</p> 
<b>ECS on Spot Instances in action<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/mapboxv1.jpg"><img class="alignright wp-image-2190 size-medium" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/mapboxv1-300x150.jpg" alt="" width="300" height="150" /></a></b> 
<p>Want to see how customers are already powering their ECS clusters on Spot Instances? Our friends at <a href="https://www.mapbox.com/">Mapbox</a> are doing just that.</p> 
<p>Mapbox&nbsp;is a platform for designing and publishing custom maps. The company uses ECS to power their entire batch processing architecture to collect and process over 100 million miles of sensor data per day that they use for powering their maps. They also optimize their batch processing architecture on ECS using Spot Instances.</p> 
<p>The Mapbox platform powers over 5,000 apps and reaches more than 200 million users each month. Its backend runs on ECS, allowing it to serve more than 1.3 billion requests per day. To learn more about their recent migration to ECS, read their recent blog post,&nbsp;<a href="https://www.mapbox.com/blog/switch-to-ecs/">We Switched to Amazon ECS, and You Won’t Believe What Happened Next</a>. Then, in their follow-up blog post, <a href="https://www.mapbox.com/blog/caches-to-cash/">Caches to Cash</a>, learn how they are running their entire platform on Spot Instances, allowing them to save upwards of 50–90% on their EC2 costs.</p> 
<b>Conclusion</b> 
<p>We hope that you are as excited as we are about running your containerized applications at scale and cost effectively using Spot Instances. For more information, see the following pages:</p> 
<ul> 
<li><a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_GetStarted.html">Getting Started with Amazon ECS</a></li> 
<li><a href="https://aws.amazon.com/ec2/spot/getting-started/">Getting Started with Amazon EC2 Spot Instances</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.html">Getting Started with AWS CloudFormation</a></li> 
</ul> 
<p>If you have comments or suggestions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ec2/" rel="tag">EC2</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/spot/" rel="tag">spot</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2071');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Workflow Layer (Part 4 of 4)</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-06-01T17:43:43+00:00">01 JUN 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2"><span property="articleSection">Amazon EC2</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-batch/" title="View all posts in AWS Batch"><span property="articleSection">AWS Batch</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions"><span property="articleSection">AWS Step Functions</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2161" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2161&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Workflow+Layer+%28Part+4+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2161');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the fourth in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<ul> 
<li>Job</li> 
<li>Batch</li> 
<li>Workflow</li> 
</ul> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2</a>, you built a Docker container for each job that needed to run as part of your workflow, and stored them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>.</p> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3</a>, you tackled the batch layer and built a scalable, elastic, and easily maintainable batch engine using <a href="https://aws.amazon.com/batch">AWS Batch</a>. This solution took care of dynamically scaling your compute resources in response to the number of runnable jobs in your job queue length as well as managed job placement.<span id="more-2161"></span></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In part 4, you build out the workflow layer of your solution using <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> and <a href="https://aws.amazon.com/lambda">AWS Lambda</a>. You then run an end-to-end genomic analysis―specifically known as exome secondary analysis―for many times at a cost of less than $1 per exome.</p> 
<p>Step Functions makes it easy to coordinate the components of your applications using visual workflows. Building applications from individual components that each perform a single function lets you scale and change your workflow quickly. You can use the graphical console to arrange and visualize the components of your application as a series of steps, which simplify building and running multi-step applications. You can change and add steps without writing code, so you can easily evolve your application and innovate faster.</p> 
<p>An added benefit of using Step Functions to define your workflows is that the state machines you create are immutable. While you can delete a state machine, you cannot alter it after it is created. For regulated workloads where auditing is important, you can be assured that state machines you used in production cannot be altered.</p> 
<p>In this blog post, you will create a Lambda state machine to orchestrate your batch workflow. For more information on how to create a basic state machine, please see this <a href="https://docs.aws.amazon.com/step-functions/latest/dg/hello-lambda.html">Step Functions tutorial</a>.</p> 
<p>All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<b id="toc_0">Build a state machine building block</b> 
<p>To skip the following steps, we have provided an <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/workflow/deploy_state_machine.yaml">AWS CloudFormation template</a> that can deploy your Step Functions state machine. You can use this in combination with the setup you did in part 3 to quickly set up the environment in which to run your analysis.</p> 
<p>The state machine is composed of smaller state machines that submit a job to AWS Batch, and then poll and check its execution.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/Polling-State-Machine.png"><img class="aligncenter size-medium wp-image-2174" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/Polling-State-Machine-300x265.png" alt="" width="300" height="265" /></a></p> 
<p>The steps in this building block state machine are as follows:</p> 
<ol> 
<li><strong>A job is submitted.</strong><br /> Each analytical module/job has its own Lambda function for submission and calls the <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/runner/batch_submit_job.py">batchSubmitJob Lambda function</a> that you built in the previous blog post. You will build these specialized Lambda functions in the following section.</li> 
<li><strong>The state machine queries the AWS Batch API for the job status.</strong><br /> This is also a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/runner/batch_get_job_status.py">Lambda function</a>.</li> 
<li><strong>The job status is checked to see if the job has completed.</strong><br /> If the job status equals SUCCESS, proceed to log the final job status. If the job status equals FAILED, end the execution of the state machine. In all other cases, wait 30 seconds and go back to Step 2.</li> 
</ol> 
<p>Here is the JSON representing this state machine.</p> 
<pre><code class="language-none">{
&quot;Comment&quot;: &quot;A simple example that submits a Job to AWS Batch&quot;,
&quot;StartAt&quot;: &quot;SubmitJob&quot;,
&quot;States&quot;: {
&quot;SubmitJob&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:&lt;account-id&gt;::function:batchSubmitJob&quot;,
&quot;Next&quot;: &quot;GetJobStatus&quot;
},
&quot;GetJobStatus&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:&lt;account-id&gt;:function:batchGetJobStatus&quot;,
&quot;Next&quot;: &quot;CheckJobStatus&quot;,
&quot;InputPath&quot;: &quot;$&quot;,
&quot;ResultPath&quot;: &quot;$.status&quot;
},
&quot;CheckJobStatus&quot;: {
&quot;Type&quot;: &quot;Choice&quot;,
&quot;Choices&quot;: [
{
&quot;Variable&quot;: &quot;$.status&quot;,
&quot;StringEquals&quot;: &quot;FAILED&quot;,
&quot;End&quot;: true
},
{
&quot;Variable&quot;: &quot;$.status&quot;,
&quot;StringEquals&quot;: &quot;SUCCEEDED&quot;,
&quot;Next&quot;: &quot;GetFinalJobStatus&quot;
}
],
&quot;Default&quot;: &quot;Wait30Seconds&quot;
},
&quot;Wait30Seconds&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 30,
&quot;Next&quot;: &quot;GetJobStatus&quot;
},
&quot;GetFinalJobStatus&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:&lt;account-id&gt;:function:batchGetJobStatus&quot;,
&quot;End&quot;: true
}
}
}</code></pre> 
<b id="toc_1">Building the Lambda functions for the state machine</b> 
<p>You need two basic Lambda functions for this state machine. The first one submits a job to AWS Batch and the second checks the status of the AWS Batch job that was submitted.</p> 
<p>In AWS Step Functions, you specify an input as JSON that is read into your state machine. Each state receives the aggregate of the steps immediately preceding it, and you can specify which components a state passes on to its children. Because you are using Lambda functions to execute tasks, one of the easiest routes to take is to modify the input JSON, represented as a Python dictionary, within the Lambda function and return the entire dictionary back for the next state to consume.</p> 
<b id="toc_2">Building the batchSubmitIsaacJob Lambda function</b> 
<p>For Step 1 above, you need a Lambda function for each of the steps in your analysis workflow. As you created a generic Lambda function in the previous post to submit a batch job (batchSubmitJob), you can use that function as the basis for the specialized functions you’ll include in this state machine. Here is such a Lambda function for the Isaac aligner.</p> 
<pre><code class="language-none">from __future__ import print_function
import boto3
import json
import traceback
lambda_client = boto3.client('lambda')
def lambda_handler(event, context):
try:
# Generate output put
bam_s3_path = '/'.join([event['resultsS3Path'], event['sampleId'], 'bam/'])
depends_on = event['dependsOn'] if 'dependsOn' in event else []
# Generate run command
command = [
'--bam_s3_folder_path', bam_s3_path,
'--fastq1_s3_path', event['fastq1S3Path'],
'--fastq2_s3_path', event['fastq2S3Path'],
'--reference_s3_path', event['isaac']['referenceS3Path'],
'--working_dir', event['workingDir']
]
if 'cmdArgs' in event['isaac']:
command.extend(['--cmd_args', event['isaac']['cmdArgs']])
if 'memory' in event['isaac']:
command.extend(['--memory', event['isaac']['memory']])
# Submit Payload
response = lambda_client.invoke(
FunctionName='batchSubmitJob',
InvocationType='RequestResponse',
LogType='Tail',
Payload=json.dumps(dict(
dependsOn=depends_on,
containerOverrides={
'command': command,
},
jobDefinition=event['isaac']['jobDefinition'],
jobName='-'.join(['isaac', event['sampleId']]),
jobQueue=event['isaac']['jobQueue']
)))
response_payload = response['Payload'].read()
# Update event
event['bamS3Path'] = bam_s3_path
event['jobId'] = json.loads(response_payload)['jobId']
return event
except Exception as e:
traceback.print_exc()
raise e</code></pre> 
<p>In the Lambda console, create a Python 2.7 Lambda function named batchSubmitIsaacJob and paste in the above code. Use the LambdaBatchExecutionRole that you created in the previous post. For more information, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html">Step 2.1: Create a Hello World Lambda Function</a>.</p> 
<p>This Lambda function reads in the inputs passed to the state machine it is part of, formats the data for the batchSubmitJob Lambda function, invokes that Lambda function, and then modifies the event dictionary to pass onto the subsequent states. You can repeat these for each of the other tools, which can be found in the tools//lambda/lambda_function.py script in the GitHub repo.</p> 
<b id="toc_3">Building the batchGetJobStatus Lambda function</b> 
<p>For Step 2 above, the process queries the AWS Batch DescribeJobs API action with jobId to identify the state that the job is in. You can put this into a Lambda function to integrate it with Step Functions.</p> 
<p>In the Lambda console, create a new Python 2.7 function with the LambdaBatchExecutionRole IAM role. Name your function batchGetJobStatus and paste in the following code. This is similar to the <code>batch-get-job-python27</code> Lambda blueprint.</p> 
<pre><code class="language-none">from __future__ import print_function
import boto3
import json
print('Loading function')
batch_client = boto3.client('batch')
def lambda_handler(event, context):
# Log the received event
print(&quot;Received event: &quot; + json.dumps(event, indent=2))
# Get jobId from the event
job_id = event['jobId']
try:
response = batch_client.describe_jobs(
jobs=[job_id]
)
job_status = response['jobs'][0]['status']
return job_status
except Exception as e:
print(e)
message = 'Error getting Batch Job status'
print(message)
raise Exception(message)</code></pre> 
<b id="toc_4">Structuring state machine input</b> 
<p>You have structured the state machine input so that general file references are included at the top-level of the JSON object, and any job-specific items are contained within a nested JSON object. At a high level, this is what the input structure looks like:</p> 
<pre><code class="language-none">{
&quot;general_field_1&quot;: &quot;value1&quot;,
&quot;general_field_2&quot;: &quot;value2&quot;,
&quot;general_field_3&quot;: &quot;value3&quot;,
&quot;job1&quot;: {},
&quot;job2&quot;: {},
&quot;job3&quot;: {}
}</code></pre> 
<b id="toc_5">Building the full state machine</b> 
<p>By chaining these state machine components together, you can quickly build flexible workflows that can process genomes in multiple ways. The development of the larger state machine that defines the entire workflow uses four of the above building blocks. You use the Lambda functions that you built in the previous section. Rename each building block submission to match the tool name.</p> 
<p>We have provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/workflow/deploy_state_machine.yaml">CloudFormation template</a> to deploy your state machine and the associated IAM roles. In the <a href="https://console.aws.amazon.com/cloudformation">CloudFormation console</a>, select Create Stack, choose your template (deploy_state_machine.yaml), and enter in the ARNs for the Lambda functions you created.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/cfn_parameters.png"><img class="aligncenter size-full wp-image-2167" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/cfn_parameters.png" alt="" width="1898" height="1232" /></a></p> 
<p>Continue through the rest of the steps and deploy your stack. Be sure to check the box next to <strong>“I acknowledge that AWS CloudFormation might create IAM resources.”</strong></p> 
<p>Once the CloudFormation stack is finished deploying, you should see the following image of your state machine.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/statemachine_workflow.png"><img class="aligncenter size-full wp-image-2168" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/statemachine_workflow.png" alt="" width="1795" height="1691" /></a></p> 
<p>In short, you first submit a job for Isaac, which is the aligner you are using for the analysis. Next, you use parallel state to split your output from “GetFinalIsaacJobStatus” and send it to both your variant calling step, Strelka, and your QC step, Samtools Stats. These then are run in parallel and you annotate the results from your Strelka step with snpEff.</p> 
<b id="toc_6">Putting it all together</b> 
<p>Now that you have built all of the components for a genomics secondary analysis workflow, test the entire process.</p> 
<p>We have provided sequences from an Illumina sequencer that cover a region of the genome known as the exome. Most of the positions in the genome that we have currently associated with disease or human traits reside in this region, which is 1–2% of the entire genome. The workflow that you have built works for both analyzing an exome, as well as an entire genome.</p> 
<p>Additionally, we have provided prebuilt reference genomes for Isaac, located at:</p> 
<pre><code class="language-none">s3://aws-batch-genomics-resources/reference/</code></pre> 
<p>If you are interested, we have provided a script that sets up all of that data. To execute that script, run the following command on a large EC2 instance:</p> 
<pre><code class="language-none">make reference REGISTRY=&lt;your-ecr-registry&gt;</code></pre> 
<p>Indexing and preparing this reference takes many hours on a large-memory EC2 instance. Be careful about the costs involved and note that the data is available through the prebuilt reference genomes.</p> 
<b id="toc_7">Starting the execution</b> 
<p>In a previous section, you established a provenance for the JSON that is fed into your state machine. For ease, we have auto-populated the input JSON for you to the state machine. You can also find this in the GitHub repo under <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/workflow/test.input.json">workflow/test.input.json</a>:</p> 
<pre><code class="language-none">{
&quot;fastq1S3Path&quot;: &quot;s3://aws-batch-genomics-resources/fastq/SRR1919605_1.fastq.gz&quot;,
&quot;fastq2S3Path&quot;: &quot;s3://aws-batch-genomics-resources/fastq/SRR1919605_2.fastq.gz&quot;,
&quot;referenceS3Path&quot;: &quot;s3://aws-batch-genomics-resources/reference/hg38.fa&quot;,
&quot;resultsS3Path&quot;: &quot;s3://&lt;bucket&gt;/genomic-workflow/results&quot;,
&quot;sampleId&quot;: &quot;NA12878_states_1&quot;,
&quot;workingDir&quot;: &quot;/scratch&quot;,
&quot;isaac&quot;: {
&quot;jobDefinition&quot;: &quot;isaac-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/highPriority-myenv&quot;,
&quot;referenceS3Path&quot;: &quot;s3://aws-batch-genomics-resources/reference/isaac/&quot;
},
&quot;samtoolsStats&quot;: {
&quot;jobDefinition&quot;: &quot;samtools_stats-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/lowPriority-myenv&quot;
},
&quot;strelka&quot;: {
&quot;jobDefinition&quot;: &quot;strelka-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/highPriority-myenv&quot;,
&quot;cmdArgs&quot;: &quot; --exome &quot;
},
&quot;snpEff&quot;: {
&quot;jobDefinition&quot;: &quot;snpeff-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/lowPriority-myenv&quot;,
&quot;cmdArgs&quot;: &quot; -t hg38 &quot;
}
}</code></pre> 
<p>You are now at the stage to run your full genomic analysis. Copy the above to a new text file, change paths and ARNs to the ones that you created previously, and save your JSON input as input.states.json.</p> 
<p>In the CLI, execute the following command. You need the ARN of the state machine that you created in the previous post:</p> 
<pre><code class="language-none">aws stepfunctions start-execution --state-machine-arn &lt;your-state-machine-arn&gt; --input file://input.states.json</code></pre> 
<p>Your analysis has now started. By using Spot Instances with AWS Batch, you can quickly scale out your workflows while concurrently optimizing for cost. While this is not guaranteed, most executions of the workflows presented here should cost under $1 for a full analysis.</p> 
<b id="toc_8">Monitoring the execution</b> 
<p>The output from the above CLI command gives you the ARN that describes the specific execution. Copy that and navigate to the Step Functions console. Select the state machine that you created previously and paste the ARN into the search bar.</p> 
<p>The screen shows information about your specific execution. On the left, you see where your execution currently is in the workflow.</p> 
<p>In the following screenshot, you can see that your workflow has successfully completed the alignment job and moved onto the subsequent steps, which are variant calling and generating quality information about your sample.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/stepfunctions_midworkflow.png"><img class="aligncenter size-full wp-image-2170" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/stepfunctions_midworkflow.png" alt="" width="1945" height="1170" /></a></p> 
<p>You can also navigate to the AWS Batch console and see that progress of all of your jobs reflected there as well.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/batch_dashboard.png"><img class="aligncenter size-full wp-image-2171" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/batch_dashboard.png" alt="" width="1945" height="687" /></a></p> 
<p>Finally, after your workflow has completed successfully, check out the S3 path to which you wrote all of your files. If you run a <code>ls –recursive</code> command on the S3 results path, specified in the input to your state machine execution, you should see something similar to the following:</p> 
<pre><code class="language-none">2017-05-02 13:46:32 6475144340 genomic-workflow/results/NA12878_run1/bam/sorted.bam
2017-05-02 13:46:34    7552576 genomic-workflow/results/NA12878_run1/bam/sorted.bam.bai
2017-05-02 13:46:32         45 genomic-workflow/results/NA12878_run1/bam/sorted.bam.md5
2017-05-02 13:53:20      68769 genomic-workflow/results/NA12878_run1/stats/bam_stats.dat
2017-05-02 14:05:12        100 genomic-workflow/results/NA12878_run1/vcf/stats/runStats.tsv
2017-05-02 14:05:12        359 genomic-workflow/results/NA12878_run1/vcf/stats/runStats.xml
2017-05-02 14:05:12  507577928 genomic-workflow/results/NA12878_run1/vcf/variants/genome.S1.vcf.gz
2017-05-02 14:05:12     723144 genomic-workflow/results/NA12878_run1/vcf/variants/genome.S1.vcf.gz.tbi
2017-05-02 14:05:12  507577928 genomic-workflow/results/NA12878_run1/vcf/variants/genome.vcf.gz
2017-05-02 14:05:12     723144 genomic-workflow/results/NA12878_run1/vcf/variants/genome.vcf.gz.tbi
2017-05-02 14:05:12   30783484 genomic-workflow/results/NA12878_run1/vcf/variants/variants.vcf.gz
2017-05-02 14:05:12    1566596 genomic-workflow/results/NA12878_run1/vcf/variants/variants.vcf.gz.tbi</code></pre> 
<b id="toc_9">Modifications to the workflow</b> 
<p>You have now built and run your genomics workflow. While diving deep into modifications to this architecture are beyond the scope of these posts, we wanted to leave you with several suggestions of how you might modify this workflow to satisfy additional business requirements.</p> 
<ul> 
<li><strong>Job tracking with Amazon DynamoDB</strong><br /> In many cases, such as if you are offering Genomics-as-a-Service, you might want to track the state of your jobs with DynamoDB to get fine-grained records of how your jobs are running. This way, you can easily identify the cost of individual jobs and workflows that you run.</li> 
<li><strong>Resuming from failure</strong><br /> Both AWS Batch and Step Functions natively support job retries and can cover many of the standard cases where a job might be interrupted. There may be cases, however, where your workflow might fail in a way that is unpredictable. In this case, you can use <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html">custom error handling with AWS Step Functions</a> to build out a workflow that is even more resilient. Also, you can build in fail states into your state machine to fail at any point, such as if a batch job fails after a certain number of retries.</li> 
<li><strong>Invoking Step Functions from Amazon API Gateway</strong><br /> You can use API Gateway to build an API that acts as a “front door” to Step Functions. You can create a POST method that contains the input JSON to feed into the state machine you built. For more information, see the <a href="https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/">Implementing Serverless Manual Approval Steps in AWS Step Functions and Amazon API Gateway</a> blog post.</li> 
</ul> 
<b id="toc_10">Conclusion</b> 
<p>While the approach we have demonstrated in this series has been focused on genomics, it is important to note that this can be generalized to nearly any high-throughput batch workload. We hope that you have found the information useful and that it can serve as a jump-start to building your own batch workloads on AWS with native AWS services.</p> 
<p>For more information about how AWS can enable your genomics workloads, be sure to check out the <a href="https://aws.amazon.com/health/genomics/">AWS Genomics</a> page.</p> 
<p>Other posts in this four-part series:</p> 
<ul> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
</ul> 
<p>Please leave any questions and comments below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/batch/" rel="tag">Batch</a>, <a href="https://aws.amazon.com/blogs/compute/tag/genomics/" rel="tag">genomics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/lambda/" rel="tag">lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a>, <a href="https://aws.amazon.com/blogs/compute/tag/workflow/" rel="tag">workflow</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2161');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building High-Throughput Genomic Batch Workflows on AWS: Batch Layer (Part 3 of 4)</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-31T17:45:09+00:00">31 MAY 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-batch/" title="View all posts in AWS Batch"><span property="articleSection">AWS Batch</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2142" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2142&amp;disqus_title=Building+High-Throughput+Genomic+Batch+Workflows+on+AWS%3A++Batch+Layer+%28Part+3+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2142');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the third in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<ul> 
<li>Job</li> 
<li>Batch</li> 
<li>Workflow</li> 
</ul> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2</a>, you built a Docker container for each job that needed to run as part of your workflow, and stored them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>.<span id="more-2142"></span></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In Part 3, you tackle the batch layer and build a scalable, elastic, and easily maintainable batch engine using <a href="https://aws.amazon.com/batch">AWS Batch</a>.</p> 
<p>AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. It dynamically provisions the optimal quantity and type of compute resources (for example, CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs that you submit. With AWS Batch, you do not need to install and manage your own batch computing software or server clusters, which allows you to focus on analyzing results, such as those of your genomic analysis.</p> 
<b id="toc_0">Integrating applications into AWS Batch</b> 
<p>If you are new to AWS Batch, we recommend reading <a href="http://docs.aws.amazon.com/batch/latest/userguide/get-set-up-for-aws-batch.html">Setting Up AWS Batch</a> to ensure that you have the proper permissions and AWS environment.</p> 
<p>After you have a working environment, you define several types of resources:</p> 
<ul> 
<li>IAM roles that provide service permissions</li> 
<li>A compute environment that launches and terminates compute resources for jobs</li> 
<li>A custom Amazon Machine Image (AMI)</li> 
<li>A job queue to submit the units of work and to schedule the appropriate resources within the compute environment to execute those jobs</li> 
<li>Job definitions that define how to execute an application</li> 
</ul> 
<p>After the resources are created, you’ll test the environment and create an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function to send generic jobs to the queue.</p> 
<p>This genomics workflow covers the basic steps. For more information, see <a href="http://docs.aws.amazon.com/batch/latest/userguide/Batch_GetStarted.html">Getting Started with AWS Batch</a>.</p> 
<b id="toc_1">Creating the necessary IAM roles</b> 
<p>AWS Batch simplifies batch processing by managing a number of underlying AWS services so that you can focus on your applications. As a result, you create IAM roles that give the service permissions to act on your behalf. In this section, deploy the <a href="https://aws.amazon.com/cloudformation">AWS CloudFormation</a> template <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/setup/iam.template.yaml">included in the GitHub repository</a> and extract the ARNs for later use.</p> 
<p>To deploy the stack, go to the top level in the <a href="https://github.com/awslabs/aws-batch-genomics">repo</a> with the following command:</p> 
<pre><code class="language-none">aws cloudformation create-stack --template-body file://batch/setup/iam.template.yaml --stack-name iam --capabilities CAPABILITY_NAMED_IAM</code></pre> 
<p>You can capture the output from this stack in the <strong>Outputs</strong> tab in the CloudFormation console:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/iam_cfn_output.png"><img class="aligncenter size-full wp-image-2145" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/iam_cfn_output.png" alt="" width="1945" height="450" /></a></p> 
<b id="toc_2">Creating the compute environment</b> 
<p>In AWS Batch, you will set up a managed compute environments. Managed compute environments automatically launch and terminate compute resources on your behalf based on the aggregate resources needed by your jobs, such as vCPU and memory, and simple boundaries that you define.</p> 
<p>When defining your compute environment, specify the following:</p> 
<ul> 
<li>Desired instance types in your environment</li> 
<li>Min and max vCPUs in the environment</li> 
<li>The Amazon Machine Image (AMI) to use</li> 
<li>Percentage value for bids on the <a href="https://aws.amazon.com/ec2/spot/">Spot Market</a> and VPC subnets that can be used.</li> 
</ul> 
<p>AWS Batch then provisions an elastic and heterogeneous pool of <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances based on the aggregate resource requirements of jobs sitting in the RUNNABLE state. If a mix of CPU and memory-intensive jobs are ready to run, AWS Batch provisions the appropriate ratio and size of CPU and memory-optimized instances within your environment. For this post, you will use the simplest configuration, in which instance types are set to “optimal” allowing AWS Batch to choose from the latest C, M, and R EC2 instance families.</p> 
<p>While you could create this compute environment in the console, we provide the following CLI commands. Replace the subnet IDs and key name with your own private subnets and key, and the image-id with the image you will build in the next section.</p> 
<pre><code class="language-none">ACCOUNTID=&lt;your account id&gt;
SERVICEROLE=&lt;from output in CloudFormation template&gt;
IAMFLEETROLE=&lt;from output in CloudFormation template&gt;
JOBROLEARN=&lt;from output in CloudFormation template&gt;
SUBNETS=&lt;comma delimited list of subnets&gt;
SECGROUPS=&lt;your security groups&gt;
SPOTPER=50 # percentage of on demand
IMAGEID=&lt;ami-id corresponding to the one you created&gt;
INSTANCEROLE=&lt;from output in CloudFormation template&gt;
REGISTRY=${ACCOUNTID}.dkr.ecr.us-east-1.amazonaws.com
KEYNAME=&lt;your key name&gt;
MAXCPU=1024 # max vCPUs in compute environment
ENV=myenv
# Creates the compute environment
aws batch create-compute-environment --compute-environment-name genomicsEnv-$ENV --type MANAGED --state ENABLED --service-role ${SERVICEROLE} --compute-resources type=SPOT,minvCpus=0,maxvCpus=$MAXCPU,desiredvCpus=0,instanceTypes=optimal,imageId=$IMAGEID,subnets=$SUBNETS,securityGroupIds=$SECGROUPS,ec2KeyPair=$KEYNAME,instanceRole=$INSTANCEROLE,bidPercentage=$SPOTPER,spotIamFleetRole=$IAMFLEETROLE</code></pre> 
<b id="toc_3">Creating the custom AMI for AWS Batch</b> 
<p>While you can use default Amazon ECS-optimized AMIs with AWS Batch, you can also provide your own image in managed compute environments. Use this feature to provision additional scratch EBS storage on each of the instances that AWS Batch launches, and to encrypt both the Docker and scratch EBS volumes.</p> 
<p>The process for <a href="http://docs.aws.amazon.com/batch/latest/userguide/create-batch-ami.html">creating a custom AMI for AWS Batch</a> is well-documented. Because AWS Batch has the <a href="https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec">same requirements for your AMI</a> as Amazon ECS, use the default <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=Amazon+ECS-Optimized+Amazon+Linux+AMI&amp;page=1&amp;ref_=nav_search_box">Amazon ECS-optimized Amazon Linux AMI</a> as a base and change it in the following ways:</p> 
<ul> 
<li>Attach a 1 TB scratch volume to <code>/dev/sdb</code></li> 
<li>Set the EBS encryption options on both the Docker volume and the new scratch volume</li> 
<li>Modify the <code>/etc/fstab</code> file so that the scratch volume is mounted to <code>/docker_scratch</code> on system start</li> 
</ul> 
<p>The first two tasks can be addressed in the console. Use the standard EC2 instance launch process to spin up a small t2.micro instance of the ECS-optimized AMI. In the <strong>Add Storage</strong> step, add the scratch volume and make sure that you select the <strong>Encrypted</strong> boxes for both the Docker and scratch volume.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_ecs_setup.png"><img class="aligncenter size-full wp-image-2146" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_ecs_setup.png" alt="" width="1950" height="554" /></a></p> 
<p>After your instance has launched, record the IP address and then SSH into the instance. Copy and paste the following code:</p> 
<pre><code class="language-none">
sudo yum -y update
sudo mkfs -t ext4 /dev/xvdb
sudo mkdir /docker_scratch
sudo echo -e '/dev/xvdb\t/docker_scratch\text4\tdefaults\t0\t0' | sudo tee -a /etc/fstab
sudo mount –a
sudo stop ecs
sudo rm -rf /var/lib/ecs/data/ecs_agent_data.json
</code></pre> 
<p>This auto-mounts your scratch volume to <code>/docker_scratch</code>, which is your scratch directory for batch processing. The last two commands stop the ECS agent and remove any persistent data checkpoint files. Next, <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-ebs.html">create your new AMI</a> and record the image ID.</p> 
<b id="toc_4">Creating the job queues</b> 
<p>AWS Batch job queues are used to coordinate the submission of batch jobs. Your jobs are submitted to job queues, which can be mapped to one or more compute environments. Job queues have priority relative to each other. You can also specify the order in which they consume resources from your compute environments.</p> 
<p>In this solution, use two job queues. The first is for high priority jobs, such as alignment or variant calling. Set this with a high priority (1000) and map back to the previously created compute environment. Next, set a second job queue for low priority jobs, such as quality statistics generation. To create these compute environments, enter the following CLI commands:</p> 
<pre><code class="language-none">aws batch create-job-queue --job-queue-name highPriority-${ENV} --compute-environment-order order=0,computeEnvironment=genomicsEnv-${ENV}  --priority 1000 --state ENABLED
aws batch create-job-queue --job-queue-name lowPriority-${ENV} --compute-environment-order order=0,computeEnvironment=genomicsEnv-${ENV}  --priority 1 --state ENABLED</code></pre> 
<b id="toc_5">Creating the job definitions</b> 
<p>To run the Isaac aligner container image locally, supply the Amazon S3 locations for the FASTQ input sequences, the reference genome to align to, and the output BAM file. For more information, see <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/README.md">tools/isaac/README.md</a>.</p> 
<p>The Docker container itself also requires some information on a suitable mountable volume so that it can read and write files temporary files without running out of space.</p> 
<p>Note: In the following example, the FASTQ files as well as the reference files to run are in a publicly available bucket.</p> 
<pre><code class="language-none">FASTQ1=s3://aws-batch-genomics-resources/fastq/SRR1919605_1.fastq.gz
FASTQ2=s3://aws-batch-genomics-resources/fastq/SRR1919605_2.fastq.gz
REF=s3://aws-batch-genomics-resources/reference/isaac/
BAM=s3://mybucket/genomic-workflow/test_results/bam/
mkdir ~/scratch
docker run --rm -ti -v $(HOME)/scratch:/scratch $REPO_URI --bam_s3_folder_path $BAM \
--fastq1_s3_path $FASTQ1 \
--fastq2_s3_path $FASTQ2 \
--reference_s3_path $REF \
--working_dir /scratch </code></pre> 
<p>Locally running containers can typically expand their CPU and memory resource headroom. In AWS Batch, the CPU and memory requirements are hard limits and are allocated to the container image at runtime.</p> 
<p>Isaac is a fairly resource-intensive algorithm, as it creates an uncompressed index of the reference genome in memory to match the query DNA sequences. The large memory space is shared across multiple CPU threads, and Isaac can scale almost linearly with the number of CPU threads given to it as a parameter.</p> 
<p>To fit these characteristics, choose an optimal instance size to maximize the number of CPU threads based on a given large memory footprint, and deploy a Docker container that uses all of the instance resources. In this case, we chose a host instance with 80+ GB of memory and 32+ vCPUs. The following code is example JSON that you can pass to the AWS CLI to create a job definition for Isaac.</p> 
<pre><code class="language-none">aws batch register-job-definition --job-definition-name isaac-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/isaac&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:80000,
&quot;vcpus&quot;:32,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'</code></pre> 
<p>You can copy and paste the following code for the other three job definitions:</p> 
<pre><code class="language-none">aws batch register-job-definition --job-definition-name strelka-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/strelka&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:32000,
&quot;vcpus&quot;:32,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'
aws batch register-job-definition --job-definition-name snpeff-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/snpeff&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:10000,
&quot;vcpus&quot;:4,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'
aws batch register-job-definition --job-definition-name samtoolsStats-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/samtools_stats&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:10000,
&quot;vcpus&quot;:4,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'</code></pre> 
<p>The value for “image” comes from the previous post on creating a Docker image and publishing to ECR. The value for jobRoleArn you can find from the output of the CloudFormation template that you deployed earlier. In addition to providing the number of CPU cores and memory required by Isaac, you also give it a storage volume for scratch and staging. The volume comes from the previously defined custom AMI.</p> 
<b id="toc_6">Testing the environment</b> 
<p>After you have created the Isaac job definition, you can submit the job using the <a href="http://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html">AWS Batch submitJob</a> API action. While the base mappings for Docker run are taken care of in the job definition that you just built, the specific job parameters should be specified in the container overrides section of the API call. Here’s what this would look like in the CLI, using the same parameters as in the bash commands shown earlier:</p> 
<pre><code class="language-none">aws batch submit-job --job-name testisaac --job-queue highPriority-${ENV} --job-definition isaac-${ENV}:1 --container-overrides '{
&quot;command&quot;: [
&quot;--bam_s3_folder_path&quot;, &quot;s3://mybucket/genomic-workflow/test_batch/bam/&quot;,
&quot;--fastq1_s3_path&quot;, &quot;s3://aws-batch-genomics-resources/fastq/ SRR1919605_1.fastq.gz&quot;,
&quot;--fastq2_s3_path&quot;, &quot;s3://aws-batch-genomics-resources/fastq/SRR1919605_2.fastq.gz&quot;,
&quot;--reference_s3_path&quot;, &quot;s3://aws-batch-genomics-resources/reference/isaac/&quot;,
&quot;--working_dir&quot;, &quot;/scratch&quot;,
&quot;—cmd_args&quot;, &quot; --exome &quot;,]
}'</code></pre> 
<p>When you execute a submitJob call, jobId is returned. You can then track the progress of your job using the describeJobs API action:</p> 
<pre><code class="language-none">aws batch describe-jobs –jobs &lt;jobId returned from submitJob&gt;</code></pre> 
<p>You can also track the progress of all of your jobs in the AWS Batch console dashboard.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_dashboard.png"><img class="aligncenter size-full wp-image-2147" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_dashboard.png" alt="" width="1945" height="687" /></a></p> 
<p>To see exactly where a RUNNING job is at, use the link in the AWS Batch console to direct you to the appropriate location in CloudWatch logs.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_cwl.png"><img class="aligncenter size-full wp-image-2148" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_cwl.png" alt="" width="1945" height="1016" /></a></p> 
<b id="toc_7">Completing the batch environment setup</b> 
<p>To finish, create a Lambda function to submit a generic AWS Batch job.</p> 
<p>In the Lambda console, create a Python 2.7 Lambda function named batchSubmitJob. Copy and paste the following code. This is similar to the <code>batch-submit-job-python27</code> Lambda blueprint. Use the LambdaBatchExecutionRole that you created earlier. For more information about creating functions, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html">Step 2.1: Create a Hello World Lambda Function</a>.</p> 
<pre><code class="language-none">from __future__ import print_function
import json
import boto3
batch_client = boto3.client('batch')
def lambda_handler(event, context):
# Log the received event
print(&quot;Received event: &quot; + json.dumps(event, indent=2))
# Get parameters for the SubmitJob call
# http://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html
job_name = event['jobName']
job_queue = event['jobQueue']
job_definition = event['jobDefinition']
# containerOverrides, dependsOn, and parameters are optional
container_overrides = event['containerOverrides'] if event.get('containerOverrides') else {}
parameters = event['parameters'] if event.get('parameters') else {}
depends_on = event['dependsOn'] if event.get('dependsOn') else []
try:
response = batch_client.submit_job(
dependsOn=depends_on,
containerOverrides=container_overrides,
jobDefinition=job_definition,
jobName=job_name,
jobQueue=job_queue,
parameters=parameters
)
# Log response from AWS Batch
print(&quot;Response: &quot; + json.dumps(response, indent=2))
# Return the jobId
event['jobId'] = response['jobId']
return event
except Exception as e:
print(e)
message = 'Error getting Batch Job status'
print(message)
raise Exception(message)</code></pre> 
<b id="toc_8">Conclusion</b> 
<p>In part 3 of this series, you successfully set up your data processing, or batch, environment in AWS Batch. We also provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/setup/create_batch_env.py">Python script</a> in the corresponding GitHub repo that takes care of all of the above CLI arguments for you, as well as building out the job definitions for all of the jobs in the workflow: Isaac, Strelka, SAMtools, and snpEff. You can check the script’s README for additional documentation.</p> 
<p>In Part 4, you’ll cover the workflow layer using <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> and <a href="https://aws.amazon.com/lambda">AWS Lambda</a>.</p> 
<p>Other posts in this four-part series:</p> 
<ul> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
</ul> 
<p>Please leave any questions and comments below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/batch/" rel="tag">Batch</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecr/" rel="tag">ECR</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2142');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2017-05-31T11:13:18+00:00">31 MAY 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2"><span property="articleSection">Amazon EC2</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2091" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2091&amp;disqus_title=Building+Scalable+Applications+and+Microservices%3A+Adding+Messaging+to+Your+Toolbox&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2091');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/Jakub.jpeg"><img class="alignnone size-full wp-image-2082" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/Jakub.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Jakub Wojciak, Senior Software Development Engineer</strong></p> 
<p>Throughout our careers, we developers keep adding new tools to our development toolboxes. These range from the programming languages we learn, use, and become experts in, to architectural components such as HTTP servers, load balancers, and databases (both relational and NoSQL).</p> 
<p>I’d like to kick off a series of posts to introduce you to the architectural components of messaging solutions. Expand your toolbox with this indispensable tool for building modern, scalable services and applications. In the coming months, I will update this post with links that dive deeper into each topic and illustrate messaging use cases using <a href="http://aws.amazon.com/sqs/">Amazon Simple Queue Service (SQS)</a> and<a href="http://aws.amazon.com/sns/"> Amazon Simple Notification Service (SNS)</a>.<span id="more-2091"></span></p> 
<p><strong>What is messaging?</strong><br /> Messaging involves passing messages around, but it’s different from email or text messages, because it is intended for communication between software components, not between people. Enterprise messaging happens at a higher level than that of UDP packets or direct TCP connections (although it does frequently use these protocols).</p> 
<p>A message typically contains the payload — whatever information your application sends: XML, JSON, binary data, and so on. You can also add optional attributes and metadata to a message.</p> 
<p>A SQL or NoSQL database often has a server that stores data. Similarly, a messaging server or service allows a place for your messages to be stored temporarily and transmitted.</p> 
<p><strong>The queue and the topic</strong><br /> For a database service, the main resource is a table. In a messaging service, the two main resources are the queue and the topic.</p> 
<p>A queue is like a buffer. You can put messages into a queue, and you can retrieve messages from a queue. The software that puts messages into a queue is called a message producer and the software that retrieves messages is called a message consumer.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageQueueDiagramFINAL.png"><img class="size-full wp-image-2080 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageQueueDiagramFINAL.png" alt="" width="950" height="396" /></a></p> 
<p>A topic is like a broadcasting station. You can publish messages to a topic, and anyone interested in these messages can subscribe to the topic. Then, the interested parties are notified about the published messages. The software that broadcasts topics is called a topic publisher and the software that subscribes to broadcasts is called a topic subscriber.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageTopicDiagramFINAL.png"><img class="size-full wp-image-2079 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageTopicDiagramFINAL.png" alt="" width="694" height="390" /></a><strong>When should you use messaging?</strong><br /> There are some common use cases that might instantly make you think “I should use messaging for that!” Here are some of these use cases (to be discussed in greater detail in future posts).</p> 
<ul> 
<li><strong>Service-to-service communication</strong><br /> You have two services or systems that need to communicate with each other. Let’s say a website (the frontend) has to update customer’s delivery address in a customer relationship management (CRM) system (the backend). Alternatively, you can set up a load balancer in front of the backend CRM service and call its API actions directly from the frontend website. You can also set up a queue and have the frontend website code send messages to the queue and have the backend CRM service to consume them.</li> 
<li><strong>Asynchronous work item backlogs</strong><br /> You have a service that has to track a backlog of actions to be executed. Let’s say a hotel booking system needs to cancel a booking and this process takes a long time (from a few seconds to a minute). You can execute the cancellation synchronously, but then you risk annoying the customer who has to wait for the webpage to load. You can also track all pending cancellations in your database and keep polling and executing cancellations. Alternatively, you can put a message into a queue and have the same hotel booking system consume messages from that queue and perform asynchronous cancellations.</li> 
<li><strong>State change notifications</strong><br /> You have a service that manages some resource and other services that receive updates about changes to those resources. Let’s say an inventory tracking system tracks products stocked in a warehouse. Whenever the stock is sold out, the website must stop offering that product. Whenever the stock is close to being depleted, the purchasing system must place an order for more items. Those systems can keep querying the inventory system to learn about these changes (or even directly examine the database—yuck!). Alternatively, the inventory system can publish notifications about stock changes to a topic and any interested program can subscribe to learn about those changes.</li> 
</ul> 
<p><strong>When should you not use messaging?</strong><br /> According to the law of the instrument, <a href="https://en.wikipedia.org/wiki/Law_of_the_instrument">“If all you have is a hammer, everything looks like a nail.”</a> In other words, it’s important to know when a particular technology won’t fit well with your use case. For example, you have a relational database that you can store large binary files in… but you probably shouldn’t.</p> 
<p>Messaging has its own set of commonly encountered anti-patterns (also to be discussed in greater detail in future posts).</p> 
<ul> 
<li><strong>Message selection</strong><br /> It’s tempting to have the ability to receive messages selectively from a queue —that match a particular set of attributes, or even match an ad-hoc logical query. For example, a service requests a message with a particular attribute because it contains a response to another message that the service sent out. This can lead to a scenario where there are messages in the queue that no one is polling for and are never consumed. (Note: This problem doesn’t exist for message routing or filtering, which are evaluated when messages are&nbsp;sent&nbsp;to a destination queue or topic.)</li> 
<li><strong>Very large messages or files</strong><br /> Most messaging protocols and implementations work best with reasonably sized messages (in the tens or hundreds of KBs). As message sizes grow, it’s best to use a dedicated file (or blob) storage system, such as Amazon S3, and pass a reference to an object in that store in the message itself. A dedicated file (or blob) store typically has much better support for uploading data in chunks with the ability to retry or resume downloads from a particular fragment.</li> 
</ul> 
<p><strong>Key features of messaging systems</strong><br /> Messaging servers and services offer much more than just produce/consume or publish/subscribe functionality. Thus, although it might seem easy to create your own message passing implementation on top of your own data store, consider all the extra features that a full-fledged messaging service provides. Here’s a list of a few but not—by any means—all messaging features:</p> 
<ul> 
<li><strong>Push or pull delivery</strong><br /> Most messaging services provide both options for consuming messages. Pull means continuously querying whether the messaging service has any new messages. Push means that the messaging service notifies you when a message is available. The notification about the new message might be a special packet sent over the messaging protocol. It might also be an HTTP call that the messaging service makes to your API endpoint. You can also use long-polling, which combines both push and pull functionality.</li> 
<li><strong>Dead letter queues</strong><br /> What can your application do if a queue contains a message that you can’t process? Most messaging services allow you to configure a dead-letter queue for messages that you fail to process a certain number of times. This makes it easy to set them aside for further inspection without blocking the queue processing or spending CPU cycles on a message that can never be consumed successfully.</li> 
<li><strong>Delay queues and scheduled messages</strong><br /> What if you want to postpone the processing of a particular message until a specific time? Many messaging services support setting a specific delivery time for a message. If you need to have a common delay for all messages, you can set up a delay queue.</li> 
<li><strong>Ordering, priorities, duplicates</strong><br /> Messaging services provide you with a variety of options that affect the delivery of messages:<p></p> 
<ul> 
<li>A choice between ordered delivery with limited maximum throughput or unordered delivery with virtually unlimited throughput</li> 
<li>Message priorities, where a higher priority message can skip over other messages in the queue</li> 
<li>Transactionality or best-effort acknowledgments of messages</li> 
</ul> </li> 
</ul> 
<p>When designing your system with messaging in mind, ask yourself the following questions:</p> 
<ul> 
<li>Do you need to process messages exactly in the order in which they were sent?</li> 
<li>Could your application parallelize the workload and process messages out of order?</li> 
<li>Do you want your application to consume certain messages at a higher priority than other messages?</li> 
<li>What happens if your application fails to process a message midway? Can you handle processing the same message again?</li> 
</ul> 
<p><strong>How can you get started?</strong></p> 
<p>If you have to configure and start a messaging server, it might take an extra effort to start using messaging. Instead, you can start to use message queues and topics today, using Amazon SQS and Amazon SNS. For more information, visit the following resources, and get started creating message queues and topics with just a few API actions:</p> 
<ul> 
<li><a href="https://aws.amazon.com/sqs/getting-started/">Getting started with Amazon SQS</a></li> 
<li><a href="https://aws.amazon.com/sns/getting-started/">Getting started with Amazon SNS</a></li> 
</ul> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-notification-service/" rel="tag">Amazon Simple Notification Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-queue-service/" rel="tag">Amazon Simple Queue Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sns/" rel="tag">Amazon SNS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sqs/" rel="tag">Amazon SQS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloud-messaging/" rel="tag">cloud messaging</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-queues/" rel="tag">message queues</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-topics/" rel="tag">message topics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/notifications/" rel="tag">notifications</a>, <a href="https://aws.amazon.com/blogs/compute/tag/pubsub/" rel="tag">pub/sub</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2091');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Job Layer (Part 2 of 4)</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-31T09:45:02+00:00">31 MAY 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2123" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2123&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Job+Layer+%28Part+2+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2123');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the second in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<ul> 
<li>Job</li> 
<li>Batch</li> 
<li>Workflow</li> 
</ul> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In Part 2, you tackle the job layer and package a set of bioinformatics applications into Docker containers and store them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>. We illustrate some common patterns and best practices for these containers, such as how you can effectively use <a href="https://aws.amazon.com/s3">Amazon S3</a> to exchange data between jobs.<span id="more-2123"></span></p> 
<p>ECR is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, simplifying your development to a production workflow. ECR eliminates the need to operate your own container repositories or worry about scaling the underlying infrastructure by hosting your images in a highly available and scalable architecture. You can integrate with <a href="https://aws.amazon.com/iam">IAM</a> to provide resource-level control for each repository.</p> 
<p>All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<b id="toc_0">Packaging an application in a Docker container</b> 
<p>Genomic analysis often relies on open source software that is developed by academic groups or open-sourced by industry leaders. These applications have a range of requirements for libraries and reference data, and are typically executed using a Linux command line interface.</p> 
<p>Docker containers provide a consistent, reproducible run-time environment for applications to run in, which results in reproducible results. Consequently, containerization of the applications using Docker has received much attention from the bioinformatics community, resulting in the development of application registries such as <a href="https://dockstore.org/">Dockstore.org</a>, <a href="http://biocontainers.pro/">BioContainers</a>, and the <a href="https://galaxyproject.org/admin/tools/docker/">Galaxy Tool Shed</a>. In this post, we cover several good practices for packing genomics applications in Docker containers, including:</p> 
<ul> 
<li>Building your Dockerfile</li> 
<li>Dealing with job multitenancy</li> 
<li>Sharing data between jobs</li> 
</ul> 
<b id="toc_1">Building your Dockerfile</b> 
<p>Your Dockerfile contains all of the commands that you use to package your Docker container. In it, you pick a base image to build from, include any metadata to attribute to the image, describe how to build and configure the environment, and how to access the code running within it.</p> 
<p>We recommend that you adopt a standard set of conventions for your Dockerfiles. Add metadata to describe the contained application, and have an order set of sections for application packaging needs. Here is an example from the provided <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/docker/Dockerfile">Isaac Dockerfile</a>:</p> 
<pre><code class="language-none">FROM python:2.7
# Metadata
LABEL container.base.image=&quot;python:2.7&quot;
LABEL software.name=&quot;iSAAC&quot;
LABEL software.version=&quot;03.17.03.01&quot;
LABEL software.description=&quot;Aligner for sequencing data&quot;
LABEL software.website=&quot;https://github.com/Illumina/Isaac3&quot;
LABEL software.documentation=&quot;https://github.com/Illumina/Isaac3/blob/master/src/markdown/manual.md&quot;
LABEL software.license=&quot;GPLv3&quot;
LABEL tags=&quot;Genomics&quot;
RUN apt-get -y update &amp;&amp; \
apt-get -y install zlib1g-dev gnuplot &amp;&amp; \
apt-get clean
RUN pip install boto3 awscli
RUN git clone https://github.com/Illumina/Isaac3.git &amp;&amp; cd /Isaac3 &amp;&amp; git checkout 6f0191a4e0d4b332e8f34b7ced57dc6e6eb4f2f1
RUN mkdir /iSAAC-build /isaac
WORKDIR /iSAAC-build
RUN /Isaac3/src/configure --prefix=/isaac
RUN make
RUN make install
WORKDIR /
RUN rm -rf /Isaac3 /iSAAC-build
RUN chmod -R +x /isaac/bin/
ENV PATH=&quot;/isaac/bin:$PATH&quot;
ENV LD_LIBRARY_PATH=&quot;/usr/local/lib:/usr/lib:/isaac/libexec:${LD_LIBRARY_PATH}&quot;
COPY isaac/src/run_isaac.py /
COPY common_utils /common_utils
ENTRYPOINT [&quot;python&quot;, &quot;/run_isaac.py&quot;]</code></pre> 
<p>There are many ways to architect Docker containers, but we wanted to consolidate some recommendations that we have observed our customers successfully using:</p> 
<ul> 
<li>Work off of a base image that satisfies most dependencies across a set of applications. In the code provided, we often use the <a href="https://github.com/docker-library/python/blob/a248f4583c5f788e1af02016f762cdc323ee5765/2.7/Dockerfile">official <code>python:2.7</code> image</a> from <a href="https://hub.docker.com/_/python/">Docker Hub</a>.</li> 
<li>In the above Dockerfile, you can see that we have separated out the metadata, underlying system and library dependencies, application-specific dependencies, and the installation requirements into a logical ordered set for easier maintenance. It’s worth noting that if you are building a production application, you would traditionally have a golden set of images to build from and application artifacts to install that have been validated with internal processes.</li> 
<li>Instead of building large datasets into the container itself, we recommend that you download reference data at runtime instead. This allows the decoupling of the algorithm from reference data updates, which could happen nightly. It also allows you to download a subset of all reference data for an algorithm that is specific to the running analysis, for example the particular species under study.</li> 
<li>Provide an application entry point to both expose and limit the default functionality of the container image. In this example, we created a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/src/run_isaac.py">simple Python script</a> that takes care of downloading the dependencies from S3, such as reference data for your analysis, on the fly from a set of provided runtime parameters and stage results back into S3. We dive deeper into this script in the following section.</li> 
</ul> 
<p>Often these shared dependencies are a mix of packaged code that is easily installable (in this case via pip) or private modules usually shared as part of internal source repositories, as is the case here.</p> 
<p>When you build the Docker images, you should take care to include both the necessary private modules within the context of the build. The example below shows how you would accomplish that given a directory context with some dependencies a few levels higher that the Dockerfile. In the project, we provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/docker/Makefile">Makefile</a> for taking care of some of these items, but for clarity’s sake we issue the necessary Docker commands.</p> 
<pre><code class="language-none"># Given the following partial directory tree structure
# .
# └── tools
#     ├── common\_utils
#     │   ├── __init__.py
#     │   ├── job_utils.py
#     │   └── s3_utils.py
#     └── isaac
#         └── docker
#             ├── Dockerfile
#             └── Makefile
# cd &lt;git repository&gt;/tools/isaac/docker
$ docker build -t isaac:03.17.03.01 \
-t isaac:latest \
-f Dockerfile ../..</code></pre> 
<b id="toc_2">Job multitenancy and sharing data between jobs</b> 
<p>Many bioinformatics tools have been developed to run in any Linux environment, and not necessarily optimized for cloud computing or multitenancy. To overcome these challenges, you can use a simple Python wrapper script for each tool that facilitates the deployment of a job.</p> 
<p>Our tools have several of the same requirements, such as the need to read and write from S3 and deal with job multitenancy. For these common utilities, we built a separate <a href="https://github.com/awslabs/aws-batch-genomics/tree/master/tools/common_utils"><code>common_utils</code></a> package to import during the creation of the Docker image. These utilities deal with the previously mentioned common requirements, such as:</p> 
<ul> 
<li style="list-style-type: none"> 
<ul> 
<li><strong>Container placement</strong></li> 
</ul> </li> 
</ul> 
<p>To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute. The <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/common_utils/s3_utils.py"><code>tools/common_utils/s3_utils.py</code></a> script contains the functions required to leverage S3.</p> 
<ul> 
<li style="list-style-type: none"> 
<ul> 
<li><strong>Multitenancy</strong></li> 
</ul> </li> 
</ul> 
<p>Multiple container jobs may run concurrently on the same instance. In these situations, it’s essential that your job writes to a unique subdirectory. An easy way to do this is to create a subfolder using a UUID and have your application write all of your data there.</p> 
<ul> 
<li style="list-style-type: none"> 
<ul> 
<li><strong>Cleanup</strong></li> 
</ul> </li> 
</ul> 
<p>As your jobs complete and write the output back to S3, you can delete the scratch data on your instance generated by that job. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances. As you ensure that you’re writing to a unique subdirectory in the multitenancy solution, you can simply delete that subdirectory to minimize your storage footprint.</p> 
<p>Each of the Python wrappers takes in all of the requisite data dependencies, residing in S3, as command-line arguments and any other necessary commands to run the tool it wraps. It then handles all of the file downloading, running the bioinformatics tool, and uploading the files back to S3. For more information about each of these tools, see the READMEs for each individual tool.</p> 
<b id="toc_3">Deploying images to Amazon ECR</b> 
<p>Next, publish the Docker images to ECR. The first example below creates an ECR repository and collects the URI you provide to Docker in order to push the image to ECR. If a repository already exists for the container image, query for it, as shown in the second example.</p> 
<pre><code class="language-none"># Create an ECR repository for Isaac, then copy the `repositoryUri` into a variable
$ REPO\_URI=$(aws ecr create-repository \
--repository-name isaac \
--output text --query &quot;repository.repositoryUri&quot;)
# If the repository already exists, then
# query ECR for the `repositoryUri`
$ REPO\_URI=$(aws ecr describe-repositories \
--repository-names isaac \
--output text --query &quot;repositories[0].repositoryUri&quot;)</code></pre> 
<p>After you have a repository URI, you can tag the container image and push it to ECR.</p> 
<pre><code class="language-none">$ eval $(aws ecr get-login)
$ docker tag isaac:latest $(REPO\_URI):latest
$ docker push $(REPO\_URI):latest
$ docker tag isaac:03.17.03.01 $(REPO\_URI):03.17.03.01
$ docker push $(REPO\_URI):03.17.03.01</code></pre> 
<b id="toc_4">Conclusion</b> 
<p>In part 2 of this series, we showed a practical example of packaging up a bioinformatics application within a Docker container, and publishing the container image to an ECR repository. Along the way, we discussed some generally applicable best practices and design decisions specific to this demonstration project.</p> 
<p>Now that you have built one of the Docker containers, you can go ahead and build each of the containers. We have provided all of the necessary code within a GitHub repository and within that repository are specific instructions, as well as some helpers for building container images using GNU make.</p> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3</a> of this series, you’ll dive deep into the batch processing layer and how to leverage the packaged applications within AWS Batch.</p> 
<p>Other posts in this four-part series:</p> 
<ul> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
</ul> 
<p>Please leave any questions and comments below.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2123');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
