<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/LambdaTrigger-626x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Implementing Default Directory Indexes in Amazon S3-backed Amazon CloudFront Origins Using Lambda@Edge</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ronnie Eichler</span></span> | on 
<time property="datePublished" datetime="2017-10-18T07:35:23+00:00">18 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-cloudfront/" title="View all posts in Amazon CloudFront*"><span property="articleSection">Amazon CloudFront*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2895" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2895&amp;disqus_title=Implementing+Default+Directory+Indexes+in+Amazon+S3-backed+Amazon+CloudFront+Origins+Using+Lambda%40Edge&amp;disqus_url=https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2895');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>With the recent launch of <a href="https://aws.amazon.com/lambda/edge/">Lambda@Edge</a>, it’s now possible for you to provide even more robust functionality to your static websites. <a href="https://aws.amazon.com/cloudfront">Amazon CloudFront</a> is a content distribution network service. In this post, I show how you can use Lambda@Edge along with the CloudFront origin access identity (OAI) for Amazon S3 and still provide simple URLs (such as www.example.com/about/ instead of www.example.com/about/index.html).<span id="more-2895"></span></p> 
<h3>Background</h3> 
<p>Amazon S3 is a great platform for <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html">hosting a static website</a>. You don’t need to worry about managing servers or underlying infrastructure—you just publish your static to content to an S3 bucket. S3 provides a DNS name such as &lt;<strong>bucket-name</strong>&gt;.s3-website-&lt;<strong>AWS-region</strong>&gt;.amazonaws.com. Use this name for your website by creating a CNAME record in your domain’s DNS environment (or Amazon Route 53) as follows:</p> 
<blockquote> 
<p>www.example.com -&gt; &lt;<strong>bucket-name</strong>&gt;.s3-website-&lt;<strong>AWS-region</strong>&gt;.amazonaws.com</p> 
</blockquote> 
<p>You can also put CloudFront in front of S3 to further scale the performance of your site and cache the content closer to your users. CloudFront can enable HTTPS-hosted sites, by either using a custom Secure Sockets Layer (SSL) certificate or a managed certificate from <a href="https://aws.amazon.com/blogs/aws/new-aws-certificate-manager-deploy-ssltls-based-apps-on-aws/">AWS Certificate Manager</a>. In addition, CloudFront also offers integration with <a href="https://aws.amazon.com/waf/">AWS WAF</a>, a web application firewall. As you can see, it’s possible to achieve some robust functionality by using S3, CloudFront, and other managed services and not have to worry about maintaining underlying infrastructure.</p> 
<p>One of the key concerns that you might have when implementing any type of WAF or CDN is that you want to force your users to go through the CDN. If you implement CloudFront in front of S3, you can achieve this by using an <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html">OAI</a>. However, in order to do this, you cannot use the HTTP endpoint that is exposed by S3’s static website hosting feature. Instead, CloudFront must use the S3 REST endpoint to fetch content from your origin so that the request can be authenticated using the OAI. This presents some challenges in that the REST endpoint does not support redirection to a default index page.</p> 
<p>CloudFront does allow you to specify a default root object (index.html), but it only works on the root of the website (such as http://www.example.com &gt; http://www.example.com/index.html). It does not work on any subdirectory (such as http://www.example.com/about/). If you were to attempt to request this URL through CloudFront, CloudFront would do a S3 GetObject API call against a key that does not exist.</p> 
<p>Of course, it is a bad user experience to expect users to always type index.html at the end of every URL (or even know that it should be there). Until now, there has not been an easy way to provide these simpler URLs (equivalent to the DirectoryIndex Directive in an Apache Web Server configuration) to users through CloudFront. Not if you still want to be able to restrict access to the S3 origin using an OAI. However, with the release of <a href="https://aws.amazon.com/lambda/edge/">Lambda@Edge</a>, you can use a JavaScript function running on the CloudFront edge nodes to look for these patterns and request the appropriate object key from the S3 origin.</p> 
<h3>Solution</h3> 
<p>In this example, you use the compute power at the CloudFront edge to inspect the request as it’s coming in from the client. Then re-write the request so that CloudFront requests a default index object (index.html in this case) for any request URI that ends in ‘/’.</p> 
<p>When a request is made against a web server, the client specifies the object to obtain in the request. You can use this URI and apply a regular expression to it so that these URIs get resolved to a default index object before CloudFront requests the object from the origin. Use the following code:</p> 
<code class="lang-js">'use strict';
exports.handler = (event, context, callback) =&gt; {
// Extract the request from the CloudFront event that is sent to Lambda@Edge 
var request = event.Records[0].cf.request;
// Extract the URI from the request
var olduri = request.uri;
// Match any '/' that occurs at the end of a URI. Replace it with a default index
var newuri = olduri.replace(/\/$/, '\/index.html');
// Log the URI as received by CloudFront and the new URI to be used to fetch from origin
console.log(&quot;Old URI: &quot; + olduri);
console.log(&quot;New URI: &quot; + newuri);
// Replace the received URI with the URI that includes the index page
request.uri = newuri;
// Return to CloudFront
return callback(null, request);
};
</code> 
<p>To get started, create an S3 bucket to be the origin for CloudFront:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/17/Picture1-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/17/Picture1-1-921x1024.png" /></a></p> 
<p>On the other screens, you can just accept the defaults for the purposes of this walkthrough. If this were a production implementation, I would recommend enabling bucket logging and specifying an existing S3 bucket as the destination for access logs. These logs can be useful if you need to troubleshoot issues with your S3 access.</p> 
<p>Now, put some content into your S3 bucket. For this walkthrough, create two simple webpages to demonstrate the functionality: &nbsp;A page that resides at the website root, and another that is in a subdirectory.</p> 
<p><strong>&lt;s3bucketname&gt;/index.html</strong></p> 
<code class="lang-html">&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Root home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the root directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code> 
<p><strong>&lt;s3bucketname&gt;/subdirectory/index.html</strong></p> 
<code class="lang-html">&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Subdirectory home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the /subdirectory/ directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code> 
<p>When uploading the files into S3, you can accept the defaults. You add a bucket policy as part of the CloudFront distribution creation that allows CloudFront to access the S3 origin. You should now have an S3 bucket that looks like the following:</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3root.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3root.png" /></a> 
<p class="wp-caption-text">Root of bucket</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3subdirectory.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3subdirectory.png" /></a> 
<p class="wp-caption-text">Subdirectory in bucket</p> 
<p>Next, create a CloudFront distribution that your users will use to access the content. Open the CloudFront console, and choose <strong>Create Distribution</strong>. For <strong>Select a delivery method for your content</strong>, under <strong>Web</strong>, choose <strong>Get Started</strong>.</p> 
<p>On the next screen, you set up the distribution. Below are the options to configure:</p> 
<li><strong>Origin Domain Name</strong>: &nbsp;Select the S3 bucket that you created earlier.</li> 
<li><strong>Restrict Bucket Access:&nbsp;</strong>Choose&nbsp;<strong>Yes</strong>.</li> 
<li><strong>Origin Access Identity:&nbsp;</strong>Create a new identity.</li> 
<li><strong>Grant Read Permissions on Bucket</strong>: Choose&nbsp;<strong>Yes, Update Bucket Policy</strong>.</li> 
<li><strong>Object Caching</strong>: Choose&nbsp;<strong>Customize</strong> (I am changing the behavior to avoid having CloudFront cache objects, as this could affect your ability to troubleshoot while implementing the Lambda code). 
<li><strong>Minimum TTL</strong>: 0</li> 
<li><strong>Maximum TTL</strong>: 0</li> 
<li><strong>Default TTL</strong>: 0</li> 
</ul> </li> 
<p>You can accept all of the other defaults. Again, this is a proof-of-concept exercise. After you are comfortable that the CloudFront distribution is working properly with the origin and Lambda code, you can re-visit the preceding values and make changes before implementing it in production.</p> 
<p>CloudFront distributions can take several minutes to deploy (because the changes have to propagate out to all of the edge locations). After that’s done, test the functionality of the S3-backed static website. Looking at the distribution, you can see that CloudFront assigns a domain name:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/CfDistribution-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/CfDistribution-1.png" /></a></p> 
<p>Try to access the website using a combination of various URLs:</p> 
<p>http://&lt;domainname&gt;/: &nbsp;<strong>Works</strong></p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/
*   Trying 54.192.192.214...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.214) port 80 (#0)
&gt; GET / HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; ETag: &quot;cb7e2634fe66c1fd395cf868087dd3b9&quot;
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: Miss from cloudfront
&lt; X-Amz-Cf-Id: -D2FSRwzfcwyKZKFZr6DqYFkIf4t7HdGw2MkUF5sE6YFDxRJgi0R1g==
&lt; Content-Length: 209
&lt; Content-Type: text/html
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:16 GMT
&lt; Via: 1.1 6419ba8f3bd94b651d416054d9416f1e.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Root home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the root directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>This is because CloudFront is configured to request a default root object (index.html) from the origin.</p> 
<p>http://&lt;domainname&gt;/subdirectory/: &nbsp;<strong>Doesn’t work</strong></p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/subdirectory/
*   Trying 54.192.192.214...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.214) port 80 (#0)
&gt; GET /subdirectory/ HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; ETag: &quot;d41d8cd98f00b204e9800998ecf8427e&quot;
&lt; x-amz-server-side-encryption: AES256
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: Miss from cloudfront
&lt; X-Amz-Cf-Id: Iqf0Gy8hJLiW-9tOAdSFPkL7vCWBrgm3-1ly5tBeY_izU82ftipodA==
&lt; Content-Length: 0
&lt; Content-Type: application/x-directory
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:24 GMT
&lt; Via: 1.1 6419ba8f3bd94b651d416054d9416f1e.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>If you use a tool such like cURL to test this, you notice that CloudFront and S3 are returning a blank response. The reason for this is that the subdirectory does exist, but it does not resolve to an S3 object. Keep in mind that S3 is an object store, so there are no real directories. User interfaces such as the S3 console present a hierarchical view of a bucket with folders based on the presence of forward slashes, but behind the scenes the bucket is just a collection of keys that represent stored objects.</p> 
<p>http://&lt;domainname&gt;/subdirectory/index.html: &nbsp;<strong>Works</strong></p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/subdirectory/index.html
*   Trying 54.192.192.130...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.130) port 80 (#0)
&gt; GET /subdirectory/index.html HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Date: Thu, 20 Jul 2017 20:35:15 GMT
&lt; ETag: &quot;ddf87c487acf7cef9d50418f0f8f8dae&quot;
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: RefreshHit from cloudfront
&lt; X-Amz-Cf-Id: bkh6opXdpw8pUomqG3Qr3UcjnZL8axxOH82Lh0OOcx48uJKc_Dc3Cg==
&lt; Content-Length: 227
&lt; Content-Type: text/html
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:45 GMT
&lt; Via: 1.1 3f2788d309d30f41de96da6f931d4ede.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Subdirectory home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the /subdirectory/ directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>This request works as expected because you are referencing the object directly. Now, you implement the Lambda@Edge function to return the default index.html page for any subdirectory. Looking at the example JavaScript code, here’s where the magic happens:</p> 
<code class="lang-js">var newuri = olduri.replace(/\/$/, '\/index.html');
</code> 
<p>You are going to use a JavaScript regular expression to match any ‘/’ that occurs at the end of the URI and replace it with ‘/index.html’. This is the equivalent to what S3 does on its own with static website hosting. However, as I mentioned earlier, you can’t rely on this if you want to use a policy on the bucket to restrict it so that users must access the bucket through CloudFront. That way, all requests to the S3 bucket must be authenticated using the S3 REST API. Because of this, you implement a Lambda@Edge function that takes any client request ending in ‘/’ and append a default ‘index.html’ to the request before requesting the object from the origin.</p> 
<p>In the Lambda console, choose <strong>Create function</strong>. On the next screen, skip the blueprint selection and choose <strong>Author from scratch</strong>, as you’ll use the sample code provided.</p> 
<p>Next, configure the trigger. Choosing the empty box shows a list of available triggers. Choose <strong>CloudFront</strong> and select your CloudFront distribution ID (created earlier). For this example, leave <strong>Cache Behavior</strong> as <strong>*</strong> and <strong>CloudFront Event</strong> as <strong>Origin Request</strong>. Select the <strong>Enable trigger and replicate box</strong> and choose <strong>Next</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/LambdaTrigger.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/LambdaTrigger.png" /></a></p> 
<p>Next, give the function a name and a description. Then, copy and paste the following code:</p> 
<code class="lang-js">'use strict';
exports.handler = (event, context, callback) =&gt; {
// Extract the request from the CloudFront event that is sent to Lambda@Edge 
var request = event.Records[0].cf.request;
// Extract the URI from the request
var olduri = request.uri;
// Match any '/' that occurs at the end of a URI. Replace it with a default index
var newuri = olduri.replace(/\/$/, '\/index.html');
// Log the URI as received by CloudFront and the new URI to be used to fetch from origin
console.log(&quot;Old URI: &quot; + olduri);
console.log(&quot;New URI: &quot; + newuri);
// Replace the received URI with the URI that includes the index page
request.uri = newuri;
// Return to CloudFront
return callback(null, request);
};
</code> 
<p>Next, define a role that grants permissions to the Lambda function. For this example, choose <strong>Create new role from template</strong>, <strong>Basic Edge Lambda permissions</strong>. This creates a new IAM role for the Lambda function and grants the following permissions:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;,
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:logs:*:*:*&quot;
]
}
]
}
</code> 
<p>In a nutshell, these are the permissions that the function needs to create the necessary CloudWatch log group and log stream, and to put the log events so that the function is able to write logs when it executes.</p> 
<p>After the function has been created, you can go back to the browser (or cURL) and re-run the test for the subdirectory request that failed previously:</p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/subdirectory/
*   Trying 54.192.192.202...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.202) port 80 (#0)
&gt; GET /subdirectory/ HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Date: Thu, 20 Jul 2017 21:18:44 GMT
&lt; ETag: &quot;ddf87c487acf7cef9d50418f0f8f8dae&quot;
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: Miss from cloudfront
&lt; X-Amz-Cf-Id: rwFN7yHE70bT9xckBpceTsAPcmaadqWB9omPBv2P6WkIfQqdjTk_4w==
&lt; Content-Length: 227
&lt; Content-Type: text/html
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:45 GMT
&lt; Via: 1.1 3572de112011f1b625bb77410b0c5cca.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Subdirectory home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the /subdirectory/ directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>You have now configured a way for CloudFront to return a default index page for subdirectories in S3!</p> 
<h3>Summary</h3> 
<p>In this post, you used <a href="http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html">Lambda@Edge</a> to be able to use CloudFront with an S3 origin access identity and serve a default root object on subdirectory URLs. To find out some more about this use-case, see <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html">Lambda@Edge integration with CloudFront</a>&nbsp;in&nbsp;our documentation.</p> 
<p>If you have questions or suggestions, feel free to comment below. For troubleshooting or implementation help, check out the Lambda <a href="https://forums.aws.amazon.com/forum.jspa?forumID=186">forum</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2895');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/withoutcold-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Analyzing Performance for Amazon Rekognition Apps Written on AWS Lambda Using AWS X-Ray</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Bharath Kumar</span></span> | on 
<time property="datePublished" datetime="2017-10-12T11:30:41+00:00">12 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-x-ray/" title="View all posts in AWS X-Ray*"><span property="articleSection">AWS X-Ray*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/analyzing-performance-for-amazon-rekognition-apps-written-on-aws-lambda-using-aws-x-ray/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2975" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2975&amp;disqus_title=Analyzing+Performance+for+Amazon+Rekognition+Apps+Written+on+AWS+Lambda+Using+AWS+X-Ray&amp;disqus_url=https://aws.amazon.com/blogs/compute/analyzing-performance-for-amazon-rekognition-apps-written-on-aws-lambda-using-aws-x-ray/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2975');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/xray/"> AWS X-Ray </a> helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. It allows you to view, filter, and gain insights to identify issues and opportunities for optimization.</p> 
<p>X-Ray helps you to both understand the performance of your app and to annotate important parameters. In this post, I look at a sample <a href="https://aws.amazon.com/rekognition/">Amazon Rekognition</a> app that stores images in <a href="https://aws.amazon.com/s3/">Amazon S3</a>. The app calls the&nbsp;<a href="http://docs.aws.amazon.com/rekognition/latest/dg/API_RecognizeCelebrities.html"><code class="lang-json">RecognizeCelebrities</code></a> action from an <a href="https://aws.amazon.com/lambda/"> AWS Lambda </a> function. The function uses X-Ray features to analyze the app’s performance in recognizing any celebrity in a single image or in multiple images. These features include:</p> 
<li>Annotations and traces</li> 
<li>Histogram</li> 
<li>Metadata</li> 
<li>Exceptions</li> 
<p><span id="more-2975"></span></p> 
<b>Sample app overview</b> 
<p>In the <a href="https://github.com/awslabs/aws-xray-rekognition-lambda-sample">sample app</a>, you can upload images and store them in the <strong>imagestoragexray</strong> bucket. Then you can choose <strong>Recognize faces</strong> for an individual image. This calls a Lambda function, which then calls <code class="lang-json">RecognizeCelebrities</code> to recognize the celebrities in the image. The Lambda function returns to the client a comma-separated list of celebrities that it found in the image.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/architecture-1024x576-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/architecture-1024x576-1.png" /></a> 
<p class="wp-caption-text">Overview of the sample app’s architecture</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/servicemap.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/servicemap-1024x472.png" /></a> 
<p class="wp-caption-text">X-Ray service map from the sample app</p> 
<p>To search for a celebrity in the collection of images, you can type the celebrity’s name in the text box and choose <strong>Search celeb</strong>. This calls another Lambda function that runs <code class="lang-json">RecognizeCelebrities</code> on all the images in the <strong>imagestoragexray</strong> bucket. The Lambda function then returns the number of times that the celebrity was found in all the images.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/Picture3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/Picture3.png" /></a> 
<p class="wp-caption-text">Example of a sample app webpage with stored images</p> 
<b>Annotations and traces</b> 
<p>Annotations are indexed for grouping traces in the X-Ray console, based on parameters stored in the annotations. Annotations help you determine the performance of particular images.</p> 
<p>Using annotations to record the celebrity image file name, you can filter to see traces for performance of your app on specific images. For example, all the celebrities recognized in the analyzed images are annotated with the average confidence with which Amazon Rekognition matched the celebrity. The number of faces found in an image is also annotated.</p> 
<p>Resourceful annotations such as these provide you with a comprehensive outlook about your app’s performance.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/annotations.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/annotations-1024x388.png" /></a> 
<p class="wp-caption-text">X-Ray annotation in the sample app</p> 
<h3>Understanding Lambda function cold starts</h3> 
<p>When a Lambda function is invoked for the first time, or after it’s updated, Lambda launches a container based on the configuration settings that you provided. It takes time to set up a container and do the necessary bootstrapping, also known as a cold start.</p> 
<p>The latency of a Lambda function is the amount of time between when a request starts and when it completes. Due to the cold start behavior, Lambda function executions may take longer on the first invocation or after a function has been updated. For subsequent invocations, Lambda tries to reuse the container to reduce the latency.</p> 
<p>X-Ray helps you trace the cold start time as well as the overall latency for a Lambda function. In the following screenshot showing the 3.5-second duration of the Lambda function, you see that it took 473 milliseconds to initialize the function. For more information about execution time, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html">AWS Lambda: How It Works</a>.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/coldstart.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/coldstart-1024x441.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing initialization time for the Lambda function</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/withoutcold-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/withoutcold-1-1024x389.png" /></a> 
<p class="wp-caption-text">X-Ray trace on subsequent invocations of the Lambda function without the cold start</p> 
<h3>Determining the time for image analysis on celebrities</h3> 
<p>You can determine that the time Amazon Rekognition takes differs with the number of celebrity images in the photo. For example, a photo without a celebrity takes a shorter time to process, compared to a photo with a celebrity that Amazon Rekognition was able to recognize. In the following screenshots, you can see that the Lambda function that calls Amazon Rekognition to find images with Steve Jobs took 1.7 seconds to complete. This is compared to 858 milliseconds, using the same Lambda function for an image with a random landscape</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobslandscape.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobslandscape-1024x189.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the time that a Lambda function takes to call Amazon Rekognition and return</p> 
<p>By choosing the particular traces that show up in the trace list for your annotation filter, you can investigate further. Specifically, you can look at the performance of your Amazon Rekognition call.</p> 
<p>Amazon Rekognition took 1.3 seconds to analyze and provide results for the image with Steve Jobs. However, it took only 829 milliseconds to analyze and provide results for the random landscape image.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobstrace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobstrace-1024x424.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the time that Amazon Rekognition takes to analyze the Steve Jobs image</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/landscapetrace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/landscapetrace-1024x409.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the time that Amazon Rekognition takes to analyze the random landscape image</p> 
<h3>Determining the time for image analysis on multiple faces</h3> 
<p>X-Ray can also help you to look at the performance of your app for images with multiple faces. Using the annotations feature, you can record the number of faces that Amazon Rekognition recognized in the image. Through this annotation, you can filter traces for images with a specific number of face counts that were recognized or not recognized in Amazon Rekognition, as shown in the following screenshot.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/multiplefaces-facecount.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/multiplefaces-facecount-1024x221.png" /></a> 
<p class="wp-caption-text">X-Ray annotations for face counts in images recognized by Amazon Rekognition</p> 
<p>In the images below, notice that Amazon Rekognition takes more time to analyze images that contain more faces. For example, Amazon Rekognition took 8.9 seconds to analyze an image with 15 faces compared to only 6.0 seconds for an image with 5 faces. You can use the filter expression <em>Annotation.Facecount &gt; “5”</em> to view requests for which Amazon Rekognition recognized more than 5 faces.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount15trace-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount15trace-1-1024x405.png" /></a> 
<p class="wp-caption-text">X-Ray traces for an image with 15 face counts</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount5trace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount5trace-1024x430.png" /></a> 
<p class="wp-caption-text">X-Ray traces for an image with 5 face counts</p> 
<b>Histograms</b> 
<p>When you select a node or edge on an X-Ray service map, the console shows a latency distribution histogram. It shows duration on the x-axis, and the percentage of requests that match each duration on the y-axis. Using this histogram, you can look at calls that have a high latency and try to improve their performance.</p> 
<h3>Using the latency histogram</h3> 
<p>In your service map, you can choose individual nodes to look at the response latency for calls. When you choose the Amazon Rekognition node AWS::rekognition in the service map, you can see the corresponding response latency from Amazon Rekognition, as shown in the following screenshot.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/xraylatency.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/xraylatency-1024x783.png" /></a> 
<p class="wp-caption-text">X-Ray response distribution histogram for response latency</p> 
<p>Notice that the response latency increases with the number of face counts in the image. As you noted previously, image analysis time also increases with the face count increase.</p> 
<p>For example, the random landscape image’s response latency is less than 1 second, whereas calls to Amazon Rekognition for images with one face count have a response latency between 1 second and 1.5 seconds. Images with multiple face counts have a much higher response latency, greater than 1.5 seconds.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/between1and15.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/between1and15-1024x485.png" /></a> 
<p class="wp-caption-text">X-Ray traces for images with a response latency between 1 second and 1.5 seconds</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/highlatencyxraytraces.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/highlatencyxraytraces-1024x784.png" /></a> 
<p class="wp-caption-text">X-Ray response distribution histogram for a higher response latency</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/histogram.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/histogram-1024x490.png" /></a> 
<p class="wp-caption-text">X-Ray traces for images with multiple face counts that have a higher response latency</p> 
<b>Metadata</b> 
<p>The metadata feature in X-Ray enables you to store app information for later reference. However, you cannot filter out traces based on the metadata. The sample app stores the response from Amazon Rekognition in metadata for your reference.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/metadata.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/metadata-1024x549.png" /></a> 
<p class="wp-caption-text">X-Ray metadata in the sample app</p> 
<b>Exceptions</b> 
<p>X-Ray also provides information on exceptions from other AWS services in the trace. For the sample app, Amazon Rekognition responded back with an exception when the RecognizeCelebrities call was made on a non-image file. You can also filter requests that had an exception in Amazon Rekognition, using the following expression:<br /> <code>service(&quot;rekognition&quot;) { error = true }</code></p> 
<p>For more information, see <a href="http://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html">Searching for Traces in the AWS X-Ray Console with Filter Expressions</a>.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptiontrace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptiontrace-1024x448.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing an error from Amazon Rekognition</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptioninfo.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptioninfo.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the Amazon Rekognition exception in more detail</p> 
<b>Summary</b> 
<p>X-Ray helps you analyze your app and its performance. Through the annotations feature, you can index and filter specific traces. Annotating specific parameters in your app helps you get a comprehensive overview of your app’s performance specific to these parameters. The histogram helps you analyze calls with high response latency and gives you visibility into the time that individual services take. The metadata feature helps you store any relevant information that might be useful to improve your app’s performance. X-Ray also provides information on exceptions that happened on a particular trace to an AWS service.</p> 
<p>Overall, by using X-Ray you can both improve performance of your app and understand the underlying services that your app uses. Use this information to troubleshoot and improve specific portions of your app, and save both time and money.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2975');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Clean up Your Container Images with Amazon ECR Lifecycle Policies</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-10-11T16:33:49+00:00">11 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/clean-up-your-container-images-with-amazon-ecr-lifecycle-policies/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3025" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3025&amp;disqus_title=Clean+up+Your+Container+Images+with+Amazon+ECR+Lifecycle+Policies&amp;disqus_url=https://aws.amazon.com/blogs/compute/clean-up-your-container-images-with-amazon-ecr-lifecycle-policies/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3025');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post comes from the desk of Brent Langston.</em></p> 
<p>—</p> 
<p>Starting today, customers can keep their container image repositories tidy by automatically removing old or unused images using <strong>lifecycle policies</strong>, now available as part of <a href="https://aws.amazon.com/ecr">Amazon EC2 Container Registry (Amazon ECR)</a>.</p> 
<p>Amazon ECR is a fully managed Docker container registry that makes it easy to store manage and deploy Docker container images without worrying about the typical challenges of scaling a service to handle pulling hundreds of images at one time. This scale means that development teams using Amazon ECR actively often find that their repositories fill up with many container image versions. This makes it difficult to find the code changes that matter and incurs unnecessary storage costs. Previously, cleaning up your repository meant spending time to manually delete old images, or writing and executing scripts.</p> 
<p>Now, lifecycle policies allow you to define a set of rules to remove old container images automatically. You can also preview rules to see exactly which container images are affected when the rule runs. This allows repositories to be better organized, makes it easier to find the code revisions that matter, and lowers storage costs.</p> 
<p>Let’s take a look&nbsp;at how lifecycle policies work.</p> 
<p><span id="more-3025"></span></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/ecr-lifecyclepolicies.png" /></p> 
<b>Ground&nbsp;Rules</b> 
<p>One of the biggest benefits of deploying code in containers is the ability to quickly and easily roll back to a previous version. You can deploy with less risk because, if something goes wrong, it is easy to revert back to the previous container version and know that your application will run like it did before the failed deployment. Most people probably never roll back past a few versions. If your situation is similar, then one simple lifecycle rule might be to just keep the last 30 images.</p> 
<h3>Last 30 Images</h3> 
<p>In your ECR registry, choose <strong>Dry-Run Lifecycle Rules, Add</strong>.</p> 
<li>For <strong>Image Status</strong>, select <strong>Untagged</strong>.</li> 
<li>Under <strong>Match criteria</strong>, for <strong>Count Type</strong>, enter <strong>Image Count More Than</strong>.</li> 
<li>For <strong>Count Number</strong>, enter <strong>30</strong>.</li> 
<li>For <strong>Rule action</strong>, choose <strong>expire</strong>.</li> 
<p>Choose <strong>Save</strong>. To see which images would be cleaned up, <strong>Save and dry-run rules</strong>.</p> 
<p>Of course, there are teams who, for compliance reasons, might prefer to keep certain images for a period of time, rather than keeping by count. For that situation, you can choose to clean up images older than 90 days.</p> 
<h3>Last 90 Days</h3> 
<p>Select the rule that you just created and choose Edit. Change the parameters to keep only 90 days of untagged images:</p> 
<li>Under <strong>Match criteria</strong>, for <strong>Count Type</strong>, enter <strong>Since Image Pushed</strong></li> 
<li>For <strong>Count Number</strong>, enter <strong>90</strong>.</li> 
<li>For <strong>Count Unit</strong>, enter <strong>days</strong>.</li> 
<b>Tags</b> 
<p>Certainly 90 days is an arbitrary timeframe, and your team might have policies in place that would require a longer timeframe for certain kinds of images. If that’s the case, but you still want to continue with the spring cleaning, you can consider getting rid of images that are tag prefixed.</p> 
<p>Here is the list of rules I came up with to groom untagged, development, staging, and production images:</p> 
<li>Remove untagged images over 90 days old</li> 
<li>Remove development tagged images over 90 days old</li> 
<li>Remove staging tagged images over 180 days old</li> 
<li>Remove production tagged images over 1 year old</li> 
<p>As you can see, the new Amazon ECR lifecycle policies are powerful, and help you easily keep the images you need, while cleaning out images you may never use again. This feature is available starting today, in all <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">regions where Amazon ECR is available</a>, at no extra charge. For more information, see <a href="http://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html">Amazon ECR Lifecycle Policies</a> in the AWS&nbsp;technical documentation.</p> 
<p>— Brent<br /> <a href="https://twitter.com/brentContained">@brentContained</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3025');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture1-3-629x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Improved Testing on the AWS Lambda Console</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Orr Weinstein</span></span> | on 
<time property="datePublished" datetime="2017-10-02T11:10:46+00:00">02 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/improved-testing-on-the-aws-lambda-console/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2928" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2928&amp;disqus_title=Improved+Testing+on+the+AWS+Lambda+Console&amp;disqus_url=https://aws.amazon.com/blogs/compute/improved-testing-on-the-aws-lambda-console/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2928');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>(This post has been written by Chris Tate, SDE on the Lambda Console team)</p> 
<p>Today, <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> released three console enhancements:</p> 
<li>A quicker creation flow that lets you quickly create a function with the minimum working configuration, so that you can start iterating faster.</li> 
<li>A streamlined configuration page with Lambda function settings logically grouped into cards, which makes locating and making changes much easier.</li> 
<li>Persisting multiple events to help test your function.</li> 
<p>This post focuses on persisting test events, and I discuss how I’ve been using this new feature. Now when you are testing on the Lambda console, you can save up to 10 test events per function, and each event can be up to 6 megabytes in size, the maximum payload size for synchronous invocations. The events are saved for the logged-in user, so that two different users in the same account have their own set of events.</p> 
<b>Testing Lambda functions</b> 
<p>As a Lambda console developer, when I work on side projects at home, I sometimes use our development server. I’ve been using this new feature to test a Lambda function in one of my projects. The function is probably more complicated than it should be, because it can be triggered by an Alexa skill, Amazon CloudWatch schedule, or an Amazon API Gateway API. If you have had a similarly complicated function, you may have run into the same problem I did: &nbsp;How do you test?</p> 
<p>For quick testing, I used the console but the console used to save only one test event at a time. To work around this, my solution was a text file with three different JSON events, one for each trigger. I would copy whatever event I needed into the Lambda console, tweak it, and choose Test. This would become particularly annoying when I wanted to quickly test all three.</p> 
<p>I also switch between my laptop and desktop depending on my mood. For that reason, I needed to make sure this text file with the events were shared in some way, as the console only locally saved one test event to the current browser. But now you don’t have to worry about any of that.</p> 
<b>Walkthrough</b> 
<p>In the Lambda console, go to the detail page of any function, and select <strong>Configure test events</strong> from the test events dropdown (the dropdown beside the orange test button). In the dialog box, you can manage 10 test events for your function. First, paste your Alexa trigger event in the dialog box and type an event name, such as AlexaTrigger.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture2-2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture2-2.png" /></a></p> 
<p>Choose <strong>Create</strong>. After it saves, you see AlexaTrigger in the Test list.</p> 
<p>When you open the dialog box again by choosing <strong>Configure test events</strong>, you are in edit mode.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture1-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture1-3.png" /></a></p> 
<p>To add another event, choose <strong>Create new test event</strong>. Now you can choose from a list of templates or any of your previously saved test events. This is very useful for a couple of reasons:</p> 
<li>First, when you want to slightly tweak one of your existing events and still keep the earlier version intact.</li> 
<li>Second, when you are not sure how to structure a particular event from an event source. You can use one of the sample event templates and tweak them to your needs. Skip it when you know what your event should be.</li> 
<p>Paste in your CloudWatch schedule event, give it a name, and choose <strong>Create</strong>. Repeat for API Gateway.</p> 
<p>Now that you have three events saved, you can quickly switch between them and repeatedly test. Furthermore, if you’re on your desktop but you created the test events on your laptop, there’s no problem. You can still see all your events and you can switch back and forth seamlessly between different computers.</p> 
<b>Conclusion</b> 
<p>This feature should allow you to more easily test your Lambda functions through the console. If you have more suggestions, add a comment to this post or submit feedback through the console. We actually read the feedback, believe it!</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2928');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Creating a Cost-Efficient Amazon ECS Cluster for Scheduled Tasks</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-09-28T10:44:39+00:00">28 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/creating-a-cost-efficient-amazon-ecs-cluster-for-scheduled-tasks/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2947" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2947&amp;disqus_title=Creating+a+Cost-Efficient+Amazon+ECS+Cluster+for+Scheduled+Tasks&amp;disqus_url=https://aws.amazon.com/blogs/compute/creating-a-cost-efficient-amazon-ecs-cluster-for-scheduled-tasks/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2947');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/28/mperi_profile.jpeg" /></td> 
</tr> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><b>Madhuri Peri</b><br /> Sr. DevOps Consultant</td> 
</tr> 
</tbody> 
</table> 
<p>When you use Amazon Relational Database Service (<a href="https://aws.amazon.com/rds/">Amazon RDS</a>), depending on the logging levels on the RDS instances and the volume of transactions, you could generate a lot of log data. To ensure that everything is running smoothly, many customers search for log error patterns using different log aggregation and visualization systems, such as Amazon Elasticsearch Service, Splunk, or other tool of their choice. A module needs to periodically retrieve the RDS logs using the SDK, and then send them to Amazon S3. From there, you can stream them to your log aggregation tool.</p> 
<p>One option is writing an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function to retrieve the log files. However, because of the time that this function needs to execute, depending on the volume of log files retrieved and transferred, it is possible that Lambda could time out on many instances.&nbsp; Another approach is launching an Amazon EC2 instance that runs this job periodically. However, this would require you to run an EC2 instance continuously, not an optimal use of time or money.</p> 
<p>Using&nbsp;the new Amazon CloudWatch integration with Amazon EC2 Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>), you&nbsp;can&nbsp;trigger this job to run in a container on an existing Amazon ECS cluster. Additionally, this would allow you to improve costs by running containers on a fleet of Spot Instances.</p> 
<p>In this post, I will show you how to use the new scheduled tasks (<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduled_tasks.html">cron</a>) feature in Amazon ECS and launch tasks&nbsp;using CloudWatch events, while leveraging Spot Fleet to maximize availability and cost optimization for containerized workloads.</p> 
<p><span id="more-2947"></span></p> 
<b>Architecture</b> 
<p>The following diagram shows how the various components described schedule a task that retrieves log files from&nbsp;Amazon RDS database instances, and deposits the logs into an S3 bucket.</p> 
<p>Amazon ECS cluster container instances are using Spot Fleet, which is a perfect match for the workload that needs to run when it can. This improves cluster costs.</p> 
<p>The task definition defines which Docker image to retrieve from the Amazon&nbsp;EC2 Container Registry (Amazon ECR)&nbsp;repository and run on the Amazon ECS cluster.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/917_scheduledtasks-arch-1024x790.png" /></p> 
<p>The container image has Python code functions to make AWS API calls using boto3. It iterates over the RDS database instances, retrieves the logs, and deposits them in the S3 bucket. Many customers choose these logs to be delivered to their centralized log-store. CloudWatch Events defines the schedule for when the container task has to be launched.</p> 
<b>Walkthrough</b> 
<p>To provide the basic framework, we have built an AWS CloudFormation <a href="https://github.com/awslabs/aws-ecs-scheduled-tasks/blob/master/cloudformation/ecs-spot-fleet.yaml">template</a> that creates the following resources:</p> 
<li>Amazon ECR repository for storing the Docker image to be used in the task definition</li> 
<li>S3 bucket that holds the transferred logs</li> 
<li>Task definition, with image name and S3 bucket as environment variables provided via input parameter</li> 
<li>CloudWatch Events rule</li> 
<li>Amazon ECS cluster</li> 
<li>Amazon ECS container instances using Spot Fleet</li> 
<li>IAM roles required for the container instance profiles</li> 
<h3>Before you begin</h3> 
<p>Ensure that <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git</a>, <a href="https://docs.docker.com/engine/installation/">Docker</a>, and the <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">AWS CLI</a> are installed on your computer.</p> 
<p>In your AWS account, instantiate one Amazon Aurora instance using the console. For more information, see <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.CreateInstance.html">Creating an Amazon Aurora DB Cluster</a>.</p> 
<h3>Implementation Steps</h3> 
<ol> 
<li>Clone the code from GitHub that performs RDS API calls to retrieve the log files.<br /> <code>git clone https://github.com/awslabs/aws-ecs-scheduled-tasks.git</code></li> 
<li>Build and tag the image.<br /> <code>cd aws-ecs-scheduled-tasks/container-code/src &amp;&amp; ls</code><p></p> <code class="lang-bash">Dockerfile		rdslogsshipper.py	requirements.txt</code> <p><code>docker build -t rdslogsshipper .</code></p> <code class="lang-bash">Sending build context to Docker daemon 9.728 kB
Step 1 : FROM python:3
---&gt; 41397f4f2887
Step 2 : WORKDIR /usr/src/app
---&gt; Using cache
---&gt; 59299c020e7e
Step 3 : COPY requirements.txt ./
---&gt; 8c017e931c3b
Removing intermediate container df09e1bed9f2
Step 4 : COPY rdslogsshipper.py /usr/src/app
---&gt; 099a49ca4325
Removing intermediate container 1b1da24a6699
Step 5 : RUN pip install --no-cache-dir -r requirements.txt
---&gt; Running in 3ed98b30901d
Collecting boto3 (from -r requirements.txt (line 1))
Downloading boto3-1.4.6-py2.py3-none-any.whl (128kB)
Collecting botocore (from -r requirements.txt (line 2))
Downloading botocore-1.6.7-py2.py3-none-any.whl (3.6MB)
Collecting s3transfer&lt;0.2.0,&gt;=0.1.10 (from boto3-&gt;-r requirements.txt (line 1))
Downloading s3transfer-0.1.10-py2.py3-none-any.whl (54kB)
Collecting jmespath&lt;1.0.0,&gt;=0.7.1 (from boto3-&gt;-r requirements.txt (line 1))
Downloading jmespath-0.9.3-py2.py3-none-any.whl
Collecting python-dateutil&lt;3.0.0,&gt;=2.1 (from botocore-&gt;-r requirements.txt (line 2))
Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
Collecting docutils&gt;=0.10 (from botocore-&gt;-r requirements.txt (line 2))
Downloading docutils-0.14-py3-none-any.whl (543kB)
Collecting six&gt;=1.5 (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore-&gt;-r requirements.txt (line 2))
Downloading six-1.10.0-py2.py3-none-any.whl
Installing collected packages: six, python-dateutil, docutils, jmespath, botocore, s3transfer, boto3
Successfully installed boto3-1.4.6 botocore-1.6.7 docutils-0.14 jmespath-0.9.3 python-dateutil-2.6.1 s3transfer-0.1.10 six-1.10.0
---&gt; f892d3cb7383
Removing intermediate container 3ed98b30901d
Step 6 : COPY . .
---&gt; ea7550c04fea
Removing intermediate container b558b3ebd406
Successfully built ea7550c04fea</code> </li> 
<li>Run the <a href="https://github.com/awslabs/aws-ecs-scheduled-tasks.git">CloudFormation stack</a> and get the names for the Amazon ECR repo and S3 bucket. In the stack, choose <b>Outputs</b>.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/917_scheduledtasks-cf.png" /></li> 
<li>Open the ECS console and choose <b>Repositories</b>. The rdslogs repo has been created. Choose <b>View Push Commands</b> and follow the instructions to connect to the repository and push the image for the code that you built in Step 2. The screenshot shows the final result:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/917_scheduledtasks-cf2.png" /></li> 
<li>Associate the CloudWatch scheduled task with the created Amazon ECS Task Definition, using a new CloudWatch event rule that is scheduled to run at intervals. The following rule is scheduled to run every 15 minutes:<br /> <code>aws --profile default --region us-west-2 events put-rule --name demo-ecs-task-rule&nbsp; --schedule-expression &quot;rate(15 minutes)&quot;</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;RuleArn&quot;: &quot;arn:aws:events:us-west-2:12345678901:rule/demo-ecs-task-rule&quot;
}</code> </li> 
<li>CloudWatch requires IAM permissions to place a task on the Amazon ECS cluster when the CloudWatch event rule is executed, in addition to an IAM role that can be assumed by CloudWatch Events. This is done in three steps: 
<ol> 
<li>Create the IAM role to be assumed by CloudWatch.<br /> <code>aws --profile default --region us-west-2 iam create-role --role-name Test-Role --assume-role-policy-document file://event-role.json</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;Role&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;AssumeRolePolicyDocument&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Version&quot;: &quot;2012-10-17&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Statement&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Action&quot;: &quot;sts:AssumeRole&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Effect&quot;: &quot;Allow&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Principal&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Service&quot;: &quot;events.amazonaws.com&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;RoleId&quot;: &quot;AROAIRYYLDCVZCUACT7FS&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;CreateDate&quot;: &quot;2017-07-14T22:44:52.627Z&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;RoleName&quot;: &quot;Test-Role&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Path&quot;: &quot;/&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Arn&quot;: &quot;arn:aws:iam::12345678901:role/Test-Role&quot;
&nbsp; &nbsp; }
}</code> <p>The following is an example of the event-role.json&nbsp;file used earlier:</p> <code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;events.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}</code> </li> 
<li>Create the IAM policy defining the ECS cluster and task definition. You need to get these values from the CloudFormation outputs and resources.<br /> <code>aws --profile default --region us-west-2 iam create-policy --policy-name test-policy --policy-document file://event-policy.json</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;Policy&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;PolicyName&quot;: &quot;test-policy&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;CreateDate&quot;: &quot;2017-07-14T22:51:20.293Z&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;AttachmentCount&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;IsAttachable&quot;: true,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;PolicyId&quot;: &quot;ANPAI7XDIQOLTBUMDWGJW&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;DefaultVersionId&quot;: &quot;v1&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Path&quot;: &quot;/&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Arn&quot;: &quot;arn:aws:iam::123455678901:policy/test-policy&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;UpdateDate&quot;: &quot;2017-07-14T22:51:20.293Z&quot;
&nbsp; &nbsp; }
}</code> <p>The following is an example of the event-policy.json&nbsp;file used earlier:</p> <code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ecs:RunTask&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:ecs:*::task-definition/&quot;
],
&quot;Condition&quot;: {
&quot;ArnLike&quot;: {
&quot;ecs:cluster&quot;: &quot;arn:aws:ecs:*::cluster/&quot;
}
}
}
]
}</code> </li> 
<li>Attach the IAM policy to the role.<br /> <code>aws --profile default --region us-west-2 iam attach-role-policy --role-name Test-Role --policy-arn arn:aws:iam::1234567890:policy/test-policy</code></li> 
</ol> </li> 
<li>Associate the CloudWatch rule created earlier to place the task on the ECS cluster. The following command shows an example. Replace the AWS account ID and region with your settings.<br /> <code>aws events put-targets --rule demo-ecs-task-rule --targets &quot;Id&quot;=&quot;1&quot;,&quot;Arn&quot;=&quot;arn:aws:ecs:us-west-2:12345678901:cluster/test-cwe-blog-ecsCluster-15HJFWCH4SP67&quot;,&quot;EcsParameters&quot;={&quot;TaskDefinitionArn&quot;=&quot;arn:aws:ecs:us-west-2:12345678901:task-definition/test-cwe-blog-taskdef:8&quot;},&quot;RoleArn&quot;=&quot;arn:aws:iam::12345678901:role/Test-Role&quot;</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;FailedEntries&quot;: [],&nbsp;
&nbsp; &nbsp; &quot;FailedEntryCount&quot;: 0
}</code> </li> 
</ol> 
<p>That’s it. The logs now run based on the defined schedule.</p> 
<p>To test this, open the Amazon ECS console, select the Amazon ECS cluster that you created, and then choose <b>Tasks, Run New Task</b>. Select the task definition created by the CloudFormation template, and the cluster should be selected automatically. As this runs, the S3 bucket should be populated with the RDS logs for the instance.</p> 
<b>Conclusion</b> 
<p>In this post, you’ve seen that the choices for workloads that need to run at a scheduled time include Lambda with CloudWatch events or EC2 with cron. However, sometimes the job could run outside of Lambda execution time limits or be not cost-effective for an EC2 instance.</p> 
<p>In such cases, you can schedule the tasks on an ECS cluster using CloudWatch rules. In addition, you can use a Spot Fleet cluster with Amazon ECS for cost-conscious workloads that do not have hard requirements on execution time or instance availability in the Spot Fleet. For more information, see <a href="https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/">Powering your Amazon ECS Cluster with Amazon EC2 Spot Instances and Scheduled Events</a>.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<p>— Madhuri</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2947');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using Enhanced Request Authorizers in Amazon API Gateway</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Stefano Buliani</span></span> | on 
<time property="datePublished" datetime="2017-09-27T10:05:56+00:00">27 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/" title="View all posts in Security, Identity, &amp; Compliance*"><span property="articleSection">Security, Identity, &amp; Compliance*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/using-enhanced-request-authorizers-in-amazon-api-gateway/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2917" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2917&amp;disqus_title=Using+Enhanced+Request+Authorizers+in+Amazon+API+Gateway&amp;disqus_url=https://aws.amazon.com/blogs/compute/using-enhanced-request-authorizers-in-amazon-api-gateway/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2917');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Recently, AWS introduced a new type of authorizer in <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway</a>, <a href="https://aws.amazon.com/about-aws/whats-new/2017/09/amazon-api-gateway-now-supports-enhanced-request-authorizers/" target="_blank" rel="noopener noreferrer">enhanced request authorizers</a>. Previously, custom authorizers received only the bearer token included in the request and the ARN of the API Gateway method being called. Enhanced request authorizers receive all of the headers, query string, and path parameters as well as the request context. This enables you to make more sophisticated authorization decisions based on parameters such as the client IP address, user agent, or a query string parameter alongside the client bearer token.</p> 
<h3>Enhanced request authorizer configuration</h3> 
<p>From the API Gateway console, you can declare a new enhanced request authorizer by selecting the <strong>Request</strong> option as the AWS Lambda event payload:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-10.13.18-AM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-10.13.18-AM-215x300.png" /></a></p> 
<p>&nbsp;</p> 
<p>Just like normal custom authorizers, API Gateway can cache the policy returned by your Lambda function. With enhanced request authorizers, however, you can also specify the values that form the unique key of a policy in the cache. For example, if your authorization decision is based on both the bearer token and the IP address of the client, both values should be part of the unique key in the policy cache. The identity source parameter lets you specify these values as <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html" target="_blank" rel="noopener noreferrer">mapping expressions</a>:</p> 
<li>The bearer token appears in the Authorization header</li> 
<li>The client IP address is stored in the sourceIp parameter of the request context.</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-1.54.04-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-1.54.04-PM-300x163.png" /></a></p> 
<p>&nbsp;</p> 
<h3>Using enhanced request authorizers with Swagger</h3> 
<p>You can also define enhanced request authorizers in your Swagger (<a href="https://www.openapis.org" target="_blank" rel="noopener noreferrer">Open API</a>) definitions. In the following example, you can see that all of the options configured in the API Gateway console are available as custom extensions in the API definition. For example, the <i>identitySource</i> field is a comma-separated list of mapping expressions.</p> 
<code class="lang-yaml">securityDefinitions:
IpAuthorizer:
type: &quot;apiKey&quot;
name: &quot;IpAuthorizer&quot;
in: &quot;header&quot;
x-amazon-apigateway-authtype: &quot;custom&quot;
x-amazon-apigateway-authorizer:
authorizerResultTtlInSeconds: 300
identitySource: &quot;method.request.header.Authorization, context.identity.sourceIp&quot;
authorizerUri: &quot;arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:XXXXXXXXXX:function:py-ip-authorizer/invocations&quot;
type: &quot;request&quot;
</code> 
<p>After you have declared your authorizer in the security definitions section, you can use it in your API methods:</p> 
<code class="lang-yaml">---
swagger: &quot;2.0&quot;
info:
title: &quot;request-authorizer-demo&quot;
basePath: &quot;/dev&quot;
paths:
/hello:
get:
security:
- IpAuthorizer: []
...
</code> 
<h3>Enhanced request authorizer Lambda functions</h3> 
<p>Enhanced request authorizer <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda</a> functions receive an event object that is similar to proxy integrations. It contains all of the information about a request, excluding the body.</p> 
<code class="lang-json">{
&quot;methodArn&quot;: &quot;arn:aws:execute-api:us-east-1:XXXXXXXXXX:xxxxxx/dev/GET/hello&quot;,
&quot;resource&quot;: &quot;/hello&quot;,
&quot;requestContext&quot;: {
&quot;resourceId&quot;: &quot;xxxx&quot;,
&quot;apiId&quot;: &quot;xxxxxxxxx&quot;,
&quot;resourcePath&quot;: &quot;/hello&quot;,
&quot;httpMethod&quot;: &quot;GET&quot;,
&quot;requestId&quot;: &quot;9e04ff18-98a6-11e7-9311-ef19ba18fc8a&quot;,
&quot;path&quot;: &quot;/dev/hello&quot;,
&quot;accountId&quot;: &quot;XXXXXXXXXXX&quot;,
&quot;identity&quot;: {
&quot;apiKey&quot;: &quot;&quot;,
&quot;sourceIp&quot;: &quot;58.240.196.186&quot;
},
&quot;stage&quot;: &quot;dev&quot;
},
&quot;queryStringParameters&quot;: {},
&quot;httpMethod&quot;: &quot;GET&quot;,
&quot;pathParameters&quot;: {},
&quot;headers&quot;: {
&quot;cache-control&quot;: &quot;no-cache&quot;,
&quot;x-amzn-ssl-client-hello&quot;: &quot;AQACJAMDAAAAAAAAAAAAAAAAAAAAAAAAAAAA…&quot;,
&quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,
&quot;X-Forwarded-For&quot;: &quot;54.240.196.186, 54.182.214.90&quot;,
&quot;Accept&quot;: &quot;*/*&quot;,
&quot;User-Agent&quot;: &quot;PostmanRuntime/6.2.5&quot;,
&quot;Authorization&quot;: &quot;hello&quot;
},
&quot;stageVariables&quot;: {},
&quot;path&quot;: &quot;/hello&quot;,
&quot;type&quot;: &quot;REQUEST&quot;
}
</code> 
<p>The following enhanced request authorizer snippet is written in Python and compares the source IP address against a list of valid IP addresses. The comments in the code explain what happens in each step.</p> 
<code class="lang-python">...
VALID_IPS = [&quot;58.240.195.186&quot;, &quot;201.246.162.38&quot;]
def lambda_handler(event, context):
# Read the client’s bearer token.
jwtToken = event[&quot;headers&quot;][&quot;Authorization&quot;]
# Read the source IP address for the request form 
# for the API Gateway context object.
clientIp = event[&quot;requestContext&quot;][&quot;identity&quot;][&quot;sourceIp&quot;]
# Verify that the client IP address is allowed.
# If it’s not valid, raise an exception to make sure
# that API Gateway returns a 401 status code.
if clientIp not in VALID_IPS:
raise Exception('Unauthorized')
# Only allow hello users in!
if not validate_jwt(userId):
raise Exception('Unauthorized')
# Use the values from the event object to populate the 
# required parameters in the policy object.
policy = AuthPolicy(userId, event[&quot;requestContext&quot;][&quot;accountId&quot;])
policy.restApiId = event[&quot;requestContext&quot;][&quot;apiId&quot;]
policy.region = event[&quot;methodArn&quot;].split(&quot;:&quot;)[3]
policy.stage = event[&quot;requestContext&quot;][&quot;stage&quot;]
# Use the scopes from the bearer token to make a 
# decision on which methods to allow in the API.
policy.allowMethod(HttpVerb.GET, '/hello')
# Finally, build the policy.
authResponse = policy.build()
return authResponse
...
</code> 
<h3>Conclusion</h3> 
<p>API Gateway customers build complex APIs, and authorization decisions often go beyond the simple properties in a JWT token. For example, users may be allowed to call the “list cars” endpoint but only with a specific subset of filter parameters. With enhanced request authorizers, you have access to all request parameters. You can centralize all of your application’s access control decisions in a Lambda function, making it easier to manage your application security.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2917');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Automating Amazon EBS Snapshot Management with AWS Step Functions and Amazon CloudWatch Events</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-09-18T13:06:35+00:00">18 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-elastic-block-storage-ebs/" title="View all posts in Amazon Elastic Block Storage (EBS)*"><span property="articleSection">Amazon Elastic Block Storage (EBS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2753" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2753&amp;disqus_title=Automating+Amazon+EBS+Snapshot+Management+with+AWS+Step+Functions+and+Amazon+CloudWatch+Events&amp;disqus_url=https://aws.amazon.com/blogs/compute/automating-amazon-ebs-snapshot-management-with-aws-step-functions-and-amazon-cloudwatch-events/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2753');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Brittany.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Brittany.jpeg" /></a></p> 
<p><strong>Brittany Doncaster, Solutions Architect</strong></p> 
<p>Business continuity is important for building mission-critical workloads on AWS. As an AWS customer, you might define recovery point objectives (RPO) and recovery time objectives (RTO) for different tier applications in your business. After the RPO and RTO requirements are defined, it is up to your architects to determine how to meet those requirements.</p> 
<p>You probably store persistent data in <a href="https://aws.amazon.com/ebs/">Amazon EBS</a> volumes, which live within a single Availability Zone. And, following best practices, you take snapshots of your EBS volumes to back up the data on <a href="https://aws.amazon.com/s3">Amazon S3</a>, which provides 11 9's of durability. If you are following these best practices, then you've probably recognized the need to manage the number of snapshots you keep for a particular EBS volume and delete older, unneeded snapshots. Doing this cleanup helps save on storage costs.</p> 
<p>Some customers also have policies stating that backups need to be stored a certain number of miles away as part of a disaster recovery (DR) plan. To meet these requirements, customers copy their EBS snapshots to the DR region. Then, the same snapshot management and cleanup has to also be done in the DR region.</p> 
<p>All of this snapshot management logic consists of different components. You would first tag your snapshots so you could manage them. Then, determine how many snapshots you currently have for a particular EBS volume and assess that value against a retention rule. If the number of snapshots was greater than your retention value, then you would clean up old snapshots. And finally, you might copy the latest snapshot to your DR region. All these steps are just an example of a simple snapshot management workflow. But how do you automate something like this in AWS? How do you do it without servers?</p> 
<p>One of the most powerful AWS services released in 2016 was <a href="http://aws.amazon.com/cloudwatch">Amazon CloudWatch Events</a>. It enables you to build event-driven IT automation, based on events happening within your AWS infrastructure. CloudWatch Events integrates with <a href="https://aws.amazon.com/lambda">AWS Lambda</a> to let you execute your custom code when one of those events occurs. However, the actions to take based on those events aren't always composed of a single Lambda function. Instead, your business logic may consist of multiple steps (like in the case of the example snapshot management flow described earlier). And you may want to run those steps in sequence or in parallel. You may also want to have retry logic or exception handling for each step.</p> 
<p><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> serves just this purpose―to help you coordinate your functions and microservices. Step Functions enables you to simplify your effort and pull the error handling, retry logic, and workflow logic out of your Lambda code. Step Functions integrates with Lambda to provide a mechanism for building complex serverless applications. Now, you can kick off a Step Functions state machine based on a CloudWatch event.</p> 
<p>In this post, I discuss how you can target Step Functions in a CloudWatch Events rule. This allows you to have event-driven snapshot management based on snapshot completion events firing in CloudWatch Event rules.</p> 
<p>As an example of what you could do with Step Functions and CloudWatch Events, we've developed a <a href="https://github.com/awslabs/aws-step-functions-ebs-snapshot-mgmt">reference architecture</a> that performs management of your EBS snapshots.</p> 
<b id="toc_1">Automating EBS Snapshot Management with Step Functions</b> 
<p>This architecture assumes that you have already <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html">set up CloudWatch Events to create the snapshots on a schedule</a> or that you are using some other means of creating snapshots according to your needs.</p> 
<p>This architecture covers the pieces of the workflow that need to happen after a snapshot has been created.</p> 
<li>It <a href="https://aws.amazon.com/blogs/aws/new-cloudwatch-events-for-ebs-snapshots/">creates a CloudWatch Events rule</a> to invoke a Step Functions state machine execution when an EBS snapshot is created.</li> 
<li>The state machine then tags the snapshot, cleans up the oldest snapshots if the number of snapshots is greater than the defined number to retain, and copies the snapshot to a DR region.</li> 
<li>When the DR region snapshot copy is completed, another state machine kicks off in the DR region. The new state machine has a similar flow and uses some of the same Lambda code to clean up the oldest snapshots that are greater than the defined number to retain.</li> 
<li>Also, both state machines demonstrate how you can use Step Functions to handle errors within your workflow. Any errors that are caught during execution result in the execution of a Lambda function that writes a message to an SNS topic. Therefore, if any errors occur, you can subscribe to the SNS topic and get notified.</li> 
<p>The following is an architecture diagram of the reference architecture:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture1_Arch.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture1_Arch.png" /></a></p> 
<b id="toc_2">Creating the Lambda functions and Step Functions state machines</b> 
<p>First, pull the code from GitHub and use the <a href="https://aws.amazon.com/cli">AWS CLI</a> to create S3 buckets for the Lambda code in the primary and DR regions. For this example, assume that the primary region is us-west-2 and the DR region is us-east-2. Run the following commands, replacing the italicized text in &lt;&gt; with your own unique bucket names.</p> 
<code class="language-none">git clone https://github.com/awslabs/aws-step-functions-ebs-snapshot-mgmt.git
cd aws-step-functions-ebs-snapshot-mgmt/
aws s3 mb s3://<i>&lt;primary region bucket name&gt;</i> --region us-west-2
aws s3 mb s3://<i>&lt;DR region bucket name&gt;</i> --region us-east-2</code> 
<p>Next, use the <a href="https://github.com/awslabs/serverless-application-model">Serverless Application Model</a> (SAM), which uses AWS CloudFormation to deploy the Lambda functions and Step Functions state machines in the primary and DR regions. Replace the italicized text in &lt;&gt; with the S3 bucket names that you created earlier.</p> 
<code class="language-none">aws cloudformation package --template-file PrimaryRegionTemplate.yaml --s3-bucket <i>&lt;primary region bucket name&gt;</i>  --output-template-file tempPrimary.yaml --region us-west-2
aws cloudformation deploy --template-file tempPrimary.yaml --stack-name ebsSnapshotMgmtPrimary --capabilities CAPABILITY_IAM --region us-west-2
aws cloudformation package --template-file DR_RegionTemplate.yaml --s3-bucket <i>&lt;DR region bucket name&gt;</i> --output-template-file tempDR.yaml  --region us-east-2
aws cloudformation deploy --template-file tempDR.yaml --stack-name ebsSnapshotMgmtDR --capabilities CAPABILITY_IAM --region us-east-2</code> 
<b id="toc_3">CloudWatch event rule verification</b> 
<p>The CloudFormation templates deploy the following resources:</p> 
<li>The Lambda functions that are coordinated by Step Functions</li> 
<li>The Step Functions state machine</li> 
<li>The SNS topic</li> 
<li>The CloudWatch Events rules that trigger the state machine execution</li> 
<p>So, all of the CloudWatch event rules have been created for you by performing the preceding commands. The next section demonstrates how you could create the CloudWatch event rule manually. To jump straight to testing the workflow, see the “Testing in your Account” section. Otherwise, you begin by setting up the CloudWatch event rule in the primary region for the createSnapshot event and also the CloudWatch event rule in the DR region for the copySnapshot command.</p> 
<p>First, open the CloudWatch console in the primary region.</p> 
<p>Choose <strong>Create Rule</strong> and create a rule for the createSnapshot command, with your newly created Step Function state machine as the target.</p> 
<p>For <strong>Event Source</strong>, choose <strong>Event Pattern</strong> and specify the following values:</p> 
<li><strong>Service Name</strong>: EC2</li> 
<li><strong>Event Type</strong>: EBS Snapshot Notification</li> 
<li><strong>Specific Event</strong>: createSnapshot</li> 
<p>For <strong>Target</strong>, choose <strong>Step Functions state machine</strong>, then choose the state machine created by the CloudFormation commands. Choose <strong>Create a new role for this specific resource</strong>. Your completed rule should look like the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture2_CWE_Rule.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture2_CWE_Rule.png" /></a></p> 
<p>Choose <strong>Configure Details</strong> and give the rule a name and description.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture3_CWE_Rule_config.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture3_CWE_Rule_config.png" /></a></p> 
<p>Choose <strong>Create Rule</strong>. You now have a CloudWatch Events rule that triggers a Step Functions state machine execution when the EBS snapshot creation is complete.</p> 
<p>Now, set up the CloudWatch Events rule in the DR region as well. This looks almost same, but is based off the copySnapshot event instead of createSnapshot.</p> 
<p>In the upper right corner in the console, switch to your DR region. Choose <strong>CloudWatch</strong>, <strong>Create Rule</strong>.</p> 
<p>For <strong>Event Source</strong>, choose <strong>Event Pattern</strong> and specify the following values:</p> 
<li><strong>Service Name</strong>: EC2</li> 
<li><strong>Event Type</strong>: EBS Snapshot Notification</li> 
<li><strong>Specific Event</strong>: copySnapshot</li> 
<p>For <strong>Target</strong>, choose <strong>Step Functions state machine</strong>, then select the state machine created by the CloudFormation commands. Choose <strong>Create a new role for this specific resource</strong>. Your completed rule should look like in the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture4_CWE_Target.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture4_CWE_Target.png" /></a></p> 
<p>As in the primary region, choose <strong>Configure Details</strong> and then give this rule a name and description. Complete the creation of the rule.</p> 
<b id="toc_4">Testing in your account</b> 
<p>To test this setup, open the EC2 console and choose <strong>Volumes</strong>. Select a volume to snapshot. Choose <strong>Actions</strong>, <strong>Create Snapshot</strong>, and then create a snapshot.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture5_EBS_Snapshot.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture5_EBS_Snapshot.png" /></a></p> 
<p>This results in a new execution of your state machine in the primary and DR regions. You can view these executions by going to the Step Functions console and selecting your state machine.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture6_Steps.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture6_Steps.png" /></a></p> 
<p>From there, you can see the execution of the state machine.</p> 
<p>Primary region state machine:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture7_Steps_Execute.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture7_Steps_Execute.png" /></a></p> 
<p>DR region state machine:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture8_Steps_Execute_DR.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/05/Picture8_Steps_Execute_DR.png" /></a></p> 
<p>I've also provided CloudFormation templates that perform all the earlier setup without using git clone and running the CloudFormation commands. Choose the <strong>Launch Stack</strong> buttons below to launch the primary and DR region stacks in Dublin and Ohio, respectively. From there, you can pick up at the Testing in Your Account section above to finish the example. All of the code for this example architecture is located in the <a href="https://github.com/awslabs/aws-step-functions-ebs-snapshot-mgmt">aws-step-functions-ebs-snapshot-mgmt</a> AWSLabs repo.</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=eu-west-1#/stacks/new?stackName=step-functions-ebs-mgmt-primary&amp;templateURL=https://s3-eu-west-1.amazonaws.com/step-functions-ref-archs-eu-west-1/PrimaryRegionTemplateV2.json"><img width="100%" src="https://camo.githubusercontent.com/c8a1bc998060aa99bd4f459aee30a9a762b914d6/687474703a2f2f646f63732e6177732e616d617a6f6e2e636f6d2f415753436c6f7564466f726d6174696f6e2f6c61746573742f5573657247756964652f696d616765732f636c6f7564666f726d6174696f6e2d6c61756e63682d737461636b2d627574746f6e2e706e67" /></a><br /> <strong>DR Region us-east-2 (Ohio) </strong></p> 
<b id="toc_5">Summary</b> 
<p>This reference architecture is just an example of how you can use Step Functions and CloudWatch Events to build event-driven IT automation. The possibilities are endless:</p> 
<li>Use this pattern to perform other common cleanup type jobs such as managing Amazon RDS snapshots, old versions of Lambda functions, or old Amazon ECR images—all triggered by scheduled events.</li> 
<li>Use Trusted Advisor events to identify unused EC2 instances or EBS volumes, then coordinate actions on them, such as alerting owners, stopping, or snapshotting.</li> 
<p>Happy coding and please let me know what useful state machines you build!</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2753');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Automate Your IT Operations Using AWS Step Functions and Amazon CloudWatch Events</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-09-14T11:29:33+00:00">14 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/aws-identity-and-access-management-iam/" title="View all posts in AWS Identity and Access Management (IAM)*"><span property="articleSection">AWS Identity and Access Management (IAM)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/management-tools/aws-personal-health-dashboard/" title="View all posts in AWS Personal Health Dashboard*"><span property="articleSection">AWS Personal Health Dashboard*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/" title="View all posts in Security, Identity, &amp; Compliance*"><span property="articleSection">Security, Identity, &amp; Compliance*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2832" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2832&amp;disqus_title=Automate+Your+IT+Operations+Using+AWS+Step+Functions+and+Amazon+CloudWatch+Events&amp;disqus_url=https://aws.amazon.com/blogs/compute/automate-your-it-operations-using-aws-step-functions-and-amazon-cloudwatch-events/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2832');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/download.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/download.jpeg" /></a><br /> <strong>Rob Percival, Associate Solutions Architect</strong></p> 
<p>Are you interested in reducing the operational overhead of your AWS Cloud infrastructure? One way to achieve this is to automate the response to operational events for resources in your AWS account.</p> 
<p><a href="https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/">Amazon CloudWatch Events</a> provides a near real-time stream of system events that describe the changes and notifications for your AWS resources. From this stream, you can create rules to route specific events to <a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a>, <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>, and <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">other AWS services</a> for further processing and automated actions.</p> 
<p>In this post, learn how you can use Step Functions to orchestrate <a href="https://aws.amazon.com/serverless/">serverless</a> IT automation workflows in response to CloudWatch events sourced from <a href="http://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html">AWS Health</a>, a service that monitors and generates events for your AWS resources. As a real-world example, I show automating the response to a scenario where an <a href="https://aws.amazon.com/iam/">IAM</a> user <a href="http://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys">access key</a> has been exposed.</p> 
<b id="toc_0">Serverless workflows with Step Functions and Lambda</b> 
<p>Step Functions makes it easy to develop and orchestrate components of operational response automation using visual workflows. Building automation workflows from individual Lambda functions that perform discrete tasks lets you develop, test, and modify the components of your workflow quickly and seamlessly. As serverless services, Step Functions and Lambda also provide the benefits of more productive development, reduced operational overhead, and no costs incurred outside of when the workflows are actively executing.</p> 
<b id="toc_1">Example workflow</b> 
<p>As an example, this post focuses on automating the response to an event generated by AWS Health when an IAM access key has been publicly exposed on GitHub. This is a diagram of the automation workflow:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/CWEArch.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/CWEArch.png" /></a></p> 
<p>AWS proactively monitors popular code repository sites for IAM access keys that have been publicly exposed. Upon detection of an exposed IAM access key, AWS Health generates an AWS_RISK_CREDENTIALS_EXPOSED event in the AWS account related to the exposed key. A configured CloudWatch Events rule detects this event and invokes a Step Functions state machine. The state machine then orchestrates the automated workflow that deletes the exposed IAM access key, summarizes the recent API activity for the exposed key, and sends the summary message to an Amazon SNS topic to notify the subscribers―in that order.</p> 
<p>The corresponding Step Functions state machine diagram of this automation workflow can be seen below:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/CWEStateMachine.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/CWEStateMachine.png" /></a></p> 
<p>While this particular example focuses on IT automation workflows in response to the AWS_RISK_CREDENTIALS_EXPOSEDevent sourced from AWS Health, it can be generalized to integrate with other events from these services, <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html">other event-generating AWS services</a>, and even <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html">run on a time-based schedule</a>.</p> 
<b id="toc_2">Walkthrough</b> 
<p>To follow along, use the code and resources found in the <a href="https://github.com/aws/aws-health-tools/tree/master/automated-actions/AWS_RISK_CREDENTIALS_EXPOSED">aws-health-tools</a> GitHub repo. The code and resources include an <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> template, in addition to instructions on how to use it.</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=AWSHealthCredsExposed&amp;templateURL=https://s3.amazonaws.com/aws-health-tools-assets/cloudformation-templates/risk_credentials_exposed.output.yaml"><img width="100%" src="https://camo.githubusercontent.com/c8a1bc998060aa99bd4f459aee30a9a762b914d6/687474703a2f2f646f63732e6177732e616d617a6f6e2e636f6d2f415753436c6f7564466f726d6174696f6e2f6c61746573742f5573657247756964652f696d616765732f636c6f7564666f726d6174696f6e2d6c61756e63682d737461636b2d627574746f6e2e706e67" /></a></p> 
<p>The Step Functions state machine execution starts with the exposed keys event details in JSON, a sanitized example of which is provided below:</p> 
<code class="language-json">{
&quot;version&quot;: &quot;0&quot;,
&quot;id&quot;: &quot;121345678-1234-1234-1234-123456789012&quot;,
&quot;detail-type&quot;: &quot;AWS Health Event&quot;,
&quot;source&quot;: &quot;aws.health&quot;,
&quot;account&quot;: &quot;123456789012&quot;,
&quot;time&quot;: &quot;2016-06-05T06:27:57Z&quot;,
&quot;region&quot;: &quot;us-east-1&quot;,
&quot;resources&quot;: [],
&quot;detail&quot;: {
&quot;eventArn&quot;: &quot;arn:aws:health:us-east-1::event/AWS_RISK_CREDENTIALS_EXPOSED_XXXXXXXXXXXXXXXXX&quot;,
&quot;service&quot;: &quot;RISK&quot;,
&quot;eventTypeCode&quot;: &quot;AWS_RISK_CREDENTIALS_EXPOSED&quot;,
&quot;eventTypeCategory&quot;: &quot;issue&quot;,
&quot;startTime&quot;: &quot;Sat, 05 Jun 2016 15:10:09 GMT&quot;,
&quot;eventDescription&quot;: [
{
&quot;language&quot;: &quot;en_US&quot;,
&quot;latestDescription&quot;: &quot;A description of the event is provided here&quot;
}
],
&quot;affectedEntities&quot;: [
{
&quot;entityValue&quot;: &quot;ACCESS_KEY_ID_HERE&quot;
}
]
}
}</code> 
<p>After it's invoked, the state machine execution proceeds as follows.</p> 
<b id="toc_3">Step 1: Delete the exposed IAM access key pair</b> 
<p>The first thing you want to do when you determine that an IAM access key has been exposed is to delete the key pair so that it can no longer be used to make API calls. This Step Functions <em>task</em> state deletes the exposed access key pair detailed in the incoming event, and retrieves the IAM user associated with the key to look up API activity for the user in the next step. The user name, access key, and other details about the event are passed to the next step as JSON.</p> 
<p>This state contains a powerful error-handling feature offered by Step Functions <em>task</em> states called a <a href="http://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html#amazon-states-language-fallback-states"><em>catch</em> configuration</a>. Catch configurations allow you to reroute and continue state machine invocation at new states depending on potential errors that occur in your <em>task</em> function. In this case, the <em>catch</em> configuration skips to Step 3. It immediately notifies your security team that errors were raised in the <em>task</em> function of this step (Step 1), when attempting to look up the corresponding IAM user for a key or delete the user's access key.</p> 
<p><strong>Note:</strong> Step Functions also offers a <a href="http://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html#amazon-states-language-retrying-after-error"><em>retry</em> configuration</a> for when you would rather retry a <em>task</em> function that failed due to error, with the option to specify an increasing time interval between attempts and a maximum number of attempts.</p> 
<b id="toc_4">Step 2: Summarize recent API activity for key</b> 
<p>After you have deleted the access key pair, you'll want to have some immediate insight into whether it was used for malicious activity in your account. Another <em>task</em> state, this step uses <a href="https://aws.amazon.com/cloudtrail/">AWS CloudTrail</a> to look up and summarize the most recent API activity for the IAM user associated with the exposed key. The summary is in the form of counts for each API call made and resource type and name affected. This summary information is then passed to the next step as JSON. This step requires information that you obtained in Step 1. Step Functions ensures the successful completion of Step 1 before moving to Step 2.</p> 
<b id="toc_5">Step 3: Notify security</b> 
<p>The summary information gathered in the last step can provide immediate insight into any malicious activity on your account made by the exposed key. To determine this and further secure your account if necessary, you must notify your security team with the gathered summary information.</p> 
<p>This final <em>task</em> state generates an email message providing in-depth detail about the event using the API activity summary, and publishes the message to an SNS topic subscribed to by the members of your security team.</p> 
<p>If the <em>catch</em> configuration of the <em>task</em> state in Step 1 was triggered, then the security notification email instead directs your security team to log in to the console and navigate to the <a href="http://docs.aws.amazon.com/health/latest/ug/getting-started-phd.html">Personal Health Dashboard</a> to view more details on the incident.</p> 
<b id="toc_6">Lessons learned</b> 
<p>When implementing this use case with Step Functions and Lambda, consider the following:</p> 
<li>One of the most important parts of implementing automation in response to operational events is to ensure visibility into the response and resolution actions is retained. Step Functions and Lambda enable you to orchestrate your granular response and resolution actions that provides direct visibility into the state of the automation workflow.</li> 
<li>This basic workflow currently executes these steps serially with a <em>catch</em> configuration for error handling. More sophisticated workflows can leverage the <a href="http://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html">parallel execution</a>, <a href="http://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-choice-state.html">branching logic</a>, and <a href="http://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-wait-state.html">time delay</a> functionality provided by Step Functions.</li> 
<li>Catch and retry configurations for task states allow for orchestrating reliable workflows while maintaining the granularity of each Lambda function. Without leveraging a catch configuration in Step 1, you would have had to duplicate code from the function in Step 3 to ensure that your security team was notified on failure to delete the access key.</li> 
<li>Step Functions and Lambda are serverless services, so there is no cost for these services when they are not running. Because this IT automation workflow only runs when an IAM access key is exposed for this account (which is hopefully rare!), the total monthly cost for this workflow is essentially $0.</li> 
<b id="toc_7">Conclusion</b> 
<p>Automating the response to operational events for resources in your AWS account can free up the valuable time of your engineers. Step Functions and Lambda enable granular IT automation workflows to achieve this result while gaining direct visibility into the orchestration and state of the automation.</p> 
<p>For more examples of how to use Step Functions to automate the operations of your AWS resources, or if you'd like to see how Step Functions can be used to build and orchestrate serverless applications, visit <a href="https://aws.amazon.com/step-functions/getting-started/">Getting Started</a> on the <a href="https://aws.amazon.com/step-functions">Step Functions</a> website.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2832');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Manage Kubernetes Clusters on AWS Using CoreOS Tectonic</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Arun Gupta</span></span> | on 
<time property="datePublished" datetime="2017-09-13T12:44:55+00:00">13 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-coreos-tectonic/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2819" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2819&amp;disqus_title=Manage+Kubernetes+Clusters+on+AWS+Using+CoreOS+Tectonic&amp;disqus_url=https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-coreos-tectonic/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2819');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>There are multiple ways to run a&nbsp;<a href="https://github.com/kubernetes/community/blob/master/sig-aws/kubernetes-on-aws.md">Kubernetes cluster on Amazon Web Services</a> (AWS).&nbsp;The <a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/">first post</a> in this series explained how to manage a Kubernetes cluster on AWS using <a href="https://github.com/kubernetes/kops">kops</a>. This second post explains how to manage a Kubernetes cluster on AWS using <a href="https://coreos.com/tectonic/">CoreOS Tectonic</a>.</p> 
<b>Tectonic overview</b> 
<p>Tectonic delivers the most current upstream version of Kubernetes with additional features. It is a commercial offering from CoreOS and adds the following features over the upstream:</p> 
<li><strong>Installer</strong><br /> Comes with a graphical installer that installs a highly available Kubernetes cluster. Alternatively, the cluster can be installed using AWS CloudFormation templates or Terraform scripts.</li> 
<li><strong>Operators</strong><br /> An <a href="https://coreos.com/operators">operator</a> is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user. This release includes an <a href="https://coreos.com/blog/introducing-the-etcd-operator.html">etcd operator</a> for rolling upgrades and a Prometheus operator for monitoring capabilities.</li> 
<li><strong>Console</strong><br /> A web console provides a full view of applications running in the cluster. It also allows you to deploy applications to the cluster and start the rolling upgrade of the cluster.</li> 
<li><strong>Monitoring</strong><br /> Node CPU and memory metrics are powered by the Prometheus operator. The graphs are available in the console. A large set of preconfigured Prometheus alerts are also available.</li> 
<li><strong>Security</strong><br /> Tectonic ensures that cluster is always up to date with the most recent patches/fixes. Tectonic clusters also enable role-based access control (RBAC). Different roles can be mapped to an LDAP service.</li> 
<li><strong>Support</strong><br /> CoreOS provides commercial support for clusters created using Tectonic.</li> 
<p>Tectonic can be installed on AWS using a <a href="https://coreos.com/tectonic/docs/latest/install/aws/index.html">GUI installer</a>&nbsp;or <a href="https://coreos.com/tectonic/docs/latest/install/aws/aws-terraform.html">Terraform</a> scripts. The installer prompts you for the information needed to boot the Kubernetes cluster, such as AWS access and secret key, number of master and worker nodes, and instance size for the master and worker nodes. The cluster can be created after all the options are specified. Alternatively, Terraform assets can be downloaded and the cluster can be created later. This post shows using the installer.</p> 
<b>CoreOS License and Pull Secret</b> 
<p>Even though Tectonic is a commercial offering, a cluster for up to 10 nodes can be created by creating a free account at <a href="https://account.coreos.com/signup/summary/tectonic-2016-12">Get Tectonic for Kubernetes</a>. After signup, a CoreOS License and Pull Secret files are provided on your <a href="http://account.coreos.com/">CoreOS account page</a>. Download these files as they are needed by the installer to boot the cluster.</p> 
<b>IAM user permission</b> 
<p>The IAM user to create the Kubernetes cluster must have access to the following services and features:</p> 
<li>Amazon Route 53</li> 
<li>Amazon EC2</li> 
<li>Elastic Load Balancing</li> 
<li>Amazon S3</li> 
<li>Amazon VPC</li> 
<li>Security groups</li> 
<p>Use the <a href="https://coreos.com/tectonic/docs/latest/files/aws-policy.json">aws-policy</a> policy to grant the required permissions for the IAM user.</p> 
<b>DNS configuration</b> 
<p>A subdomain is required to create the cluster, and it must be registered as a public Route 53 hosted zone. The zone is used to host and expose the console web application. It is also used as the static namespace for the Kubernetes API server. This allows <code>kubectl</code> to be able to talk directly with the master.</p> 
<p>The domain may be registered using Route 53. Alternatively, a domain may be registered at a third-party registrar. This post uses a&nbsp;<code>kubernetes-aws.io</code> domain registered at a third-party registrar and a <code>tectonic</code> subdomain within it.</p> 
<p>Generate a Route 53 hosted zone using the <a href="https://aws.amazon.com/cli">AWS CLI</a>. Download&nbsp;<a href="https://github.com/stedolan/jq/wiki/Installation">jq</a>&nbsp;to run this command:</p> 
ID=$(uuidgen) &amp;&amp; \
aws route53 create-hosted-zone \
--name tectonic.kubernetes-aws.io \
--caller-reference $ID \
| jq .DelegationSet.NameServers 
<p>The command shows an output such as the following:</p> 
[
&quot;ns-1924.awsdns-48.co.uk&quot;,
&quot;ns-501.awsdns-62.com&quot;,
&quot;ns-1259.awsdns-29.org&quot;,
&quot;ns-749.awsdns-29.net&quot;
] 
<p>Create NS records for the domain with your registrar.&nbsp;Make sure that the NS records can be resolved using a utility like <a href="https://www.digwebinterface.com/">dig web interface</a>. A sample output would look like the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-1.png" /></a></p> 
<p>The bottom of the screenshot shows NS records configured for the subdomain.</p> 
<b>Download and run the Tectonic installer</b> 
<p>Download the <a href="https://releases.tectonic.com/tectonic-1.7.1-tectonic.1.tar.gz">Tectonic installer</a>&nbsp;(version 1.7.1) and extract it. The latest installer can always be found at <a href="https://coreos.com/tectonic">coreos.com/tectonic</a>. Start the installer:</p> 
./tectonic/tectonic-installer/$PLATFORM/installer 
<p>Replace <code>$PLATFORM</code> with either <code>darwin</code> or <code>linux</code>. The installer opens your default browser and prompts you to select the cloud provider. Choose Amazon Web Services as the platform. Choose <b>Next Step</b>.</p> 
<p>Specify the Access Key ID and Secret Access Key for the IAM role that you created earlier. This allows the installer to create resources required for the Kubernetes cluster. This also gives the installer full access to your AWS account. Alternatively, to protect the integrity of your main AWS credentials, use a temporary session token to generate temporary credentials.</p> 
<p>You also need to choose a region in which to install the cluster. For the purpose of this post, I chose a region close to where I live, Northern California. Choose <b>Next Step</b>.</p> 
<p>Give your cluster a name. This name is part of the static namespace for the master and the address of the console.</p> 
<p>To enable in-place update to the Kubernetes cluster, select the checkbox next to <b>Automated Updates</b>. It also enables update to the etcd and Prometheus operators. This feature may become a default in future releases.</p> 
<p>Choose <b>Upload “tectonic-license.txt”</b> and upload the previously downloaded license file.</p> 
<p>Choose <b>Upload “config.json”</b> and upload the previously downloaded pull secret file. Choose <b>Next Step</b>.</p> 
<p>Let the installer generate a CA certificate and key. In this case, the browser may not recognize this certificate, which I discuss later in the post. Alternatively, you can provide a CA certificate and a key in PEM format issued by an authorized certificate authority. Choose Next Step.</p> 
<p>Use the SSH key for the region specified earlier. You also have an option to generate a new key. This allows you to later connect using SSH into the Amazon EC2 instances provisioned by the cluster. Here is the command that can be used to log in:</p> 
ssh –i &lt;key&gt; core@&lt;ec2-instance-ip&gt; 
<p>Choose <b>Next Step</b>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-2.png" /></a></p> 
<p>Define the number and instance type of master and worker nodes. In this case, create a 6 nodes cluster. Make sure that the worker nodes have enough processing power and memory to run the containers.</p> 
<p>An etcd cluster is used as persistent storage for all of Kubernetes API objects. This cluster is required for the Kubernetes cluster to operate. There are three ways to use the etcd cluster as part of the Tectonic installer:</p> 
<li>(Default) Provision the cluster using EC2 instances. Additional EC2 instances are used in this case.</li> 
<li>Use an alpha support for cluster provisioning using the etcd operator. The etcd operator is used for automated operations of the etcd master nodes for the cluster itself, in addition to for etcd instances that are created for application usage. The etcd cluster is provisioned within the Tectonic installer.</li> 
<li>Bring your own pre-provisioned etcd cluster.</li> 
<p>Use the first option in this case.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-3.png" /></a></p> 
<p>For more information about choosing the appropriate instance type, see the <a href="https://coreos.com/etcd/docs/latest/op-guide/hardware.html">etcd hardware recommendation</a>. Choose <strong>Next Step</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-4.png" /></a></p> 
<p>Specify the networking options. The installer can create a new public VPC or use a pre-existing public or private VPC. Make sure that the <a href="https://coreos.com/tectonic/docs/latest/install/aws/requirements.html#subnetvpc-requirements">VPC requirements</a> are met for an existing VPC.</p> 
<p>Give a DNS name for the cluster. Choose the domain for which the Route 53 hosted zone was configured earlier, such as <code>tectonic.kubernetes-aws.io</code>. Multiple clusters may be created under a single domain. The cluster name and the DNS name would typically match each other.</p> 
<p>To select the CIDR range, choose <strong>Show Advanced Settings</strong>. You can also choose the Availability Zones for the master and worker nodes. By default, the master and worker nodes are spread across multiple Availability Zones in the chosen region. This makes the cluster highly available.</p> 
<p>Leave the other values as default. Choose <strong>Next Step</strong>.</p> 
<p>Specify an email address and password to be used as credentials to log in to the console. Choose <strong>Next Step</strong>.</p> 
<p>At any point during the installation, you can choose <strong>Save progress</strong>. This allows you to save configurations specified in the installer. This configuration file can then be used to restore progress in the installer at a later point.</p> 
<p>To start the cluster installation, choose <strong>Submit</strong>. At another time, you can download the Terraform assets by choosing <strong>Manually boot</strong>. This allows you to boot the cluster later.</p> 
<p>The logs from the Terraform scripts are shown in the installer. When the installation is complete, the console shows that the Terraform scripts were successfully applied, the domain name was resolved successfully, and that the console has started. The domain works successfully if the DNS resolution worked earlier, and it’s the address where the console is accessible.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-5.png" /></a></p> 
<p>Choose <strong>Download assets</strong> to download assets related to your cluster. It contains your generated CA, kubectl configuration file, and the Terraform state. This download is an important step as it allows you to delete the cluster later.</p> 
<p>Choose <strong>Next Step</strong> for the final installation screen. It allows you to access the Tectonic console, gives you instructions about how to configure <code>kubectl</code> to manage this cluster, and finally deploys an application using <code>kubectl</code>.</p> 
<p>Choose <strong>Go to my Tectonic Console</strong>. In our case, it is also accessible at <code>http://cluster.tectonic.kubernetes-aws.io/</code>.</p> 
<p>As I mentioned earlier, the browser does not recognize the self-generated CA certificate. Choose <strong>Advanced</strong> and connect to the console. Enter the login credentials specified earlier in the installer and choose <strong>Login</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-6.png" /></a></p> 
<p>The Kubernetes upstream and console version are shown under <strong>Software Details</strong>. Cluster health shows <strong>All systems go</strong> and it means that the API server and the backend API can be reached.</p> 
<p>To view different Kubernetes resources in the cluster choose, the resource in the left navigation bar. For example, all deployments can be seen by choosing <strong>Deployments</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-7.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-7.png" /></a></p> 
<p>By default, resources in the <code>all</code> namespace are shown. Other namespaces may be chosen by clicking on a menu item on the top of the screen. Different administration tasks such as managing the namespaces, getting list of the nodes and RBAC can be configured as well.</p> 
<b>Download and run Kubectl</b> 
<p>Kubectl is required to manage the Kubernetes cluster. The latest version of kubectl can be downloaded using the following command:</p> 
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl 
<p>It can also be conveniently installed using the <a href="https://brew.sh/">Homebrew package manager</a>. To find and access a cluster, Kubectl needs a kubeconfig file. By default, this configuration file is at <code>~/.kube/config</code>. This file is created when a Kubernetes cluster is created from your machine. However, in this case, download this file from the console.</p> 
<p>In the console, choose <strong>admin</strong>, <strong>My Account</strong>, <strong>Download Configuration</strong> and follow the steps to download the kubectl configuration file. Move this file to <code>~/.kube/config</code>. If kubectl has already been used on your machine before, then this file already exists. Make sure to take a backup of that file first.</p> 
<p>Now you can run the commands to view the list of deployments:</p> 
~ $ kubectl get deployments --all-namespaces
NAMESPACE         NAME                                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-system       etcd-operator                           1         1         1            1           43m
kube-system       heapster                                1         1         1            1           40m
kube-system       kube-controller-manager                 3         3         3            3           43m
kube-system       kube-dns                                1         1         1            1           43m
kube-system       kube-scheduler                          3         3         3            3           43m
tectonic-system   container-linux-update-operator         1         1         1            1           40m
tectonic-system   default-http-backend                    1         1         1            1           40m
tectonic-system   kube-state-metrics                      1         1         1            1           40m
tectonic-system   kube-version-operator                   1         1         1            1           40m
tectonic-system   prometheus-operator                     1         1         1            1           40m
tectonic-system   tectonic-channel-operator               1         1         1            1           40m
tectonic-system   tectonic-console                        2         2         2            2           40m
tectonic-system   tectonic-identity                       2         2         2            2           40m
tectonic-system   tectonic-ingress-controller             1         1         1            1           40m
tectonic-system   tectonic-monitoring-auth-alertmanager   1         1         1            1           40m
tectonic-system   tectonic-monitoring-auth-prometheus     1         1         1            1           40m
tectonic-system   tectonic-prometheus-operator            1         1         1            1           40m
tectonic-system   tectonic-stats-emitter                  1         1         1            1           40m 
<p>This output is similar to the one shown in the console earlier. Now, this <code>kubectl</code> can be used to manage your resources.</p> 
<b>Upgrade the Kubernetes cluster</b> 
<p>Tectonic allows the in-place upgrade of the cluster. This is an experimental feature as of this release. The clusters can be updated either automatically, or with manual approval.</p> 
<p>To perform the update, choose <strong>Administration</strong>, <strong>Cluster Settings</strong>. If an earlier Tectonic installer, version 1.6.2 in this case, is used to install the cluster, then this screen would look like the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-8.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-8.png" /></a></p> 
<p>Choose <strong>Check for Updates</strong>. If any updates are available, choose <strong>Start Upgrade</strong>. After the upgrade is completed, the screen is refreshed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-9.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-9.png" /></a></p> 
<p>This is an experimental feature in this release and so should only be used on clusters that can be easily replaced. This feature may become a fully supported in a future release. For more information about the upgrade process, see <a href="https://coreos.com/tectonic/docs/latest/admin/upgrade.html">Upgrading Tectonic &amp; Kubernetes</a>.</p> 
<b>Delete the Kubernetes cluster</b> 
<p>Typically, the Kubernetes cluster is a long-running cluster to serve your applications. After its purpose is served, you may delete it. It is important to delete the cluster as this ensures that all resources created by the cluster are appropriately cleaned up.</p> 
<p>The easiest way to delete the cluster is using the assets downloaded in the last step of the installer. Extract the downloaded zip file. This creates a directory like <code>&lt;cluster-name&gt;_TIMESTAMP</code>. In that directory, give the following command to delete the cluster:</p> 
TERRAFORM_CONFIG=$(pwd)/.terraformrc terraform destroy --force 
<p>This destroys the cluster and all associated resources.</p> 
<p>You may have forgotten to download the assets. There is a copy of the assets in the directory <code>tectonic/tectonic-installer/darwin/clusters</code>. In this directory, another directory with the name <code>&lt;cluster-name&gt;_TIMESTAMP</code> contains your assets.</p> 
<b>Conclusion</b> 
<p>This post explained how to manage Kubernetes clusters using the CoreOS Tectonic graphical installer. &nbsp;For more details, see <a href="https://coreos.com/tectonic/docs/latest/install/aws/index.html">Graphical Installer with AWS</a>. If the installation does not succeed, see the helpful <a href="https://coreos.com/tectonic/docs/latest/install/aws/troubleshooting.html">Troubleshooting tips</a>. After the cluster is created, see the <a href="https://coreos.com/tectonic/docs/latest/tutorials/index.html">Tectonic tutorials</a> to learn how to deploy, scale, version, and delete an application.</p> 
<p>Future posts in this series will explain other ways of creating and running a Kubernetes cluster on AWS.</p> 
<p>—<a href="https://twitter.com/arungupta">Arun</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2819');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture5.png" /> 
<b class="lb-b blog-post-title" property="name headline">Delivering Graphics Apps with Amazon AppStream 2.0</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Deepak Suryanarayanan</span></span> | on 
<time property="datePublished" datetime="2017-09-12T10:02:21+00:00">12 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/amazon-appstream-2-0/" title="View all posts in Amazon AppStream 2.0*"><span property="articleSection">Amazon AppStream 2.0*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/" title="View all posts in Desktop &amp; App Streaming*"><span property="articleSection">Desktop &amp; App Streaming*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/delivering-graphics-apps-with-amazon-appstream-2-0/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2792" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2792&amp;disqus_title=Delivering+Graphics+Apps+with+Amazon+AppStream+2.0&amp;disqus_url=https://aws.amazon.com/blogs/compute/delivering-graphics-apps-with-amazon-appstream-2-0/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2792');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>Sahil Bahri, Sr. Product Manager, Amazon AppStream 2.0</strong></p> 
<p><strong><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture6-300x207.png" /></a></strong></p> 
<p>Do you need to provide a workstation class experience for users who run graphics apps? With Amazon AppStream 2.0, you can stream graphics apps from AWS to a web browser running on any supported device. AppStream 2.0 offers a choice of GPU instance types. The range includes the newly launched Graphics Design instance, which allows you to offer a fast, fluid user experience at a fraction of the cost of using a graphics workstation, without upfront investments or long-term commitments.</p> 
<p>In this post, I discuss the Graphics Design instance type in detail, and how you can use it to deliver a graphics application such as Siemens NX―a popular CAD/CAM application that we have been testing on AppStream 2.0 with engineers from Siemens PLM.</p> 
<h3>Graphics Instance Types on AppStream 2.0</h3> 
<p>First, a quick recap on the GPU&nbsp;instance types available with AppStream 2.0. In July, 2017, we launched graphics support for AppStream 2.0 with <a href="https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-appstream-2-0-now-supports-graphics-applications/">two new instance types</a> that Jeff Barr discussed on the <a href="https://aws.amazon.com/blogs/aws/new-gpu-powered-streaming-instances-for-amazon-appstream-2-0/">AWS Blog</a>:</p> 
<li>Graphics Desktop</li> 
<li>Graphics Pro</li> 
<p>Many customers in industries such as engineering, media, entertainment, and oil and gas are using these instances to deliver high-performance graphics applications to their users. These instance types are based on dedicated NVIDIA GPUs and can run the most demanding graphics applications, including those that rely on CUDA graphics API libraries.</p> 
<p>Last week, we added a new lower-cost instance type: <a href="https://aws.amazon.com/about-aws/whats-new/2017/09/introducing-amazon-appstream-2-graphics-design-a-new-lower-cost-instance-type-for-streaming-graphics-applications/">Graphics Design</a>. This instance type is a great fit for engineers, 3D modelers, and designers who use graphics applications that rely on the hardware acceleration of DirectX, OpenGL, or OpenCL APIs, such as Siemens NX, Autodesk AutoCAD, or Adobe Photoshop. The Graphics Design instance is based on AMD’s FirePro S7150x2 Server GPUs and equipped with AMD Multiuser GPU technology. The instance type uses virtualized GPUs to achieve lower costs, and is available in four instance sizes to scale and match the requirements of your applications.</p> 
<table style="border: 2px solid black;border-collapse: collapse;margin-left: auto;margin-right: auto"> 
<tbody> 
<tr style="border-bottom: 1px solid black;background-color: #e0e0e0"> 
<td style="border-right: 1px solid black;padding: 4px"><b>Instance</b></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>vCPUs</strong></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Instance RAM (GiB)</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center"><strong>GPU Memory (GiB)</strong></td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><b>stream.graphics-design.large</b></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">2</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">7.5 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">1</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>stream.graphics-design.xlarge</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">15.3 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">2</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>stream.graphics-design.2xlarge</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">30.5 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>stream.graphics-design.4xlarge</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">16</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">61 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8</td> 
</tr> 
</tbody> 
</table> 
<p>The following table compares all three graphics instance types on AppStream 2.0, along with example applications you could use with each.</p> 
<table style="border: 2px solid black;border-collapse: collapse;margin-left: auto;margin-right: auto"> 
<tbody> 
<tr style="border-bottom: 1px solid black;background-color: #e0e0e0"> 
<td style="border-right: 1px solid black;padding: 4px"><b>&nbsp;</b></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Graphics Design</strong></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Graphics Desktop</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center"><strong>Graphics Pro</strong></td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><b>Number of instance sizes</b></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">1</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">3</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>GPU memory range<br /> </strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">1–8 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8–32 GiB</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>vCPU range</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">2–16</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">16–32</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Memory range</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">7.5–61 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">15 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">122–488 GiB</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Graphics libraries supported</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">AMD FirePro S7150x2</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">NVIDIA GRID K520</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">NVIDIA Tesla M60</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Price range (N. Virginia AWS Region)</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">$0.25 – $2.00/hour</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">$0.5/hour</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">$2.05 – $8.20/hour</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Example applications</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">Adobe Premiere Pro, AutoDesk Revit, Siemens NX</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">AVEVA E3D, SOLIDWORKS</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">AutoDesk Maya, Landmark DecisionSpace, Schlumberger Petrel</td> 
</tr> 
</tbody> 
</table> 
<h3>Example graphics instance set up with Siemens NX</h3> 
<p>In the section, I walk through setting up Siemens NX with Graphics Design instances on AppStream 2.0. After set up is complete, users can able to access NX from within their browser and also access their design files from a file share. You can also use these steps to set up and test your own graphics applications on AppStream 2.0. Here’s the workflow:</p> 
<ol> 
<li>Create a file share to load and save design files.</li> 
<li>Create an AppStream 2.0 image with Siemens NX installed.</li> 
<li>Create an AppStream 2.0 fleet and stack.</li> 
<li>Invite users to access Siemens NX through a browser.</li> 
<li>Validate the setup.</li> 
</ol> 
<p>To learn more about AppStream 2.0 concepts and set up, see the previous post <a href="https://aws.amazon.com/blogs/compute/scaling-your-desktop-application-streams-with-amazon-appstream-2-0/">Scaling Your Desktop Application Streams with Amazon AppStream 2.0</a>. For a deeper review of all the setup and maintenance steps, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html">Amazon AppStream 2.0 Developer Guide</a>.</p> 
<h4><span style="color: #000080"><strong>Step 1: Create a file share to load and save design files</strong></span></h4> 
<p><strong>To launch and configure the file server</strong></p> 
<ol> 
<li>Open the EC2 console and choose <strong>Launch Instance</strong>.</li> 
<li>Scroll to the <strong>Microsoft Windows Server 2016 Base Image</strong> and choose <strong>Select</strong>.</li> 
<li>Choose an instance type and size for your file server (I chose the general purpose m4.large instance). Choose <strong>Next: Configure Instance Details</strong>.</li> 
<li>Select a VPC and subnet. You launch AppStream 2.0 resources in the same VPC. Choose <strong>Next: Add Storage.</strong></li> 
<li>If necessary, adjust the size of your EBS volume. Choose <strong>Review and Launch, Launch</strong>.</li> 
<li>On the Instances page, give your file server a name, such as My File Server.</li> 
<li>Ensure that the security group associated with the file server instance allows for incoming traffic from the security group that you select for your AppStream 2.0 fleets or image builders. You can use the default security group and select the same group while creating the image builder and fleet in later steps.</li> 
</ol> 
<p>Log in to the file server using a remote access client such as Microsoft Remote Desktop. For more information about connecting to an EC2 Windows instance, see&nbsp;<a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/connecting_to_windows_instance.html#connect-rdp">Connect to Your Windows Instance</a>.</p> 
<p><strong>To enable file sharing</strong></p> 
<ol> 
<li>Create a new folder (such as C:\My Graphics Files) and upload the shared files to make available to your users.</li> 
<li>From the Windows control panel, enable network discovery.</li> 
<li>Choose <strong>Server Manager</strong>, <strong>File and Storage Services, Volumes</strong>.</li> 
<li>Scroll to Shares and choose <strong>Start the</strong> <strong>Add Roles and Features Wizard</strong>. Go through the wizard to install the <strong>File Server and Share</strong> role.</li> 
<li>From the left navigation menu, choose <strong>Shares</strong>.</li> 
<li>Choose <strong>Start the New Share Wizard</strong> to set up your folder as a file share.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture1.png" /></a></li> 
<li>Open the context (right-click) menu on the share and choose <strong>Properties, Permissions, Customize Permissions</strong>.</li> 
<li>Choose <strong>Permissions, Add</strong>. Add <strong>Read</strong> and <strong>Execute</strong> permissions for everyone on the network.</li> 
</ol> 
<h4><span style="color: #000080"><strong>Step 2: &nbsp;Create an AppStream 2.0 image with Siemens NX installed</strong></span></h4> 
<p><strong>To connect to the image builder and install applications</strong></p> 
<ol> 
<li>Open the <a href="https://us-west-2.console.aws.amazon.com/appstream2/home?region=us-west-2#/stacks">AppStream 2.0 management console</a> and choose <strong>Images</strong>,<strong> Image Builder</strong>,<strong> Launch Image Builder</strong>.</li> 
<li>Create a graphics design image builder in the same VPC as your file server.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture2.png" /></a></li> 
<li>From the <strong>Image builder</strong> tab, select your image builder and choose <strong>Connect</strong>. This opens a new browser tab and display a desktop to log in to.</li> 
<li>Log in to your image builder as ImageBuilderAdmin.</li> 
<li>Launch the <strong>Image Assistant</strong>.</li> 
<li>Download and install Siemens NX and other applications on the image builder. I added Blender and Firefox, but you could replace these with your own applications.</li> 
<li>To verify the user experience, you can test the application performance on the instance.</li> 
</ol> 
<p>Before you finish creating the image, you must mount the file share by enabling a few Microsoft Windows services.</p> 
<p><strong>To mount the file share</strong></p> 
<ol> 
<li>Open services.msc and check the following services:</li> 
</ol> 
<li>DNS Client</li> 
<li>Function Discovery Resource Publication</li> 
<li>SSDP Discovery</li> 
<li>UPnP Device H</li> 
<ol start="2"> 
<li>If any of the preceding services have <strong>Startup Type</strong> set to <strong>Manual</strong>, open the context (right-click) menu on the service and choose <strong>Start</strong>. Otherwise, open the context (right-click) menu on the service and choose <strong>Properties</strong>. For <strong>Startup Type</strong>, choose <strong>Manual</strong>, <strong>Apply</strong>. To start the service, choose <strong>Start</strong>.</li> 
<li>From the Windows control panel, enable network discovery.</li> 
<li>Create a batch script that mounts a file share from the storage server set up earlier. The file share is mounted automatically when a user connects to the AppStream 2.0 environment.</li> 
</ol> 
<p style="padding-left: 90px"><strong>Logon Script Location:</strong> C:\Users\Public\logon.bat</p> 
<p style="padding-left: 90px"><strong>Script Contents: </strong></p> 
<p style="padding-left: 90px">:loop</p> 
<p style="padding-left: 90px">net use H: <em>\\path\to\network\share</em><em>&nbsp;</em></p> 
<p style="padding-left: 90px">PING localhost -n 30 &gt;NUL</p> 
<p style="padding-left: 90px">IF NOT EXIST H:\ GOTO loop</p> 
<ol start="5"> 
<li>Open gpedit.msc and choose <strong>User Configuration</strong>, <strong>Windows Settings</strong>, <strong>Scripts</strong>. Set logon.bat as the user logon script.</li> 
<li>Next, create a batch script that makes the mounted drive visible to the user.</li> 
</ol> 
<p style="padding-left: 90px"><strong>Logon Script Location:</strong> C:\Users\Public\startup.bat</p> 
<p style="padding-left: 90px"><strong>Script Contents: </strong><br /> REG DELETE “HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Policies\Explorer” /v “NoDrives” /f</p> 
<ol start="7"> 
<li>Open <strong>Task Scheduler</strong> and choose <strong>Create Task</strong>.</li> 
<li>Choose <strong>General</strong>, provide a task name, and then choose <strong>Change User or Group</strong>.</li> 
<li>For <strong>Enter the object name to select</strong>, enter <strong>SYSTEM</strong> and choose <strong>Check Names</strong>, <strong>OK</strong>.</li> 
<li>Choose <strong>Triggers</strong>, <strong>New</strong>. For <strong>Begin the task</strong>, choose <strong>At startup</strong>. Under <strong>Advanced Settings</strong>, change <strong>Delay task for</strong> to 5 minutes. Choose <strong>OK</strong>.</li> 
<li>Choose <strong>Actions</strong>, <strong>New</strong>. Under <strong>Settings</strong>, for <strong>Program/script</strong>, enter <strong>C:\Users\Public\startup.bat</strong>. Choose <strong>OK</strong>.</li> 
<li>Choose <strong>Conditions</strong>. Under <strong>Power</strong>, clear the <strong>Start the task only if the computer is on AC power</strong> Choose <strong>OK</strong>.</li> 
<li>To view your scheduled task, choose <strong>Task Scheduler Library</strong>. Close Task Scheduler when you are done.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture3.png" /></a></li> 
</ol> 
<h4><span style="color: #000080">Step 3: &nbsp;Create an AppStream 2.0 fleet and stack</span></h4> 
<p><strong>To create a fleet and stack</strong></p> 
<ol> 
<li>In the AppStream 2.0 management console, choose <strong>Fleets, Create Fleet. </strong></li> 
<li>Give the fleet a name, such as Graphics-Demo-Fleet, that uses the newly created image and the same VPC as your file server.</li> 
<li>Choose <strong>Stacks</strong>, <strong>Create Stack</strong>. Give the stack a name, such as Graphics-Demo-Stack.</li> 
<li>After the stack is created, select it and choose <strong>Actions</strong>, <strong>Associate Fleet</strong>. Associate the stack with the fleet you created in step 1.</li> 
</ol> 
<h4><span style="color: #000080">Step 4: &nbsp;Invite users to access Siemens NX through a browser</span></h4> 
<p><strong>To invite users</strong></p> 
<ol> 
<li>Choose <strong>User Pools</strong>, <strong>Create User</strong> to create users.</li> 
<li>Enter a name and email address for each user.</li> 
<li>Select the users just created, and choose <strong>Actions</strong>,<strong> Assign Stack</strong> to provide access to the stack created in step 2. You can also provide access using SAML 2.0 and connect to your Active Directory if necessary. For more information, see the <a href="https://aws.amazon.com/blogs/compute/enabling-identity-federation-with-ad-fs-3-0-and-amazon-appstream-2-0/">Enabling Identity Federation with AD FS 3.0 and Amazon AppStream 2.0</a> post.</li> 
</ol> 
<p>Your user receives an email invitation to set up an account and use a web portal to access the applications that you have included in your stack.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture4.png" /></a></p> 
<h4><span style="color: #000080">Step 5: &nbsp;Validate the setup</span></h4> 
<p><strong>Time for a test drive with Siemens NX on AppStream 2.0!</strong></p> 
<ol> 
<li>Open the link for the AppStream 2.0 web portal shared through the email invitation. The web portal opens in your default browser. You must sign in with the temporary password and set a new password. After that, you get taken to your app catalog.</li> 
<li>Launch Siemens NX and interact with it using the demo files available in the shared storage folder – My Graphics Files.<em>&nbsp;</em></li> 
</ol> 
<p>After I launched NX, I captured the screenshot below. The Siemens PLM team also recorded a <a href="https://www.youtube.com/watch?v=lOWPcFjwkCE">video with NX running on AppStream 2.0</a>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture5.png" /></a></p> 
<h3>Summary</h3> 
<p>In this post, I discussed the GPU instances available for delivering rich graphics applications to users in a web browser. While I demonstrated a simple setup, you can scale this out to launch a production environment with users signing in using <a href="https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-appstream-2-0-now-supports-microsoft-active-directory-domains/">Active Directory credentials,&nbsp;</a> accessing <a href="https://aws.amazon.com/about-aws/whats-new/2017/05/amazon-appstream-2-0-now-offers-persistent-storage-for-end-users-files-backed-by-amazon-s3/">persistent storage with Amazon S3</a>, and using other commonly requested features reviewed in the <a href="https://aws.amazon.com/blogs/aws/amazon-appstream-2-0-launch-recap-domain-join-simple-network-setup-and-lots-more/">Amazon AppStream 2.0 Launch Recap – Domain Join, Simple Network Setup, and Lots More</a> post.</p> 
<p>To learn more about AppStream 2.0 and capabilities added this year, see <a href="https://aws.amazon.com/appstream2/resources/">Amazon AppStream 2.0 Resources</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2792');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
