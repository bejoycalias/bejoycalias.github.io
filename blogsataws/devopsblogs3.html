<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/devopsblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li class="active"><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="devopsblogs1.html">Page 1</a>|<a href="devopsblogs2.html">Page 2</a>|<a href="devopsblogs3.html">Page 3</a>|<a href="devopsblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building a Secure Cross-Account Continuous Delivery Pipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Anuj Sharma</span></span> | on 
<time property="datePublished" datetime="2017-05-16T08:58:11+00:00">16 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Most organizations create multiple AWS accounts because they provide the highest level of resource and security isolation. In this blog post, I will discuss how to use cross account <a title="undefined" href="https://aws.amazon.com/iam/" target="null">AWS Identity and Access Management (IAM)</a> access to orchestrate continuous integration and continuous deployment.</p> 
<b>Do I need multiple accounts?</b> 
<p>If you answer “yes” to any of the following questions you should consider creating more AWS accounts:</p> 
<li>Does your business require administrative isolation between workloads? Administrative isolation by account is the most straightforward way to grant independent administrative groups different levels of administrative control over AWS resources based on workload, development lifecycle, business unit (BU), or data sensitivity.</li> 
<li>Does your business require limited visibility and discoverability of workloads? Accounts provide a natural boundary for visibility and discoverability. Workloads cannot be accessed or viewed unless an administrator of the account enables access to users managed in another account.</li> 
<li>Does your business require isolation to minimize blast radius? Separate accounts help define boundaries and provide natural blast-radius isolation to limit the impact of a critical event such as a security breach, an unavailable <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">AWS Region or Availability Zone</a>, account suspensions, and so on.</li> 
<li>Does your business require a particular workload to operate within <a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html">AWS service limits</a> without impacting the limits of another workload? You can use AWS account service limits to impose restrictions on a business unit, development team, or project. For example, if you create an AWS account for a project group, you can limit the number of Amazon Elastic Compute Cloud (<a href="https://aws.amazon.com/ec2/">Amazon EC2</a>) or high performance computing (HPC) instances that can be launched by the account.</li> 
<li>Does your business require strong isolation of recovery or auditing data? If regulatory requirements require you to control access and visibility to auditing data, you can isolate the data in an account separate from the one where you run your workloads (for example, by writing <a href="https://aws.amazon.com/cloudtrail/?sc_channel=PS&amp;sc_campaign=acquisition_US&amp;sc_publisher=google&amp;sc_medium=cloudtrail_b_test_q32016&amp;sc_content=cloudtrail_bmm&amp;sc_detail=%2Bcloudtrail&amp;sc_category=cloudtrial&amp;sc_segment=105093175122&amp;sc_matchtype=b&amp;sc_country=US&amp;s_kwcid=AL!4422!3!105093175122!b!!g!!%2Bcloudtrail&amp;ef_id=WFGCgwAABYUoeNPB:20170502153559:s">AWS CloudTrail</a> logs to a different account).</li> 
<li>Do your workloads depend on specific instance reservations to support high availability (HA) or disaster recovery (DR) capacity requirements? Reserved Instances (RIs) ensure reserved capacity for services such as Amazon EC2 and Amazon Relational Database Service (<a href="https://aws.amazon.com/rds/">Amazon RDS</a>) at the individual account level.</li> 
<b>Use case</b> 
<p>The identities in this use case are set up as follows:</p> 
<li><strong>DevAccount </strong></li> 
<p>Developers check the code into an <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a> repository. It stores all the repositories as a single source of truth for application code. Developers have full control over this account. This account is usually used as a sandbox for developers.</p> 
<li><strong>ToolsAccount</strong></li> 
<p>A central location for all the tools related to the organization, including continuous delivery/deployment services such as <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> and <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a>. Developers have limited/read-only access in this account. The Operations team has more control.</p> 
<li><strong>TestAccount</strong></li> 
<p>Applications using the CI/CD orchestration for test purposes are deployed from this account. Developers and the Operations team have limited/read-only access in this account.</p> 
<li><strong>ProdAccount</strong></li> 
<p>Applications using the CI/CD orchestration tested in the ToolsAccount are deployed to production from this account. Developers and the Operations team have limited/read-only access in this account.</p> 
<b>Solution</b> 
<p>In this solution, we will check in sample code for an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function in the Dev account. This will trigger the pipeline (created in AWS CodePipeline) and run the build using AWS CodeBuild in the Tools account. The pipeline will then deploy the Lambda function to the Test and Prod accounts.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/CrossAccount-CI-CD.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/CrossAccount-CI-CD.jpg" /></a></p> 
<p>&nbsp;</p> 
<b>Setup</b> 
<ol> 
<li>Clone this repository. It contains the <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation templates </a>that we will use in this walkthrough.</li> 
</ol> 
<code class="lang-bash">git clone https://github.com/awslabs/aws-refarch-cross-account-pipeline.git
</code> 
<ol start="2"> 
<li>Follow the instructions in the repository README to push the sample AWS Lambda application to an AWS CodeCommit repository in the Dev account.</li> 
<li>Install the AWS Command Line Interface as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">here</a>. To prepare your access keys or assume-role to make calls to AWS, configure the AWS CLI as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">here</a>.</li> 
</ol> 
<b></b> 
<b>Walkthrough</b> 
<p><strong>Note:</strong> Follow the steps in the order they’re written. Otherwise, the resources might not be created correctly.</p> 
<ol> 
<li>In the Tools account, deploy this CloudFormation template. It will create <a href="http://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys">the customer master keys</a> (CMK) in <a href="http://docs.aws.amazon.com/kms/latest/developerguide/overview.html">AWS Key Management Service</a> (AWS KMS), grant access to Dev, Test, and Prod accounts to use these keys, and create an <a href="https://aws.amazon.com/s3/">Amazon S3</a> bucket to hold artifacts from AWS CodePipeline.</li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --stack-name pre-reqs \
--template-file ToolsAcct/pre-reqs.yaml --parameter-overrides \
DevAccount=ENTER_DEV_ACCT TestAccount=ENTER_TEST_ACCT \
ProductionAccount=ENTER_PROD_ACCT</code> 
<p>In the output section of the CloudFormation console, make a note of the <a href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">Amazon Resource Number</a> (ARN) of the CMK and the S3 bucket name. You will need them in the next step.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/10/Screen-Shot-2017-04-18-at-5.36.32-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/10/Screen-Shot-2017-04-18-at-5.36.32-PM.png" /></a></p> 
<ol start="2"> 
<li>In the Dev account, which hosts the AWS CodeCommit repository, deploy this CloudFormation template. This template will create the IAM roles, which will later be assumed by the pipeline running in the Tools account. Enter the AWS account number for the Tools account and the CMK ARN.</li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --stack-name toolsacct-codepipeline-role \
--template-file DevAccount/toolsacct-codepipeline-codecommit.yaml \
--capabilities CAPABILITY_NAMED_IAM \
--parameter-overrides ToolsAccount=ENTER_TOOLS_ACCT CMKARN=FROM_1st_Step</code> 
<ol start="3"> 
<li>In the Test and Prod accounts where you will deploy the Lambda code, execute this CloudFormation template. This template creates IAM roles, which will later be assumed by the pipeline to create, deploy, and update the sample AWS Lambda function through CloudFormation.</li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --stack-name toolsacct-codepipeline-cloudformation-role \
--template-file TestAccount/toolsacct-codepipeline-cloudformation-deployer.yaml \
--capabilities CAPABILITY_NAMED_IAM \
--parameter-overrides ToolsAccount=ENTER_TOOLS_ACCT CMKARN=FROM_1st_STEP  \
S3Bucket=FROM_1st_STEP</code> 
<ol start="4"> 
<li>In the Tools account, which hosts AWS CodePipeline, execute this CloudFormation template. This creates a pipeline, but does not add permissions for the cross accounts (Dev, Test, and Prod).</li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --stack-name sample-lambda-pipeline \
--template-file ToolsAcct/code-pipeline.yaml \
--parameter-overrides DevAccount=ENTER_DEV_ACCT TestAccount=ENTER_TEST_ACCT \
ProductionAccount=ENTER_PROD_ACCT CMKARN=FROM_1st_STEP \
S3Bucket=FROM_1st_STEP--capabilities CAPABILITY_NAMED_IAM</code> 
<ol start="5"> 
<li>In the Tools account, execute this CloudFormation template, which give access to the role created in step 4. This role will be assumed by AWS CodeBuild to decrypt artifacts in the S3 bucket. This is the same template that was used in step 1, but with different parameters.</li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --stack-name pre-reqs \
--template-file ToolsAcct/pre-reqs.yaml \
--parameter-overrides CodeBuildCondition=true</code> 
<ol start="6"> 
<li>In the Tools account, execute this CloudFormation template, which will do the following: 
<ol start="a"> 
<li>Add the IAM role created in step 2. This role is used by AWS CodePipeline in the Tools account for checking out code from the AWS CodeCommit repository in the Dev account.</li> 
<li>Add the IAM role created in step 3. This role is used by AWS CodePipeline in the Tools account for deploying the code package to the Test and Prod accounts.</li> 
</ol> </li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --stack-name sample-lambda-pipeline \
--template-file ToolsAcct/code-pipeline.yaml \
--parameter-overrides CrossAccountCondition=true \
--capabilities CAPABILITY_NAMED_IAM</code> 
<b>What did we just do?</b> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/details-cross-account-pipeline.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/details-cross-account-pipeline.png" /></a></p> 
<ol> 
<li>The pipeline created in step 4 and updated in step 6 checks out code from the AWS CodeCommit repository. It uses the IAM role created in step 2. The IAM role created in step 4 has permissions to assume the role created in step 2. This role will be assumed by AWS CodeBuild to decrypt artifacts in the S3 bucket, as described in step 5.</li> 
<li>The IAM role created in step 2 has permission to check out code. See <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/DevAccount/toolsacct-codepipeline-codecommit.yaml#L35">here</a>.</li> 
<li>The IAM role created in step 2 also has permission to upload the checked-out code to the S3 bucket created in step 1. It uses the KMS keys created in step 1 for server-side encryption.</li> 
<li>Upon successfully checking out the code, AWS CodePipeline triggers AWS CodeBuild. The AWS CodeBuild project created in step 4 is configured to use the CMK created in step 1 for cryptography operations. See <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/ToolsAcct/code-pipeline.yaml#L94">here</a>. The AWS CodeBuild role is created later in step 4. In step 5, access is granted to the AWS CodeBuild role to allow the use of the CMK for cryptography.</li> 
<li>AWS CodeBuild uses <a href="https://pypi.python.org/pypi/pip">pip</a> to install any libraries for the sample Lambda function. It also executes the <a href="http://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html">aws cloudformation package</a> command to create a Lambda function deployment package, uploads the package to the specified S3 bucket, and adds a reference to the uploaded package to the CloudFormation template. See <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/ToolsAcct/code-pipeline.yaml#L118">here</a>.</li> 
<li>Using the role created in step 3, AWS CodePipeline executes the transformed CloudFormation template (received as an output from AWS CodeBuild) in the Test account. The AWS CodePipeline role created in step 4 has permissions to assume the IAM role created in step 3, as described in step 5.</li> 
<li>The IAM role assumed by AWS CodePipeline passes the role to an IAM role that can be assumed by CloudFormation. AWS CloudFormation creates and updates the Lambda function using the code that was built and uploaded by AWS CodeBuild.</li> 
</ol> 
<p>This is what the pipeline looks like using the sample code:<br /> <a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/Screen-Shot-2017-04-27-at-10.56.56-AM.png"><img class="aligncenter wp-image-1011 size-medium" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/Screen-Shot-2017-04-27-at-10.56.56-AM.png" /></a></p> 
<b>Conclusion</b> 
<p>Creating multiple AWS accounts provides the highest degree of isolation and is appropriate for a number of use cases. However, keeping a centralized account to orchestrate continuous delivery and deployment using AWS CodePipeline and AWS CodeBuild eliminates the need to duplicate the delivery pipeline. You can secure the pipeline through the use of cross account IAM roles and the encryption of artifacts using AWS KMS. For more information, see <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html">Providing Access to an IAM User in Another AWS Account That You Own in the IAM User Guide</a>.</p> 
<b>References</b> 
<li><a href="https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Security_Strategy.pdf">AWS Multiple Account Security Strategy</a></li> 
<li><a href="https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Billing_Strategy.pdf">AWS Multiple Account Billing Strategy</a></li> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Use AWS CloudFormation to Automate the Creation of an S3 Bucket with Cross-Region Replication Enabled</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Rajakumar Sampathkumar</span></span> | on 
<time property="datePublished" datetime="2017-05-15T13:16:10+00:00">15 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/use-aws-cloudformation-to-automate-the-creation-of-an-s3-bucket-with-cross-region-replication-enabled/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-938" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=938&amp;disqus_title=Use+AWS+CloudFormation+to+Automate+the+Creation+of+an+S3+Bucket+with+Cross-Region+Replication+Enabled&amp;disqus_url=https://aws.amazon.com/blogs/devops/use-aws-cloudformation-to-automate-the-creation-of-an-s3-bucket-with-cross-region-replication-enabled/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-938');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>At the request of many of our customers, in this blog post, we will discuss how to use <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> to create an <a href="https://aws.amazon.com/s3/">S3</a> bucket with <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html">cross-region replication</a> enabled. We’ve included a CloudFormation template with this post that uses an <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>-backed custom resource to create source and destination buckets.</p> 
<p><strong></strong></p> 
<strong> <h3>What is S3 cross-region replication?</h3> </strong> 
<p><strong></strong></p> 
<p>Cross-region replication is a bucket-level feature that enables automatic, asynchronous copying of objects across buckets in different AWS regions. You can create two buckets in two different regions and use the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html#cfn-s3-bucket-replicationconfiguration">ReplicationConfiguration</a> property to replicate the objects from one bucket to the other. For example, you can have a bucket in us-east-1 and replicate the bucket objects to a bucket in us-west-2.</p> 
<p>For more information, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr-what-is-isnot-replicated.html">What Is and Is Not Replicated in Cross-Region Replication</a>.</p> 
<p><strong></strong></p> 
<strong> <h3>Challenge</h3> </strong> 
<p><strong></strong></p> 
<p>When you enable cross-region replication, the replicated objects will be stored in only one destination (an S3 bucket). The destination bucket must already exist and it must be in an AWS region different from your source bucket.</p> 
<p>Using CloudFormation, you cannot create the destination bucket in a region different from the region in which you are creating your stack. To create the destination bucket, you can:</p> 
<li>Use another CloudFormation template.</li> 
<li>Use <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">AWS Lambda-backed custom resources</a> in the same template.</li> 
<p><strong></strong></p> 
<strong> <h3>Solution overview</h3> </strong> 
<p><strong></strong></p> 
<p>The CloudFormation template provided with this post uses an AWS Lambda-backed custom resource to create an S3 destination bucket in one region and a source S3 bucket in the same region as the CloudFormation endpoint.</p> 
<p><strong>Note</strong>: In this scenario, CloudFormation is not aware of the destination bucket created by AWS Lambda. For this reason, CloudFormation will not delete this resource when the stack is deleted.</p> 
<p><span id="more-938"></span></p> 
<p><strong></strong></p> 
<strong> <h3>How does it work?</h3> </strong> 
<p><strong></strong></p> 
<p>Launch the stack and provide the following custom values to the CloudFormation template. These (user input) values will be passed as <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html">parameters</a> to the template.</p> 
<li>OriginalBucketName</li> 
<li>ReplicationBucketName</li> 
<li>ReplicationRegion (different from the source region from which you are launching the stack)</li> 
<p>After the parameters are received by the template, the CloudFormation stack creates these IAM roles:</p> 
<li>A Lambda execution role with access to Amazon CloudWatch Logs, Amazon EC2, and Amazon S3</li> 
<li>An S3 role with AmazonS3FullAccess</li> 
<p>The AWS Lambda function is created after the roles are created. Lambda triggers the creation of the S3 destination bucket in the region specified in the CloudFormation template. Versioning is enabled on the bucket.</p> 
<p>When the destination bucket is available, CloudFormation initiates the creation of the source bucket with cross-region replication enabled. The destination bucket is the target for cross-region replication.</p> 
<p><strong>Note</strong>: The creation of the IAM role and Lambda function is automated in the template. You do not need not create them manually.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/03/AWS-Blogpost-Create-S3-bucket-with-CRR-using-cloud-formation-v2-952x1024.jpg" /></p> 
<p><strong></strong></p> 
<strong> <h3>Automated deployment</h3> </strong> 
<p><strong></strong></p> 
<p>The step-by-step instructions in this section show you how you can automate the creation of an S3 bucket with cross-region replication enabled. After you click the button, the bucket will be created in approximately two minutes.</p> 
<p><strong>Note</strong>: Running this solution may result in charges to your AWS account. These include possible charges for Amazon S3 and AWS Lambda.</p> 
<p>1. Sign in to the AWS Management Console and open the AWS CloudFormation console. Choose the <strong>Launch Stack</strong> button to create the AWS CloudFormation stack (S3CrossRegionReplication).</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=S3CrossRegionReplication&amp;templateURL=https://s3.amazonaws.com/blog-s3-crr-automate/cfn-s3-x-region-replication_final.yml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/03/30/Launch-Stack.png" /></a></p> 
<p><em>The template will be loaded from an S3 bucket automatically.</em></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_1-1024x597.png" /></p> 
<p>2. On the <strong>Specify details</strong> page, change the stack name, if required. Provide the following custom values to the CloudFormation template. These (user input) values will be passed as parameters to the template.</p> 
<li>OriginalBucketName</li> 
<li>ReplicationBucketName</li> 
<li>ReplicationRegion</li> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_2-1024x584.png" /></p> 
<p>Choose <strong>Next</strong>.</p> 
<p>3. On the <strong>Options</strong> page, you can specify tags for your AWS CloudFormation template, if you like, and then choose <strong>Next</strong>.</p> 
<p>Permissions are built in the template. You don’t have to choose an IAM role.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_3-1024x674.png" /></p> 
<p>Choose <strong>Next</strong>.</p> 
<p>4. On the <strong>Review</strong> page, review your template details. Select the acknowledgement check box, and then choose <strong>Create</strong> to create the stack.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_4-1024x749.png" /></p> 
<p>You can also <a href="https://s3.amazonaws.com/blog-s3-crr-automate/cfn-s3-x-region-replication_final.yml">download the template</a> and use it as a starting point for your own implementation. The template is launched in the US East (N. Virginia) region by default. To launch the CloudFormation stack in a different AWS region, use the region selector in the console navigation bar after you click <strong>Launch stack</strong>.</p> 
<p><strong>Note</strong>: Because this solution uses AWS Lambda, which is currently available in selected regions only, be sure you launch this solution in an AWS region where Lambda is available. For more information, see <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS service offerings by region</a>.</p> 
<p><strong></strong></p> 
<strong> <h3>Conclusion</h3> </strong> 
<p><strong></strong></p> 
<p>In this blog post, we showed you how to use a single AWS CloudFormation template and AWS Lambda-backed custom resources to create an S3 bucket with cross-region replication enabled.<br /> &nbsp;<br /> <em>I would like to thank my colleague Arun Tunuri for his contributions in designing the CloudFormation template.</em></p> 
<p>&nbsp;</p> 
<hr /> 
<p><strong></strong></p> 
<strong> <h3>About the author</h3> </strong> 
<p><strong></strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/d695a2abcda41fc682c5ef06305d8d21.jpeg" /><strong>Rajakumar Sampathkumar</strong> is a Senior Technical Account Manager for Amazon Web Services. In his spare time, he is a passionate author and likes to spend quality time with his kids and nature.</p> 
<p>&nbsp;<br /> &nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-938');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Performing Blue/Green Deployments with AWS CodeDeploy and Auto Scaling Groups</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Jeff Levine</span></span> | on 
<time property="datePublished" datetime="2017-04-18T23:26:52+00:00">18 APR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Jeff Levine is a Solutions Architect for Amazon Web Services.</em></p> 
<p>Amazon Web Services offers services that enable organizations to leverage the power of the cloud for their development and deployment needs. <a href="https://aws.amazon.com/codedeploy/">AWS CodeDeploy</a> makes it possible to automate the deployment of code to either <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> or on-premises instances. AWS CodeDeploy now supports <a href="https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/">blue/green deployments</a>. In this blog post, I will discuss the benefits of blue/green deployments and show you how to perform one.</p> 
<p><span id="more-870"></span></p> 
<p><strong>The benefits of blue/green deployments</strong></p> 
<p>Blue/green deployment involves two production environments:</p> 
<li>Blue is the active environment.</li> 
<li>Green is for the release of a new version.</li> 
<p>Here are some of the advantages of a blue/green deployment:</p> 
<li>You can perform testing on the green environment without disrupting the blue environment.</li> 
<li>Switching to the green environment involves no downtime. It only requires the redirecting of user traffic.</li> 
<li>Rolling back from the green environment to the blue environment in the event of a problem is easier because you can redirect traffic to the blue environment without having to rebuild it.</li> 
<li>You can incorporate the principle of infrastructure immutability by provisioning fresh instances when you need to make changes. In this way, you avoid configuration drift.</li> 
<p>AWS CodeDeploy offers two ways to perform blue/green deployments:</p> 
<li>In the first approach, AWS CodeDeploy makes a copy of an Auto Scaling group. It, in turn, provisions new Amazon EC2 instances, deploys the application to these new instances, and then redirects traffic to the newly deployed code.</li> 
<li>In the second approach, you use instance tags or an Auto Scaling group to select the instances that will be used for the green environment. AWS CodeDeploy then deploys the code to the tagged instances.</li> 
<p>So how do you set up your first blue environment? A best practice is to start with an in-place deployment. You can also start with an existing, empty Auto Scaling group.</p> 
<p><strong>An example of blue/green deployments</strong></p> 
<p>Let’s take a look at an example of how to use Auto Scaling groups to perform a blue/green deployment.</p> 
<p><strong>Overview</strong></p> 
<p>In the following figure, the example environment includes an Amazon EC2 instance that serves as a workstation for AWS CodeDeploy. A release manager or developer could use this workstation to deploy new versions of code. The blue environment consists of an Auto Scaling group that provisions two more instances to function as web servers. The web servers will initially contain the first version of an application and the AWS CodeDeploy agent. A load balancer directs traffic to the two web servers in a round-robin manner.</p> 
<p>The release manager uses the workstation instance to push a new version of the application to AWS CodeDeploy and starts a blue-green deployment. AWS CodeDeploy creates a copy of the Auto Scaling group. It launches two new web server instances just like the original two. AWS CodeDeploy installs the new version of the application and then redirects the load balancer to the new instances. The original instances continue to be part of the original Auto Scaling group. They can be reattached to the load balancer, if needed.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/18/CDBG-topology.png" /></p> 
<p><strong>Prerequisites for building the example</strong></p> 
<p>Here are the things you will need to build out this example.</p> 
<li>An IAM user with permissions to use Amazon EC2, Amazon S3, Amazon VPC, AWS CodeDeploy, and AWS CloudFormation.</li> 
<li>An AWS region and Availability Zone in which you can provision the environment.</li> 
<li>An Amazon EC2 key pair.</li> 
<li>Working knowledge of the aforementioned services and the AWS Management Console, and familiarity with connecting to an Amazon EC2 instance.</li> 
<p><strong>Other Considerations</strong></p> 
<p>You will incur charges from AWS for the use of the underlying AWS services in this example. The Amazon EC2 t2.micro instances and Amazon S3 storage might be covered under the <a href="https://aws.amazon.com/free/">AWS Free Tier</a>, depending on your eligibility. The resources provided in this example are for training purposes. Be sure to consider the security needs of your organization when implementing techniques similar to those described in this blog post.</p> 
<p>Step 1: Create the initial environment</p> 
<ol> 
<li>Download an archive containing the sample template from this <a href="https://github.com/awslabs/codedeploy-blue-green" target="_blank">location</a> and save it in a convenient location.</li> 
<li>Sign in to the AWS Management Console and open the AWS CloudFormation console at <a href="https://console.aws.amazon.com/cloudformation/">https://console.aws.amazon.com/cloudformation/</a>.</li> 
<li>If this is a new AWS CloudFormation account, click <strong>Create New Stack</strong>. Otherwise, click <strong>Create Stack</strong>.</li> 
<li>Under <strong>Upload a template to Amazon S3</strong>, click <strong>Choose File</strong>, choose the YAML file from the archive you downloaded, and then click <strong>Next</strong>.</li> 
<li>In <strong>Specify Details</strong>, in <strong>Stack name</strong>, type <code>bluegreen</code>.</li> 
<li>In <strong>AZName</strong>, select one of the Availability Zones. (In this blog post, I am using us-east-1a.)</li> 
<li>In <strong>BlueGreenKeyPairName</strong>, select the key pair to use.</li> 
<li>In <strong>NamePrefix</strong>, use the default value of <code>bluegreen</code>&nbsp;unless you are already running an application with a name that starts with <code>bluegreen</code>. The name prefix is used to assign name tags to the created resources. Click <strong>Next</strong>.</li> 
<li>On the Options page, click <strong>Next</strong>.</li> 
<li>Select the acknowledgement box to allow the creation of IAM resources, and then click <strong>Create</strong>. It will take CloudFormation about 10 minutes to create the sample environment. In addition to creating the infrastructure resources shown in the diagram, the CloudFormation template also sets up an AWS CodeDeploy application and blue/green deployment group.</li> 
</ol> 
<p>Step 2: Review initial environment</p> 
<ol> 
<li>Look at the CloudFormation stack outputs. You should see something similar to the following.&nbsp;<strong>WorkstationIP</strong> is the IP address of the workstation instance.&nbsp;<strong>AutoScalingGroup</strong> and <strong>LoadBalancer</strong> are the DNS names created by CloudFormation for the Auto Scaling group and the Elastic Load Balancing load balancer.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/CloudFormationOutputs2-1.png" /></li> 
<li>Copy the <strong>LoadBalancer</strong> value into your browser and browse to that link. The following application should be displayed. This PHP application queries the Amazon EC2 instance metadata. If you refresh the page, you will see the IP address and instance ID change in accordance with the round-robin load balancing algorithm.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/appversion1-v2-1.png" /></li> 
<li>Go to the EC2 console and display the instances. You will see three running instances associated with this example: the workstation and the two web server instances created by the Auto Scaling group. The web server instances make up the blue environment.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/instances1-v2.png" /></li> 
</ol> 
<p>Step 3: Deploy the new version of code</p> 
<ol> 
<li><a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html">Connect to the workstation instance</a> at the address displayed in <strong>WorkStationIP</strong>. This instance is running the Ubuntu operating system, so the user name is <strong>ubuntu</strong>. After you sign in, you will see two directories. The scripts directory contains Bourne shell scripts. The newversion directory contains an update to the PHP application.</li> 
<li>Here is the PHP code for the new version in newversion/content/index.php. The only difference from the initially installed code is the application version number.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/newappcode-1.png" /></li> 
<li>Now look at the following scripts/pushnewversion.sh shell script. It uses the <strong>aws deploy push command</strong> to bundle the code and upload it to Amazon S3.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/pushversion.png" /></li> 
<li>Run the pushnewversion.sh script. You will see a message that tells you how to deploy the code with the AWS command line interpreter, but we will use the AWS CodeDeploy console to do this instead.</li> 
<li>Open the AWS CodeDeploy console at <a href="https://console.aws.amazon.com/codedeploy">https://console.aws.amazon.com/codedeploy</a>.</li> 
<li>Click the link for bluegreen-app. If you chose a name other than the default for <strong>NamePrefix</strong>, click that name instead. Expand <strong>Revisions</strong>. You will see the revision you just pushed from the AWS CodeDeploy workstation. Click <strong>Deploy revision</strong>.</li> 
<li>On the <strong>Create deployment</strong> page, select the bluegreen-app application and the bluegreen-dg deployment group. Leave all the other default values in place, and then click <strong>Deploy</strong>. AWS CodeDeploy will provision the Auto Scaling group and instances, deploy the code, set up health checks, and redirect traffic to the new instances. This process will take a few minutes. When the deployment is complete, the deployment should appear, as shown here. AWS CodeDeploy skips the termination of the original instances because of the settings in the deployment group.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/codedeployresults.png" /></li> 
</ol> 
<p>Step 4: Review the updated environment</p> 
<ol> 
<li>Browse to the DNS name for the load balancer. You should see the new version of the application, as shown here. The application version has changed from 1 to 2, as expected.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/appversion2-v2.png" /></li> 
<li>Go to the EC2 console and display the instances. You will see four instances that have been tagged by the Auto Scaling group and launch configuration. The instances with IP addresses 10.200.11.11 and 10.200.11.192 are the ones we saw before in the blue environment. The deployment process created the instances with IP addresses 10.200.11.13 and 10.200.22 that are now part of the green environment.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/instances2-v2-1.png" /></li> 
<li>Go to the <a href="https://console.aws.amazon.com/ec2/autoscaling/home">Auto Scaling console</a>. You will see that there are now two Auto Scaling groups, each of which has two instances. The Auto Scaling group whose names begins with CodeDeploy was created during the deployment process.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/19/AutoScaling2-v2-1.png" /></li> 
</ol> 
<p>You have now successfully completed a blue/green deployment using AWS CodeDeploy.</p> 
<p>Step 5: Cleanup</p> 
<ol> 
<li>Return to the session on the AWS CodeDeploy workstation.</li> 
<li>Run the scripts/cleanup.sh script. This will remove the deployment bundle and shut down the Auto Scaling groups.</li> 
<li>Go to the CloudFormation console, select the stack you created, and delete it.</li> 
</ol> 
<p><strong>Conclusion</strong></p> 
<p>AWS CodeDeploy enables developers to automate code deployments to Amazon EC2 and on-premises instances. The blue/green deployment option enables release managers to create a new production environment and makes it easier to roll back to the previous environment if problems arise. For more information about AWS CodeDeploy, see the <a href="https://aws.amazon.com/documentation/codedeploy/">AWS CodeDeploy documentation</a>. You can get started in just a few clicks.</p> 
<p>Enjoy life in the blue/green world!</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Use Parameter Store to Securely Access Secrets and Config Data in AWS CodeDeploy</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chirag Dhull</span></span> | on 
<time property="datePublished" datetime="2017-04-11T10:57:28+00:00">11 APR 2017</time> | 
<a href="https://aws.amazon.com/blogs/devops/use-parameter-store-to-securely-access-secrets-and-config-data-in-aws-codedeploy/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. Many customers use AWS CodeDeploy to automate application deployment because it provides a uniform method for:<br /> • Updating applications across development, staging, and production environments.<br /> • Handling the complexity of updating applications and avoiding service downtime.</p> 
<p>&nbsp;<br /> Visit this <a href="https://aws.amazon.com/blogs/mt/use-parameter-store-to-securely-access-secrets-and-config-data-in-aws-codedeploy/">post</a> on the management tools blog and learn how to simplify your AWS CodeDeploy workflows by using Parameter Store to store and reference a configuration secret.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Introducing Amazon CloudWatch Logs Integration for AWS OpsWorks Stacks</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Kai Rubarth</span></span> | on 
<time property="datePublished" datetime="2017-04-10T11:59:44+00:00">10 APR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/new-stuff/" title="View all posts in New stuff"><span property="articleSection">New stuff</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/introducing-amazon-cloudwatch-logs-integration-for-aws-opsworks-stacks/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="http://aws.amazon.com/opsworks">AWS OpsWorks Stacks</a> now supports <a href="http://aws.amazon.com/cloudwatch">Amazon CloudWatch Logs</a>. This benefits all users who want to stream their log files from OpsWorks instances to CloudWatch. This enables you to take advantage of CloudWatch Logs features such as centralized log archival, real-time monitoring of log data, or generating CloudWatch alarms. Until now, OpsWorks customers had to manually install and configure the CloudWatch Logs agent on every instance they wanted to ship log data from. Now, with the built-in CloudWatch Logs integration these features are made available with just a few clicks across all instances in a layer.</p> 
<p><span id="more-852"></span></p> 
<p>The OpsWorks CloudWatch logs integration is compatible with all Chef 11 and Chef 12 Linux stacks. To get started, simply click the new <strong>CloudWatch Logs</strong> tab in your layer settings screen. OpsWorks agent command logs, which include Chef recipe output, are enabled by default; to enable logging for custom log files, simply input an absolute path or file pattern for every log file you want shipped.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/04/07/OpsWorks-CloudWatchLogs.png" /></p> 
<p>It’s that simple. OpsWorks creates log groups and log streams for you, and starts streaming your system and application logs within a few minutes.</p> 
<p>For more information, see <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/monitoring-cloudwatch-logs.html">Using Amazon CloudWatch Logs with AWS OpsWorks Stacks</a> in the <em>AWS OpsWorks User Guide</em>.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using AWS Lambda and Amazon DynamoDB in an Automated Approach to Managing AWS CloudFormation Template Parameters and Mappings</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Anuj Sharma</span></span> | on 
<time property="datePublished" datetime="2017-04-04T10:15:33+00:00">04 APR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/custom-lookup-using-aws-lambda-and-amazon-dynamodb/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> gives you an easy way to codify the creation and management of related AWS resources. The optional <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html">Mappings</a> and <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html">Parameters</a> sections of CloudFormation templates help you organize and parameterize your templates so you can quickly customize your stack. As organizations adopt Infrastructure as Code best practices, the number of mappings and parameters can quickly grow. In this blog post, we’ll discuss how <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> and <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a> can be used to simplify updates, reuse, quick lookups, and reporting for these mappings and parameters.</p> 
<h3>Solution overview</h3> 
<p>There are three parts to the solution described in this post. You’ll find sample code for this solution in this <a href="https://github.com/awslabs/custom-lookup-lambda">AWS Labs GitHub repository</a>.</p> 
<ol> 
<li><strong>DynamoDB table</strong>: Used as a central location to store and update all key-value pairs used in the ‘Mappings’ and ‘Parameters’ sections of the CloudFormation template. This could be a centralized table for the whole organization, with a partition key consisting of the team name and environment (for example, development, test, production) and a sort key for the application name. For more information about the types of keys supported by DynamoDB, see <a href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html#HowItWorks.CoreComponents.PrimaryKey">Core Components</a> in the Amazon DynamoDB Developer Guide.</li> 
</ol> 
<p>Here is the sample data in the table. “teamname-environment” is the partition key and “appname” is the sort key.</p> 
<code class="lang-json">{
&quot;teamname-environment&quot;: &quot;team1-dev&quot;,
&quot;appname&quot;: &quot;app1&quot;,
&quot;mappings&quot;: {
&quot;elbsubnet1&quot;: &quot;subnet-123456&quot;,
&quot;elbsubnet2&quot;: &quot;subnet-234567&quot;,
&quot;appsubnet1&quot;: &quot;subnet-345678&quot;,
&quot;appsubnet2&quot;: &quot;subnet-456789&quot;,
&quot;vpc&quot;: &quot;vpc-123456&quot;,
&quot;appname&quot;:&quot;app1&quot;,
&quot;costcenter&quot;:&quot;123456&quot;,
&quot;teamname&quot;:&quot;team1&quot;,
&quot;environment&quot;:&quot;dev&quot;,
&quot;certificate&quot;:&quot;arn-123456qwertyasdfgh&quot;,
&quot;compliancetype&quot;:&quot;pci&quot;,
&quot;amiid&quot;:&quot;ami-123456&quot;,
&quot;region&quot;:&quot;us-west-1&quot;,
&quot;publichostedzoneid&quot;:&quot;Z234asdf1234asdf&quot;,
&quot;privatehostedzoneid&quot;:&quot;Z345SDFGCVHD123&quot;,
&quot;hostedzonename&quot;:&quot;demo.internal&quot;
}
}
</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/29/Screen-Shot-2017-03-26-at-3.43.48-PM.png" /><br /> &nbsp;</p> 
<ol start="2"> 
<li><strong>Lambda function:</strong> Accepts the inputs of the primary keys, looks up the DynamoDB table, and returns all the key-value data.</li> 
<li><strong>Custom lookup resource:</strong> Makes a call to the Lambda function, with the inputs of primary keys (accepted by the Lambda function) and retrieves the key-value data. All CloudFormation templates can duplicate this&nbsp;generic custom resource.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/29/custom-lookup-workflow.png" /></p> 
<p>&nbsp;</p> 
<p>The diagram shows the interaction of services and resources in this solution.</p> 
<ol> 
<li>Users create a DynamoDB table and, using the DynamoDB console, AWS SDK, or AWS CLI, insert the mappings, parameters, and key-value data.</li> 
<li>CloudFormation templates have a custom resource, which calls a Lambda function. The combination of team name and application environment (“teamname-environment”) is the partition key input. Application Name (“appname”) is the sort key input to the Lambda function.</li> 
<li>The Lambda function queries the DynamoDB table based on the inputs.</li> 
<li>DynamoDB responds to the Lambda function with the results.</li> 
<li>The Lambda function responds to the custom resource in the CloudFormation stack, with the key-value data.</li> 
<li>The key-value data retrieved by the custom resource is then used by other resources in the stack using <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html">GetAtt Intrinsic function</a>.</li> 
</ol> 
<p>&nbsp;</p> 
<b>Using the sample code</b> 
<p>To use the sample code for this solution, follow these steps:</p> 
<ol> 
<li>Clone the repository.</li> 
</ol> 
<code class="lang-bash">git clone https://github.com/awslabs/custom-lookup-lambda.git
cd custom-lookup-lambda
</code> 
<ol start="2"> 
<li>The values in the sample-mappings.json file will be inserted into DynamoDB. Each record in sample-mappings.json corresponds to an item in DynamoDB. The mappings object contains the mapping.</li> 
<li>This solution uses Python as a <a href="http://docs.aws.amazon.com/lambda/latest/dg/python-programming-model-handler-types.html">programming model</a> for AWS Lambda. If you haven’t set up your local development environment for Python, follow <a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create-deploy-python-common-steps.html#python-common-setup-venv">these steps</a>, and then install the <a href="https://pypi.python.org/pypi/awscli/1.11.63">awscli python package</a>.</li> 
</ol> 
<code class="lang-bash">pip install awscli
</code> 
<ol start="4"> 
<li>To prepare your access keys or assume-role to make calls to AWS, configure the AWS Command Line Interface as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">here</a>. The IAM user or the assumed role used to make API calls must have, at minimum, <a href="https://github.com/awslabs/custom-lookup-lambda/blob/master/sample-user-policy.json">this access</a>. You can attach <a href="https://github.com/awslabs/custom-lookup-lambda/blob/master/sample-user-policy.json">this policy</a> to the IAM user or IAM group if you are using access keys, or to the IAM role if you are assuming a role. For more information, see <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-using.html#attach-managed-policy-console">Attaching Managed Policies</a> in the IAM User Guide.</li> 
<li>Run insertrecord.sh to create the DynamoDB table named custom-lookup and insert the items in sample-mappings.json.</li> 
</ol> 
<code class="lang-bash">./insertrecord.sh</code> 
<p>This script does the following:</p> 
<li>Installs <a href="https://pypi.python.org/pypi/boto3">boto3</a> and <a href="https://pypi.python.org/pypi/requests">requests</a> Python packages through <a href="https://pypi.python.org/pypi/pip">pip</a>.</li> 
<li>Executes the Python script to create the DynamoDB table (custom-lookup) and puts the data in sample-mappings.json.</li> 
<ol start="6"> 
<li>Run deployer.sh to package and create the Lambda function.</li> 
</ol> 
<code class="lang-bash">./deployer.sh</code> 
<p>This script does the following:</p> 
<li>Installs <a href="https://pypi.python.org/pypi/boto3">boto3</a> and <a href="https://pypi.python.org/pypi/requests">requests</a> Python packages through <a href="https://pypi.python.org/pypi/pip">pip</a>.</li> 
<li>Uses <a href="http://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html">CloudFormation package cli</a> to package the Lambda function (as coded in lambda-cloudformation.yaml).</li> 
<li>Uploads the packaged code to the S3 bucket (as prompted when you execute this script).</li> 
<li>Uses <a href="http://docs.aws.amazon.com/cli/latest/reference/cloudformation/deploy/index.html">CloudFormation deploy cli </a>to deploy the Lambda function. lambda-cloudformation.yaml also <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html">exports</a> the Lambda function’s<a href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html"> Amazon Resource Name (ARN)</a> using <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html#w1ab2c19c12d587c13">GetAtt intrinsic function</a>, which can be <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html">imported</a> by other stacks.</li> 
<ol start="7"> 
<li>Using sample-stack.yaml, create a stack that makes a call to the Lambda function created in step 6. The function queries the DynamoDB table and produces as output the values corresponding to team1-dev and app1.</li> 
</ol> 
<code class="lang-bash">aws cloudformation deploy --template-file sample-stack.yaml --stack-name sample-stack
</code> 
<ol start="8"> 
<li>Examine the output in the AWS CloudFormation console. You should see the values retrieved from the DynamoDB table. The Fn::GetAtt function allows you to use the values retrieved by the custom resource Lambda function.</li> 
</ol> 
<p>For example, if the custom resource Lambda function is called using resource name as CUSTOMLOOKUP in the sample-stack, the value of key=amiid will be used in the stack using !GetAtt CUSTOMLOOKUP.amiid. Likewise, the value of key=vpc will be used in the stack using !GetAtt CUSTOMLOOKUP.vpc and so on.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/29/Screen-Shot-2017-03-26-at-2.53.28-PM.png" /></p> 
<b>Conclusion</b> 
<p>In this blog post, we showed how to use an AWS CloudFormation custom resource backed by an AWS Lambda function to query Amazon DynamoDB to retrieve key-value data, thereby replacing the Mappings and Parameter sections of the CloudFormation template. This solution provides a more automated approach to managing template parameters and mappings. You can use the DynamoDB table to simplify updates, reuse, quick lookups, and reporting for these mappings and parameters.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Implementing DevSecOps Using AWS CodePipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ramesh Adabala</span></span> | on 
<time property="datePublished" datetime="2017-03-23T10:57:15+00:00">23 MAR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/implementing-devsecops-using-aws-codepipeline/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/devops/what-is-devops/" target="_blank">DevOps </a>is a combination of cultural philosophies, practices, and tools that emphasizes collaboration and communication between software developers and IT infrastructure teams while automating an organization’s ability to deliver applications and services rapidly, frequently, and more reliably.</p> 
<p><a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/" target="_blank">CI/CD</a> stands for continuous integration and continuous deployment. These concepts represent everything related to automation of application development and the deployment pipeline — from the moment a developer adds a change to a central repository until that code winds up in production.</p> 
<p>DevSecOps covers security of and in the CI/CD pipeline, including <a href="https://d0.awsstatic.com/whitepapers/compliance/Intro_to_Security_by_Design.pdf" target="_blank">automating security operations and auditing</a>. The goals of DevSecOps are to:</p> 
<li>Embed security knowledge into DevOps teams so that they can secure the pipelines they design and automate.</li> 
<li>Embed application development knowledge and automated tools and processes into security teams so that they can provide security at scale in the cloud.</li> 
<p>The <a href="https://d0.awsstatic.com/whitepapers/AWS_CAF_Security_Perspective.pdf" target="_blank">Security Cloud Adoption Framework (CAF) whitepaper</a> provides prescriptive controls to improve the security posture of your AWS accounts. These controls are in line with a DevOps blog post published last year about the <a href="https://aws.amazon.com/blogs/devops/it-governance-in-a-dynamic-devops-environment/" target="_blank">control-monitor-fix governance model</a>.</p> 
<p>Security CAF controls are grouped into four categories:</p> 
<li><strong>Directive</strong>: controls establish the governance, risk, and compliance models on AWS.</li> 
<li><strong>Preventive</strong>: controls protect your workloads and mitigate threats and vulnerabilities.</li> 
<li><strong>Detective</strong>: controls provide full visibility and transparency over the operation of your deployments in AWS.</li> 
<li><strong>Responsive</strong>: controls drive remediation of potential deviations from your security baselines.</li> 
<p>To embed the DevSecOps discipline in the enterprise, AWS customers are automating CAF controls using a combination of AWS and third-party solutions.</p> 
<li>AWS solutions: <a href="http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html" target="_blank">Amazon Cloudwatch Alarms</a>,&nbsp; <a href="https://aws.amazon.com/cloudtrail/" target="_blank">AWS CloudTrail</a>,&nbsp; <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html" target="_blank">Amazon CloudWatch Events</a>, <a href="https://aws.amazon.com/lambda/" target="_blank">AWS Lambda</a>, <a href="https://aws.amazon.com/config/" target="_blank">AWS Config</a></li> 
<li>Third-party solutions: <a href="https://aws.amazon.com/marketplace/seller-profile?id=59a38c69-0c0e-490c-8427-e4c4ed1b4371" target="_blank">Dome9</a>, <a href="https://aws.amazon.com/marketplace/seller-profile?id=fad58384-1745-4c81-a502-a081c858ea67" target="_blank">Evident.IO</a></li> 
<p>In this blog post, I will show you how to use a CI/CD pipeline to automate preventive and detective security controls. I’ll use an example that show how you can take the creation of a simple security group through the CI/CD pipeline stages and enforce security CAF controls at various stages of the deployment. I’ll use <a href="https://aws.amazon.com/codepipeline/" target="_blank">AWS CodePipeline</a> to orchestrate the steps in a continuous delivery pipeline.</p> 
<p>These resources are being used in this example:</p> 
<li>An <a href="https://aws.amazon.com/cloudformation/" target="_blank">AWS CloudFormation</a> template to create the demo pipeline.</li> 
<li>A <a href="https://aws.amazon.com/lambda/" target="_blank">Lambda </a>function to perform the static code analysis of the CloudFormation template.</li> 
<li>A <a href="https://aws.amazon.com/lambda/" target="_blank">Lambda </a>function to perform dynamic stack validation for the security groups in scope.</li> 
<li>An <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html" target="_blank">S3 bucket</a> as the sample code repository.</li> 
<li>An <a href="https://aws.amazon.com/cloudformation/" target="_blank">AWS CloudFormation</a> source template file to create the <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html" target="_blank">security groups</a>.</li> 
<li>Two <a href="https://aws.amazon.com/vpc/" target="_blank">VPCs</a> to deploy the test and production security groups.</li> 
<p>These are the high-level security checks enforced by the pipeline:</p> 
<li>During the Source stage, static code analysis for any open security groups. The pipeline will fail if there are any violations.</li> 
<li>During the Test stage, dynamic analysis to make sure port 22 (SSH) is open only to the approved IP CIDR range. The pipeline will fail if there are any violations.</li> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/23/demo_pipeline1.png" /></p> 
<p>&nbsp;</p> 
<p>These are the pipeline stages:</p> 
<p>1. <strong>Source stage</strong>: In this example, the pipeline gets the CloudFormation code that creates the security group from S3, the code repository service.</p> 
<p>This stage passes the CloudFormation template and pipeline name to a Lambda function, CFNValidateLambda. This function performs the static code analysis. It uses the regular expression language to find patterns and identify security group policy violations. If it finds violations, then Lambda fails the pipeline and includes the violation details.</p> 
<p>Here is the regular expression that Lambda function using for static code analysis of the open SSH port:</p> 
<code class="lang-json">&quot;^.*Ingress.*(([fF]rom[pP]ort|[tT]o[pP]ort).\s*:\s*u?.(22).*[cC]idr[iI]p.\s*:\s*u?.((0\.){3}0\/0)|[cC]idr[iI]p.\s*:\s*u?.((0\.){3}0\/0).*([fF]rom[pP]ort|[tT]o[pP]ort).\s*:\s*u?.(22))&quot;</code> 
<p>2. <strong>Test stage</strong>: After the static code analysis is completed successfully, the pipeline executes the following steps:</p> 
<p>a. <strong>Create stack</strong>: This step creates the stack in the test VPC, as described in the test configuration.</p> 
<p>b. <strong>Stack validation</strong>: This step triggers the StackValidationLambda Lambda function. It passes the stack name and pipeline name in the event parameters. Lambda validates the security group for the following security controls. If it finds violations, then Lambda deletes the stack, stops the pipeline, and returns an error message.</p> 
<p>The following is the sample Python code used by AWS Lambda to check if the SSH port is open to the approved IP CIDR range (in this example, 72.21.196.67/32):</p> 
<code class="lang-python">for n in regions:
client = boto3.client('ec2', region_name=n)
response = client.describe_security_groups(
Filters=[{'Name': 'tag:aws:cloudformation:stack-name', 'Values': [stackName]}])
for m in response['SecurityGroups']:
if &quot;72.21.196.67/32&quot; not in str(m['IpPermissions']):
for o in m['IpPermissions']:
try:
if int(o['FromPort']) &lt;= 22 &lt;= int(o['ToPort']):
result = False
failReason = &quot;Found Security Group with port 22 open to the wrong source IP range&quot;
offenders.append(str(m['GroupId']))
except:
if str(o['IpProtocol']) == &quot;-1&quot;:
result = False
failReason = &quot;Found Security Group with port 22 open to the wrong source IP range&quot;
offenders.append(str(n) + &quot; : &quot; + str(m['GroupId']))
</code> 
<p>c. <strong>Approve test stack</strong>: This step creates a manual approval task for stack review. This step could be eliminated for automated deployments.</p> 
<p>d. <strong>Delete test stack</strong>: After all the stack validations are successfully completed, this step deletes the stack in the test environment to avoid unnecessary costs.</p> 
<p>3. <strong>Production stage</strong>: After the static and dynamic security checks are completed successfully, this stage creates the stack in the production VPC using the production configuration supplied in the template.</p> 
<p>a. <strong>Create change set</strong>: This step creates the change set for the resources in the scope.</p> 
<p>b. <strong>Execute change set</strong>: This step executes the change set and creates/updates the security group in the production VPC.</p> 
<p><strong>&nbsp;</strong></p> 
<h3>Source code and CloudFormation template</h3> 
<p>You’ll find the source code at <a href="https://github.com/awslabs/automating-governance-sample/tree/master/DevSecOps-Blog-Code">https://github.com/awslabs/automating-governance-sample/tree/master/DevSecOps-Blog-Code</a></p> 
<p><a href="https://github.com/awslabs/automating-governance-sample/blob/master/DevSecOps-Blog-Code/basic-sg-3-cfn.json">basic-sg-3-cfn.json</a> creates the pipeline in AWS CodePipeline with all the stages previously described. It also creates the static code analysis and stack validation Lambda functions.</p> 
<p>The CloudFormation template points to a shared S3 bucket. The codepipeline-lambda.zip file contains the Lambda functions. Before you run the template, upload the zip file to your S3 bucket and then update the CloudFormation template to point to your S3 bucket location.</p> 
<p>The CloudFormation template uses the codepipe-single-sg.zip file, which contains the sample security group and test and production configurations. Update these configurations with your VPC details, and then upload the modified zip file to your S3 bucket.</p> 
<p>Update these parts of the code to point to your S3 bucket:</p> 
<code class="lang-json"> &quot;S3Bucket&quot;: {
&quot;Default&quot;: &quot;codepipeline-devsecops-demo&quot;,
&quot;Description&quot;: &quot;The name of the S3 bucket that contains the source artifact, which must be in the same region as this stack&quot;,
&quot;Type&quot;: &quot;String&quot;
},
&quot;SourceS3Key&quot;: {
&quot;Default&quot;: &quot;codepipe-single-sg.zip&quot;,
&quot;Description&quot;: &quot;The file name of the source artifact, such as myfolder/myartifact.zip&quot;,
&quot;Type&quot;: &quot;String&quot;
},
&quot;LambdaS3Key&quot;: {
&quot;Default&quot;: &quot;codepipeline-lambda.zip&quot;,
&quot;Description&quot;: &quot;The file name of the source artifact of the Lambda code, such as myfolder/myartifact.zip&quot;,
&quot;Type&quot;: &quot;String&quot;
},
&quot;OutputS3Bucket&quot;: {
&quot;Default&quot;: &quot;codepipeline-devsecops-demo&quot;,
&quot;Description&quot;: &quot;The name of the output S3 bucket that contains the processed artifact, which must be in the same region as this stack&quot;,
&quot;Type&quot;: &quot;String&quot;
},</code> 
<p>After the stack is created, AWS CodePipeline executes the pipeline and starts deploying the sample CloudFormation template. In the default template, security groups have wide-open ports (0.0.0.0/0), so the pipeline execution will fail. Update the CloudFormation template in codepipe-single-sg.zip with more restrictive ports and then upload the modified zip file to S3 bucket. Open the AWS CodePipeline console, and choose the <strong>Release Change</strong> button. This time the pipeline will successfully create the security groups.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/23/demo_pipeline2.png" /></p> 
<p>You could expand the security checks in the pipeline to include other AWS resources, not just security groups. The following table shows the sample controls you could enforce in the pipeline using the static and dynamic analysis Lambda functions.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/23/demo_pipeline3.png">Developer Tools forum</a>.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Replicating and Automating Sync-Ups for a Repository with AWS CodeCommit</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Cherry Zhou</span></span> | on 
<time property="datePublished" datetime="2017-03-13T17:01:41+00:00">13 MAR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/replicating-and-automating-sync-ups-for-a-repository-with-aws-codecommit/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>by Chenwei (Cherry) Zhou, Software Development Engineer</p> 
<hr /> 
<p>&nbsp;</p> 
<p>Many of our customers have expressed interest in the following scenarios:</p> 
<li>Backing up or replicating an AWS CodeCommit repository to another AWS region.</li> 
<li>Automatically backing up repositories currently hosted on other services (for example, GitHub or BitBucket) to AWS CodeCommit.</li> 
<p>In this blog post, we’ll show you how to automate the replication of a source repository to a repository in AWS CodeCommit. Your source repository could be another AWS CodeCommit repository, a local repository, or a repository hosted on other Git services.</p> 
<p>To replicate your repository, you’ll first need to set up a repository in AWS CodeCommit to use as your backup/replica repository. After replicating the contents in your source repository to the backup repository, we’ll demonstrate how you can set up a scheduled job to periodically sync up your source repository with the backup/replica.</p> 
<p><span id="more-705"></span></p> 
<p><strong>Where do I host this?</strong></p> 
<p>You can host your local repository and schedule your task on your own machine or on an Amazon EC2 instance. For an example of how to set up an EC2 instance for access to an AWS CodeCommit repository, including a sample AWS CloudFormation template for launching the instance, see <a href="http://docs.aws.amazon.com/devops/latest/gsg/setup-codecommit-launch-instance.html">Launch an Amazon EC2 Instance to Access the AWS CodeCommit Repository</a> in the AWS for DevOps Guide.</p> 
<p>&nbsp;</p> 
<p><strong>Part 1: Set Up a Replica Repository</strong></p> 
<p>In this section, we’ll create an AWS CodeCommit repository and replicate your source repository to it.</p> 
<ol> 
<li>If you haven’t already done so, <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html">set up for AWS CodeCommit</a>. Then follow the steps to <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/how-to-create-repository.html">create a CodeCommit repository</a> in the region of your choice. Choose a name that will help you remember that this repository is a replica or backup repository. For example, you could create a repository in the US East (Ohio) region and name it <em><code class="lang-git">MyReplicaRepo</code></em>. This is the name and region we’ll use in this post.</li> 
<li>Use the <code class="lang-git">git clone --mirror</code> command to clone the source repository, including the directory where you want to create the local repo, to your local computer. You are not cloning the repository you just created in AWS CodeCommit. You are cloning the repository you want to replicate or back up to that AWS CodeCommit repository. For example, to clone a sample application created for AWS demonstration purposes and hosted on GitHub <em>(https://github.com/awslabs/aws-demo-php-simple-app.git)</em> to a local repo in a directory named <em><code class="lang-git">my-repo-replica</code></em>:</li> 
</ol> 
git clone --mirror https://github.com/awslabs/aws-demo-php-simple-app.git <span style="color: #c24040"><em>my-repo-replica</em></span> 
<p><strong>IMPORTANT</strong></p> 
<li>DO NOT use your working directory as the local clone repository. Your work-in-progress commits would also be pushed for backup.</li> 
<li>DO NOT make local changes to this local repository. It should be used for sync-up operations only.</li> 
<li>DO NOT manually push any changes to this replica repository. It will cause conflicts later when your scheduled job pushes changes in the source repository. Treat it as a read-only repository, and push all of your development changes to your source repository.</li> 
<ol start="3"> 
<li>Change directories to the directory where you made the clone:</li> 
</ol> 
cd <span style="color: #c24040"><em>my-repo-replica</em></span> 
<ol start="4"> 
<li>Use the&nbsp;<strong><code class="lang-git">git remote add</code> </strong><em><code class="lang-git">RemoteName RemoteRepositoryURL</code></em> command to add the AWS CodeCommit repository you created as a remote repository for the local repo. Use an appropriate nickname, such as <em><code class="lang-git">sync</code></em>. (Because this is a mirror, the default nickname, <strong>origin</strong>, will already be in use.) For example, to add your AWS CodeCommit repository&nbsp;<em><code class="lang-git">MyReplicaRepo</code></em> as a remote for&nbsp;<em><code class="lang-git">my-repo-replica</code></em> with the nickname <em>sync</em>:</li> 
</ol> 
git remote add <span style="color: #c24040"><em>sync</em></span> ssh://git-codecommit.us-east-2.amazonaws.com/v1/repos/<span style="color: #c24040"><em>MyReplicaRepo</em></span> 
<p style="padding-left: 30px">When you push large repositories, consider using SSH instead of HTTPS. When you push a large change, a large number of changes, or a large repository, long-running HTTPS connections are often terminated prematurely due to networking issues or firewall settings. For more information about setting up AWS CodeCommit for SSH, see <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html">For SSH Connections on Linux, macOS, or Unix</a> or <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-windows.html">For SSH Connections on Windows</a>.</p> 
<p style="padding-left: 30px"><strong>Tip</strong></p> 
<p style="padding-left: 30px">Use the git remote show command to review the list of remotes set for your local repo.</p> 
<ol start="5"> 
<li>Run the <code class="lang-git">git push</code> <em><code class="lang-git">sync</code></em> <code class="lang-git">--mirror</code> command to push to your replica repository.</li> 
</ol> 
<li>If you named your remote for the replica repository something else, replace&nbsp;<em><code class="lang-git">sync</code></em> with your remote name.</li> 
<li>The&nbsp;<code class="lang-git">--mirror</code> option specifies that all refs under <strong>refs/</strong> (which includes, but is not limited to, <strong>refs/heads/</strong>, <strong>refs/remotes/</strong>, and <strong>refs/tags/</strong>) will be mirrored to the remote repository. If you only want to push branches and commits, but don’t care if you push other references such as tags, you can use the <code class="lang-git">--all</code> option instead.</li> 
<p>&nbsp;</p> 
<p>Your replica repository is now ready for sync-up operations. To do a manual sync, run <code class="lang-git">git pull</code> to pull from your original repository, and then run <code class="lang-git">git push</code>&nbsp;<em><code class="lang-git">sync</code></em> <code class="lang-git">--mirror</code> to push to the replica repository. Again, do not push any local changes to your replica repository at any time.</p> 
<p>&nbsp;</p> 
<p><strong>Part 2: Create a Periodic Sync Job</strong></p> 
<p>You can use a number of tools to set up an automated sync job. In this section, we’ll briefly cover four common tools: a cron job (Linux), a task in Windows Task Scheduler (Windows), a launchd instance (macOS), and, for those users who already have a Jenkins server set up, a Freestyle project with build triggers. Feel free to use whatever tools are best for you.</p> 
<p><strong>Note</strong></p> 
<p>Some hosted repositories offer options for syncing repositories, such as Git hooks, notifications, and other triggers. To learn more about those options, consult the documentation for your source repository system.</p> 
<p>&nbsp;</p> 
<p>All of the following approaches rely on commands that pull the latest changes from the source repository to your local clone repo, and then mirror those changes to your AWS CodeCommit repository. They can be summed up as follows:</p> 
cd <span style="color: #c24040"><em>/path/to/your/local/repo </em></span>git pull
git push <span style="color: #c24040"><em>sync</em></span> --mirror 
<p>Where and how you save and schedule these commands depends on your operating system and tool(s). We’ve included just a few options/examples from a variety of approaches.</p> 
<p>&nbsp;</p> 
<p><strong>In Linux:</strong></p> 
<ol> 
<li>At the terminal, run the crontab -e command to edit your crontab file in your default editor.</li> 
<li>Add a line for a new cron job that will change directories to your local clone repo, pull from your source repository, and mirror any changes to your AWS CodeCommit repository on the schedule you specify. For example, to run a daily job at 2:45 A.M. for a local repo named&nbsp;<em><code class="lang-git">my-repo-replica</code></em> in the ~/tmp directory where you nicknamed your remote (the AWS CodeCommit repository) <em><code class="lang-ggit">sync</code></em>, your new line might look like this:</li> 
</ol> 
45 2 * * * cd <em>~</em>/tmp/<span style="color: #c24040"><em>my-repo-replica</em></span> &amp;&amp; git pull &amp;&amp; git push <span style="color: #c24040"><em>sync</em></span> --mirror 
<ol start="3"> 
<li>Save the crontab file and exit your editor.</li> 
</ol> 
<p>&nbsp;</p> 
<p><strong>In Windows:</strong></p> 
<ol> 
<li>Create a batch file that contains the command to change directories to your local clone repo, pull from your source repository, and mirror any changes up to your AWS CodeCommit repository. For example, if you created your local repo&nbsp;<em><code class="lang-git">my-repo-replica</code></em> in a c:\temp directory, and you nicknamed your remote (the AWS CodeCommit repository) <em><code class="lang-ggit">sync</code></em>, your file might look like this:</li> 
</ol> 
cd /d c:\temp\<em><span style="color: #c24040">my-repo-replica</span>
</em>git pull
git push <span style="color: #c24040"><em>sync</em></span> --mirror 
<ol start="2"> 
<li>Save the batch file with a name like <em><code class="lang-git">my-repo-backup.bat</code></em>.</li> 
<li>Open Task Scheduler. (Not sure how? The simplest way is to open a command line and run <strong>msc</strong>.)</li> 
<li>In <strong>Actions</strong>, choose <strong>Create Basic Task</strong>, and then follow the steps in the wizard.</li> 
</ol> 
<p><strong>&nbsp;</strong></p> 
<p><strong>In macOS:</strong></p> 
<ol> 
<li>Create a shell script that contains the command to change directories to your local clone repo, pull from your source repository, and mirror any changes up to your AWS CodeCommit repository. For example, if you created your local repo&nbsp;<em><code class="lang-git">my-repo-replica</code></em> in a ~/Documents directory, and you nicknamed your remote (the AWS CodeCommit repository) <em><code class="lang-ggit">sync</code></em>, your file might look like this:</li> 
</ol> 
cd ~/Documents/<em><span style="color: #c24040">my-repo-replica</span>
</em>git pull
git push <span style="color: #c24040"><em>sync</em></span> --mirror 
<ol start="2"> 
<li>Save the shell script with a name like <em><code class="lang-git">my-repo-backup.sh</code></em>.</li> 
<li>Create a <strong>launchd</strong> property list file that runs the shell script on the schedule you specify. For example, if you stored&nbsp;<em><code class="lang-git">my-repo-backup.sh</code></em> in ~/Documents, to run the script daily at 2:45 A.M., your plist file might look like this:</li> 
</ol> 
<code class="lang-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;
&lt;plist version=&quot;1.0&quot;&gt;
&lt;dict&gt;
&lt;key&gt;Label&lt;/key&gt;
&lt;string&gt;com.example.codecommit.backup&lt;/string&gt;
&lt;key&gt;ProgramArguments&lt;/key&gt;
&lt;array&gt;
&lt;string&gt;~/Documents/<em>my-repo-backup.sh</em>&lt;/string&gt;
&lt;/array&gt;
&lt;key&gt;StartCalendarInterval&lt;/key&gt;
&lt;dict&gt;
&lt;key&gt;Minute&lt;/key&gt;
&lt;integer&gt;45&lt;/integer&gt;
&lt;key&gt;Hour&lt;/key&gt;
&lt;integer&gt;2&lt;/integer&gt;
&lt;/dict&gt;
&lt;/dict&gt;
&lt;/plist&gt;
</code> 
<ol start="4"> 
<li>Save your plist file in ~/Library/LaunchAgents, /Library/LaunchAgents, or /Library/LaunchDaemons folder, depending on the definition you want for the job.</li> 
<li>Run the <strong>launtchctl</strong> command to load your job. For example, if you want to load a plist file named&nbsp;<em><code class="lang-git">codecommit.sync.plist</code></em> in ~/Library/LaunchAgents, your command might look like this:</li> 
</ol> 
launchctl load ~/Library/LaunchAgents/<span style="color: #c24040"><em>codecommit.sync.plist</em></span> 
<p>&nbsp;</p> 
<p><strong>For Jenkins:</strong></p> 
<ol> 
<li>Open Jenkins.</li> 
<li>Create a new job as a Freestyle project.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/09/New-Item.png" /></p> 
<ol start="3"> 
<li>In the <strong>Build Triggers</strong> section, select <strong>Build periodically</strong>, and set up a schedule for the task. Jenkins uses cron expressions to run periodic tasks. For more information, see the <a href="https://github.com/jenkinsci/jenkins/blob/master/core/src/main/resources/hudson/triggers/TimerTrigger/help-spec.html">Jenkins documentation for the syntax of cron</a>.</li> 
</ol> 
<p style="padding-left: 30px">If you are replicating a GitHub or BitBucket repository, you can also set the task to build when the <a href="https://developer.github.com/webhooks/">Git hook</a> is triggered.</p> 
<p style="padding-left: 30px">The following example builds once a day between midnight and 1 A.M.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/09/Build-Triggers.png" /></p> 
<ol start="4"> 
<li>In the <strong>Build</strong> section, add a build step and choose <strong>Execute Windows batch command</strong> or <strong>Execute Shell</strong>. Then write a script and implement the Git operations:</li> 
</ol> 
cd <span style="color: #c24040"><em>/path/to/your/local/repo </em></span>git pull
git push <span style="color: #c24040"><em>sync</em></span> --mirror 
<p style="padding-left: 30px">Note: Jenkins may require the full path for Git.</p> 
<p style="padding-left: 30px">The following example is a Windows batch command file, with the full path for Git on the host.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/03/09/Build.png" /></p> 
<ol start="5"> 
<li>Save the configuration for the task.</li> 
</ol> 
<p>&nbsp;</p> 
<p>Your AWS CodeCommit replica repository will now be automatically updated with any changes to your source repository as scheduled.</p> 
<p>We hope you’ve enjoyed this blog post. If you have questions or suggestions for future blog post, please leave it in the comments below or visit our <a href="https://forums.aws.amazon.com/forum.jspa?forumID=189&amp;start=0">user forum</a>!</p> 
<p>&nbsp;</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Extending AWS CodeBuild with Custom Build Environments</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">John Pignata</span></span> | on 
<time property="datePublished" datetime="2017-02-17T13:40:26+00:00">17 FEB 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/extending-aws-codebuild-with-custom-build-environments/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-591" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=591&amp;disqus_title=Extending+AWS+CodeBuild+with+Custom+Build+Environments&amp;disqus_url=https://aws.amazon.com/blogs/devops/extending-aws-codebuild-with-custom-build-environments/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-591');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/codebuild/" target="_blank">AWS CodeBuild</a> is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodeBuild provides curated build environments for programming languages and runtimes such as Java, Ruby, Python, Go, Node.js, Android, and Docker. It can be extended through the use of custom build environments to support many more.</p> 
<p>Build environments are Docker images that include a complete file system with everything required to build and test your project. To use a custom build environment in a CodeBuild project, you build a container image for your platform that contains your build tools, push it to a Docker container registry such as <a href="https://aws.amazon.com/ecr/" target="_blank">Amazon EC2 Container Registry (ECR)</a>, and reference it in the project configuration. When building your application, CodeBuild will retrieve the Docker image from the container registry specified in the project configuration and use the environment to compile your source code, run your tests, and package your application.</p> 
<p>In this post, we’ll create a build environment for <a href="http://www.php.net/" target="_blank">PHP</a> applications and walk through the steps to configure CodeBuild to use this environment.</p> 
<p><span id="more-591"></span></p> 
<h3>Requirements</h3> 
<p>In order to follow this tutorial and build the Docker container image, you need to have the <a href="https://www.docker.com/products/overview#/install_the_platform" target="_blank">Docker platform</a>, the <a href="http://aws.amazon.com/cli" target="_blank">AWS Command Line Interface</a>, and <a href="https://git-scm.com/downloads" target="_blank">Git</a> installed.</p> 
<h3>Create the demo resources</h3> 
<p>To begin, we’ll clone <a href="https://github.com/awslabs/codebuild-images" target="_blank">codebuild-images</a> from GitHub. It contains an <a href="https://aws.amazon.com/cloudformation/" target="_blank">AWS CloudFormation</a> template that we’ll use to create resources for our demo: a source code repository in <a href="https://aws.amazon.com/codecommit/" target="_blank">AWS CodeCommit</a> and a Docker image repository in Amazon ECR. The repository also includes PHP sample code and tests that we’ll use to demonstrate our custom build environment.</p> 
<ol> 
<li>Clone the Git repository: 
<code class="lang-bash">git clone https://github.com/awslabs/codebuild-images.git
cd codebuild-images</code> 
<li>Create the CloudFormation stack using the template.yml file. You can use<a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html" target="_blank"> the CloudFormation console to create the stack</a>&nbsp;or you can use the <a href="https://aws.amazon.com/cli/" target="_blank">AWS Command Line Interface</a>: 
<code class="lang-bash">aws cloudformation create-stack \
--stack-name codebuild-php \
--parameters ParameterKey=EnvName,ParameterValue=php \
--template-body file://template.yml &gt; /dev/null &amp;&amp; \
aws cloudformation wait stack-create-complete \
--stack-name codebuild-php &amp;&amp; \
aws cloudformation describe-stacks \
--stack-name codebuild-php \
--output table \
--query Stacks[0].Outputs</code> 
</ol> 
<p>After the stack has been created, CloudFormation will return two outputs:</p> 
<li><strong>BuildImageRepositoryUri</strong>: the URI of the Docker repository that will host our build environment image.</li> 
<li><strong>SourceCodeRepositoryCloneUrl</strong>: the clone URL of the Git repository that will host our sample PHP code.</li> 
<h3>Build and push the Docker image</h3> 
<p>Docker images are specified using a <a href="https://docs.docker.com/engine/reference/builder/" target="_blank">Dockerfile</a>, which contains the instructions for assembling the image. The Dockerfile included in the PHP build environment contains these instructions:</p> 
<code class="lang-docker">FROM php:7
ARG composer_checksum=55d6ead61b29c7bdee5cccfb50076874187bd9f21f65d8991d46ec5cc90518f447387fb9f76ebae1fbbacf329e583e30
ARG composer_url=https://raw.githubusercontent.com/composer/getcomposer.org/ba0141a67b9bd1733409b71c28973f7901db201d/web/installer
ENV COMPOSER_ALLOW_SUPERUSER=1
ENV PATH=$PATH:vendor/bin
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
curl \
git \
python-dev \
python-pip \
zlib1g-dev \
&amp;&amp; pip install awscli \
&amp;&amp; docker-php-ext-install zip \
&amp;&amp; curl -o installer &quot;$composer_url&quot; \
&amp;&amp; echo &quot;$composer_checksum *installer&quot; | shasum –c –a 384 \
&amp;&amp; php installer --install-dir=/usr/local/bin --filename=composer \
&amp;&amp; rm -rf /var/lib/apt/lists/*
</code> 
<p>This Dockerfile inherits all of the instructions from the <a href="https://hub.docker.com/_/php/" target="_blank">official PHP Docker image</a>, which installs the PHP runtime. On top of that base image, the build process will install Python, Git, the AWS CLI, and <a href="https://getcomposer.org/" target="_blank">Composer</a>, a dependency management tool for PHP. We’ve installed the AWS CLI and Git as tools we can use during builds. For example, using the AWS CLI, we could trigger a notification from <a href="https://aws.amazon.com/sns/" target="_blank">Amazon Simple Notification Service (SNS)</a> when a build is complete or we could use Git to create a new tag to mark a successful build. Finally, the build process cleans up files created by the packaging tools, as recommended in <a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/" target="_blank">Best practices for writing Dockerfiles</a>.</p> 
<p>Next, we’ll build and push the custom build environment.</p> 
<ol> 
<li>Provide&nbsp;authentication details for our registry to the local Docker engine by executing the output of the <a href="http://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_AWSCLI.html#AWSCLI_get-login" target="_blank">login helper</a> provided by the AWS CLI: 
<code class="lang-bash">aws ecr get-login
</code> 
<li>Build and push the Docker image. We’ll use the repository URI returned in the CloudFormation stack output (<em>BuildImageRepositoryUri</em>) as the image tag: 
<code class="lang-bash">cd php
docker build -t [BuildImageRepositoryUri] .
docker push [BuildImageRepositoryUri]</code> 
</ol> 
<p>After running these commands, your Docker image is pushed into Amazon ECR and ready to build your project.</p> 
<h3>Configure the Git repository</h3> 
<p>The repository we cloned includes a small PHP sample that we can use to test our PHP build environment. The <a href="https://github.com/awslabs/codebuild-images/blob/master/php/sample/RomanNumerals.php" target="_blank">sample function</a> converts Roman numerals to Arabic numerals. The repository also includes a <a href="https://github.com/awslabs/codebuild-images/blob/master/php/sample/tests/RomanNumeralsTest.php" target="_blank">sample test</a> to exercise this function. The sample also includes a YAML file called a <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html" target="_blank">build spec</a> that contains commands and related settings that CodeBuild uses to run a build:</p> 
<code class="lang-yaml">version: 0.1
phases:
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - composer install
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - phpunit tests</code> 
<p>This build spec configures CodeBuild to run two commands during the build:</p> 
<li><code class="lang-yaml">composer install</code> to install <a href="https://github.com/awslabs/codebuild-images/blob/master/php/sample/composer.json" target="_blank">configured dependencies</a>, such as <a href="https://phpunit.de/" target="_blank">PHPUnit</a> before the build.</li> 
<li><code class="lang-yaml">phpunit tests</code> to run the unit tests during the build.</li> 
<p>We will push the sample application to the CodeCommit repo created by the CloudFormation stack. You’ll need to grant your IAM user the required level of access to the AWS services required for CodeCommit and you’ll need to configure your Git client with the appropriate credentials. See <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html" target="_blank">Setup for HTTPS Users Using Git Credentials</a> in the CodeCommit documentation for detailed steps.</p> 
<p>We’re going to initialize a Git repository for our sample, configure our origin, and push the sample to the master branch in CodeCommit.</p> 
<ol> 
<li>Initialize a new Git repository in the sample directory: 
<code class="lang-bash">cd sample
git init</code> 
<li>Add and commit the sample files to the repository: 
<code class="lang-bash">git add .
git commit -m &quot;Initial commit&quot;</code> 
<li>Configure the git remote and push the sample to it. We’ll use the repository clone URL returned in the CloudFormation stack output (<em>SourceCodeRepositoryCloneUrl</em>) as the remote URL: 
<code class="lang-bash">git remote add origin [SourceCodeRepositoryCloneUrl]
git push origin master</code> 
</ol> 
<p>Now that our sample application has been pushed into source control and our build environment image has been pushed into our Docker registry, we’re ready to create a CodeBuild project and start our first build.</p> 
<h3>Configure the CodeBuild project</h3> 
<p>In this section, we’ll walk through the steps for configuring CodeBuild to use the custom build environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/02/16/build-configuration-1012x1024.png" /></p> 
<ol> 
<li>In the AWS Management Console, open the AWS CodeBuild console, and then choose <strong>Create project</strong>.</li> 
<li>In <strong>Project name</strong>, type <strong>php-demo</strong>.</li> 
<li>From <strong>Source provider</strong>, choose <strong>AWS CodeCommit</strong>. &nbsp;From <strong>Repository</strong>, choose <strong>codebuild-sample-php</strong>.</li> 
<li>In <strong>Environment image</strong>, select <strong>Specify a Docker image</strong>. From <strong>Custom image type</strong>, choose <strong>Amazon ECR</strong>. From <strong>Amazon ECR Repository</strong>, choose <strong>codebuild/php</strong>. &nbsp;From <strong>Amazon ECR image</strong>, choose <strong>latest</strong>.</li> 
<li>In <strong>Build specification</strong>, select <strong>Use the buildspec.yml in the source code root directory</strong>.</li> 
<li>In <strong>Artifacts type</strong>, choose <strong>No artifacts</strong>.</li> 
<li>Choose <strong>Continue</strong> and then choose <strong>Save and Build</strong>.</li> 
<li>On the next page, from <strong>Branch</strong>, choose <strong>master</strong> and then choose <strong>Start build</strong>.</li> 
</ol> 
<p>CodeBuild will pull the build environment image from Amazon ECR and use it to test our application. CodeBuild will show us the status of each build step, the last 20 lines of log messages generated by the build process, and provide a link to <a href="https://aws.amazon.com/cloudwatch/details/" target="_blank">Amazon CloudWatch Logs</a> for more debugging output.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/02/16/build-succeeded-1024x426.png" /></p> 
<h3>Summary</h3> 
<p>CodeBuild supports a number of platforms and languages out of the box. By using custom build environments, it can be extended to other runtimes. In this post, we built a PHP environment and demonstrated how to use it to test PHP applications.</p> 
<p>We’re excited to see how customers extend and use CodeBuild to enable continuous integration and continuous delivery for their applications. Please leave questions or suggestions in the comments or share what you’ve learned extending CodeBuild for your own projects.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-591');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Run Umbraco CMS with Flexible Load Balancing on AWS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ihab Shaaban</span></span> | on 
<time property="datePublished" datetime="2017-01-06T14:26:18+00:00">06 JAN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/run-umbraco-cms-with-flexible-load-balancing-on-aws/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-487" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=487&amp;disqus_title=Run+Umbraco+CMS+with+Flexible+Load+Balancing+on+AWS&amp;disqus_url=https://aws.amazon.com/blogs/devops/run-umbraco-cms-with-flexible-load-balancing-on-aws/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-487');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In version 7.3, Umbraco CMS the popular open source CMS introduced the flexible load balancing feature, which makes the setup of load-balanced applications a lot easier. In this blog post, we’ll follow the guidelines in the <a title="undefined" href="https://our.umbraco.org/documentation/Getting-Started/Setup/Server-Setup/load-balancing/" target="_blank">Umbraco documentation</a> to set up a load-balanced Umbraco application on AWS. We’ll let <a title="undefined" href="https://aws.amazon.com/elasticbeanstalk/" target="_blank">AWS Elastic Beanstalk</a> manage the deployments, load balancing, auto scaling, and health monitoring for us.</p> 
<h3>Application Architecture</h3> 
<p>When you use the flexible load balancing feature, any updates to Umbraco content will be stored in a queue in the master database. Each server in the load-balanced environment will automatically download, process, and cache the updates from the queue, so no matter which server is selected by the <a title="undefined" href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">Elastic Load Balancing</a> to handle the request, the user will always receive the same content. Umbraco administration doesn’t work correctly if accessed from a load-balanced server. For this reason, we’ll set up a non-balanced environment to be accessed only by the administrators and editors.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-01-Arch-med.png" /></p> 
<p><span id="more-487"></span></p> 
<h3>Create Elastic Beanstalk Environments</h3> 
<p>We’ll start by creating a single instance environment for administrators and editors. This environment will have the master database server as an RDS instance. In the Elastic Beanstalk console, choose <strong>Create New Application</strong>. Type a name for the application, and then choose <strong>Create</strong>. When you see the message “No environments currently exist for this application”, choose <strong>Create one now</strong>. Select <strong>Web server environment</strong> for the environment tier.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-02-Create-med.png" /></p> 
<p>On the <strong>Create a new environment page</strong>, choose <strong>.NET</strong> for Platform, and then choose <strong>Configure more options</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-03-Create-med.png" /></p> 
<p>On the next page, under <strong>Capacity</strong>, set the <strong>Environment type</strong> to <strong>Single instance</strong>. Under <strong>Database</strong>, for <strong>Engine</strong>, choose <strong>sqlserver</strong>. Set the <strong>Storage</strong> field to, at minimum, <strong>20 GB</strong>, review the information, and then choose <strong>Create environment</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-04-Create-med.png" /></p> 
<p>Next, we’ll create a load-balanced environment for the front-end users. Follow the steps you used to create the first Elastic Beanstalk environment. When you reach the configuration page, select <strong>High availability</strong>, and then choose <strong>Create environment</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-05-Create-med.png" /></p> 
<h3>Configure RDS Database Server Security Groups</h3> 
<p>When we created the RDS DB instance through the first environment, Elastic Beanstalk automatically configured the security to allow the servers in this environment to access the database. We’ll configure the security for the second environment manually. By following these steps, the front-end servers will be able to connect to the database:</p> 
<ol> 
<li>In the Elastic Beanstalk console, copy the name of the security group for the front-end environment. You will need this for step 7 of this procedure.</li> 
<li>In the admin environment, choose <strong>Configuration</strong>, and then choose <strong>RDS</strong>.</li> 
<li>Choose the<strong> View in RDS Console</strong> link.</li> 
<li>On the <strong>Details</strong> tab in the RDS console, in the <strong>Security and Network</strong> section, choose <strong>Security Groups</strong>.</li> 
<li>Choose the name of the active security group to open it in the EC2 console.</li> 
<li>In the EC2 console, you should see that Elastic Beanstalk has already added a rule for the admin environment.</li> 
<li>Choose <strong>Edit</strong>, and then choose <strong>Add Rule</strong>. For <strong>Type</strong>, choose <strong>MS SQL</strong>. For <strong>Source</strong>, paste the name of the security group for the front-end environment.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-06-Configure-med.png" /></p> 
<h3>Handle Session State in a Load Balanced Environment</h3> 
<p>By default, ASP.NET stores the user’s session in memory. This means the user will lose session information if the next request goes to a different server. To prevent this from happening while keeping the default session provider, <a title="undefined" href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-sticky-sessions.html" target="_blank">configure sticky sessions for your load balancer</a>.</p> 
<p>In the Elastic Beanstalk console, navigate to the front-end environment configuration. You can use the <strong>Sessions</strong> section under the <strong>Load Balancing</strong> settings to specify whether the load balancer for the application will allow session stickiness. Select the <strong>Enable Session Stickiness</strong> check box.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-07-Configure-med.png" /></p> 
<p>When you enable session stickiness, <a title="undefined" href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">ELB</a> will send all requests for a specific user to the same back-end web server. This can result in an imbalance of requests across all back ends, with some servers having more load than others. When you scale down, the termination of an instance can result in a loss of sessions assigned to it. For this reason, consider storing the sessions in a central location like a SQL Server database and use the <a title="undefined" href="https://msdn.microsoft.com/en-us/library/aa478952.aspx" target="_blank">SqlSessionStateStore</a> provider.</p> 
<p>AWS offers other options, for example, storing the sessions in a NoSQL database using the <a title="undefined" href="http://docs.aws.amazon.com/AWSSdkDocsNET/V3/DeveloperGuide/net-dg-dynamodb-session.html" target="_blank">DynamoDBSessionStore</a> provider or using the <a title="undefined" href="http://aws.amazon.com/elasticache/" target="_blank">Amazon ElastiCache</a> to store the sessions in the cloud as in-memory cache. For information, see the <a title="undefined" href="https://blogs.aws.amazon.com/net/post/TxMREMF0459SXT/ElastiCache-as-an-ASP-NET-Session-Store" target="_blank">ElastiCache as an ASP.NET Session Store</a> blog post.</p> 
<h3>A One-Step Alternative: CloudFormation Template</h3> 
<p>You can download and use this <a title="undefined" href="https://github.com/awslabs/aws-cfn-umbraco-load-balancing-template" target="_blank">CloudFormation template</a> to create resources similar to those created in the preceding steps. If you use the template, you will still have to prepare and publish your own version of Umbraco from a local machine. We’ll do that next.</p> 
<p>The template is written to create the RDS database as a separate resource from the environments. You’ll see in the previous steps the DB is tied to the admin environment (for example, if you delete the environment, the DB will be deleted, too). This works well during development or testing, but is not ideal for production. If you should accidentally delete your database, by default, Elastic Beanstalk creates a snapshot on environment termination so it can be recovered. For information about how to decouple the DB from your environment, see <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html" target="_blank">Using Elastic Beanstalk with Amazon RDS</a> in the Elastic Beanstalk Developer Guide.</p> 
<h3>Prepare the Umbraco CMS Locally</h3> 
<p>To keep things simple, we’ll download Umbraco from <a title="undefined" href="https://our.umbraco.org/download" target="_blank">our.umbraco.org/download</a>. As of this writing, the current version is v7.5.</p> 
<p>From IIS on the local machine, add a new website with the following settings:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/30/Umbraco-08-Prepare-med.png" /></p> 
<h3>File and Folder Permissions</h3> 
<p>To ensure the Umbraco installation will run smoothly, grant the read and write permissions to the application pool user for Umbraco’s files and folders. In IIS, right-click the UmbracoApp website, and then click <strong>Edit Permissions</strong>. Grant modify or full-control permissions to IIS_IUSRS. For information, see <a title="undefined" href="https://our.umbraco.org/documentation/Getting-Started/Setup/Server-Setup/permissions" target="_blank">File and folder permissions</a> in the Umbraco documentation.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-09-Prepare-med.png" /></p> 
<p>When we deploy the application to AWS, we need to update the permissions for IIS_IUSRS, too. It isn’t easy to access and grant permissions to each Windows server in the environment. If we create an .ebextension, Elastic Beanstalk can automate the permissions process. It can also allow us to run extra commands during the application deployment:</p> 
<code class="lang-yaml">commands:
create_default_website_folder:
command: if not exist &quot;C:\inetpub\wwwroot&quot; mkdir &quot;C:\inetpub\wwwroot&quot;
update_iis_user_permissions:
command: Icacls.exe &quot;C:\inetpub\wwwroot&quot; /grant IIS_IUSRS:(OI)(CI)F
</code> 
<p>Here is the .ebextension saved as a .config file:</p> 
C:\inetpub\wwwroot\UmbracoCms\App_Data\.ebextensions\update_iis_permissions.config 
<p>Although this one is written in YAML, you can write .ebextensions in JSON, too. You can extend the .ebextension by writing a test to ensure the commands run on the first deployment only and not on redeployments. For more information, see the <a title="undefined" href="https://blogs.aws.amazon.com/net/post/Tx3N7JHP24J1EJ5/Configuring-Advanced-Logging-on-AWS-Elastic-Beanstalk" target="_blank">Configuring Advanced Logging on AWS Elastic Beanstalk</a> blog post or the <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html" target="_blank">.ebextensions documentation</a>.</p> 
<h3>Custom Machine Key</h3> 
<p>By default, ASP.NET generates a unique machine key for each server. In a load-balanced environment, this will cause validation errors and invalid view state. To fix this issue, make sure the machine key is the same on all servers. One of the simplest ways to generate a custom key is from IIS.</p> 
<p>In IIS, on the <strong>Machine Key</strong> page, click <strong>Generate Keys</strong>. Change the settings as follows, and click <strong>Apply</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-10-Prepare-med.png" /></p> 
<h3>Create an Empty Database</h3> 
<p>Before running the CMS installation, we’ll create an empty database in our RDS DB server. Open AWS Explorer in Visual Studio, right-click the DB instance we created in the first step, and then select <strong>Create SQL Server Database</strong>.</p> 
<p>If you encounter any problems connecting to the RDS server, add the IP address of the development machine to the RDS DB security group. Make sure your Windows Firewall allows outbound access for 1433 port.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-11-Prepare-med.png" /></p> 
<h3>Run the Umbraco CMS Installation</h3> 
<p>In IIS, click <strong>Browse</strong>. The Umbraco installation should start. Enter the required information and use the RDS DB information to complete the fields for the database configuration step.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-12-Prepare-med.png" /></p> 
<h3>Publish Umbraco CMS to AWS</h3> 
<p>Finally, open the UmbracoApp folder in Visual Studio. From the <strong>File</strong> menu, click <strong>Open</strong>. Click <strong>Web site</strong>, click <strong>File system</strong>, and then navigate to C:\inetpub\wwwroot\UmbracoCms. Right-click the project, and then select <strong>Publish to AWS</strong> to deploy to the admin environment and then deploy again to the front-end environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-13-Deploy-med.png" /></p> 
<p>The <a title="undefined" href="https://aws.amazon.com/ebs/" target="_blank">EBS</a> volumes attached to the instances in the environments are isolated and not shared. When compared to other load-balanced solutions that use shared storage, there is no extra work required to separate Umbraco logging file paths, change XML cache content settings, or update the configuration for Lucene/Examine indexes.</p> 
<h3>Conclusion</h3> 
<p>You now have an Umbraco application that is ready to scale up or down on AWS, and you can take this further using Elastic Beanstalk, there are many options to customize your environments, for example, associating a <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customdomains.html" target="_blank">custom domain</a> name or enabling <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html" target="_blank">HTTPS</a>.</p> 
<p>We hope you found the information in this post helpful. If you have questions or other feedback, please leave it in the comments below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-487');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
