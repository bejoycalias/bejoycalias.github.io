<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/devopsblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li class="active"><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="devopsblogs1.html">Page 1</a>|<a href="devopsblogs2.html">Page 2</a>|<a href="devopsblogs3.html">Page 3</a>|<a href="devopsblogs4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Build Serverless AWS CodeCommit Workflows using Amazon CloudWatch Events and JGit</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Barclay</span></span> | on 
<time property="datePublished" datetime="2017-08-03T13:25:33+00:00">03 AUG 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/developer-tools/aws-codecommit/" title="View all posts in AWS CodeCommit*"><span property="articleSection">AWS CodeCommit*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/build-serverless-aws-codecommit-workflows-using-amazon-cloudwatch-events-and-jgit/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><img src="https://s3.amazonaws.com/chrisb/samdengler.jpeg" /> Sam Dengler is a Solutions Architect at Amazon Web Services</p> 
<b>Summary</b> 
<p><a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a> now supports <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html">AWS CodeCommit</a> <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/EventTypes.html#codecommit_event_type">Repository State Changes</a> event types for activities like pushing new code to a repository. Using these new event types, customers can build Amazon CloudWatch Event rules to match AWS CodeCommit events and route them to one or more targets like an Amazon SNS Topic, AWS Step Functions state machine, or AWS Lambda function to trigger automated workflows to process repository changes.</p> 
<p>In this blog, I will provide three examples for using AWS Lambda and JGit to build cost-effective serverless solutions to securely process AWS CodeCommit repository state changes:</p> 
<li>Replicate CodeCommit Repository</li> 
<li>Enforce Git Commit Message Policy</li> 
<li>Backup Git Archive to Amazon S3</li> 
<p>Source code and Amazon CloudFormation templates for the examples are located in the following GitHub repository: <a href="https://github.com/awslabs/serverless-codecommit-examples">https://github.com/awslabs/serverless-codecommit-examples</a>.</p> 
<b>AWS CodeCommit CloudWatch Events</b> 
<p>Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in AWS resources. Below is an example Amazon CloudWatch Event for one of the new AWS CodeCommit Repository State Changes event types, referenceUpdated. Any change to the repository will trigger a referenceUpdated event, however triggers for particular branches can be filtered using the referenceType and referenceName fields in the event details.</p> 
<pre><code class="lang-json">{
&quot;version&quot;: &quot;0&quot;,
&quot;id&quot;: &quot;01234567-0123-0123-0123-012345678901&quot;,
&quot;detail-type&quot;: &quot;CodeCommit Repository State Change&quot;,
&quot;source&quot;: &quot;aws.codecommit&quot;,
&quot;account&quot;: &quot;123456789012&quot;,
&quot;time&quot;: &quot;2017-06-12T10:23:43Z&quot;,
&quot;region&quot;: &quot;us-east-1&quot;,
&quot;resources&quot;: [
&quot;arn:aws:codecommit:us-east-1:123456789012:myRepo&quot;
],
&quot;detail&quot;: {
&quot;event&quot;: &quot;referenceUpdated&quot;,
&quot;repositoryName&quot;: &quot;myRepo&quot;,
&quot;referenceType&quot;: &quot;head&quot;,
&quot;referenceName&quot;: &quot;myBranch&quot;,
&quot;commitId&quot;: &quot;3e5983EXAMPLE&quot;,
&quot;oldCommitId&quot;: &quot;1a7813EXAMPLE&quot;
}
}</code></pre> 
<p>We will use the Amazon CloudWatch Event’s fields to create a pattern to match the events for which we will trigger a target, in this case an AWS Lambda function.</p> 
<b>Accessing AWS CodeCommit using HTTPS URLs</b> 
<p>The HTTPS URL method for accessing an AWS CodeCommit repository is particularly suited to a serverless solution because an Amazon Lambda execution container already provides temporary AWS IAM key credentials associated to the function’s AWS IAM Execution Role. The function’s Execution Role is associated to one or more AWS IAM Policies, in which you specify permissions allowing the function to access AWS resources. For each function, we will limit the AWS IAM polices to only the AWS CodeCommit repository, Amazon S3 bucket, or Amazon SNS topics, following the AWS best practice to <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege">grant least privileged access</a>.</p> 
<p>For example, the AWS IAM Policy snippet below restricts the Amazon Lambda function to pull from the source repository and push to the target repository.</p> 
<pre><code class="lang-yaml">Policies:
- Version: '2012-10-17'
&nbsp;   Statement:
&nbsp; &nbsp; &nbsp; - Effect: Allow
&nbsp; &nbsp; &nbsp; &nbsp; Resource: !Sub 'arn:aws:codecommit:${AWS::Region}:${AWS::AccountId}:${SourceRepositoryName}'
&nbsp; &nbsp; &nbsp; &nbsp; Action:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - 'codecommit:GetRepository'
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - 'codecommit:GitPull'
&nbsp; &nbsp; &nbsp; - Effect: Allow
&nbsp; &nbsp; &nbsp; &nbsp; Resource: !Sub 'arn:aws:codecommit:${TargetRepositoryRegion}:${AWS::AccountId}:${TargetRepositoryName}'
&nbsp; &nbsp; &nbsp; &nbsp; Action:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - 'codecommit:GetRepository'
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - 'codecommit:GitPush'</code></pre> 
<p>When using the HTTPS URL access method, a credential helper is configured for the Git client, which executes the “aws codecommit credential-helper” command to provide a SigV4 compatible user name and password using AWS IAM credentials (<a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html#setting-up-https-unixes-credential-helper">see more</a>). When using JGit as the Git client, a CredentialsProvider can be supplied to Git commands to achieve the same result.</p> 
<p>The Spring Cloud Config project provides an implementation of the JGit CredentialsProvider for AwsCodeCommit (<a href="https://github.com/spring-cloud/spring-cloud-config/blob/dd08a3bbf4f0ee0bbde0107f63677f180ac786cc/spring-cloud-config-server/src/main/java/org/springframework/cloud/config/server/support/AwsCodeCommitCredentialProvider.java">source</a>), which conveniently uses the AWS DefaultAWSCredentialsProviderChain to discover AWS credentials in the standard priority order supported by Amazon Lambda. The <a href="https://github.com/spring-cloud/spring-cloud-config/blob/dd08a3bbf4f0ee0bbde0107f63677f180ac786cc/spring-cloud-config-server/src/main/java/org/springframework/cloud/config/server/support/AwsCodeCommitCredentialProvider.java#L207">AwsCodeCommit.calculateCodeCommitPassword</a> method is particularly interesting to review as an example of SigV4 transformation logic.</p> 
<p>Cloning a repository is repeated across examples, and the functionality has been delegated to a supporting CloneCommandBuilder Class below.</p> 
<pre><code class="lang-java">public class CloneCommandBuilder {
&nbsp; &nbsp; private File directory;
&nbsp; &nbsp; public CloneCommandBuilder() throws IOException {
&nbsp; &nbsp; &nbsp; &nbsp; directory = Files.createTempDirectory(null).toFile();
&nbsp; &nbsp; }
&nbsp; &nbsp; public CloneCommand buildCloneCommand(String sourceUrl) {
&nbsp; &nbsp; &nbsp; &nbsp; return buildCloneCommand(sourceUrl, new AwsCodeCommitCredentialProvider());
&nbsp; &nbsp; }
&nbsp; &nbsp; public CloneCommand buildCloneCommand(String sourceUrl,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; AwsCodeCommitCredentialProvider credentialsProvider) {
&nbsp; &nbsp; &nbsp; &nbsp; return new CloneCommand().setDirectory(directory)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .setURI(sourceUrl)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .setCredentialsProvider(credentialsProvider)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .setBare(true);
&nbsp; &nbsp; }
}</code></pre> 
<p>Next we’ll look at some examples to process the repository events using Amazon Lambda.</p> 
<b>Example 1: Replicate CodeCommit Repository</b> 
<p>Customers often need to replicate commits from one repository to another to support disaster recovery or cross region CI/CD pipelines. In this example, the Amazon Lambda function will clone a repository from the source and push to the target. This example is intended to update an existing target repository, which should not be empty before configuring replication prior to configuring replication.</p> 
<p>Please note, Amazon Lambda functions are limited to 1.5GB memory, 512MB ephemeral disk capacity (“/tmp” space), and a 5 minute execution time. If your repository is unable to be processed within these limits, please see the <a href="https://aws.amazon.com/blogs/devops/replicating-and-automating-sync-ups-for-a-repository-with-aws-codecommit/">Replicating and Automating Sync-Ups for a Repository with AWS CodeCommit</a> blog article for an alternative approach to replicate repositories using an Amazon EC2 instance.</p> 
<p>Let’s take a look at some code!</p> 
<pre><code class="lang-java">public class ReplicateRepositoryHandler
&nbsp; &nbsp; &nbsp; &nbsp; implements RequestHandler&lt;CodeCommitEvent, HandlerResponse&gt; {
&nbsp; &nbsp; private static Logger logger = Logger.getLogger(ReplicateRepositoryHandler.class);
&nbsp; &nbsp; private final String targetUrl;
&nbsp; &nbsp; private final AwsCodeCommitCredentialProvider credentialsProvider;
&nbsp; &nbsp; public ReplicateRepositoryHandler() {
&nbsp; &nbsp; &nbsp; &nbsp; String targetName = System.getenv(&quot;TARGET_REPO_NAME&quot;);
&nbsp; &nbsp; &nbsp; &nbsp; String targetRegion = System.getenv(&quot;TARGET_REPO_REGION&quot;);
&nbsp; &nbsp; &nbsp; &nbsp; CodeCommitMetadata target = new CodeCommitMetadata(targetName, targetRegion);
&nbsp; &nbsp; &nbsp; &nbsp; targetUrl = target.getCloneUrlHttp();
&nbsp; &nbsp; &nbsp; &nbsp; credentialsProvider = new AwsCodeCommitCredentialProvider();
&nbsp; &nbsp; }
// ...</code></pre> 
<p>On instantiation, the Amazon Lambda function discovers the target AWS CodeCommit repository HTTPS URL by querying the repository metadata using the target repository name and region. This discovery process is repeated across the examples, and the code has been delegated to the CodeCommitMetadata class below.</p> 
<pre><code class="lang-java">public class CodeCommitMetadata {
&nbsp; &nbsp; private RepositoryMetadata repositoryMetadata;
&nbsp; &nbsp; public CodeCommitMetadata(String repoName, String repoRegion) {
&nbsp; &nbsp; &nbsp; &nbsp; AWSCodeCommitClientBuilder builder = AWSCodeCommitClientBuilder.standard();
&nbsp; &nbsp; &nbsp; &nbsp; AWSCodeCommit client = builder.withRegion(repoRegion).build();
&nbsp; &nbsp; &nbsp; &nbsp; GetRepositoryRequest request = new GetRepositoryRequest();
&nbsp; &nbsp; &nbsp; &nbsp; request.withRepositoryName(repoName);
&nbsp; &nbsp; &nbsp; &nbsp; GetRepositoryResult result = client.getRepository(request);
&nbsp; &nbsp; &nbsp; &nbsp; repositoryMetadata = result.getRepositoryMetadata();
&nbsp; &nbsp; }
&nbsp; &nbsp; public String getCloneUrlHttp() {
&nbsp; &nbsp; &nbsp; &nbsp; return repositoryMetadata.getCloneUrlHttp();
&nbsp; &nbsp; }
}</code></pre> 
<p>When the Amazon Lambda function is triggered by the AWS CloudWatch Event, the source repository name and region in the event are used to discover the source repository HTTPS URL. We use JGit to clone the source repository from this URL into a local repository stored in a temporary directory in the Amazon Lambda execution container.</p> 
<pre><code class="lang-java">public HandlerResponse handleRequest(CodeCommitEvent event, Context context) {
&nbsp; &nbsp; try {
&nbsp; &nbsp; &nbsp; &nbsp; String sourceName = event.getDetail().getRepositoryName();
&nbsp; &nbsp; &nbsp; &nbsp; String sourceRegion = event.getRegion();
&nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; // clone source repository
&nbsp; &nbsp; &nbsp; &nbsp; CodeCommitMetadata source = new CodeCommitMetadata(sourceName, sourceRegion);
&nbsp; &nbsp; &nbsp; &nbsp; String sourceUrl = source.getCloneUrlHttp();
&nbsp; &nbsp; &nbsp; &nbsp; Git git = new CloneCommandBuilder().buildCloneCommand(sourceUrl).call();
// ...</code></pre> 
<p>Once we’ve cloned the local repository to the Amazon Lambda execution container, the last step is to set the target AWS CodeCommit repository as a new remote location and push the local references using the reference specification “+refs/*:refs/*”.</p> 
<pre><code class="lang-java">// push target repository
git.push().setCredentialsProvider(credentialsProvider)
&nbsp;  &nbsp; &nbsp;&nbsp; &nbsp; .setRemote(targetUrl)
&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; .setRefSpecs(new RefSpec(&quot;+refs/*:refs/*&quot;))
&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; .call();
// ...</code></pre> 
<p>In the next example, we’ll review how we can build a Lambda function to enforce commit message policies.</p> 
<b>Example 2: Enforce Git Commit Message Policy</b> 
<p>Some customers choose to enforce policies on a Git repository to maintain code quality. In this example, we use the same tools described above to clone a repository and validate the commit messages from the Git log using a regular expression.</p> 
<pre><code class="lang-java">public class PolicyEnforcerHandler implements RequestHandler&lt;CodeCommitEvent, HandlerResponse&gt; {
&nbsp; &nbsp; private static Logger logger = LoggerFactory.getLogger(ArchiveRepositoryHandler.class);
&nbsp; &nbsp; private final String mainBranch;
&nbsp; &nbsp; private final String snsTopicArn;
&nbsp; &nbsp; private final Pattern pattern;
&nbsp; &nbsp; private final AmazonSNS snsClient;
&nbsp; &nbsp; public PolicyEnforcerHandler() {
&nbsp; &nbsp; &nbsp; &nbsp; mainBranch = System.getenv(&quot;MAIN_BRANCH_NAME&quot;);
&nbsp; &nbsp; &nbsp; &nbsp; snsTopicArn = System.getenv(&quot;SNS_TOPIC_ARN&quot;);
&nbsp; &nbsp; &nbsp; &nbsp; String messageRegex = System.getenv(&quot;MESSAGE_REGEX&quot;);
&nbsp; &nbsp; &nbsp; &nbsp; pattern = Pattern.compile(messageRegex);
&nbsp; &nbsp; &nbsp; &nbsp; String snsRegion = snsTopicArn.split(&quot;:&quot;)[3];
&nbsp; &nbsp; &nbsp; &nbsp; snsClient = AmazonSNSClientBuilder.standard().withRegion(snsRegion).build();
&nbsp; &nbsp; }
// ...</code></pre> 
<p>On instantiation, the Amazon Lambda function compiles the regular expression for message validation and creates an Amazon SNS client to send notifications.</p> 
<pre><code class="lang-java">@Override
public HandlerResponse handleRequest(CodeCommitEvent event, Context context) {
&nbsp; &nbsp; String sourceName = event.getDetail().getRepositoryName();
&nbsp; &nbsp; String sourceRegion = event.getRegion();
&nbsp; &nbsp; String commitId = event.getDetail().getCommitId();
&nbsp; &nbsp; String oldCommitId = event.getDetail().getOldCommitId();
&nbsp; &nbsp; try {
&nbsp; &nbsp; &nbsp; &nbsp; // clone source repository
&nbsp; &nbsp; &nbsp; &nbsp; CodeCommitMetadata source = new CodeCommitMetadata(sourceName, sourceRegion);
&nbsp; &nbsp; &nbsp; &nbsp; String sourceUrl = source.getCloneUrlHttp();
&nbsp; &nbsp; &nbsp; &nbsp; Git git = new CloneCommandBuilder().buildCloneCommand(sourceUrl).call();
&nbsp; &nbsp; &nbsp; &nbsp; // ...</code></pre> 
<p>When the Amazon Lambda function is triggered by the AWS CloudWatch Event, the process to clone the repository is the same, discovering the AWS CodeCommit HTTPS URL from the AWS CloudWatch Event and cloning a bare Git repository to the Amazon Lambda execution container.</p> 
<pre><code class="lang-java">// use the OldCommitId, or default to the main branch
String toGitReference = Optional.ofNullable(oldCommitId).orElse(mainBranch);
Repository repository = git.getRepository();
ObjectId to = repository.resolve(toGitReference);
ObjectId from = repository.resolve(commitId);
// ...</code></pre> 
<p>JGit RevWalk is used to determine the range of commits over which to validate the message policy. When commits are added to an existing branch, AWS CodeCommit will emit an referenceUpdated event, which includes commitId and oldCommitId fields that establish the range of commits.</p> 
<p>When commits are added to a new branch, AWS CodeCommit will emit a referenceCreated event, which includes a commitId but not the oldCommitId. In this case, we will use the main branch name to determine the common ancestry of the commit chains, called the <a href="https://git-scm.com/docs/git-merge-base">merge base</a>, in order to establish the range of commits.</p> 
<pre><code class="lang-java">// create a RevWalk and set the range of commits
try (RevWalk walk = new RevWalk(repository)) {
&nbsp; &nbsp; walk.markStart(walk.parseCommit(from));
&nbsp; &nbsp; walk.markUninteresting(walk.parseCommit(to));
// iterate the list of commits and validate each message
&nbsp; &nbsp; for (RevCommit commit : walk) {
&nbsp; &nbsp;     Matcher matcher = pattern.matcher(commit.getShortMessage());
// publish a message to the topic if the message does not match
&nbsp; &nbsp; &nbsp; &nbsp; if (!matcher.find()) {
&nbsp; &nbsp; &nbsp; &nbsp;     String message = buildMessage(commit);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; logger.info(message);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; snsClient.publish(snsTopicArn, message);
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp;walk.dispose();
}
// ...</code></pre> 
<p>Once the range has been established, we iterate the list of commit messages, testing each against the message policy regular expression. If the message does not match the regular expression, then it is out of compliance from the policy, and a message is published to the Amazon SNS topic for notification.</p> 
<p>In the next example, I’ll review how to backup an archive of the files in a Git repository.</p> 
<b>Example 3: Backup Git Archive to Amazon S3</b> 
<p>The previous examples have focused on the bare Git repository <a href="https://git-scm.com/book/en/v2/Git-Internals-Git-Objects">objects</a>, however there are some use cases for processing the files in the Git repository at a particular reference. In this example, I’ll build a Lambda function to create a zip of the files in the repository and store it in Amazon S3 as a backup.</p> 
<pre><code class="lang-java">public class ArchiveRepositoryHandler
&nbsp; &nbsp; &nbsp; &nbsp; implements RequestHandler&lt;CodeCommitEvent, HandlerResponse&gt; {
&nbsp; &nbsp; private static Logger logger = LoggerFactory.getLogger(ArchiveRepositoryHandler.class);
private final String ZIP_FORMAT = &quot;zip&quot;
&nbsp; &nbsp; private final String targetS3Bucket;
&nbsp; &nbsp; private final AmazonS3 s3Client;
&nbsp; &nbsp; public ArchiveRepositoryHandler() {
&nbsp; &nbsp; &nbsp; &nbsp; targetS3Bucket = System.getenv(&quot;TARGET_S3_BUCKET&quot;);
&nbsp; &nbsp; &nbsp; &nbsp; s3Client = AmazonS3ClientBuilder.defaultClient();
&nbsp; &nbsp; &nbsp; &nbsp; ArchiveCommand.registerFormat(ZIP_FORMAT, new ZipFormat());
&nbsp; &nbsp; }
// ...</code></pre> 
<p>On instantiation, the Amazon Lambda function creates an Amazon S3 client and registers the ZipFormat with JGit.</p> 
<pre><code class="lang-java">@Override
public HandlerResponse handleRequest(CodeCommitEvent event, Context context) {
&nbsp; &nbsp; String sourceName = event.getDetail().getRepositoryName();
&nbsp; &nbsp; String sourceRegion = event.getRegion();
&nbsp; &nbsp; String commitId = event.getDetail().getCommitId();
&nbsp; &nbsp; try {
&nbsp; &nbsp; &nbsp; &nbsp; // clone source repository
&nbsp; &nbsp; &nbsp; &nbsp; CodeCommitMetadata source = new CodeCommitMetadata(sourceName, sourceRegion);
&nbsp; &nbsp; &nbsp; &nbsp; String sourceUrl = source.getCloneUrlHttp();
&nbsp; &nbsp; &nbsp; &nbsp; Git git = new CloneCommandBuilder().buildCloneCommand(sourceUrl).call();
// ...</code></pre> 
<p>When the Amazon Lambda function is triggered by the AWS CloudWatch Event, the process to clone the repository is the same, discovering the AWS CodeCommit HTTPS URL from the AWS CloudWatch Event and cloning a bare Git repository to the Amazon Lambda execution container.</p> 
<pre><code class="lang-java">// create and upload archive for commitId
File file = Files.createTempFile(null, null).toFile();
try (OutputStream out = new FileOutputStream(file)) {
ObjectId objectId = git.getRepository().resolve(commitId);
git.archive().setTree(objectId)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .setFormat(ZIP_FORMAT)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .setOutputStream(out)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; .call();
String key = sourceName + &quot;.&quot; + commitId + &quot;.&quot; + ZIP_FORMAT;
s3Client.putObject(targetS3Bucket, key, file);
}
// ...</code></pre> 
<p>Once the repository has been cloned, we use a JGit ArchiveCommand to create a zip artifact representing the working files of repository at the commit triggering the event. The generated zip artifact is then uploaded to Amazon S3 using the repository name and commit shortId as the key.</p> 
<b>Conclusion</b> 
<p>AWS CloudWatch Event’s support for AWS CodeCommit Repository State Changes event types opens possibilities to build event-driven source code workflow automation using the same AWS CloudWatch Events service that acts as an event bus across many AWS services. Combining this new capability with Amazon Lambda, the JGit client, and AWS IAM policy controls provides builders with a set of tools to build serverless solutions that securely access AWS resources, scale on demand, and are cost effective.</p> 
<p>In this blog, I’ve demonstrated three example solutions built using these tools, however AWS CodeCommit’s integration with AWS CloudWatch Events allows you to integrate with other AWS CloudWatch Events targets, like Amazon SQS or AWS Step Functions.</p> 
<p>I encourage you to visit the GitHub repository (<a href="https://github.com/awslabs/serverless-codecommit-examples">https://github.com/awslabs/serverless-codecommit-examples</a>), which has instructions to launch these examples in your own AWS account. Please share your ideas and questions in the comments below, or submit pull requests and issues to the GitHub repository!</p> 
<footer> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Create Multiple Builds from the Same Source Using Different AWS CodeBuild Build Specification Files</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Prakash Palanisamy</span></span> | on 
<time property="datePublished" datetime="2017-08-01T14:25:49+00:00">01 AUG 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/developer-tools/aws-codebuild/" title="View all posts in AWS CodeBuild*"><span property="articleSection">AWS CodeBuild*</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/developer-tools/" title="View all posts in Developer Tools*"><span property="articleSection">Developer Tools*</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/create-multiple-builds-from-the-same-source-using-different-aws-codebuild-build-specification-files/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In June 2017, <a href="https://forums.aws.amazon.com/ann.jspa?annID=4762" target="_blank" rel="noopener noreferrer">AWS CodeBuild announced</a> you can now specify an alternate build specification file name or location in an AWS CodeBuild project.</p> 
<p>In this post, I’ll show you how to use different build specification files in the same repository to create different builds. You’ll find the source code for this post in our&nbsp;<a href="https://github.com/awslabs/aws-codebuild-multiple-buildspec" target="_blank" rel="noopener noreferrer">GitHub repo</a>.</p> 
<h3>Requirements</h3> 
<p>The <a href="https://aws.amazon.com/cli/" target="_blank" rel="noopener noreferrer">AWS CLI</a> must be installed and configured.</p> 
<h3>Solution Overview</h3> 
<p>I have created a C program (cbsamplelib.c) that will be used to create a shared library and another utility program (cbsampleutil.c) to use that library. I’ll use a Makefile to compile these files.</p> 
<p>I need to put this sample application in RPM and DEB packages so end users can easily deploy them. I have created a build specification file for RPM. It will use make to compile this code and the RPM specification file (cbsample.rpmspec) configured in the build specification to create the RPM package. Similarly, I have created a build specification file for DEB. It will create the DEB package based on the control specification file (cbsample.control) configured in this build specification.</p> 
<p><span id="more-1464"></span></p> 
<h4>RPM Build Project:</h4> 
<p>The following build specification file (<a href="https://github.com/awslabs/aws-codebuild-multiple-buildspec/blob/master/buildspec-rpm.yml" target="_blank" rel="noopener noreferrer">buildspec-rpm.yml</a>) uses build specification version 0.2. As described in <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html#build-spec-ref-versions" target="_blank" rel="noopener noreferrer">the documentation</a>, this version has different syntax for environment variables. This build specification includes multiple phases:</p> 
<li>As part of the install phase, the required packages is installed using yum.</li> 
<li>During the pre_build phase, the required directories are created and the required files, including the RPM build specification file, are copied to the appropriate location.</li> 
<li>During the build phase, the code is compiled, and then the RPM package is created based on the RPM specification.</li> 
<p>As defined in the artifact section, the RPM file will be uploaded as a build artifact.</p> 
<pre><code class="lang-yaml">version: 0.2
env:
&nbsp; variables:
&nbsp; &nbsp; build_version: &quot;0.1&quot;
phases:
&nbsp; install:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - yum install rpm-build make gcc glibc -y
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - curr_working_dir=`pwd`
&nbsp; &nbsp; &nbsp; - mkdir -p ./{RPMS,SRPMS,BUILD,SOURCES,SPECS,tmp}
&nbsp; &nbsp; &nbsp; - filename=&quot;cbsample-$build_version&quot;
&nbsp; &nbsp; &nbsp; - echo $filename
&nbsp; &nbsp; &nbsp; - mkdir -p $filename
&nbsp; &nbsp; &nbsp; - cp ./*.c ./*.h Makefile $filename
&nbsp; &nbsp; &nbsp; - tar -zcvf /root/$filename.tar.gz $filename
&nbsp; &nbsp; &nbsp; - cp /root/$filename.tar.gz ./SOURCES/
&nbsp; &nbsp; &nbsp; - cp cbsample.rpmspec ./SPECS/
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Triggering RPM build&quot;
&nbsp; &nbsp; &nbsp; - rpmbuild --define &quot;_topdir `pwd`&quot; -ba SPECS/cbsample.rpmspec
&nbsp; &nbsp; &nbsp; - cd $curr_working_dir
artifacts:
&nbsp; files:
&nbsp; &nbsp; - RPMS/x86_64/cbsample*.rpm
&nbsp; discard-paths: yes
</code></pre> 
<p>Using cb-centos-project.json as a reference, create the input JSON file for the CLI command. This project uses an AWS CodeCommit repository named codebuild-multispec and a file named buildspec-rpm.yml as the build specification file. To create the RPM package, we need to specify a custom image name. I’m using the latest CentOS 7 image available in the Docker Hub. I’m using a role named CodeBuildServiceRole. It contains permissions similar to those defined in CodeBuildServiceRole.json. (You need to change the resource fields in the policy, as appropriate.)</p> 
<pre><code class="lang-json">{
&nbsp; &nbsp; &quot;name&quot;: &quot;rpm-build-project&quot;,
&nbsp; &nbsp; &quot;description&quot;: &quot;Project which will build RPM from the source.&quot;,
&nbsp; &nbsp; &quot;source&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;CODECOMMIT&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/codebuild-multispec&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;buildspec&quot;: &quot;buildspec-rpm.yml&quot;
&nbsp; &nbsp; },
&nbsp; &nbsp; &quot;artifacts&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;S3&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;codebuild-demo-artifact-repository&quot;
&nbsp; &nbsp; },
&nbsp; &nbsp; &quot;environment&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;LINUX_CONTAINER&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;image&quot;: &quot;centos:7&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;computeType&quot;: &quot;BUILD_GENERAL1_SMALL&quot;
&nbsp; &nbsp; },
&nbsp; &nbsp; &quot;serviceRole&quot;: &quot;arn:aws:iam::012345678912:role/service-role/CodeBuildServiceRole&quot;,
&nbsp; &nbsp; &quot;timeoutInMinutes&quot;: 15,
&nbsp; &nbsp; &quot;encryptionKey&quot;: &quot;arn:aws:kms:eu-west-1:012345678912:alias/aws/s3&quot;,
&nbsp; &nbsp; &quot;tags&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;key&quot;: &quot;Name&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;value&quot;: &quot;RPM Demo Build&quot;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; ]
}
</code></pre> 
<p>After the <code class="lang-bash">cli-input-json</code> file is ready, execute the following command to create the build project.</p> 
<pre><code class="lang-json">$ aws codebuild create-project --name CodeBuild-RPM-Demo --cli-input-json file://cb-centos-project.json
{
&quot;project&quot;: {
&quot;name&quot;: &quot;CodeBuild-RPM-Demo&quot;, 
&quot;serviceRole&quot;: &quot;arn:aws:iam::012345678912:role/service-role/CodeBuildServiceRole&quot;, 
&quot;tags&quot;: [
{
&quot;value&quot;: &quot;RPM Demo Build&quot;, 
&quot;key&quot;: &quot;Name&quot;
}
], 
&quot;artifacts&quot;: {
&quot;namespaceType&quot;: &quot;NONE&quot;, 
&quot;packaging&quot;: &quot;NONE&quot;, 
&quot;type&quot;: &quot;S3&quot;, 
&quot;location&quot;: &quot;codebuild-demo-artifact-repository&quot;, 
&quot;name&quot;: &quot;CodeBuild-RPM-Demo&quot;
}, 
&quot;lastModified&quot;: 1500559811.13, 
&quot;timeoutInMinutes&quot;: 15, 
&quot;created&quot;: 1500559811.13, 
&quot;environment&quot;: {
&quot;computeType&quot;: &quot;BUILD_GENERAL1_SMALL&quot;, 
&quot;privilegedMode&quot;: false, 
&quot;image&quot;: &quot;centos:7&quot;, 
&quot;type&quot;: &quot;LINUX_CONTAINER&quot;, 
&quot;environmentVariables&quot;: []
}, 
&quot;source&quot;: {
&quot;buildspec&quot;: &quot;buildspec-rpm.yml&quot;, 
&quot;type&quot;: &quot;CODECOMMIT&quot;, 
&quot;location&quot;: &quot;https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/codebuild-multispec&quot;
}, 
&quot;encryptionKey&quot;: &quot;arn:aws:kms:eu-west-1:012345678912:alias/aws/s3&quot;, 
&quot;arn&quot;: &quot;arn:aws:codebuild:eu-west-1:012345678912:project/CodeBuild-RPM-Demo&quot;, 
&quot;description&quot;: &quot;Project which will build RPM from the source.&quot;
}
}</code></pre> 
<p>When the project is created, run the following command to start the build. After the build has started, get the build ID. You can use the build ID to get the status of the build.</p> 
<pre><code class="lang-json">$ aws codebuild start-build --project-name CodeBuild-RPM-Demo
{
&nbsp; &nbsp; &quot;build&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;buildComplete&quot;: false,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;initiator&quot;: &quot;prakash&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;artifacts&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;arn:aws:s3:::codebuild-demo-artifact-repository/CodeBuild-RPM-Demo&quot;
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;projectName&quot;: &quot;CodeBuild-RPM-Demo&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;timeoutInMinutes&quot;: 15,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;buildStatus&quot;: &quot;IN_PROGRESS&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;environment&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;computeType&quot;: &quot;BUILD_GENERAL1_SMALL&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;privilegedMode&quot;: false,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;image&quot;: &quot;centos:7&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;LINUX_CONTAINER&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;environmentVariables&quot;: []
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;source&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;buildspec&quot;: &quot;buildspec-rpm.yml&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;CODECOMMIT&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/codebuild-multispec&quot;
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;currentPhase&quot;: &quot;SUBMITTED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560156.761,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;id&quot;: &quot;CodeBuild-RPM-Demo:57a36755-4d37-4b08-9c11-1468e1682abc&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;arn&quot;: &quot;arn:aws:codebuild:eu-west-1: 012345678912:build/CodeBuild-RPM-Demo:57a36755-4d37-4b08-9c11-1468e1682abc&quot;
&nbsp; &nbsp; }
}
$ aws codebuild list-builds-for-project --project-name CodeBuild-RPM-Demo
{
&nbsp; &nbsp; &quot;ids&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &quot;CodeBuild-RPM-Demo:57a36755-4d37-4b08-9c11-1468e1682abc&quot;
&nbsp; &nbsp; ]
}
$ aws codebuild batch-get-builds --ids CodeBuild-RPM-Demo:57a36755-4d37-4b08-9c11-1468e1682abc
{
&nbsp; &nbsp; &quot;buildsNotFound&quot;: [],&nbsp;
&nbsp; &nbsp; &quot;builds&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;buildComplete&quot;: true,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phases&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560157.164,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;SUBMITTED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560156.761
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;PROVISIONING&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 24,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560157.164,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560182.066
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;DOWNLOAD_SOURCE&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 15,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560182.066,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560197.906
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;INSTALL&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 19,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560197.906,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560217.515
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;PRE_BUILD&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560217.515,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560217.662
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;BUILD&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560217.662,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560217.995
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;POST_BUILD&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560217.995,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560218.074
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;UPLOAD_ARTIFACTS&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560218.074,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560218.542
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;contexts&quot;: [],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;FINALIZING&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;durationInSeconds&quot;: 4,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560218.542,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560223.128
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;phaseType&quot;: &quot;COMPLETED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560223.128
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ],&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;logs&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;groupName&quot;: &quot;/aws/codebuild/CodeBuild-RPM-Demo&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;deepLink&quot;: &quot;https://console.aws.amazon.com/cloudwatch/home?region=eu-west-1#logEvent:group=/aws/codebuild/CodeBuild-RPM-Demo;stream=57a36755-4d37-4b08-9c11-1468e1682abc&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;streamName&quot;: &quot;57a36755-4d37-4b08-9c11-1468e1682abc&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;artifacts&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;arn:aws:s3:::codebuild-demo-artifact-repository/CodeBuild-RPM-Demo&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;projectName&quot;: &quot;CodeBuild-RPM-Demo&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;timeoutInMinutes&quot;: 15,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;initiator&quot;: &quot;prakash&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;buildStatus&quot;: &quot;SUCCEEDED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;environment&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;computeType&quot;: &quot;BUILD_GENERAL1_SMALL&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;privilegedMode&quot;: false,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;image&quot;: &quot;centos:7&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;LINUX_CONTAINER&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;environmentVariables&quot;: []
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;source&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;buildspec&quot;: &quot;buildspec-rpm.yml&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;CODECOMMIT&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/codebuild-multispec&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;currentPhase&quot;: &quot;COMPLETED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500560156.761,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;endTime&quot;: 1500560223.128,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;id&quot;: &quot;CodeBuild-RPM-Demo:57a36755-4d37-4b08-9c11-1468e1682abc&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;arn&quot;: &quot;arn:aws:codebuild:eu-west-1:012345678912:build/CodeBuild-RPM-Demo:57a36755-4d37-4b08-9c11-1468e1682abc&quot;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; ]
}</code></pre> 
<h4>DEB Build Project:</h4> 
<p>In this project, we will use the build specification file named <a href="https://github.com/awslabs/aws-codebuild-multiple-buildspec/blob/master/buildspec-deb.yml" target="_blank" rel="noopener noreferrer">buildspec-deb.yml</a>. Like the RPM build project, this specification includes multiple phases. Here I use a Debian control file to create the package in DEB format. After a successful build, the DEB package will be uploaded as build artifact.</p> 
<pre><code class="lang-yaml">version: 0.2
env:
&nbsp; variables:
&nbsp; &nbsp; build_version: &quot;0.1&quot;
phases:
&nbsp; install:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - apt-get install gcc make -y
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - mkdir -p ./cbsample-$build_version/DEBIAN
&nbsp; &nbsp; &nbsp; - mkdir -p ./cbsample-$build_version/usr/lib
&nbsp; &nbsp; &nbsp; - mkdir -p ./cbsample-$build_version/usr/include
&nbsp; &nbsp; &nbsp; - mkdir -p ./cbsample-$build_version/usr/bin
&nbsp; &nbsp; &nbsp; - cp -f cbsample.control ./cbsample-$build_version/DEBIAN/control
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Building the application&quot;
&nbsp; &nbsp; &nbsp; - make
&nbsp; &nbsp; &nbsp; - cp libcbsamplelib.so ./cbsample-$build_version/usr/lib
&nbsp; &nbsp; &nbsp; - cp cbsamplelib.h ./cbsample-$build_version/usr/include
&nbsp; &nbsp; &nbsp; - cp cbsampleutil ./cbsample-$build_version/usr/bin
&nbsp; &nbsp; &nbsp; - chmod +x ./cbsample-$build_version/usr/bin/cbsampleutil
&nbsp; &nbsp; &nbsp; - dpkg-deb --build ./cbsample-$build_version
artifacts:
&nbsp; files:
&nbsp; &nbsp; - cbsample-*.deb</code></pre> 
<p>Here we use cb-ubuntu-project.json as a reference to create the CLI input JSON file. This project uses the same AWS CodeCommit repository (codebuild-multispec) but a different buildspec file in the same repository (buildspec-deb.yml). We use the default CodeBuild image to create the DEB package. We use the same IAM role (CodeBuildServiceRole).</p> 
<pre><code class="lang-json">{
&nbsp; &nbsp; &quot;name&quot;: &quot;deb-build-project&quot;,
&nbsp; &nbsp; &quot;description&quot;: &quot;Project which will build DEB from the source.&quot;,
&nbsp; &nbsp; &quot;source&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;CODECOMMIT&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/codebuild-multispec&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;buildspec&quot;: &quot;buildspec-deb.yml&quot;
&nbsp; &nbsp; },
&nbsp; &nbsp; &quot;artifacts&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;S3&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;codebuild-demo-artifact-repository&quot;
&nbsp; &nbsp; },
&nbsp; &nbsp; &quot;environment&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;LINUX_CONTAINER&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;image&quot;: &quot;aws/codebuild/ubuntu-base:14.04&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;computeType&quot;: &quot;BUILD_GENERAL1_SMALL&quot;
&nbsp; &nbsp; },
&nbsp; &nbsp; &quot;serviceRole&quot;: &quot;arn:aws:iam::012345678912:role/service-role/CodeBuildServiceRole&quot;,
&nbsp; &nbsp; &quot;timeoutInMinutes&quot;: 15,
&nbsp; &nbsp; &quot;encryptionKey&quot;: &quot;arn:aws:kms:eu-west-1:012345678912:alias/aws/s3&quot;,
&nbsp; &nbsp; &quot;tags&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;key&quot;: &quot;Name&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;value&quot;: &quot;Debian Demo Build&quot;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; ]
}</code></pre> 
<p>Using the CLI input JSON file, create the project, start the build, and check the status of the project.</p> 
<pre><code class="lang-bash">$ aws codebuild create-project --name CodeBuild-DEB-Demo --cli-input-json file://cb-ubuntu-project.json
$ aws codebuild list-builds-for-project --project-name CodeBuild-DEB-Demo
$ aws codebuild batch-get-builds --ids CodeBuild-DEB-Demo:e535c4b0-7067-4fbe-8060-9bb9de203789</code></pre> 
<p>After successful completion of the RPM and DEB builds, check the S3 bucket configured in the artifacts section for the build packages. Build projects will create a directory in the name of the build project and copy the artifacts inside it.</p> 
<pre><code class="lang-bash">$ aws s3 ls s3://codebuild-demo-artifact-repository/CodeBuild-RPM-Demo/
2017-07-20 16:16:59 &nbsp; &nbsp; &nbsp; 8108 cbsample-0.1-1.el7.centos.x86_64.rpm
$ aws s3 ls s3://codebuild-demo-artifact-repository/CodeBuild-DEB-Demo/
2017-07-20 16:37:22 &nbsp; &nbsp; &nbsp; 5420 cbsample-0.1.deb</code></pre> 
<h4>Override Buildspec During Build Start:</h4> 
<p>It’s also possible to override the build specification file of an existing project when starting a build. If we want to create the libs RPM package instead of the whole RPM, we will use the build specification file named <a href="https://github.com/awslabs/aws-codebuild-multiple-buildspec/blob/master/buildspec-libs-rpm.yml" target="_blank" rel="noopener noreferrer">buildspec-libs-rpm.yml</a>. This build specification file is similar to the earlier RPM build. The only difference is that it uses a different RPM specification file to create libs RPM.</p> 
<pre><code class="lang-yaml">version: 0.2
env:
&nbsp; variables:
&nbsp; &nbsp; build_version: &quot;0.1&quot;
phases:
&nbsp; install:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - yum install rpm-build make gcc glibc -y
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - curr_working_dir=`pwd`
&nbsp; &nbsp; &nbsp; - mkdir -p ./{RPMS,SRPMS,BUILD,SOURCES,SPECS,tmp}
&nbsp; &nbsp; &nbsp; - filename=&quot;cbsample-libs-$build_version&quot;
&nbsp; &nbsp; &nbsp; - echo $filename
&nbsp; &nbsp; &nbsp; - mkdir -p $filename
&nbsp; &nbsp; &nbsp; - cp ./*.c ./*.h Makefile $filename
&nbsp; &nbsp; &nbsp; - tar -zcvf /root/$filename.tar.gz $filename
&nbsp; &nbsp; &nbsp; - cp /root/$filename.tar.gz ./SOURCES/
&nbsp; &nbsp; &nbsp; - cp cbsample-libs.rpmspec ./SPECS/
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Triggering RPM build&quot;
&nbsp; &nbsp; &nbsp; - rpmbuild --define &quot;_topdir `pwd`&quot; -ba SPECS/cbsample-libs.rpmspec
&nbsp; &nbsp; &nbsp; - cd $curr_working_dir
artifacts:
&nbsp; files:
&nbsp; &nbsp; - RPMS/x86_64/cbsample-libs*.rpm
&nbsp; discard-paths: yes</code></pre> 
<p>Using the same RPM build project that we created earlier, start a new build and set the value of the `–buildspec-override` parameter to buildspec-libs-rpm.yml .</p> 
<pre><code class="lang-json">$ aws codebuild start-build --project-name CodeBuild-RPM-Demo --buildspec-override buildspec-libs-rpm.yml
{
&nbsp; &nbsp; &quot;build&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;buildComplete&quot;: false,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;initiator&quot;: &quot;prakash&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;artifacts&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;arn:aws:s3:::codebuild-demo-artifact-repository/CodeBuild-RPM-Demo&quot;
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;projectName&quot;: &quot;CodeBuild-RPM-Demo&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;timeoutInMinutes&quot;: 15,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;buildStatus&quot;: &quot;IN_PROGRESS&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;environment&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;computeType&quot;: &quot;BUILD_GENERAL1_SMALL&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;privilegedMode&quot;: false,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;image&quot;: &quot;centos:7&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;LINUX_CONTAINER&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;environmentVariables&quot;: []
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;source&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;buildspec&quot;: &quot;buildspec-libs-rpm.yml&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;CODECOMMIT&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;location&quot;: &quot;https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/codebuild-multispec&quot;
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;currentPhase&quot;: &quot;SUBMITTED&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;startTime&quot;: 1500562366.239,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;id&quot;: &quot;CodeBuild-RPM-Demo:82d05f8a-b161-401c-82f0-83cb41eba567&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;arn&quot;: &quot;arn:aws:codebuild:eu-west-1:012345678912:build/CodeBuild-RPM-Demo:82d05f8a-b161-401c-82f0-83cb41eba567&quot;
&nbsp; &nbsp; }
}</code></pre> 
<p>After the build is completed successfully, check to see if the package appears in the artifact S3 bucket under the CodeBuild-RPM-Demo build project folder.</p> 
<pre><code class="lang-bash">$ aws s3 ls s3://codebuild-demo-artifact-repository/CodeBuild-RPM-Demo/
2017-07-20 16:16:59 &nbsp; &nbsp; &nbsp; 8108 cbsample-0.1-1.el7.centos.x86_64.rpm
2017-07-20 16:53:54 &nbsp; &nbsp; &nbsp; 5320 cbsample-libs-0.1-1.el7.centos.x86_64.rpm</code></pre> 
<h3>Conclusion</h3> 
<p>In this post, I have shown you how multiple buildspec files in the same source repository can be used to run multiple AWS CodeBuild build projects. I have also shown you how to provide a different buildspec file when starting the build.</p> 
<p>For more information about AWS CodeBuild, see the&nbsp;<a href="https://aws.amazon.com/documentation/codebuild/" target="_blank" rel="noopener noreferrer">AWS CodeBuild documentation</a>. You can&nbsp;get started&nbsp;with AWS CodeBuild by using <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/getting-started.html" target="_blank" rel="noopener noreferrer">this step by step guide</a>.</p> 
<hr /> 
<h3>About the author</h3> 
<p><strong><img class="alignleft wp-image-1260 size-thumbnail" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/07/PP1-150x150.jpg" alt="" width="150" height="150" />Prakash Palanisamy</strong> is a Solutions Architect for Amazon Web Services. When he is not working on Serverless, DevOps or Alexa, he will be solving problems in Project Euler. He also enjoys watching educational documentaries.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/aws-codebuild/" rel="tag">AWS CodeBuild</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codebuild/" rel="tag">codebuild</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Validating AWS CloudFormation Templates</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Remek Hetman</span></span> | on 
<time property="datePublished" datetime="2017-06-28T10:49:09+00:00">28 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/validating-aws-cloudformation-templates/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>For their continuous integration and continuous deployment (CI/CD) pipeline path, many companies use tools like Jenkins, Chef, and <a href="https://aws.amazon.com/cloudformation">AWS CloudFormation</a>. Usually, the process is managed by two or more teams. One team is responsible for designing and developing an application, CloudFormation templates, and so on. The other team is generally responsible for integration and deployment.</p> 
<p>One of the challenges that a CI/CD team has is to validate the CloudFormation templates provided by the development team. Validation provides early warning about any incorrect syntax and ensures that the development team follows company policies in terms of security and the resources created by CloudFormation templates.</p> 
<p>In this post, I focus on the validation of <a href="https://aws.amazon.com/cloudformation">AWS CloudFormation</a> templates for syntax as well as in the context of business rules.</p> 
<p><span id="more-1318"></span></p> 
<h3>Scripted validation solution</h3> 
<p>For CloudFormation syntax validation, one option is to use the <a href="https://aws.amazon.com/cli">AWS CLI </a>to call the <a href="http://docs.aws.amazon.com/cli/latest/reference/cloudformation/validate-template.html">validate-template command</a>. For security and resource management, another approach is to run a Jenkins pipeline from an Amazon EC2 instance under an EC2 role that has been granted only the necessary permissions.</p> 
<p>What if you need more control over your CloudFormation templates, such as managing parameters or attributes? What if you have many development teams where permissions to the AWS environment required by one team are either too open or not open enough for another team?</p> 
<p>To have more control over the contents of your CloudFormation template, you can use the <a href="https://github.com/awslabs/aws-cloudformation-validator">cf-validator</a> Python script, which shows you how to validate different template aspects. With this script, you can validate:</p> 
<li>JSON syntax</li> 
<li>IAM capabilities</li> 
<li>Root tags</li> 
<li>Parameters</li> 
<li>CloudFormation resources</li> 
<li>Attributes</li> 
<li>Reference resources</li> 
<p>You can download this script from the <a href="https://github.com/awslabs/aws-cloudformation-validator">cf-validator</a> GitHub repo. Use the following command to run the script:</p> 
<pre>python cf-validator.py</pre> 
<p>The script takes the following parameters:</p> 
<li><strong>–cf_path [Required]</strong><br /> 
<blockquote> 
<p>The location of the CloudFormation template in JSON format. Supported location types:</p> 
<li>File system – Path to the CloudFormation template on the file system</li> 
<li>Web – URL, for example, https://my-file.com/my_cf.json</li> 
<li>Amazon S3 – Amazon S3 bucket, for example, s3://my_bucket/my_cf.json</li> 
</blockquote> </li> 
<li><strong>–cf_rules [Required]</strong><br /> 
<blockquote> 
<p>The location of the JSON file with the validation rules. This parameter supports the same locations as –cf_path. The next section of this post has more information about defining rules.</p> 
</blockquote> </li> 
<li><strong>–cf_res [Optional]</strong><br /> 
<blockquote> 
<p>The location of the JSON file with the defined AWS resources, which need to be confirmed before launching the CloudFormation template. A later section of this post has more information about resource validation.</p> 
</blockquote> </li> 
<li><strong>–allow_cap [Optional][yes/no]</strong><br /> 
<blockquote> 
<p>Controls whether you allow the creation of IAM resources by the CloudFormation template, such as policies, rules, or IAM users. The default value is no.</p> 
</blockquote> </li> 
<li><strong>–region [Optional]</strong><br /> 
<blockquote> 
<p>The AWS region where the existing resources were created. The default value is us-east-1.</p> 
</blockquote> </li> 
<h5>Defining rules</h5> 
<p>All rules are defined in the JSON format file. Rules consist of the following keys:</p> 
<li><strong>“allow_root_keys”</strong><br /> 
<blockquote> 
<p>Lists allowed root CloudFormation keys. Example of root keys are Parameters, Resources, Output, and so on. An empty list means that any key is allowed.</p> 
</blockquote> </li> 
<li><strong>“allow_parameters”</strong><br /> 
<blockquote> 
<p>Lists allowed CloudFormation parameters. For instance, to force each CloudFormation template to use only the set of parameters defined in your pipeline, list them under this key. An empty list means that any parameter is allowed.</p> 
</blockquote> </li> 
<li><strong>“allow_resources”</strong><br /> 
<blockquote> 
<p>Lists the AWS resources allowed for creation by a CloudFormation template. The format of the resource is the same as resource types in CloudFormation, but without the “AWS::” prefix. Examples:&nbsp; EC2::Instance, EC2::Volume, and so on. If you allow the creation of all resources from the given group, you can use a wildcard. For instance, if you allow all resources related to CloudFormation, you can add CloudFormation::* to the list instead of typing CloudFormation::Init, CloudFormation:Stack, and so on. An empty list means that all resources are allowed.</p> 
</blockquote> </li> 
<li><strong>“require_ref_attributes”</strong><br /> 
<blockquote> 
<p>Lists attributes (per resource) that have to be defined in CloudFormation. The value must be referenced and cannot be hardcoded. For instance, you can require that each EC2 instance must be created from a specific AMI where Image ID has to be a passed-in parameter. An empty list means that you are not requiring specific attributes to be present for a given resource.</p> 
</blockquote> </li> 
<li><strong>“allow_additional_attributes”</strong><br /> 
<blockquote> 
<p>Lists additional attributes (per resource) that can be defined and have any value in the CloudFormation template. An empty list means that any additional attribute is allowed. If you specify additional attributes for this key, then any resource attribute defined in a CloudFormation template that is not listed in this key or in the require_ref_attributes key causes validation to fail.</p> 
</blockquote> </li> 
<li><strong>“not_allow_attributes”</strong><br /> 
<blockquote> 
<p>Lists attributes (per resource) that are not allowed in the CloudFormation template. This key takes precedence over the require_ref_attributes and allow_additional_attributes keys.</p> 
</blockquote> </li> 
<h5>Rule file example</h5> 
<p>The following is an example of a rule file:</p> 
<pre><code class="lang-json">{
&quot;allow_root_keys&quot; : [&quot;AWSTemplateFormatVersion&quot;, &quot;Description&quot;, &quot;Parameters&quot;, &quot;Conditions&quot;, &quot;Resources&quot;, &quot;Outputs&quot;],
&quot;allow_parameters&quot; : [],
&quot;allow_resources&quot; : [
&quot;CloudFormation::*&quot;,
&quot;CloudWatch::Alarm&quot;,
&quot;EC2::Instance&quot;,
&quot;EC2::Volume&quot;,
&quot;EC2::VolumeAttachment&quot;,
&quot;ElasticLoadBalancing::LoadBalancer&quot;,
&quot;IAM::Role&quot;,
&quot;IAM::Policy&quot;,
&quot;IAM::InstanceProfile&quot;
],
&quot;require_ref_attributes&quot; :
{
&quot;EC2::Instance&quot; : [ &quot;InstanceType&quot;, &quot;ImageId&quot;, &quot;SecurityGroupIds&quot;, &quot;SubnetId&quot;, &quot;KeyName&quot;, &quot;IamInstanceProfile&quot; ],
&quot;ElasticLoadBalancing::LoadBalancer&quot; : [&quot;SecurityGroups&quot;, &quot;Subnets&quot;]
},
&quot;allow_additional_attributes&quot; : {},
&quot;not_allow_attributes&quot; : {}
}
</code></pre> 
<h4>Validating resources</h4> 
<p>You can use the –cf_res parameter to validate that the resources you are planning to reference in the CloudFormation template exist and are available. As a value for this parameter, point to the JSON file with defined resources. The format should be as follows:</p> 
<pre><code class="lang-json">[
{ &quot;Type&quot; : &quot;SG&quot;,
&quot;ID&quot; : &quot;sg-37c9b448A&quot;
},
{ &quot;Type&quot; : &quot;AMI&quot;,
&quot;ID&quot; : &quot;ami-e7e523f1&quot;
},
{ &quot;Type&quot; : &quot;Subnet&quot;,
&quot;ID&quot; : &quot;subnet-034e262e&quot;
}
]
</code></pre> 
<h3>Summary</h3> 
<p>At this moment, this CloudFormation template validation script supports only security groups, AMIs, and subnets. But anyone with some knowledge of Python and the boto3 package can add support for additional resources type, as needed.</p> 
<p>For more tips please visit our <a href="https://aws.amazon.com/blogs/aws/category/aws-cloud-formation/">AWS CloudFormation blog</a></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/cloudformation/" rel="tag">CloudFormation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/validation/" rel="tag">Validation</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Continuous Delivery of Nested AWS CloudFormation Stacks Using AWS CodePipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Prakash Palanisamy</span></span> | on 
<time property="datePublished" datetime="2017-06-27T23:30:29+00:00">27 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/continuous-delivery-of-nested-aws-cloudformation-stacks-using-aws-codepipeline/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In <a href="https://aws.amazon.com/blogs/aws/codepipeline-update-build-continuous-delivery-workflows-for-cloudformation-stacks/" target="_blank" rel="noopener noreferrer">CodePipeline Update – Build Continuous Delivery Workflows for CloudFormation Stacks</a>, Jeff Barr discusses infrastructure as code and how to use <a href="https://aws.amazon.com/codepipeline/" target="_blank" rel="noopener noreferrer">AWS CodePipeline</a> for continuous delivery. In this blog post, I discuss the continuous delivery of nested CloudFormation stacks using AWS CodePipeline, with <a href="https://aws.amazon.com/codecommit/" target="_blank" rel="noopener noreferrer">AWS CodeCommit</a> as the source repository and <a href="https://aws.amazon.com/codebuild/" target="_blank" rel="noopener noreferrer">AWS CodeBuild</a> as a build and <a href="https://aws.amazon.com/about-aws/whats-new/2017/03/aws-codepipeline-adds-support-for-unit-testing/" target="_blank" rel="noopener noreferrer">testing tool</a>. I deploy the stacks using <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html" target="_blank" rel="noopener noreferrer">CloudFormation change sets</a> following a <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html" target="_blank" rel="noopener noreferrer">manual approval process</a>.</p> 
<p>Here’s how to do it:</p> 
<p>In AWS CodePipeline, create a pipeline with four stages:</p> 
<li>Source (AWS CodeCommit)</li> 
<li>Build and Test (AWS CodeBuild and AWS CloudFormation)</li> 
<li>Staging (AWS CloudFormation and manual approval)</li> 
<li>Production (AWS CloudFormation and manual approval)</li> 
<p><span id="more-1238"></span></p> 
<p>Pipeline stages, the actions in each stage, and transitions between stages are shown in the following diagram.</p> 
<p><img class="alignnone wp-image-1244" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/07/Pipeline_vertical_design-2-362x1024.png" alt="" width="400" height="1131" /></p> 
<p>CloudFormation templates, test scripts, and the build specification are stored in AWS CodeCommit repositories. These files are used in the Source stage of the pipeline in AWS CodePipeline.</p> 
<p>The <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-stack.html" target="_blank" rel="noopener noreferrer">AWS::CloudFormation::Stack</a> resource type is used to create child stacks from a master stack. The CloudFormation stack resource requires the templates of the child stacks to be stored in the S3 bucket. The location of the template file is provided as a URL in the properties section of the resource definition.</p> 
<p>The following template creates three child stacks:</p> 
<li>Security (IAM, security groups).</li> 
<li>Database (an RDS instance).</li> 
<li>Web stacks (EC2 instances in an Auto Scaling group, elastic load balancer).</li> 
<pre><code class="lang-yaml">Description: Master stack which creates all required nested stacks
Parameters:
&nbsp; TemplatePath:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: S3Bucket Path where the templates are stored
&nbsp; VPCID:
&nbsp; &nbsp; Type: &quot;AWS::EC2::VPC::Id&quot;
&nbsp; &nbsp; Description: Enter a valid VPC Id
&nbsp; PrivateSubnet1:
&nbsp; &nbsp; Type: &quot;AWS::EC2::Subnet::Id&quot;
&nbsp; &nbsp; Description: Enter a valid SubnetId of private subnet in AZ1
&nbsp; PrivateSubnet2:
&nbsp; &nbsp; Type: &quot;AWS::EC2::Subnet::Id&quot;
&nbsp; &nbsp; Description: Enter a valid SubnetId of private subnet in AZ2
&nbsp; PublicSubnet1:
&nbsp; &nbsp; Type: &quot;AWS::EC2::Subnet::Id&quot;
&nbsp; &nbsp; Description: Enter a valid SubnetId of public subnet in AZ1
&nbsp; PublicSubnet2:
&nbsp; &nbsp; Type: &quot;AWS::EC2::Subnet::Id&quot;
&nbsp; &nbsp; Description: Enter a valid SubnetId of public subnet in AZ2
&nbsp; S3BucketName:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Name of the S3 bucket to allow access to the Web Server IAM Role.
&nbsp; KeyPair:
&nbsp; &nbsp; Type: &quot;AWS::EC2::KeyPair::KeyName&quot;
&nbsp; &nbsp; Description: Enter a valid KeyPair Name
&nbsp; AMIId:
&nbsp; &nbsp; Type: &quot;AWS::EC2::Image::Id&quot;
&nbsp; &nbsp; Description: Enter a valid AMI ID to launch the instance
&nbsp; WebInstanceType:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Enter one of the possible instance type for web server
&nbsp; &nbsp; AllowedValues:
&nbsp; &nbsp; &nbsp; - t2.large
&nbsp; &nbsp; &nbsp; - m4.large
&nbsp; &nbsp; &nbsp; - m4.xlarge
&nbsp; &nbsp; &nbsp; - c4.large
&nbsp; WebMinSize:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Minimum number of instances in auto scaling group
&nbsp; WebMaxSize:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Maximum number of instances in auto scaling group
&nbsp; DBSubnetGroup:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Enter a valid DB Subnet Group
&nbsp; DBUsername:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Enter a valid Database master username
&nbsp; &nbsp; MinLength: 1
&nbsp; &nbsp; MaxLength: 16
&nbsp; &nbsp; AllowedPattern: &quot;[a-zA-Z][a-zA-Z0-9]*&quot;
&nbsp; DBPassword:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Enter a valid Database master password
&nbsp; &nbsp; NoEcho: true
&nbsp; &nbsp; MinLength: 1
&nbsp; &nbsp; MaxLength: 41
&nbsp; &nbsp; AllowedPattern: &quot;[a-zA-Z0-9]*&quot;
&nbsp; DBInstanceType:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Enter one of the possible instance type for database
&nbsp; &nbsp; AllowedValues:
&nbsp; &nbsp; &nbsp; - db.t2.micro
&nbsp; &nbsp; &nbsp; - db.t2.small
&nbsp; &nbsp; &nbsp; - db.t2.medium
&nbsp; &nbsp; &nbsp; - db.t2.large
&nbsp; Environment:
&nbsp; &nbsp; Type: String
&nbsp; &nbsp; Description: Select the appropriate environment
&nbsp; &nbsp; AllowedValues:
&nbsp; &nbsp; &nbsp; - dev
&nbsp; &nbsp; &nbsp; - test
&nbsp; &nbsp; &nbsp; - uat
&nbsp; &nbsp; &nbsp; - prod
Resources:
&nbsp; SecurityStack:
&nbsp; &nbsp; Type: &quot;AWS::CloudFormation::Stack&quot;
&nbsp; &nbsp; Properties:
&nbsp; &nbsp; &nbsp; TemplateURL:
&nbsp; &nbsp; &nbsp; &nbsp; Fn::Sub: &quot;https://s3.amazonaws.com/${TemplatePath}/security-stack.yml&quot;
&nbsp; &nbsp; &nbsp; Parameters:
&nbsp; &nbsp; &nbsp; &nbsp; S3BucketName:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: S3BucketName
&nbsp; &nbsp; &nbsp; &nbsp; VPCID:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: VPCID
&nbsp; &nbsp; &nbsp; &nbsp; Environment:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: Environment
&nbsp; &nbsp; &nbsp; Tags:
&nbsp; &nbsp; &nbsp; &nbsp; - Key: Name
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Value: SecurityStack
&nbsp; DatabaseStack:
&nbsp; &nbsp; Type: &quot;AWS::CloudFormation::Stack&quot;
&nbsp; &nbsp; Properties:
&nbsp; &nbsp; &nbsp; TemplateURL:
&nbsp; &nbsp; &nbsp; &nbsp; Fn::Sub: &quot;https://s3.amazonaws.com/${TemplatePath}/database-stack.yml&quot;
&nbsp; &nbsp; &nbsp; Parameters:
&nbsp; &nbsp; &nbsp; &nbsp; DBSubnetGroup:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: DBSubnetGroup
&nbsp; &nbsp; &nbsp; &nbsp; DBUsername:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: DBUsername
&nbsp; &nbsp; &nbsp; &nbsp; DBPassword:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: DBPassword
&nbsp; &nbsp; &nbsp; &nbsp; DBServerSecurityGroup:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fn::GetAtt: SecurityStack.Outputs.DBServerSG
&nbsp; &nbsp; &nbsp; &nbsp; DBInstanceType:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: DBInstanceType
&nbsp; &nbsp; &nbsp; &nbsp; Environment:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: Environment
&nbsp; &nbsp; &nbsp; Tags:
&nbsp; &nbsp; &nbsp; &nbsp; - Key: Name
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Value: &nbsp; DatabaseStack
&nbsp; ServerStack:
&nbsp; &nbsp; Type: &quot;AWS::CloudFormation::Stack&quot;
&nbsp; &nbsp; Properties:
&nbsp; &nbsp; &nbsp; TemplateURL:
&nbsp; &nbsp; &nbsp; &nbsp; Fn::Sub: &quot;https://s3.amazonaws.com/${TemplatePath}/server-stack.yml&quot;
&nbsp; &nbsp; &nbsp; Parameters:
&nbsp; &nbsp; &nbsp; &nbsp; VPCID:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: VPCID
&nbsp; &nbsp; &nbsp; &nbsp; PrivateSubnet1:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: PrivateSubnet1
&nbsp; &nbsp; &nbsp; &nbsp; PrivateSubnet2:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: PrivateSubnet2
&nbsp; &nbsp; &nbsp; &nbsp; PublicSubnet1:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: PublicSubnet1
&nbsp; &nbsp; &nbsp; &nbsp; PublicSubnet2:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: PublicSubnet2
&nbsp; &nbsp; &nbsp; &nbsp; KeyPair:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: KeyPair
&nbsp; &nbsp; &nbsp; &nbsp; AMIId:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: AMIId
&nbsp; &nbsp; &nbsp; &nbsp; WebSG:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fn::GetAtt: SecurityStack.Outputs.WebSG
&nbsp; &nbsp; &nbsp; &nbsp; ELBSG:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fn::GetAtt: SecurityStack.Outputs.ELBSG
&nbsp; &nbsp; &nbsp; &nbsp; DBClientSG:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fn::GetAtt: SecurityStack.Outputs.DBClientSG
&nbsp; &nbsp; &nbsp; &nbsp; WebIAMProfile:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fn::GetAtt: SecurityStack.Outputs.WebIAMProfile
&nbsp; &nbsp; &nbsp; &nbsp; WebInstanceType:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: WebInstanceType
&nbsp; &nbsp; &nbsp; &nbsp; WebMinSize:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: WebMinSize
&nbsp; &nbsp; &nbsp; &nbsp; WebMaxSize:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: WebMaxSize
&nbsp; &nbsp; &nbsp; &nbsp; Environment:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ref: Environment
&nbsp; &nbsp; &nbsp; Tags:
&nbsp; &nbsp; &nbsp; &nbsp; - Key: Name
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Value: ServerStack
Outputs:
&nbsp; WebELBURL:
&nbsp; &nbsp; Description: &quot;URL endpoint of web ELB&quot;
&nbsp; &nbsp; Value:
&nbsp; &nbsp; &nbsp; Fn::GetAtt: ServerStack.Outputs.WebELBURL
</code></pre> 
<p>During the Validate stage, AWS CodeBuild checks for changes to the AWS CodeCommit source repositories. It uses the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_ValidateTemplate.html" target="_blank" rel="noopener noreferrer">ValidateTemplate</a> API to validate the CloudFormation template and copies the child templates and configuration files to the appropriate location in the S3 bucket.</p> 
<p>The following AWS CodeBuild build specification validates the CloudFormation templates listed under the TEMPLATE_FILES environment variable and copies them to the S3 bucket specified in the TEMPLATE_BUCKET environment variable in the AWS CodeBuild project. Optionally, you can use the TEMPLATE_PREFIX environment variable to specify a path inside the bucket. This updates the configuration files to use the location of the child template files. The location of the template files is provided as a parameter to the master stack.</p> 
<pre><code class="lang-yaml">version: 0.1
environment_variables:
&nbsp; plaintext:
&nbsp; &nbsp; CHILD_TEMPLATES: |
&nbsp; &nbsp; &nbsp; security-stack.yml
&nbsp; &nbsp; &nbsp; server-stack.yml
&nbsp; &nbsp; &nbsp; database-stack.yml
&nbsp; &nbsp; TEMPLATE_FILES: |
&nbsp; &nbsp; &nbsp; master-stack.yml
&nbsp; &nbsp; &nbsp; security-stack.yml
&nbsp; &nbsp; &nbsp; server-stack.yml
&nbsp; &nbsp; &nbsp; database-stack.yml
&nbsp; &nbsp; CONFIG_FILES: |
&nbsp; &nbsp; &nbsp; config-prod.json
&nbsp; &nbsp; &nbsp; config-test.json
&nbsp; &nbsp; &nbsp; config-uat.json
phases:
&nbsp; install:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; npm install jsonlint -g
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Validating CFN templates&quot;
&nbsp; &nbsp; &nbsp; - |
&nbsp; &nbsp; &nbsp; &nbsp; for cfn_template in $TEMPLATE_FILES; do
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; echo &quot;Validating CloudFormation template file $cfn_template&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; aws cloudformation validate-template --template-body file://$cfn_template
&nbsp; &nbsp; &nbsp; &nbsp; done
&nbsp; &nbsp; &nbsp; - |
&nbsp; &nbsp; &nbsp; &nbsp; for conf in $CONFIG_FILES; do
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; echo &quot;Validating CFN parameters config file $conf&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; jsonlint -q $conf
&nbsp; &nbsp; &nbsp; &nbsp; done
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Copying child stack templates to S3&quot;
&nbsp; &nbsp; &nbsp; - |
&nbsp; &nbsp; &nbsp; &nbsp; for child_template in $CHILD_TEMPLATES; do
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if [ &quot;X$TEMPLATE_PREFIX&quot; = &quot;X&quot; ]; then
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; aws s3 cp &quot;$child_template&quot; &quot;s3://$TEMPLATE_BUCKET/$child_template&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; aws s3 cp &quot;$child_template&quot; &quot;s3://$TEMPLATE_BUCKET/$TEMPLATE_PREFIX/$child_template&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fi
&nbsp; &nbsp; &nbsp; &nbsp; done
&nbsp; &nbsp; &nbsp; - echo &quot;Updating template configurtion files to use the appropriate values&quot;
&nbsp; &nbsp; &nbsp; - |
&nbsp; &nbsp; &nbsp; &nbsp; for conf in $CONFIG_FILES; do
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if [ &quot;X$TEMPLATE_PREFIX&quot; = &quot;X&quot; ]; then
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; echo &quot;Replacing \&quot;TEMPLATE_PATH_PLACEHOLDER\&quot; for \&quot;$TEMPLATE_BUCKET\&quot; in $conf&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sed -i -e &quot;s/TEMPLATE_PATH_PLACEHOLDER/$TEMPLATE_BUCKET/&quot; $conf
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; echo &quot;Replacing \&quot;TEMPLATE_PATH_PLACEHOLDER\&quot; for \&quot;$TEMPLATE_BUCKET/$TEMPLATE_PREFIX\&quot; in $conf&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sed -i -e &quot;s/TEMPLATE_PATH_PLACEHOLDER/$TEMPLATE_BUCKET\/$TEMPLATE_PREFIX/&quot; $conf
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; fi
&nbsp; &nbsp; &nbsp; &nbsp; done
artifacts:
&nbsp; files:
&nbsp; &nbsp; - master-stack.yml
&nbsp; &nbsp; - config-*.json
</code></pre> 
<p>After the template files are copied to S3, CloudFormation creates a test stack and triggers AWS CodeBuild as a test action.</p> 
<p>Then the AWS CodeBuild build specification executes <code class="lang-bash">validate-env.py</code>, the Python script used to determine whether resources created using the nested CloudFormation stacks conform to the specifications provided in the CONFIG_FILE.</p> 
<pre><code class="lang-yaml">version: 0.1
environment_variables:
&nbsp; plaintext:
&nbsp; &nbsp; CONFIG_FILE: env-details.yml
phases:
&nbsp; install:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - pip install --upgrade pip
&nbsp; &nbsp; &nbsp; - pip install boto3 --upgrade
&nbsp; &nbsp; &nbsp; - pip install pyyaml --upgrade
&nbsp; &nbsp; &nbsp; - pip install yamllint --upgrade
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Validating config file $CONFIG_FILE&quot;
&nbsp; &nbsp; &nbsp; - yamllint $CONFIG_FILE
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - echo &quot;Validating resources...&quot;
&nbsp; &nbsp; &nbsp; - python validate-env.py
&nbsp; &nbsp; &nbsp; - exit $?</code></pre> 
<p>Upon successful completion of the test action, CloudFormation deletes the test stack and proceeds to the UAT stage in the pipeline.</p> 
<p>During this stage, CloudFormation creates a change set against the UAT stack and then executes the change set. This updates the UAT environment and makes it available for acceptance testing. The process continues to a manual approval action. After the QA team validates the UAT environment and provides an approval, the process moves to the Production stage in the pipeline.</p> 
<p>During this stage, CloudFormation creates a change set for the nested production stack and the process continues to a manual approval step. Upon approval (usually by a designated executive), the change set is executed and the production deployment is completed.<br /> &nbsp;</p> 
<h3>Setting up a continuous delivery pipeline</h3> 
<p>&nbsp;<br /> I used a CloudFormation template to set up my continuous delivery pipeline. The <a href="https://github.com/awslabs/codepipeline-nested-cfn/blob/master/codepipeline-cfn-codebuild.yml" target="_blank" rel="noopener noreferrer">codepipeline-cfn-codebuild.yml</a> template, available from GitHub, sets up a full-featured pipeline.</p> 
<p>When I use the template to create my pipeline, I specify the following:</p> 
<li>AWS CodeCommit repositories.</li> 
<li>SNS topics to send approval notifications.</li> 
<li>S3 bucket name where the artifacts will be stored.</li> 
<p><img class="alignnone size-full wp-image-1172" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/07/Stack_Parameters.png" alt="" width="1894" height="802" /></p> 
<p>The CFNTemplateRepoName points to the AWS CodeCommit repository where CloudFormation templates, configuration files, and build specification files are stored.</p> 
<p>My repo contains following files:</p> 
<p><img class="alignnone size-full wp-image-1173" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/07/RepoContent.png" alt="" width="2102" height="626" /></p> 
<p>The continuous delivery pipeline is ready just seconds after clicking <strong>Create Stack</strong>. After it’s created, the pipeline executes each stage. Upon manual approvals for the UAT and Production stages, the pipeline successfully enables continuous delivery.</p> 
<p><img class="alignnone size-full wp-image-1205" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/07/CodePipeline_Flow_1.png" alt="" width="400" height="1764" /><br /> &nbsp;</p> 
<h3>Implementing a change in nested stack</h3> 
<p>&nbsp;<br /> To make changes to a child stack in a nested stack (for example, to update a parameter value or add or change resources), update the master stack. The changes must be made in the appropriate template or configuration files and then checked in to the AWS CodeCommit repository. This triggers the following deployment process:</p> 
<h3><img class="alignnone wp-image-1176 size-large" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/DeployChanges-517x1024.png" alt="" width="517" height="1024" /></h3> 
<p>&nbsp;</p> 
<h3>Conclusion</h3> 
<p>&nbsp;<br /> In this post, I showed how you can use AWS CodePipeline, AWS CloudFormation, AWS CodeBuild, and a manual approval process to create a continuous delivery pipeline for both infrastructure as code and application deployment.</p> 
<p>For more information about AWS CodePipeline, see the <a href="https://aws.amazon.com/documentation/codepipeline/" target="_blank" rel="noopener noreferrer">AWS CodePipeline documentation</a>. You can <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/getting-started-codepipeline.html" target="_blank" rel="noopener noreferrer">get started</a> in just a few clicks. All CloudFormation templates, AWS CodeBuild build specification files, and the Python script that performs the validation are available in <a href="https://github.com/awslabs/codepipeline-nested-cfn" target="_blank" rel="noopener noreferrer">codepipeline-nested-cfn</a> GitHub repository.</p> 
<hr /> 
<h3>About the author</h3> 
<p>&nbsp;<br /> <strong><img class="alignleft wp-image-1260 size-thumbnail" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/07/PP1-150x150.jpg" alt="" width="150" height="150" />Prakash Palanisamy</strong> is a Solutions Architect for Amazon Web Services. When he is not working on Serverless, DevOps or Alexa, he will be solving problems in Project Euler. He also enjoys watching educational documentaries.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/cloudformation/" rel="tag">CloudFormation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codebuild/" rel="tag">codebuild</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codecommit/" rel="tag">CodeCommit</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codedeploy/" rel="tag">CodeDeploy</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codepipeline/" rel="tag">CodePipeline</a>, <a href="https://aws.amazon.com/blogs/devops/tag/continuous-delivery/" rel="tag">Continuous Delivery</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">How to Create an AMI Builder with AWS CodeBuild and HashiCorp Packer – Part 2</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Heitor Lessa</span></span> | on 
<time property="datePublished" datetime="2017-06-22T02:04:41+00:00">22 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/how-to-create-an-ami-builder-with-aws-codebuild-and-hashicorp-packer-part-2/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h5 id="written-by-aws-solutions-architects-jason-barto-and-heitor-lessa">Written by AWS Solutions Architects Jason Barto and Heitor Lessa</h5> 
<p>&nbsp;<br /> In <a href="https://aws.amazon.com/blogs/devops/how-to-create-an-ami-builder-with-aws-codebuild-and-hashicorp-packer/">Part 1</a> of this post, we described how AWS CodeBuild, AWS CodeCommit, and HashiCorp Packer can be used to build an Amazon Machine Image (AMI) from the latest version of Amazon Linux. In this post, we show how to use AWS CodePipeline, AWS CloudFormation, and Amazon CloudWatch Events to continuously ship new AMIs. We use Ansible by Red Hat to harden the OS on the AMIs through a well-known set of security controls outlined by the Center for Internet Security in its <a href="https://benchmarks.cisecurity.org/tools2/linux/CIS_Amazon_Linux_Benchmark_v2.0.0.pdf">CIS Amazon Linux Benchmark</a>.</p> 
<p>You’ll find the source code for this post in our <a href="https://github.com/awslabs/ami-builder-packer">GitHub repo</a>.</p> 
<p><span id="introduction" class="anchor"></span>At the end of this post, we will have the following architecture:</p> 
<p><img class="alignnone wp-image-1277 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/21/ami-builder-diagram.png" alt="" width="2298" height="866" /></p> 
<b id="requirements">Requirements</b> 
<p>&nbsp;<br /> To follow along, you will need <a href="https://git-scm.com/downloads">Git</a> and a text editor. Make sure Git is configured to work with AWS CodeCommit, as described in <a href="https://aws.amazon.com/blogs/devops/how-to-build-an-ami-builder-with-codebuild-and-packer">Part 1</a>.</p> 
<b id="technologies">Technologies</b> 
<p>&nbsp;<br /> In addition to the services and products used in Part 1 of this post, we also use these AWS services and third-party software:</p> 
<p><a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.</p> 
<p><a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch Events</a> enables you to react selectively to events in the cloud and in your applications. Specifically, you can create CloudWatch Events rules that match event patterns, and take actions in response to those patterns.</p> 
<p><a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> is a continuous integration and continuous delivery service for fast and reliable application and infrastructure updates. AWS CodePipeline builds, tests, and deploys your code every time there is a code change, based on release process models you define.</p> 
<p><a href="https://aws.amazon.com/sns">Amazon SNS</a> is a fast, flexible, fully managed push notification service that lets you send individual messages or to fan out messages to large numbers of recipients. Amazon SNS makes it simple and cost-effective to send push notifications to mobile device users or email recipients. The service can even send messages to other distributed services.</p> 
<p><a href="https://www.ansible.com">Ansible</a> is a simple IT automation system that handles configuration management, application deployment, cloud provisioning, ad-hoc task-execution, and multinode orchestration.</p> 
<b id="getting-started">Getting Started</b> 
<p>&nbsp;<br /> We use CloudFormation to bootstrap the following infrastructure:</p> 
<table cellpadding="10"> 
<thead> 
<tr class="header"> 
<th>Component</th> 
<th>Purpose</th> 
</tr> 
</thead> 
<tbody> 
<tr class="odd"> 
<td><strong>AWS CodeCommit repository</strong></td> 
<td>Git repository where the AMI builder code is stored.</td> 
</tr> 
<tr class="even"> 
<td><strong>S3 bucket</strong></td> 
<td>Build artifact repository used by AWS CodePipeline and AWS CodeBuild.</td> 
</tr> 
<tr class="odd"> 
<td><strong>AWS CodeBuild project</strong></td> 
<td>Executes the AWS CodeBuild instructions contained in the build specification file.</td> 
</tr> 
<tr class="even"> 
<td><strong>AWS CodePipeline pipeline</strong></td> 
<td>Orchestrates the AMI build process, triggered by new changes in the AWS CodeCommit repository.</td> 
</tr> 
<tr class="odd"> 
<td><strong>SNS topic</strong></td> 
<td>Notifies subscribed email addresses when an AMI build is complete.</td> 
</tr> 
<tr class="even"> 
<td><strong>CloudWatch Events rule</strong></td> 
<td>Defines how the AMI builder should send a custom event to notify an SNS topic.</td> 
</tr> 
</tbody> 
</table> 
<table cellpadding="10"> 
<thead> 
<tr class="header"> 
<th>Region</th> 
<th>AMI Builder Launch Template</th> 
</tr> 
</thead> 
<tbody> 
<tr class="odd"> 
<td><strong>N. Virginia</strong> (us-east-1)</td> 
<td><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=AMI-Builder-Blogpost&amp;templateURL=https://s3-eu-west-1.amazonaws.com/ami-builder-packer/cloudformation/pipeline.yaml"><img class="alignnone wp-image-1278 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/21/deploy-to-aws.png" alt="" width="150" height="30" /></a></td> 
</tr> 
<tr class="even"> 
<td><strong>Ireland</strong> (eu-west-1)</td> 
<td><a href="https://console.aws.amazon.com/cloudformation/home?region=eu-west-1#/stacks/new?stackName=AMI-Builder-Blogpost&amp;templateURL=https://s3-eu-west-1.amazonaws.com/ami-builder-packer/cloudformation/pipeline.yaml"><img class="alignnone wp-image-1278 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/21/deploy-to-aws.png" alt="" width="150" height="30" /></a></td> 
</tr> 
</tbody> 
</table> 
<p>After launching the CloudFormation template linked here, we will have a pipeline in the AWS CodePipeline console. (Failed at this stage simply means we don’t have any data in our newly created AWS CodeCommit Git repository.)</p> 
<p><img class="alignnone wp-image-1306 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/22/ami-builder-initial-pipeline-1.png" alt="" width="277" height="500" /></p> 
<p>Next, we will clone the newly created AWS CodeCommit repository.</p> 
<p>If this is your first time connecting to a AWS CodeCommit repository, please see instructions in our documentation on <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html">Setup steps for HTTPS Connections to AWS CodeCommit Repositories</a>.</p> 
<p><strong>To clone the AWS CodeCommit repository (console)</strong></p> 
<ol style="list-style-type: decimal"> 
<li>From the AWS Management Console, open the AWS CloudFormation console.</li> 
<li>Choose the <strong>AMI-Builder-Blogpost</strong> stack, and then choose <strong>Output</strong>.</li> 
<li>Make a note of the <strong>Git repository URL</strong>.</li> 
<li>Use&nbsp;<span style="font-family: monospace"><strong>git&nbsp;</strong></span>to clone the repository.</li> 
</ol> 
<blockquote> 
<p>For example:&nbsp;<code class="lang-bash">git clone https://git-codecommit.eu-west-1.amazonaws.com/v1/repos/AMI-Builder_repo</code></p> 
</blockquote> 
<p><strong>To clone the AWS CodeCommit repository (CLI)</strong></p> 
<pre><code class="lang-bash"># Retrieve CodeCommit repo URL
git_repo=$(aws cloudformation describe-stacks --query 'Stacks[0].Outputs[?OutputKey==`GitRepository`].OutputValue' --output text --stack-name &quot;AMI-Builder-Blogpost&quot;)
# Clone repository locally
git clone ${git_repo}</code></pre> 
<h3 id="bootstrap-the-repo-with-the-ami-builder-structure">Bootstrap the Repo with the AMI Builder Structure</h3> 
<p>&nbsp;<br /> Now that our infrastructure is ready, download all the files and templates required to build the AMI.</p> 
<li>Download <a href="https://github.com/awslabs/ami-builder-packer/archive/master.zip">ami-builder-packer ZIP</a>.</li> 
<li>Extract and copy the contents to the Git repo.</li> 
<p>Your local Git repo should have the following structure:</p> 
<pre><code class="lang-bash">.
├── ami_builder_event.json
├── ansible
├── buildspec.yml
├── cloudformation
├── packer_cis.json</code></pre> 
<p>Next, push these changes to AWS CodeCommit, and then let AWS CodePipeline orchestrate the creation of the AMI:</p> 
<pre><code class="lang-bash">git add .
git commit -m &quot;My first AMI&quot;
git push origin master</code></pre> 
<h3 id="aws-codebuild-implementation-details">AWS CodeBuild Implementation Details</h3> 
<p>&nbsp;<br /> While we wait for the AMI to be created, let’s see what’s changed in our AWS CodeBuild <code>buildspec.yml</code> file:</p> 
<pre><code class="lang-yaml">...
phases:
...
build:
commands:
...
- ./packer build -color=false packer_cis.json | tee build.log
post_build:
commands:
- egrep &quot;${AWS_REGION}\:\sami\-&quot; build.log | cut -d' ' -f2 &gt; ami_id.txt
# Packer doesn't return non-zero status; we must do that if Packer build failed
- test -s ami_id.txt || exit 1
- sed -i.bak &quot;s/&lt;&lt;AMI-ID&gt;&gt;/$(cat ami_id.txt)/g&quot; ami_builder_event.json
- aws events put-events --entries file://ami_builder_event.json
...
artifacts:
files:
- ami_builder_event.json
- build.log
discard-paths: yes</code></pre> 
<p>In the build phase, we capture Packer output into a file named build.log. In the post_build phase, we take the following actions:</p> 
<ol style="list-style-type: decimal"> 
<li>Look up the AMI ID created by Packer and save its findings to a temporary file (ami_id.txt).</li> 
<li>Forcefully make AWS CodeBuild to fail if the AMI ID (ami_id.txt) is not found. This is required because Packer doesn’t fail if something goes wrong during the AMI creation process. We have to tell AWS CodeBuild to stop by informing it that an error occurred.</li> 
<li>If an AMI ID is found, we update the ami_builder_event.json file and then notify CloudWatch Events that the AMI creation process is complete.</li> 
<li>CloudWatch Events publishes a message to an SNS topic. Anyone subscribed to the topic will be notified in email that an AMI has been created.</li> 
</ol> 
<p>Lastly, the new <code>artifacts</code> phase instructs AWS CodeBuild to upload files built during the build process (<code>ami_builder_event.json</code> and <code>build.log</code>) to the S3 bucket specified in the Outputs section of the CloudFormation template. These artifacts can then be used as an input artifact in any later stage in AWS CodePipeline.</p> 
<blockquote> 
<p>For information about customizing the artifacts sequence of the buildspec.yml, see the <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">Build Specification Reference for AWS CodeBuild</a>.</p> 
</blockquote> 
<h3 id="cloudwatch-events-implementation-details">CloudWatch Events Implementation Details</h3> 
<p>&nbsp;<br /> CloudWatch Events allow you to extend the AMI builder to not only send email after the AMI has been created, but to hook up any of the supported targets to react to the AMI builder event. This event publication means you can decouple from Packer actions you might take after AMI completion and plug in other actions, as you see fit.</p> 
<blockquote> 
<p>For more information about targets in CloudWatch Events, see the <a href="http://docs.aws.amazon.com/AmazonCloudWatchEvents/latest/APIReference/API_Target.html">CloudWatch Events API Reference</a><strong>.</strong></p> 
</blockquote> 
<p>In this case, CloudWatch Events should receive the following event, match it with a rule we created through CloudFormation, and publish a message to SNS so that you can receive an email.</p> 
<p><strong>Example CloudWatch custom event</strong></p> 
<pre><code class="lang-json">[
{
&quot;Source&quot;: &quot;com.ami.builder&quot;,
&quot;DetailType&quot;: &quot;AmiBuilder&quot;,
&quot;Detail&quot;: &quot;{ \&quot;AmiStatus\&quot;: \&quot;Created\&quot;}&quot;,
&quot;Resources&quot;: [ &quot;ami-12cd5guf&quot; ]
}
]</code></pre> 
<p><strong>Cloudwatch Events rule</strong></p> 
<pre><code class="lang-json">{
&quot;detail-type&quot;: [
&quot;AmiBuilder&quot;
],
&quot;source&quot;: [
&quot;com.ami.builder&quot;
],
&quot;detail&quot;: {
&quot;AmiStatus&quot;: [
&quot;Created&quot;
]
}
}</code></pre> 
<p><strong>Example SNS message sent in email</strong></p> 
<pre><code class="lang-json">{
&quot;version&quot;: &quot;0&quot;,
&quot;id&quot;: &quot;f8bdede0-b9d7...&quot;,
&quot;detail-type&quot;: &quot;AmiBuilder&quot;,
&quot;source&quot;: &quot;com.ami.builder&quot;,
&quot;account&quot;: &quot;&lt;&lt;aws_account_number&gt;&gt;&quot;,
&quot;time&quot;: &quot;2017-04-28T17:56:40Z&quot;,
&quot;region&quot;: &quot;eu-west-1&quot;,
&quot;resources&quot;: [&quot;ami-112cd5guf &quot;],
&quot;detail&quot;: {
&quot;AmiStatus&quot;: &quot;Created&quot;
}
}</code></pre> 
<h3 id="packer-implementation-details">Packer Implementation Details</h3> 
<p>&nbsp;<br /> In addition to the build specification file, there are differences between the current version of the HashiCorp Packer template (<code>packer_cis.json</code>) and the one used in Part 1.</p> 
<p><strong>Variables</strong></p> 
<pre><code class="lang-json">  &quot;variables&quot;: {
&quot;vpc&quot;: &quot;{{env `BUILD_VPC_ID`}}&quot;,
&quot;subnet&quot;: &quot;{{env `BUILD_SUBNET_ID`}}&quot;,
“ami_name”: “Prod-CIS-Latest-AMZN-{{isotime \”02-Jan-06 03_04_05\”}}”
},</code></pre> 
<li><code>ami_name</code>: Prefixes a name used by Packer to tag resources during the <code>Builders</code> sequence.</li> 
<li><code>vpc</code> and <code>subnet</code>: Environment variables defined by the CloudFormation stack parameters.</li> 
<p>We no longer assume a default VPC is present and instead use the VPC and subnet specified in the CloudFormation parameters. CloudFormation configures the AWS CodeBuild project to use these values as environment variables. They are made available throughout the build process.</p> 
<p>That allows for more flexibility should you need to change which VPC and subnet will be used by Packer to launch temporary resources.</p> 
<p><strong>Builders</strong></p> 
<pre><code class="lang-json">  &quot;builders&quot;: [{
...
&quot;ami_name&quot;: “{{user `ami_name`| clean_ami_name}}”,
&quot;tags&quot;: {
&quot;Name&quot;: “{{user `ami_name`}}”,
},
&quot;run_tags&quot;: {
&quot;Name&quot;: “{{user `ami_name`}}&quot;,
},
&quot;run_volume_tags&quot;: {
&quot;Name&quot;: “{{user `ami_name`}}&quot;,
},
&quot;snapshot_tags&quot;: {
&quot;Name&quot;: “{{user `ami_name`}}&quot;,
},
...
&quot;vpc_id&quot;: &quot;{{user `vpc` }}&quot;,
&quot;subnet_id&quot;: &quot;{{user `subnet` }}&quot;
}],</code></pre> 
<p>We now have new properties (<code>*_tag</code>) and a new function (<code>clean_ami_name</code>) and launch temporary resources in a <code>VPC</code> and <code>subnet</code> specified in the environment variables. AMI names can only contain a certain set of ASCII characters. If the input in <code>project</code> deviates from the expected characters (for example, includes whitespace or slashes), Packer’s <code>clean_ami_name</code> function will fix it.</p> 
<blockquote> 
<p>For more information, see <a href="https://www.packer.io/docs/templates/engine.html">functions</a> on the HashiCorp Packer website.</p> 
</blockquote> 
<p><strong>Provisioners</strong></p> 
<pre><code class="lang-json">  &quot;provisioners&quot;: [
{
&quot;type&quot;: &quot;shell&quot;,
&quot;inline&quot;: [
&quot;sudo pip install ansible&quot;
]
}, 
{
&quot;type&quot;: &quot;ansible-local&quot;,
&quot;playbook_file&quot;: &quot;ansible/playbook.yaml&quot;,
&quot;role_paths&quot;: [
&quot;ansible/roles/common&quot;
],
&quot;playbook_dir&quot;: &quot;ansible&quot;,
&quot;galaxy_file&quot;: &quot;ansible/requirements.yaml&quot;
},
{
&quot;type&quot;: &quot;shell&quot;,
&quot;inline&quot;: [
&quot;rm .ssh/authorized_keys ; sudo rm /root/.ssh/authorized_keys&quot;
]
}</code></pre> 
<p>We used <code>shell</code> provisioner to apply OS patches in Part 1. Now, we use <code>shell</code> to install Ansible on the target machine and <code>ansible-local</code> to import, install, and execute Ansible roles to make our target machine conform to our standards.</p> 
<p>Packer uses <code>shell</code> to remove temporary keys before it creates an AMI from the target and temporary EC2 instance.</p> 
<h3 id="ansible-implementation-details">Ansible Implementation Details</h3> 
<p>&nbsp;<br /> Ansible provides OS patching through a custom <code>Common</code> role that can be easily customized for other tasks.</p> 
<p>CIS Benchmark and Cloudwatch Logs are implemented through two Ansible third-party roles that are defined in <code>ansible/requirements.yaml</code> as seen in the Packer template.</p> 
<p>The Ansible provisioner uses Ansible Galaxy to download these roles onto the target machine and execute them as instructed by <code>ansible/playbook.yaml</code>.</p> 
<blockquote> 
<p>For information about how these components are organized, see the <a href="http://docs.ansible.com/ansible/playbooks_roles.html">Playbook Roles and Include Statements</a> in the Ansible documentation.</p> 
</blockquote> 
<p>The following Ansible playbook (<code>ansible&lt;/playbook.yaml</code>) controls the execution order and custom properties:</p> 
<pre><code class="lang-yaml">---
- hosts: localhost
connection: local
gather_facts: true    # gather OS info that is made available for tasks/roles
become: yes           # majority of CIS tasks require root
vars:
# CIS Controls whitepaper:  http://bit.ly/2mGAmUc
# AWS CIS Whitepaper:       http://bit.ly/2m2Ovrh
cis_level_1_exclusions:
# 3.4.2 and 3.4.3 effectively blocks access to all ports to the machine
## This can break automation; ignoring it as there are stronger mechanisms than that
- 3.4.2 
- 3.4.3
# CloudWatch Logs will be used instead of Rsyslog/Syslog-ng
## Same would be true if any other software doesn't support Rsyslog/Syslog-ng mechanisms
- 4.2.1.4
- 4.2.2.4
- 4.2.2.5
# Autofs is not installed in newer versions, let's ignore
- 1.1.19
# Cloudwatch Logs role configuration
logs:
- file: /var/log/messages
group_name: &quot;system_logs&quot;
roles:
- common
- anthcourtney.cis-amazon-linux
- dharrisio.aws-cloudwatch-logs-agent</code></pre> 
<p>Both third-party Ansible roles can be easily configured through variables (<code>vars</code>). We use Ansible playbook variables to exclude CIS controls that don’t apply to our case and to instruct the CloudWatch Logs agent to stream the /var/log/messages log file to CloudWatch Logs.</p> 
<p>If you need to add more OS or application logs, you can easily duplicate the playbook and make changes. The CloudWatch Logs agent will ship configured log messages to CloudWatch Logs.</p> 
<blockquote> 
<p>For more information about parameters you can use to further customize third-party roles, download Ansible roles for the <a href="https://galaxy.ansible.com/dharrisio/aws-cloudwatch-logs-agent/">Cloudwatch Logs</a> Agent and <a href="https://galaxy.ansible.com/anthcourtney/cis-amazon-linux/">CIS Amazon Linux</a> from the Galaxy website.</p> 
</blockquote> 
<b id="commiting-changes">Committing Changes</b> 
<p>&nbsp;<br /> Now that Ansible and CloudWatch Events are configured as a part of the build process, commiting any changes to the AWS CodeComit Git Repository will triger a new AMI build process that can be followed through the AWS CodePipeline console.</p> 
<p><img class="alignnone wp-image-1281 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/21/ami-builder-pipeline.png" alt="" width="304" height="605" /></p> 
<p>When the build is complete, an email will be sent to the email address you provided as a part of the CloudFormation stack deployment. The email serves as notification that an AMI has been built and is ready for use.</p> 
<b id="summary">Summary</b> 
<p>&nbsp;<br /> We used AWS CodeCommit, AWS CodePipeline, AWS CodeBuild, Packer, and Ansible to build a pipeline that continuously builds new, hardened CIS AMIs. We used Amazon SNS so that email addresses subscribed to a SNS topic are notified upon completion of the AMI build.</p> 
<p>By treating our AMI creation process as code, we can iterate and track changes over time. In this way, it’s no different from a software development workflow. With that in mind, software patches, OS configuration, and logs that need to be shipped to a central location are only a git commit away.</p> 
<b id="next-steps">Next Steps</b> 
<p>&nbsp;<br /> Here are some ideas to extend this AMI builder:</p> 
<li>Hook up a Lambda function in Cloudwatch Events to update EC2 Auto Scaling configuration upon completion of the AMI build.</li> 
<li>Use AWS CodePipeline parallel steps to build multiple Packer images.</li> 
<li>Add a commit ID as a tag for the AMI you created.</li> 
<li>Create a scheduled Lambda function through Cloudwatch Events to clean up old AMIs based on timestamp (name or additional tag).</li> 
<li>Implement Windows support for the AMI builder.</li> 
<li>Create a cross-account or cross-region AMI build.</li> 
<p>Cloudwatch Events allow the AMI builder to decouple AMI configuration and creation so that you can easily add your own logic using targets (AWS Lambda, Amazon SQS, Amazon SNS) to add events or recycle EC2 instances with the new AMI.</p> 
<p>If you have questions or other feedback, feel free to leave it in the comments or contribute to the AMI Builder repo on GitHub.<span id="tood" class="anchor"></span></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/amazon-machine-images/" rel="tag">Amazon Machine Images</a>, <a href="https://aws.amazon.com/blogs/devops/tag/aws-codecommit/" rel="tag">AWS CodeCommit</a>, <a href="https://aws.amazon.com/blogs/devops/tag/cloudformation/" rel="tag">CloudFormation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codecommit/" rel="tag">CodeCommit</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codepipeline/" rel="tag">CodePipeline</a>, <a href="https://aws.amazon.com/blogs/devops/tag/compliance/" rel="tag">Compliance</a>, <a href="https://aws.amazon.com/blogs/devops/tag/devops/" rel="tag">DevOps</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building a Continuous Delivery Pipeline for AWS Service Catalog (Sync AWS Service Catalog with Version Control)</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Anuj Sharma</span></span> | on 
<time property="datePublished" datetime="2017-06-12T22:53:40+00:00">12 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/best-practices/" title="View all posts in Best practices"><span property="articleSection">Best practices</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/aws-service-catalog-sync-code/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/servicecatalog">AWS Service Catalog</a> enables organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multitier application architectures. You can use AWS Service Catalog to centrally manage commonly deployed IT services. It also helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need.</p> 
<p>However, as the number of Service Catalog <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#w1ab1b5c17c11">portfolios</a> and <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#w1ab1b5c17c13">products</a> increases across an organization, centralized management and scaling can become a challenge. In this blog post, I walk you through a solution that simplifies management of AWS Service Catalog portfolios and related products. This solution also enables <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_portfolios_sharing.html">portfolio sharing with other accounts</a>, <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/portfoliomgmt-tags.html">portfolio tagging</a>, and <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_portfolios_users.html">granting access to users</a>. Finally, the solution delivers <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/productmgmt-update.html">updates to the products</a> using a continuous delivery in <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a>. This enables you to maintain them in version control, thereby adopting “Infrastructure as Code” practices.</p> 
<b>Solution overview</b> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/architecture.jpeg"><img class="aligncenter size-full wp-image-1133" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/architecture.jpeg" alt="" width="734" height="584" /></a></p> 
<ol> 
<li>Authors (developers, operations, architects, etc.) create the <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation templates</a> based on the needs of their organizations. These templates are the reusable artifacts. They can be shared among various teams within the organizations. You can name these templates product-A.yaml or product-B.yaml. For example, if the template creates an <a href="https://aws.amazon.com/vpc">Amazon VPC</a> that is based on organization needs, as described in the <a href="https://aws.amazon.com/quickstart/architecture/vpc/">Amazon VPC Architecture Quick Start</a>, you can save it as product-vpc.yaml.</li> 
</ol> 
<p>The authors also define a mapping.yaml file, which includes the list of <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_products.html">products</a> that you want to include in the <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_portfolios.html">portfolio</a> and related metadata. The mapping.yaml file is the core configuration component of this solution. This file defines your portfolio and its associated permissions and products. This configuration file determines how your portfolio will look in AWS Service Catalog, after the solution deploys it. A sample mapping.yaml is described <a href="https://github.com/awslabs/aws-pipeline-to-service-catalog/blob/master/portfolio-infrastructure/mapping.yaml">here</a>. Configuration properties of this mapping.yaml are explained <a href="https://github.com/awslabs/aws-pipeline-to-service-catalog/blob/master/README.md">here</a>.</p> 
<p>&nbsp;</p> 
<ol start="2"> 
<li>Product template files and the mappings are committed to version control. In this example, we use <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a>. The folder structure on the file system looks like the following: 
<li>portfolio-infrastructure (folder name)<br /> – product-a.yaml<br /> – product-b.yaml<br /> – product-c.yaml<br /> – mapping.yaml</li> 
<li>portfolio-example (folder name)<br /> – product-c.yaml<br /> – product-d.yaml<br /> – mapping.yaml</li> 
</ul> <p>The name of the folder must start with <strong>portfolio-</strong> because the <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> function iterates through all folders whose names start with <strong>portfolio-</strong>, and syncs them with AWS Service Catalog.</p> <p>Checking in any code in the repository triggers an AWS CodePipeline orchestration and invokes the Lambda function.</p></li> 
<li>The Lambda function downloads the code from version control and iterates through all folders with names that start with <strong>portfolio-</strong>. The function gets a list of all existing portfolios in AWS Service Catalog. Then it checks whether the display name of the portfolio matches the “name” property in the mapping.yaml under each folder. If the name doesn’t match, a new portfolio is created. If the name matches, the description and owner fields are updated and synced with what is in the file. There must be only one mapping.yaml file in each folder with a name starting with <strong>portfolio-</strong>.</li> 
<li>and 5. The Lambda function iterates through the list of products in the mapping.yaml file. If the name of product matches any of the products already associated with the portfolio, a new version of the product is created and is associated with the portfolio. If the name of the product doesn’t match, a new product is created. The CloudFormation template file (as specified in the template property for that product in the mapping file) is uploaded to Amazon S3 with a unique ID. A new version of the product is created and is pointed to the unique S3 path.</li> 
</ol> 
<b>Try it out!</b> 
<p>Get started using this solution, which is available in this <a href="https://github.com/awslabs/aws-pipeline-to-service-catalog">AWSLabs GitHub repository</a>.</p> 
<ol> 
<li>Clone the repository. It contains the <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation templates</a> that we use in this walkthrough.</li> 
</ol> 
<pre><code class="lang-bash">git clone https://github.com/awslabs/aws-pipeline-to-service-catalog.git
cd aws-pipeline-to-service-catalog
</code></pre> 
<ol start="2"> 
<li>Examine mapping.yaml under the portfolio-infrastructure folder. Replace the account number with the account number with which to share the portfolio. To <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_portfolios_sharing.html">share the portfolio</a> with multiple other accounts, you can append more account numbers to the list. These account numbers must be valid AWS accounts, and must not include the account number in which this solution is being created. Optionally, edit this file and provide the values you want for the name, description, and owner properties. You can also choose to leave these values as they are, which creates a portfolio with the name, description, and owners described in the file.</li> 
<li>Optional – If you don’t have the <a href="https://aws.amazon.com/cli/">AWS Command Line Interface</a> (AWS CLI) installed, install it as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">here</a>. To prepare your access keys or assumed role to make calls to AWS, configure the AWS CLI as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">here</a>.</li> 
<li>Create a pipeline. This orchestrates continuous integration with the AWS CodeCommit repository created in step 2, and continuously syncs AWS Service Catalog with the code.</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --template-file pipeline-to-service-catalog.yaml \
--stack-name service-catalog-sync-pipeline --capabilities CAPABILITY_NAMED_IAM \
--parameter-overrides RepositoryName=blogs-pipeline-to-service-catalog
</code></pre> 
<p>This creates the following resources.</p> 
<ol> 
<li>An AWS CodeCommit repository to push the code to. You can get the repository URL to push the code from the outputs of the stack that we just created. Connect, commit, and push code to this repository as described <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/how-to-connect.html">here</a>.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/Screen-Shot-2017-05-14-at-7.23.58-PM.png"><img class="aligncenter size-full wp-image-1134" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/Screen-Shot-2017-05-14-at-7.23.58-PM.png" alt="" width="1982" height="662" /></a></p> 
<ol start="2"> 
<li style="list-style-type: none"> 
<ol start="2"> 
<li>An S3 bucket, which holds the built artifacts (CloudFormation templates) and the Lambda function code.</li> 
<li>The <a href="https://aws.amazon.com/iam/">AWS IAM </a><a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">roles</a> and <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html">policies</a>, with least privileges for this solution to work.</li> 
<li>An <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a> project, which builds the Lambda function. This <a href="http://docs.aws.amazon.com/lambda/latest/dg/python-programming-model.html">Python-based Lambda function</a> has the logic, as explained earlier.</li> 
<li>A pipeline with the following four stages: 
<li><strong>Stage-1</strong>: Checks out source from the repository created in step 2</li> 
<li><strong>Stage-2</strong>: Builds the Lambda function using AWS CodeBuild, which has the logic to sync the AWS Service Catalog products and portfolios with code.</li> 
<li><strong>Stage-3</strong>: Deploys the Lambda function using CloudFormation.</li> 
<li><strong>Stage-4</strong>: Invokes the Lambda function. Once this stage completes successfully, you see an AWS Service Catalog portfolio and two products created, as shown below.</li> 
</ul> </li> 
</ol> </li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/Screen-Shot-2017-05-07-at-4.16.27-PM.png"><img class="aligncenter wp-image-1135 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/Screen-Shot-2017-05-07-at-4.16.27-PM.png" alt="" width="2510" height="1052" /></a></p> 
<p>&nbsp;</p> 
<b>Optional next steps!</b> 
<p>You can deploy the Lambda function as we explained in this post to sync AWS Service Catalog products, portfolios, and permissions across multiple accounts that you own with version control. You can create a secure cross-account continuous delivery pipeline, as explained <a href="https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/">here</a>. To do this:</p> 
<ol> 
<li>Delete all the resources created earlier.</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation delete-stack -- stack-name service-catalog-sync-pipeline</code></pre> 
<ol start="2"> 
<li>Follow the steps in <a href="https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/">this blog post</a>. The sample Lambda function, described <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/README.md">here</a>, is the same as what I explained in this post.</li> 
</ol> 
<b>Conclusion</b> 
<p>You can use AWS Lambda to make API calls to AWS Service Catalog to keep portfolios and products in sync with a mapping file. The code includes the CloudFormation templates and the mapping file and folder structure, which resembles the portfolios in AWS Service Catalog. When checked in to an AWS CodeCommit repository, it invokes the Lambda function, orchestrated by AWS CodePipeline.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/aws-cloudformation/" rel="tag">AWS CloudFormation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/aws-codepipeline/" rel="tag">AWS CodePipeline</a>, <a href="https://aws.amazon.com/blogs/devops/tag/aws-service-catalog/" rel="tag">AWS Service Catalog</a>, <a href="https://aws.amazon.com/blogs/devops/tag/best-practices/" rel="tag">Best practices</a>, <a href="https://aws.amazon.com/blogs/devops/tag/continuous-integration/" rel="tag">Continuous Integration</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Database Continuous Integration and Automated Release Management Workflow with AWS and Datical DB</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Balaji Iyer</span></span> | on 
<time property="datePublished" datetime="2017-06-02T05:00:14+00:00">02 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/best-practices/" title="View all posts in Best practices"><span property="articleSection">Best practices</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/partners/" title="View all posts in Partners"><span property="articleSection">Partners</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/database-continuous-integration-and-automated-release-management-workflow-with-aws-and-datical-db/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Just as a herd can move only as fast as its slowest member, companies must increase the speed of all parts of their release process, especially the database change process, which is often manual. One bad database change can bring down an app or compromise data security.</p> 
<p>We need to make database code deployment as fast and easy as application release automation, while eliminating risks that cause application downtime and data security vulnerabilities. Let’s take a page from the application development playbook and bring a continuous deployment approach to the database.</p> 
<p>By creating a continuous deployment database, you can:</p> 
<li>Discover mistakes more quickly.</li> 
<li>Deliver updates faster and frequently.</li> 
<li>Help developers write better code.</li> 
<li>Automate the database release management process.</li> 
<p>The database deployment package can be promoted automatically with application code changes. With database continuous deployment, application development teams can deliver smaller, less risky deployments, making it possible to respond more quickly to business or customer needs.</p> 
<p>In our previous post, <a href="https://aws.amazon.com/blogs/devops/building-end-to-end-continuous-delivery-and-deployment-pipelines-in-aws-and-teamcity/">Building End-to-End Continuous Delivery and Deployment Pipelines in AWS</a>, we walked through steps for implementing a continuous deployment and automated delivery pipeline for your application.</p> 
<p>In this post, we walk through steps for building a continuous deployment workflow for databases using <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> (a fully managed continuous delivery service) and <a href="http://www.datical.com/solution/">Datical DB</a> (a database release automation application). We use <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a> for source code control and <a href="https://aws.amazon.com/rds/">Amazon RDS</a> for database hosting to demonstrate end-to-end database change management — from check-in to final deployment.</p> 
<p>As part of this example, we will show how a database change that does not meet standards is rejected automatically and actionable feedback is provided to the developer. Just like a code unit test, Datical DB evaluates changes and enforces your organization’s standards. In the sample use case, database table indexes of more than three columns are disallowed. In some cases, this type of index can slow performance.</p> 
<p><strong>Prerequisites</strong></p> 
<p>You’ll need an AWS account, an Amazon EC2 key pair, and administrator-level permissions for <a href="https://aws.amazon.com/iam/">AWS Identity and Access Management (IAM)</a>, AWS CodePipeline, AWS CodeCommit, Amazon RDS, <a href="https://aws.amazon.com/ec2/">Amazon EC2</a>, and <a href="https://aws.amazon.com/s3/">Amazon S3</a>.</p> 
<p>From Datical DB, you’ll need access to software.datical.com portal, your license key, a database, and JDBC drivers. You can request a free trial of Datical <a href="http://pages.datical.com/aws-blog-trial-request.html">here</a>.</p> 
<b>Overview</b> 
<p>Here are the steps:</p> 
<ol> 
<li style="text-align: left">Install and configure Datical DB.</li> 
<li style="text-align: left">Create an RDS database instance running the Oracle database engine.</li> 
<li style="text-align: left">Configure Datical DB to manage database changes across your software development life cycle (SDLC).</li> 
<li style="text-align: left">Set up database version control using AWS CodeCommit.</li> 
<li style="text-align: left">Set up a continuous integration server to stage database changes.</li> 
<li style="text-align: left">Integrate the continuous integration server with Datical DB.</li> 
<li style="text-align: left">Set up automated release management for your database through AWS CodePipeline.</li> 
<li style="text-align: left">Enforce security governance and standards with the Datical DB Rules Engine.</li> 
</ol> 
<b>1. Install and configure Datical DB</b> 
<p>Navigate to <a href="https://software.datical.com">https://software.datical.com</a> and sign in with your credentials. From the left navigation menu, expand the <strong>Common</strong> folder, and then open the <strong>Datical_DB_Folder</strong>. Choose the latest version of the application by reviewing the date suffix in the name of the folder. Download the installer for your platform — Windows (32-bit or 64-bit) or Linux (32-bit or 64-bit).</p> 
<b>Verify the JDK Version</b> 
<p>In a terminal window, run the following command to ensure you’re running JDK version 1.7.x or later.</p> 
<pre><code class="lang-code"># java –version
java version &quot;1.7.0_75&quot;
Java(TM) SE Runtime Environment (build 1.7.0_75-b13)
Java HotSpot(TM) Client VM (build 24.75-b04, mixed mode, sharing)</code></pre> 
<p>The Datical DB installer contains a graphical (GUI) and command line (CLI) interface that can be installed on Windows and Linux operating systems.</p> 
<b>Install Datical DB (GUI)</b> 
<ol> 
<li>Double-click on the installer</li> 
<li>Follow the prompts to install the application.</li> 
<li>When prompted, type the path to a valid license.</li> 
</ol> 
<h3>Install JDBC drivers</h3> 
<ol> 
<li>Open the Datical DB application.</li> 
<li>From the menu, choose <strong>Help</strong>, and then choose <strong>Install New Software</strong>.</li> 
<li>From the <strong>Work with</strong> drop-down list, choose Database Drivers – <a href="http://update.datical.com/drivers/updates">http://update.datical.com/drivers/updates</a>.<img class="alignnone wp-image-1220 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/1-2.png" alt="" width="557" height="523" /></li> 
<li>Follow the prompts to install the drivers.</li> 
</ol> 
<b>Install Datical DB (CLI)</b> 
<p>Datical DB (CLI only) can be installed on a headless Linux system. Select the correct 32-bit or 64-bit Linux installer for your system.</p> 
<ol> 
<li>Run the installer as root and install it to /usr/local/DaticalDB. <pre><code class="lang-bash">sudo java -jar ../installers/&lt;Datical Installer&gt;.jar -console</code></pre> </li> 
<li>Follow the prompts to install the application.</li> 
<li>When prompted, type the path to a valid license.</li> 
</ol> 
<h3>Install JDBC drivers</h3> 
<ol> 
<li>Copy JDBC drivers to /usr/local/DaticalDB/jdbc_drivers. <pre><code class="lang-bash">sudo mkdir /usr/local/DaticalDB/jdbc_drivers
copy JDBC Drivers from software.datical.com to /usr/local/DaticalDB/jdbc_drivers</code></pre> </li> 
<li><code class="lang-bash"></code>Copy the license file to /usr/local/DaticalDB/repl. <pre><code class="lang-bash">sudo cp &lt;license_filename&gt; /usr/local/DaticalDB/repl
sudo chmod 777 /usr/local/DaticalDB/repl/&lt;license_filename&gt;</code></pre> </li> 
</ol> 
<b>2. Create an RDS instance running the Oracle database engine</b> 
<p>Datical DB supports database engines like Oracle, MySQL, Microsoft SQL Server, PostgreSQL, and IBM DB2. The example in this post uses a DB instance running Oracle. To create a DB instance running Oracle, <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateOracleInstance.html">follow these steps</a>.<br /> Make sure that you can access the Oracle port (1521) from the location where you will be using Datical DB. Just like SQLPlus or other database management tools, Datical DB must be able to connect to the Oracle port. When you configure the security group for your RDS instance, make sure you can access port 1521 from your location.</p> 
<b>3. Manage database changes across the SDLC</b> 
<p>This one-time process is required to ensure databases are in sync so that you can manage database changes across the SDLC:</p> 
<ol> 
<li>Create a Datical DB deployment plan with connections to the databases to be managed.</li> 
<li>Baseline the first database (DEV/CI). This must be the original or best configured database – your reference database.</li> 
<li>For each additional database (TEST and PROD):<br /> a. Compare databases to ensure the application schema are in sync.<br /> b. Resolve any differences.<br /> c. Perform a change log sync to get each setup for Datical DB management.</li> 
</ol> 
<p>Datical DB creates an initial model change log from one of the databases. It also creates in each database a metadata table named DATABASECHANGELOG that will be used to track the state. Now the databases look like this:</p> 
<p><img class="alignnone wp-image-1081 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/2.png" alt="Datical DB Model" width="975" height="474" /></p> 
<p><strong>Note</strong>: In the preceding figure, the Datical DB model and metadata table are a representation of the actual model.</p> 
<b>Create a deployment plan</b> 
<ol> 
<li style="list-style-type: none"> 
<ol> 
<li>In Datical DB, right-click <strong>Deployment Plans</strong>, and choose <strong>New</strong>.</li> 
<li>On the <strong>New Deployment Plan</strong> page, type a name for your project (for example, AWS-Sample-Project), and then choose <strong>Next</strong>.</li> 
<li>Select <strong>Oracle 11g Instant Client</strong>, type a name for the database (for example, DevDatabase), and then choose <strong>Next</strong>.</li> 
<li>On the following page, provide the database connection information.<img class="alignnone wp-image-1193 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/3-1.png" alt="" width="405" height="554" /> 
<ol> 
<li>For <strong>Hostname</strong>, enter the RDS endpoint..</li> 
<li>Select <strong>SID</strong>, and then type ORCL.</li> 
<li>Type the user name and password used to connect to the RDS instance running Oracle.</li> 
<li>Before you choose <strong>Finish</strong>, choose the <strong>Test Connection</strong> button.</li> 
</ol> </li> 
</ol> </li> 
</ol> 
<p>When Datical DB creates the project, it also creates a baseline snapshot that captures the current state of the database schema. Datical DB stores the snapshot in Datical change sets for future forecasting and modification.</p> 
<b>Create a database change set</b> 
<p>A change set describes the change/refactoring to apply to the database.<br /> From the <strong>AWS-Sample-Project</strong> project in the left pane, right-click <strong>Change Log</strong>, select <strong>New</strong>, and then select <strong>Change Set</strong>. Choose the type of change to make, and then choose <strong>Next</strong>. In this example, we’re creating a table. For <strong>Table Name</strong>, type a name. Choose <strong>Add Column</strong>, and then provide information to add one or more columns to the new table. Follow the prompts, and then choose <strong>Finish</strong>.</p> 
<p><img class="alignnone size-full wp-image-1087" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/4.png" alt="Add Columns" width="975" height="230" /></p> 
<p>The new change set will be added at the end of your current change log. You can tag change sets with a sprint label. Depending on the environment, changes can be deployed based on individual labels or by the higher-level grouping construct.<br /> Datical DB also provides an option to load SQL scripts into a database, where the change sets are labeled and captured as objects. This makes them ready for deployment in other environments.</p> 
<b>Best practices for continuous delivery</b> 
<p>Change sets are stored in an XML file inside the Datical DB project. The file, changelog.xml, is stored inside the Changelog folder. (In the Datical DB UI, it is called Change Log.)</p> 
<p>Just like any other files stored in your source code repository, the Datical DB change log can be branched and merged to support agile software development, where individual work spaces are isolated until changes are merged into the parent branch.</p> 
<p>To implement this best practice, your Datical DB project should be checked into the same location as your application source code. That way, branches and merges will be applied to your Datical DB project automatically. Use unique change set IDs to avoid collisions with other scrum teams.</p> 
<b>4. Set up database version control using AWS CodeCommit</b> 
<p>To create a new CodeCommit repository, <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/how-to-create-repository.html">follow these steps</a>.</p> 
<p><strong>Note</strong>: On some versions of Windows and Linux, you might see a pop-up dialog box asking for your user name and password. This is the built-in credential management system, but it is not compatible with the credential helper for AWS CodeCommit. Choose <strong>Cancel</strong>.</p> 
<p>Commit the contents located in the Datical working directory (for example, ~/datical/AWS-Sample-Project) to the AWS CodeCommit repository.</p> 
<b>5. Set up a continuous integration server to stage database changes</b> 
<p>In this example, Jenkins is the continuous integration server. To create a Jenkins server, <a href="https://aws.amazon.com/getting-started/projects/setup-jenkins-build-server/">follow these steps</a>. Be sure your instance security group allows port 8080 access.</p> 
<pre><code class="lang-basg">sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat/jenkins.repo</code></pre> 
<p>For more information about installing Jenkins, see the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Installing+Jenkins">Jenkins wiki</a>.</p> 
<p>After setup, connect to your Jenkins server, and create a job.</p> 
<ol> 
<li>Install the following Jenkins plugins:<br /> For this project, you will need to install the following Jenkins plugins:<p></p> 
<ol> 
<li>AWS CodeCommit plugin</li> 
<li>DaticalDB4Jenkins plugin</li> 
<li>Hudson Post Build Task plugin</li> 
<li>HTML Publisher plugin</li> 
</ol> </li> 
<li>To configure Jenkins for AWS CodeCommit, <a href="https://aws.amazon.com/blogs/devops/integrating-aws-codecommit-with-jenkins/">follow these steps</a>.</li> 
<li>To configure Jenkins with Datical DB, navigate to Jenkins, choose <strong>Manage Jenkins</strong>, and then choose <strong>Configure System</strong>. In the Datical DB section, provide the requested directory information.</li> 
</ol> 
<p><img class="alignnone wp-image-1184 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/5-1.png" alt="" width="804" height="176" /></p> 
<p>For example:</p> 
<p><img class="alignnone wp-image-1185 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/6-1.png" alt="" width="804" height="167" /></p> 
<b>Add a build step:</b> 
<p>Go to your newly created Jenkins project and choose <strong>Configure</strong>. On the <strong>Build</strong> tab, under <strong>Build</strong>, choose <strong>Add build step</strong>, and choose <strong>Datical DB</strong>.</p> 
<p><img class="alignnone wp-image-1182 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/7-1.png" alt="" width="386" height="301" /></p> 
<p>In <strong>Project Dir</strong>, enter the Datical DB project directory (in this example, /var/lib/jenkins/workspace/demo/MyProject). You can use Jenkins environment variables like $WORKSPACE. The first build action&nbsp;is <strong>Check Drivers</strong>. This allow you to verify that Datical DB and Jenkins are configured correctly.</p> 
<p><img class="alignnone wp-image-1187 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/8-1.png" alt="" width="599" height="255" /></p> 
<p>Choose <strong>Save</strong>. Choose <strong>Build Now</strong> to test the configuration.</p> 
<p>After you’ve verified the drivers are installed, add forecast and deploy steps.</p> 
<b>Add forecast and deploy steps:</b> 
<p><img class="alignnone wp-image-1189 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/9-1.png" alt="" width="635" height="421" /><br /> Choose <strong>Save</strong>. Then choose <strong>Build Now</strong> to test the configuration.</p> 
<b>6. Configure the continuous integration server to publish Datical DB reports</b> 
<p>In this step, we will configure Jenkins to publish Datical DB forecast and HTML reports. In your Jenkins project, select <strong>Delete workspace before build starts</strong>.</p> 
<p><img class="alignnone wp-image-1222 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/10-2.png" alt="" width="451" height="150" /></p> 
<b>Add post-build steps</b> 
<h3>1. Archive the Datical DB reports, logs, and snapshots</h3> 
<p><img class="alignnone size-full wp-image-1096" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/11.png" alt="Archive" width="975" height="177" /><br /> To expose Datical DB reports in Jenkins, you must create a post-build task step to copy the forecast and deployment HTML reports to a location easily published, and then publish the HTML reports.</p> 
<h4>2. Copy the forecast and deploy HTML reports</h4> 
<pre><code class="lang-bash">mkdir /var/lib/jenkins/workspace/Demo/MyProject/report
cp -rv /var/lib/jenkins/workspace/Demo/MyProject/Reports/*/*/*/forecast*/* /var/lib/jenkins/workspace/Demo/MyProject/report 2&gt;NUL
cp -rv /var/lib/jenkins/workspace/Demo/MyProject/Reports/*/*/*/deploy*/deployReport.html /var/lib/jenkins/workspace/Demo/MyProject/report 2&gt;NUL</code></pre> 
<h4><code class="lang-code"><img class="alignnone wp-image-1097 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/12.png" alt="Post build task" width="975" height="364" /><br /> </code></h4> 
<p>&nbsp;</p> 
<h4>3. Publish HTML reports</h4> 
<p>Use the information in the following screen shot. Depending on the location where you configured Jenkins to build, your details might be different.</p> 
<p><img class="alignnone wp-image-1191 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/13-1.png" alt="" width="751" height="711" /></p> 
<p><strong>Note</strong>: Datical DB HTML reports use CSS, so update the JENKINS_JAVA_OPTIONS in your config file as follows:</p> 
<p>Edit /etc/sysconfig/jenkins and set JENKINS_JAVA_OPTIONS to:</p> 
<pre><code class="lang-code">JENKINS_JAVA_OPTIONS=&quot;-Djava.awt.headless=true -Dhudson.model.DirectoryBrowserSupport.CSP= &quot;
</code></pre> 
<b>7. Enable automated release management for your database through AWS CodePipeline</b> 
<p>To create an automated release process for your databases using AWS CodePipeline, follow these instructions.</p> 
<ol> 
<li>Sign in to the AWS Management Console and open the AWS CodePipeline console at <a href="http://console.aws.amazon.com/codepipeline">http://console.aws.amazon.com/codepipeline</a>.</li> 
<li>On the introductory page, choose <strong>Get started</strong>. If you see the <strong>All pipelines</strong> page, choose <strong>Create pipeline</strong>.</li> 
<li>In <strong>Step 1: Name</strong>, in <strong>Pipeline name</strong>, type <strong>DatabasePipeline</strong>, and then choose <strong>Next step</strong>.</li> 
<li>In <strong>Step 2: Source</strong>, in <strong>Source provider</strong>, choose <strong>AWS CodeCommit</strong>. In <strong>Repository name</strong>, choose the name of the AWS CodeCommit repository you created earlier. In <strong>Branch name</strong>, choose the name of the branch that contains your latest code update. Choose <strong>Next step</strong>.<br /> <img class="alignnone wp-image-1194 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/14-1.png" alt="" width="538" height="347" /><br /> <img class="alignnone wp-image-1195 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/15-1.png" alt="" width="635" height="894" /></li> 
<li>In <strong>Step 3: Build</strong>, chose <strong>Jenkins</strong>.<br /> <img class="alignnone wp-image-1196 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/16-1.png" alt="" width="513" height="330" /><br /> <img class="alignnone wp-image-1197 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/17-1.png" alt="" width="626" height="989" /></li> 
</ol> 
<p>To complete the deployment workflow, follow steps 6 through 9 in the <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html#codecommit-create-pipeline">Create a Simple Pipeline Tutorial</a>.</p> 
<b>8. Enforce database standards and compliance with the Datical DB Rules Engine</b> 
<p>The Datical DB Dynamic Rules Engine automatically inspects the Virtual Database Model to make sure that proposed database code changes are safe and in compliance with your organization’s database standards. The Rules Engine also makes it easy to identify complex changes that warrant further review and empowers DBAs to efficiently focus only on the changes that require their attention. It also provides application developers with a self-service validation capability that uses the same automated build process established for the application. The consistent evaluation provided by the Dynamic Rules Engine removes uncertainty about what is acceptable and empowers application developers to write safe, compliant changes every time.</p> 
<p>Earlier, you created a Datical DB project with no changes. To demonstrate rules, you will now create changes that violate a rule.</p> 
<p>First, create a table with four columns. Then try to create an index on the table that comprises all four columns. For some databases, having more than three columns in an index can cause performance issues. For this reason, create a rule that will prevent the creation of an index on more than three columns, long before the change is proposed for production. Like a unit test that will fail the build, the Datical DB Rules Engine fails the build at the forecast step and provides feedback to the development team about the rule and the change to fix.</p> 
<b>Create a Datical DB rule</b> 
<p>To create a Datical DB rule, open the Datical DB UI and navigate to your project. Expand the Rules folder. In this example, you will create a rule in the Forecast folder.</p> 
<p>Right-click the Forecast folder, and then select <strong>Create Rules File</strong>. In the dialog box, type a unique file name for your rule. Use a .drl extension.</p> 
<p>.<img class="alignnone wp-image-1207 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/18-2.png" alt="" width="558" height="643" /></p> 
<p>In the editor window that opens, type&nbsp;the following:</p> 
<pre><code class="lang-java">package com.datical.hammer.core.forecast
import java.util.Collection;
import java.util.List;
import java.util.Arrays;
import java.util.ArrayList;
import org.apache.commons.lang.StringUtils;
import org.apache.commons.collections.ListUtils;
import com.datical.db.project.Project;
import com.datical.hammer.core.rules.Response;
import com.datical.hammer.core.rules.Response.ResponseType;
// ************************************* Models *************************************
// Database Models
import com.datical.dbsim.model.DbModel;
import com.datical.dbsim.model.Schema;
import com.datical.dbsim.model.Table;
import com.datical.dbsim.model.Index;
import com.datical.dbsim.model.Column;
import org.liquibase.xml.ns.dbchangelog.CreateIndexType;
import org.liquibase.xml.ns.dbchangelog.ColumnType;
/* @return false if validation fails; true otherwise */
function boolean validate(List columns)
{
// FAIL If more than 3 columns are included in new index
if (columns.size() &gt; 3)
return false;
else
return true;
}
rule &quot;Index Too Many Columns Error&quot;
salience 1
when
$createIndex : CreateIndexType($indexName: indexName, $columns: columns, $tableName: tableName, $schemaName: schemaName)
eval(!validate($columns))
then
String errorMessage = &quot;The new index [&quot; + $indexName + &quot;] contains more than 3 columns.&quot;;
insert(new Response(ResponseType.FAIL, errorMessage, drools.getRule().getName()));
end</code></pre> 
<p>Save the new rule file, and then right-click the Forecast folder, and select Check Rules. You should see “Rule Validation returned no errors.”</p> 
<p>Now check your rule into source code control and request a new build. The build will fail, which is expected. Go back to Datical DB, and change the index to comprise only three columns. After your check-in, you will see a successful deployment to your RDS instance.</p> 
<p>The following forecast report shows the Datical DB rule violation:</p> 
<p><img class="alignnone wp-image-1209 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/06/01/19-2.png" alt="" width="642" height="671" /></p> 
<p>To implement database continuous delivery into your existing continuous delivery process, consider creating a separate project for your database changes that use the Datical DB forecast functionality at the same time unit tests are run on your code. This will catch database changes that violate standards before deployment.</p> 
<b>Summary:</b> 
<p>In this post, you learned how to build a modern database continuous integration and automated release management workflow on AWS. You also saw how Datical DB can be seamlessly integrated with AWS services to enable database release automation, while eliminating risks that cause application downtime and data security vulnerabilities. This fully automated delivery mechanism for databases can accelerate every organization’s ability to deploy software rapidly and reliably while improving productivity, performance, compliance, and auditability, and increasing data security. These methodologies simplify process-related overhead and make it possible for organizations to serve their customers efficiently and compete more effectively in the market.</p> 
<p>I hope you enjoyed this post. If you have any feedback, please leave a comment below.</p> 
<hr /> 
<h3 style="text-align: left">About the Authors</h3> 
<p>&nbsp;</p> 
<p style="text-align: left"><img class="alignnone wp-image-1105 size-thumbnail" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/pic-150x150.jpg" alt="Balaji Iyer" width="150" height="150" /></p> 
<p style="text-align: left"><strong>Balaji Iyer is an Enterprise Consultant for the Professional Services Team&nbsp;at Amazon Web Services</strong>. In this role, he has helped several customers successfully navigate their journey to AWS. His specialties include architecting and implementing highly scalable distributed systems, serverless architectures, large scale migrations, operational&nbsp;security, and leading strategic AWS initiatives. Before he joined Amazon, Balaji spent more than&nbsp;a decade building operating systems, big data analytics solutions, mobile services, and web applications. In his spare time, he enjoys experiencing the great outdoors and spending time with his family.</p> 
<p style="text-align: left"><img class="alignnone size-thumbnail wp-image-1106" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/29/robert-reeves-150x150.jpg" alt="" width="150" height="150" /></p> 
<p style="text-align: left"><strong>Robert Reeves&nbsp;is a Co-Founder &amp;&nbsp;Chief Technology Officer at Datical</strong>. In this role, he advocates for Datical’s customers and provides technical architecture leadership. Prior to cofounding Datical, Robert was a Director at the Austin Technology Incubator. At ATI, he provided real-world entrepreneurial expertise to ATI member companies to aid in market validation, product development, and fundraising efforts. Robert cofounded Phurnace Software in 2005. He invented and created the flagship product, Phurnace Deliver, which provides middleware infrastructure management to multiple Fortune 500 companies. As Chief Technology Officer for Phurnace, he led technical evangelism efforts, product vision, and large account technical sales efforts. After BMC Software acquired Phurnace in 2009, Robert served as Chief Architect and lead worldwide technology evangelism.</p> 
<hr /> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/best-practices/" rel="tag">Best practices</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codecommit/" rel="tag">CodeCommit</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codepipeline/" rel="tag">CodePipeline</a>, <a href="https://aws.amazon.com/blogs/devops/tag/continuous-delivery/" rel="tag">Continuous Delivery</a>, <a href="https://aws.amazon.com/blogs/devops/tag/continuous-deployment/" rel="tag">Continuous Deployment</a>, <a href="https://aws.amazon.com/blogs/devops/tag/continuous-integration/" rel="tag">Continuous Integration</a>, <a href="https://aws.amazon.com/blogs/devops/tag/database/" rel="tag">Database</a>, <a href="https://aws.amazon.com/blogs/devops/tag/daticaldb/" rel="tag">DaticalDB</a>, <a href="https://aws.amazon.com/blogs/devops/tag/development/" rel="tag">Development</a>, <a href="https://aws.amazon.com/blogs/devops/tag/devops/" rel="tag">DevOps</a>, <a href="https://aws.amazon.com/blogs/devops/tag/jenkins/" rel="tag">Jenkins</a>, <a href="https://aws.amazon.com/blogs/devops/tag/partners/" rel="tag">Partners</a>, <a href="https://aws.amazon.com/blogs/devops/tag/release-management/" rel="tag">Release Management</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">How to Create an AMI Builder with AWS CodeBuild and HashiCorp Packer</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Jason Barto</span></span> | on 
<time property="datePublished" datetime="2017-06-01T04:05:35+00:00">01 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/how-to-create-an-ami-builder-with-aws-codebuild-and-hashicorp-packer/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>Written by AWS Solutions Architects Jason Barto and Heitor Lessa</strong></p> 
<p>It’s an operational and security best practice to create and maintain custom <a href="https://aws.amazon.com/answers/configuration-management/aws-ami-design/">Amazon Machine Images</a>. Because it’s also a best practice to maintain infrastructure as code, it makes sense to use automated tooling to script the creation and configuration of AMIs that are used to quickly launch <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances.</p> 
<p>In this first of two posts, we’ll use <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a> to programmatically create AMIs for use in our environment. As a part of the AMI generation, we will apply OS patches, configure a banner statement, and install some common software, forming a solid base for future Amazon EC2-based deployments.<br /> <span id="more-1046"></span></p> 
<b>Requirements</b> 
<p>You will need <a href="https://git-scm.com/downloads">Git</a> and a text editor.</p> 
<b>Technologies</b> 
<p><a href="https://aws.amazon.com/codebuild/details/">AWS CodeBuild</a> is a fully managed build service that enables customers to compile source code, run tests, and produce software packages that are ready to deploy. It is elastic, scalable, and provides curated build environments for programming languages such as Java, Ruby, Python, Go, and Node.js. With AWS CodeBuild, you don’t need to provision, manage, and scale your own build servers. For more information, see the <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html">AWS CodeBuild User Guide</a>.</p> 
<p><a href="https://www.packer.io/">HashiCorp Packer</a> is an open source utility designed to automate the creation of identical virtual machine images for multiple platforms from a single source configuration. For more information, see the <a href="https://www.packer.io/guides/index.html">HashiCorp Packer documentation</a>.</p> 
<b>Getting Started</b> 
<p>We need to host the AWS CodeBuild and HashiCorp Packer project somewhere. AWS CodeBuild can use <a href="https://aws.amazon.com/s3/">Amazon S3</a>, <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a>, or <a href="https://github.com/">GitHub</a> as source code repositories. For this tutorial, sign in to the AWS Management Console and create an AWS CodeCommit repository. If you have not used AWS CodeCommit before, we recommend that you start with the <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/getting-started.html">Git with AWS CodeCommit Tutorial</a>.</p> 
<b>Create an AWS CodeBuild Project in the Console</b> 
<p>Create an AWS CodeBuild project. It will be used later to build our HashiCorp Packer template.</p> 
<ol> 
<li>From the AWS Management Console, open the AWS CodeBuild console.</li> 
<li>If you have no AWS CodeBuild projects, choose <strong>Get Started</strong> or, on the <strong>Build Projects</strong> page, choose <strong>Create project</strong>.</li> 
<li>On the <strong>Create project</strong> page, type a name for your AWS CodeBuild project (for example, <strong>AMI_Builder</strong>).</li> 
<li>For <strong>Source provider</strong>, choose AWS CodeCommit. From the <strong>Repository</strong> drop-down list, select the repository you created in the Getting Started step.</li> 
</ol> 
<p>AWS CodeBuild uses containers to execute the project’s build instructions and build your project. You can specify custom container images hosted in <a href="https://aws.amazon.com/ecr/">Amazon EC2 Container Registry</a> or <a href="https://hub.docker.com/">DockerHub</a>, but this tutorial uses the default managed Ubuntu container.</p> 
<ol start="5"> 
<li>For <strong>Environment Image</strong>, select <strong>Use an image managed by AWS CodeBuild</strong>. For <strong>Operating system</strong>, choose <strong>Ubuntu</strong>. For <strong>Runtime</strong>, choose <strong>Base</strong>. For <strong>Version</strong>, choose <strong>aws/codebuild/ubuntu-base:14.04</strong>.</li> 
</ol> 
<p>The next section of the page tells AWS CodeBuild where to find commands that will be executed as a part of the build. For this project, the build commands are stored in the buildspec.yml file in your repository.</p> 
<ol start="6"> 
<li>For <strong>Build specification</strong>, accept the default.</li> 
</ol> 
<p><img class="aligncenter size-full wp-image-1051" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/24/configure_project.png" alt="CodeBuild project configuration" width="674" height="863" /></p> 
<p>The buildspec.yml file will use HashiCorp Packer to execute a template and generate an Amazon EC2 AMI. There is no binary output or build result that will be used as an artifact.</p> 
<ol start="7"> 
<li>For <strong>Artifacts type</strong>, choose <strong>No artifacts</strong>.</li> 
</ol> 
<p>In the Service Role section the service role you select here will provide the AWS CodeBuild container with permissions to your AWS account. HashiCorp Packer needs permissions to create a temporary EC2 instance and an AMI, delete an EC2 instance, and perform other EC2-related actions. .</p> 
<p>For the purposes of this post, allow AWS CodeBuild to create a service role for you. You will update it later with the permissions required by HashiCorp Packer.</p> 
<ol start="8"> 
<li>For <strong>Service Role</strong> choose the default of <strong>Create a service role in your account</strong>.</li> 
<li>Choose <strong>Continue</strong>.</li> 
<li>Review the settings on the next page, and then choose <strong>Save</strong>.</li> 
</ol> 
<b>Update Service Role Permissions</b> 
<p>You have now created an AWS CodeBuild project that is connected to an AWS CodeCommit repository and is ready to build our source code. Now we need to grant AWS CodeBuild the additional permissions it will need to create, delete, and image an EC2 instance.</p> 
<ol> 
<li>Open the IAM console and click the AWS CodeBuild service role, created in the last section, to display its summary page.</li> 
<li>On the summary page, under <strong>Permissions</strong>, expand <strong>Inline policies</strong>, and click the link to create a policy.</li> 
<li>Choose <strong>Custom Policy</strong>, and then choose <strong>Select</strong>.</li> 
<li>Copy and paste the IAM policy from the <a href="https://www.packer.io/docs/builders/amazon.html#using-an-iam-instance-profile">HashiCorp Packer documentation</a> into the text area. Type a name for the policy (for example, <strong>codebuild-AMI_Builder-ec2-permissions</strong>).</li> 
<li>Choose <strong>Validate Policy</strong>, and then choose <strong>Apply Policy</strong> to link the policy with the service role.</li> 
</ol> 
<p><img class="aligncenter size-full wp-image-1052" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/24/permissions.png" alt="CodeBuild project permissions" width="975" height="485" /></p> 
<p>AWS CodeBuild should now have the permissions required to create AMIs.</p> 
<b>Initial Commit</b> 
<p>The next step is to create the HashiCorp Packer template and build specification. Check out a copy of the Git repository and create two files:</p> 
<li>The HashiCorp Packer template, amazon-linux_packer-template.json.</li> 
<li>The AWS CodeBuild configuration file, buildspec.yml.</li> 
<b>Create a HashiCorp Packer Template</b> 
<p>A HashiCorp Packer template is a JSON-formatted document that provides HashiCorp Packer with information about how to build a machine image. A HashiCorp Packer template can be composed of many keys. In this post, the focus is on the builders and provisioners keys. For more information, see <a href="https://www.packer.io/docs/templates/index.html">Templates</a> on the HashiCorp Packer website.</p> 
<p>Using a text editor, create a HashiCorp Packer template named amazon-linux_packer-template.json that contains three sections: variables, builders, and provisioners.</p> 
<p>The variables section defines two variables, <code>aws_region</code> and <code>aws_ami_name</code>. These variables can be reused later in the template. They are defined by using an environment variable <code>{{env `AWS_REGION`}}</code> and an internal HashiCorp Packer function <code>{{isotime \&quot;02Jan2006\&quot;}}</code> . For more information about HashiCorp Packer functions, see <a href="https://www.packer.io/docs/templates/engine.html">Template Engine</a> on the HashiCorp Packer website.</p> 
<p>The builders section configures the amazon-ebs builder to deploy an instance into the same region in which AWS CodeBuild is running, using a t2.micro instance, and to connect to the instance with a username of ec2-user. The source_ami_filter is being used to find the latest version of Amazon Linux available for the target region. The selected source AMI will be the base for your custom AMI. After the EC2 instance has been provisioned, amazon-ebs will create an AMI using the value of the <code>aws_ami_name</code> variable as the AMI name.</p> 
<pre><code class="lang-json">{
&quot;variables&quot;: {
&quot;aws_region&quot;: &quot;{{env `AWS_REGION`}}&quot;,
&quot;aws_ami_name&quot;: &quot;amazon-linux_{{isotime \&quot;02Jan2006\&quot;}}&quot;
},
&quot;builders&quot;: [{
&quot;type&quot;: &quot;amazon-ebs&quot;,
&quot;region&quot;: &quot;{{user `aws_region`}}&quot;,
&quot;instance_type&quot;: &quot;t2.micro&quot;,
&quot;ssh_username&quot;: &quot;ec2-user&quot;,
&quot;ami_name&quot;: &quot;{{user `aws_ami_name`}}&quot;,
&quot;ami_description&quot;: &quot;Customized Amazon Linux&quot;,
&quot;associate_public_ip_address&quot;: &quot;true&quot;,
&quot;source_ami_filter&quot;: {
&quot;filters&quot;: {
&quot;virtualization-type&quot;: &quot;hvm&quot;,
&quot;name&quot;: &quot;amzn-ami*-ebs&quot;,
&quot;root-device-type&quot;: &quot;ebs&quot;
},
&quot;owners&quot;: [&quot;137112412989&quot;, &quot;591542846629&quot;, &quot;801119661308&quot;, &quot;102837901569&quot;, &quot;013907871322&quot;, &quot;206029621532&quot;, &quot;286198878708&quot;, &quot;443319210888&quot;],
&quot;most_recent&quot;: true
}
}],
&quot;provisioners&quot;: [
{
&quot;type&quot;: &quot;shell&quot;,
&quot;inline&quot;: [
&quot;sudo yum update -y&quot;,
&quot;sudo /usr/sbin/update-motd --disable&quot;,
&quot;echo 'No unauthorized access permitted' | sudo tee /etc/motd&quot;,
&quot;sudo rm /etc/issue&quot;,
&quot;sudo ln -s /etc/motd /etc/issue&quot;,
&quot;sudo yum install -y elinks screen&quot;
]
}
]
}
</code></pre> 
<p>The provisioners section provides shell commands that Packer will use to configure the virtual machine after it has been created by the builders, but before it is captured as a machine image. The provisioners section updates the package management system and cleans up temporary keys before exiting. For more information, see <a href="https://www.packer.io/docs/provisioners/index.html">Provisioners</a>&nbsp;on the HashiCorp Packer website.</p> 
<p>AWS CodeBuild and the containers it executes operate outside of a customer’s VPC. Any actions performed as a part of your build project will not have access to private IP addresses in your VPC. As a result a public IP address must be assigned to the EC2 instance so that HashiCorp Packer can remotely connect to it and configure the system. If the instance is not going to be launched in your default VPC, you must provide HashiCorp Packer with the VPC and a public-facing subnet. You will also need to be able to use SSH to connect to the EC2 instance from the internet.</p> 
<p>To protect against malicious behavior and prevent unnecessary access, during the short life of the EC2 instance, HashiCorp Packer will use a newly created key pair and security group to deploy the EC2 instance. They will be deleted after the AMI has been created.</p> 
<p>The source_ami_filter is an alternative to source_ami, which states which AMI should be used to create the EC2 instance. source_ami_filter will query the AMIs available in your region and select the one that best matches the filter criteria. In this example, source_ami_filter is configured to find the most recent AMI that matches the expression amzn-ami*-ebs. The filter has been restricted to match AMIs owned by a limited set of AWS accounts. The owners listed are accounts owned and managed by AWS.</p> 
<b>Create a Build Specification</b> 
<p>AWS CodeBuild uses the buildspec.yml file to execute user-defined commands at various phases of the build process. This file tells AWS CodeBuild how to use HashiCorp Packer to execute the template.</p> 
<p>The buildspec.yml must contain the following:</p> 
<pre><code class="lang-yaml">---
version: 0.2
phases:
pre_build:
commands:
- echo &quot;Installing HashiCorp Packer...&quot;
- curl -qL -o packer.zip https://releases.hashicorp.com/packer/0.12.3/packer_0.12.3_linux_amd64.zip &amp;&amp; unzip packer.zip
- echo &quot;Installing jq...&quot;
- curl -qL -o jq https://stedolan.github.io/jq/download/linux64/jq &amp;&amp; chmod +x ./jq
- echo &quot;Validating amazon-linux_packer-template.json&quot;
- ./packer validate amazon-linux_packer-template.json
build:
commands:
### HashiCorp Packer cannot currently obtain the AWS CodeBuild-assigned role and its credentials
### Manually capture and configure the AWS CLI to provide HashiCorp Packer with AWS credentials
### More info here: https://github.com/mitchellh/packer/issues/4279
- echo &quot;Configuring AWS credentials&quot;
- curl -qL -o aws_credentials.json http://169.254.170.2/$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI &gt; aws_credentials.json
- aws configure set region $AWS_REGION
- aws configure set aws_access_key_id `./jq -r '.AccessKeyId' aws_credentials.json`
- aws configure set aws_secret_access_key `./jq -r '.SecretAccessKey' aws_credentials.json`
- aws configure set aws_session_token `./jq -r '.Token' aws_credentials.json`
- echo &quot;Building HashiCorp Packer template, amazon-linux_packer-template.json&quot;
- ./packer build amazon-linux_packer-template.json
post_build:
commands:
- echo &quot;HashiCorp Packer build completed on `date`&quot;
</code></pre> 
<p>During pre_build, this build file configures the AWS CodeBuild container with the tools it will need to execute the Packer template. The first tool is HashiCorp Packer, which is available for download from the <a href="https://www.packer.io/downloads.html">HashiCorp website</a>. The second tool is the <a href="https://stedolan.github.io/jq/">jq</a> utitility, used to parse JSON files. The last command in this phase validates the HashiCorp Packer template to ensure there are no syntax errors.</p> 
<p>In the build phase, the build file configures the build container’s AWS credentials using the metadata URL. This metadata will provide the AWS credentials provided by the IAM role assigned to the AWS CodeBuild project earlier to HashiCorp Packer so that it can create EC2 resources on your behalf. As a final step the HashiCorp Packer template is executed, resulting in the building of an EC2 AMI.</p> 
<b>Execute the AWS CodeBuild Project</b> 
<p>Commit the new files to your repository and push them to AWS CodeCommit. You are now ready to have AWS CodeBuild create the AMI.</p> 
<ol> 
<li>From the AWS Management Console, navigate to the AWS CodeBuild console.</li> 
<li>In the list of build projects, choose the project you created, and then choose <strong>Start build</strong>.</li> 
<li>In <strong>Start new build</strong>, choose which branch and revision of your AWS CodeCommit repository should be used to build your AMI.</li> 
<li>From the <strong>Branch</strong> drop-down list, choose <strong>master</strong>. This should populate the <strong>Source version</strong> field with the latest commit you pushed to the repository.</li> 
<li>Choose <strong>Start build</strong>. AWS CodeBuild will execute the buildspec.yml file in your repository.</li> 
</ol> 
<p><img class="aligncenter size-full wp-image-1053" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/24/build_start.png" alt="CodeBuild project start" width="726" height="480" /></p> 
<p>You will be taken to a page that provides status and results for each phase of the build. <strong>Succeeded</strong> should be displayed for every stage. If an error occurred, you can review the build logs at the bottom of the page for any error messages.</p> 
<p><img class="aligncenter size-full wp-image-1054" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/24/build_status.png" alt="CodeBuild project status" width="752" height="337" /></p> 
<p>During the build, HashiCorp Packer will generate a temporary key pair, launch an EC2 instance, remotely connect to the instance using the generated key pair, and then provision the machine, before converting the instance to an AMI and tearing down everything that was created to build the AMI. After these steps are complete, the build log should contain output that shows an AMI with an ID of something like <strong>ami-1a2b3cde</strong> was created.</p> 
<p>This AMI can now be used to create EC2 instances on demand or as part of an Auto Scaling group to elastically scale your infrastructure.</p> 
<b>Next Steps</b> 
<p>We hope you found the information in this first post helpful and will use it as a starting point for your own infrastructure as code pipeline. In this post, we created a portion of your infrastructure as code in the form of a HashiCorp Packer template. We used AWS CodeBuild to create an AMI based on the template. Your next steps could include:</p> 
<li>Building more than one HashiCorp Packer template. Multiple HashiCorp Packer templates can be used to build custom AMIs based on different source AMIs, perhaps one for Ubuntu and another for Microsoft Windows.</li> 
<li>Adding AWS CodePipeline to monitor your AWS CodeCommit repository. When a new version is committed, AWS CodePipeline will call AWS CodeBuild and re-create your AMIs automatically.</li> 
<li>Using CloudWatch Events to implement automated compliance. You can use an <a href="https://aws.amazon.com/lambda/?sc_channel=PS&amp;sc_campaign=acquisition_US&amp;sc_publisher=google&amp;sc_medium=lambda_b&amp;sc_content=lambda_e&amp;sc_detail=aws%20lambda&amp;sc_category=lambda&amp;sc_segment=186623768554&amp;sc_matchtype=e&amp;sc_country=US&amp;s_kwcid=AL!4422!3!186623768554!e!!g!!aws%20lambda&amp;ef_id=WFGCgwAABYUoeNPB:20170501191444:s">AWS Lambda</a> function to verify that all EC2 instances launched in your account are using one of your preconfigured custom AMIs.</li> 
<p>For next steps and taking advantage of other more advanced features and services please see <a href="https://aws.amazon.com/blogs/devops/how-to-create-an-ami-builder-with-aws-codebuild-and-hashicorp-packer-part-2/" title="How to Create an AMI Builder with AWS CodeBuild and HashiCorp Packer – Part 2" target="null">part 2</a> in this two-part series. The post uses AWS services such as AWS CodePipeline, AWS CloudFormation, and Amazon Cloudwatch Events to continuously release new AMIs from end to end.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/amazon-machine-images/" rel="tag">Amazon Machine Images</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codebuild/" rel="tag">codebuild</a>, <a href="https://aws.amazon.com/blogs/devops/tag/how-to/" rel="tag">How-to</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building a Secure Cross-Account Continuous Delivery Pipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Anuj Sharma</span></span> | on 
<time property="datePublished" datetime="2017-05-16T08:58:11+00:00">16 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/aws-building-a-secure-cross-account-continuous-delivery-pipeline/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Most organizations create multiple AWS accounts because they provide the highest level of resource and security isolation. In this blog post, I will discuss how to use cross account <a title="undefined" href="https://aws.amazon.com/iam/" target="null">AWS Identity and Access Management (IAM)</a> access to orchestrate continuous integration and continuous deployment.</p> 
<b>Do I need multiple accounts?</b> 
<p>If you answer “yes” to any of the following questions you should consider creating more AWS accounts:</p> 
<li>Does your business require administrative isolation between workloads? Administrative isolation by account is the most straightforward way to grant independent administrative groups different levels of administrative control over AWS resources based on workload, development lifecycle, business unit (BU), or data sensitivity.</li> 
<li>Does your business require limited visibility and discoverability of workloads? Accounts provide a natural boundary for visibility and discoverability. Workloads cannot be accessed or viewed unless an administrator of the account enables access to users managed in another account.</li> 
<li>Does your business require isolation to minimize blast radius? Separate accounts help define boundaries and provide natural blast-radius isolation to limit the impact of a critical event such as a security breach, an unavailable <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">AWS Region or Availability Zone</a>, account suspensions, and so on.</li> 
<li>Does your business require a particular workload to operate within <a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html">AWS service limits</a> without impacting the limits of another workload? You can use AWS account service limits to impose restrictions on a business unit, development team, or project. For example, if you create an AWS account for a project group, you can limit the number of Amazon Elastic Compute Cloud (<a href="https://aws.amazon.com/ec2/">Amazon EC2</a>) or high performance computing (HPC) instances that can be launched by the account.</li> 
<li>Does your business require strong isolation of recovery or auditing data? If regulatory requirements require you to control access and visibility to auditing data, you can isolate the data in an account separate from the one where you run your workloads (for example, by writing <a href="https://aws.amazon.com/cloudtrail/?sc_channel=PS&amp;sc_campaign=acquisition_US&amp;sc_publisher=google&amp;sc_medium=cloudtrail_b_test_q32016&amp;sc_content=cloudtrail_bmm&amp;sc_detail=%2Bcloudtrail&amp;sc_category=cloudtrial&amp;sc_segment=105093175122&amp;sc_matchtype=b&amp;sc_country=US&amp;s_kwcid=AL!4422!3!105093175122!b!!g!!%2Bcloudtrail&amp;ef_id=WFGCgwAABYUoeNPB:20170502153559:s">AWS CloudTrail</a> logs to a different account).</li> 
<li>Do your workloads depend on specific instance reservations to support high availability (HA) or disaster recovery (DR) capacity requirements? Reserved Instances (RIs) ensure reserved capacity for services such as Amazon EC2 and Amazon Relational Database Service (<a href="https://aws.amazon.com/rds/">Amazon RDS</a>) at the individual account level.</li> 
<b>Use case</b> 
<p>The identities in this use case are set up as follows:</p> 
<li><strong>DevAccount </strong></li> 
<p>Developers check the code into an <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a> repository. It stores all the repositories as a single source of truth for application code. Developers have full control over this account. This account is usually used as a sandbox for developers.</p> 
<li><strong>ToolsAccount</strong></li> 
<p>A central location for all the tools related to the organization, including continuous delivery/deployment services such as <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> and <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a>. Developers have limited/read-only access in this account. The Operations team has more control.</p> 
<li><strong>TestAccount</strong></li> 
<p>Applications using the CI/CD orchestration for test purposes are deployed from this account. Developers and the Operations team have limited/read-only access in this account.</p> 
<li><strong>ProdAccount</strong></li> 
<p>Applications using the CI/CD orchestration tested in the ToolsAccount are deployed to production from this account. Developers and the Operations team have limited/read-only access in this account.</p> 
<b>Solution</b> 
<p>In this solution, we will check in sample code for an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function in the Dev account. This will trigger the pipeline (created in AWS CodePipeline) and run the build using AWS CodeBuild in the Tools account. The pipeline will then deploy the Lambda function to the Test and Prod accounts.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/CrossAccount-CI-CD.jpg"><img class="aligncenter wp-image-1011 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/CrossAccount-CI-CD.jpg" alt="" width="723" height="463" /></a></p> 
<p>&nbsp;</p> 
<b>Setup</b> 
<ol> 
<li>Clone this repository. It contains the <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation templates </a>that we will use in this walkthrough.</li> 
</ol> 
<pre><code class="lang-bash">git clone https://github.com/awslabs/aws-refarch-cross-account-pipeline.git
</code></pre> 
<ol start="2"> 
<li>Follow the instructions in the repository README to push the sample AWS Lambda application to an AWS CodeCommit repository in the Dev account.</li> 
<li>Install the AWS Command Line Interface as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">here</a>. To prepare your access keys or assume-role to make calls to AWS, configure the AWS CLI as described <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">here</a>.</li> 
</ol> 
<b></b> 
<b>Walkthrough</b> 
<p><strong>Note:</strong> Follow the steps in the order they’re written. Otherwise, the resources might not be created correctly.</p> 
<ol> 
<li>In the Tools account, deploy this CloudFormation template. It will create <a href="http://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys">the customer master keys</a> (CMK) in <a href="http://docs.aws.amazon.com/kms/latest/developerguide/overview.html">AWS Key Management Service</a> (AWS KMS), grant access to Dev, Test, and Prod accounts to use these keys, and create an <a href="https://aws.amazon.com/s3/">Amazon S3</a> bucket to hold artifacts from AWS CodePipeline.</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --stack-name pre-reqs \
--template-file ToolsAcct/pre-reqs.yaml --parameter-overrides \
DevAccount=ENTER_DEV_ACCT TestAccount=ENTER_TEST_ACCT \
ProductionAccount=ENTER_PROD_ACCT</code></pre> 
<p>In the output section of the CloudFormation console, make a note of the <a href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">Amazon Resource Number</a> (ARN) of the CMK and the S3 bucket name. You will need them in the next step.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/10/Screen-Shot-2017-04-18-at-5.36.32-PM.png"><img class="aligncenter size-full wp-image-1028" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/10/Screen-Shot-2017-04-18-at-5.36.32-PM.png" alt="" width="2070" height="534" /></a></p> 
<ol start="2"> 
<li>In the Dev account, which hosts the AWS CodeCommit repository, deploy this CloudFormation template. This template will create the IAM roles, which will later be assumed by the pipeline running in the Tools account. Enter the AWS account number for the Tools account and the CMK ARN.</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --stack-name toolsacct-codepipeline-role \
--template-file DevAccount/toolsacct-codepipeline-codecommit.yaml \
--capabilities CAPABILITY_NAMED_IAM \
--parameter-overrides ToolsAccount=ENTER_TOOLS_ACCT CMKARN=FROM_1st_Step</code></pre> 
<ol start="3"> 
<li>In the Test and Prod accounts where you will deploy the Lambda code, execute this CloudFormation template. This template creates IAM roles, which will later be assumed by the pipeline to create, deploy, and update the sample AWS Lambda function through CloudFormation.</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --stack-name toolsacct-codepipeline-cloudformation-role \
--template-file TestAccount/toolsacct-codepipeline-cloudformation-deployer.yaml \
--capabilities CAPABILITY_NAMED_IAM \
--parameter-overrides ToolsAccount=ENTER_TOOLS_ACCT CMKARN=FROM_1st_STEP  \
S3Bucket=FROM_1st_STEP</code></pre> 
<ol start="4"> 
<li>In the Tools account, which hosts AWS CodePipeline, execute this CloudFormation template. This creates a pipeline, but does not add permissions for the cross accounts (Dev, Test, and Prod).</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --stack-name sample-lambda-pipeline \
--template-file ToolsAcct/code-pipeline.yaml \
--parameter-overrides DevAccount=ENTER_DEV_ACCT TestAccount=ENTER_TEST_ACCT \
ProductionAccount=ENTER_PROD_ACCT CMKARN=FROM_1st_STEP \
S3Bucket=FROM_1st_STEP--capabilities CAPABILITY_NAMED_IAM</code></pre> 
<ol start="5"> 
<li>In the Tools account, execute this CloudFormation template, which give access to the role created in step 4. This role will be assumed by AWS CodeBuild to decrypt artifacts in the S3 bucket. This is the same template that was used in step 1, but with different parameters.</li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --stack-name pre-reqs \
--template-file ToolsAcct/pre-reqs.yaml \
--parameter-overrides CodeBuildCondition=true</code></pre> 
<ol start="6"> 
<li>In the Tools account, execute this CloudFormation template, which will do the following: 
<ol start="a"> 
<li>Add the IAM role created in step 2. This role is used by AWS CodePipeline in the Tools account for checking out code from the AWS CodeCommit repository in the Dev account.</li> 
<li>Add the IAM role created in step 3. This role is used by AWS CodePipeline in the Tools account for deploying the code package to the Test and Prod accounts.</li> 
</ol> </li> 
</ol> 
<pre><code class="lang-bash">aws cloudformation deploy --stack-name sample-lambda-pipeline \
--template-file ToolsAcct/code-pipeline.yaml \
--parameter-overrides CrossAccountCondition=true \
--capabilities CAPABILITY_NAMED_IAM</code></pre> 
<b>What did we just do?</b> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/details-cross-account-pipeline.png"><img class="aligncenter wp-image-1011 size-full" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/details-cross-account-pipeline.png" alt="" width="723" height="463" /></a></p> 
<ol> 
<li>The pipeline created in step 4 and updated in step 6 checks out code from the AWS CodeCommit repository. It uses the IAM role created in step 2. The IAM role created in step 4 has permissions to assume the role created in step 2. This role will be assumed by AWS CodeBuild to decrypt artifacts in the S3 bucket, as described in step 5.</li> 
<li>The IAM role created in step 2 has permission to check out code. See <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/DevAccount/toolsacct-codepipeline-codecommit.yaml#L35">here</a>.</li> 
<li>The IAM role created in step 2 also has permission to upload the checked-out code to the S3 bucket created in step 1. It uses the KMS keys created in step 1 for server-side encryption.</li> 
<li>Upon successfully checking out the code, AWS CodePipeline triggers AWS CodeBuild. The AWS CodeBuild project created in step 4 is configured to use the CMK created in step 1 for cryptography operations. See <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/ToolsAcct/code-pipeline.yaml#L94">here</a>. The AWS CodeBuild role is created later in step 4. In step 5, access is granted to the AWS CodeBuild role to allow the use of the CMK for cryptography.</li> 
<li>AWS CodeBuild uses <a href="https://pypi.python.org/pypi/pip">pip</a> to install any libraries for the sample Lambda function. It also executes the <a href="http://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html">aws cloudformation package</a> command to create a Lambda function deployment package, uploads the package to the specified S3 bucket, and adds a reference to the uploaded package to the CloudFormation template. See <a href="https://github.com/awslabs/aws-refarch-cross-account-pipeline/blob/master/ToolsAcct/code-pipeline.yaml#L118">here</a>.</li> 
<li>Using the role created in step 3, AWS CodePipeline executes the transformed CloudFormation template (received as an output from AWS CodeBuild) in the Test account. The AWS CodePipeline role created in step 4 has permissions to assume the IAM role created in step 3, as described in step 5.</li> 
<li>The IAM role assumed by AWS CodePipeline passes the role to an IAM role that can be assumed by CloudFormation. AWS CloudFormation creates and updates the Lambda function using the code that was built and uploaded by AWS CodeBuild.</li> 
</ol> 
<p>This is what the pipeline looks like using the sample code:<br /> <a href="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/Screen-Shot-2017-04-27-at-10.56.56-AM.png"><img class="aligncenter wp-image-1011 size-medium" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/08/Screen-Shot-2017-04-27-at-10.56.56-AM.png" /></a></p> 
<b>Conclusion</b> 
<p>Creating multiple AWS accounts provides the highest degree of isolation and is appropriate for a number of use cases. However, keeping a centralized account to orchestrate continuous delivery and deployment using AWS CodePipeline and AWS CodeBuild eliminates the need to duplicate the delivery pipeline. You can secure the pipeline through the use of cross account IAM roles and the encryption of artifacts using AWS KMS. For more information, see <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html">Providing Access to an IAM User in Another AWS Account That You Own in the IAM User Guide</a>.</p> 
<b>References</b> 
<li><a href="https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Security_Strategy.pdf">AWS Multiple Account Security Strategy</a></li> 
<li><a href="https://d0.awsstatic.com/aws-answers/AWS_Multi_Account_Billing_Strategy.pdf">AWS Multiple Account Billing Strategy</a></li> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/cloudformation/" rel="tag">CloudFormation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codebuild/" rel="tag">codebuild</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codecommit/" rel="tag">CodeCommit</a>, <a href="https://aws.amazon.com/blogs/devops/tag/codepipeline/" rel="tag">CodePipeline</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Use AWS CloudFormation to Automate the Creation of an S3 Bucket with Cross-Region Replication Enabled</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Rajakumar Sampathkumar</span></span> | on 
<time property="datePublished" datetime="2017-05-15T13:16:10+00:00">15 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/use-aws-cloudformation-to-automate-the-creation-of-an-s3-bucket-with-cross-region-replication-enabled/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-938" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=938&amp;disqus_title=Use+AWS+CloudFormation+to+Automate+the+Creation+of+an+S3+Bucket+with+Cross-Region+Replication+Enabled&amp;disqus_url=https://aws.amazon.com/blogs/devops/use-aws-cloudformation-to-automate-the-creation-of-an-s3-bucket-with-cross-region-replication-enabled/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-938');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>At the request of many of our customers, in this blog post, we will discuss how to use <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> to create an <a href="https://aws.amazon.com/s3/">S3</a> bucket with <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html">cross-region replication</a> enabled. We’ve included a CloudFormation template with this post that uses an <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>-backed custom resource to create source and destination buckets.</p> 
<p><strong></strong></p> 
<strong> <h3>What is S3 cross-region replication?</h3> </strong> 
<p><strong></strong></p> 
<p>Cross-region replication is a bucket-level feature that enables automatic, asynchronous copying of objects across buckets in different AWS regions. You can create two buckets in two different regions and use the <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html#cfn-s3-bucket-replicationconfiguration">ReplicationConfiguration</a> property to replicate the objects from one bucket to the other. For example, you can have a bucket in us-east-1 and replicate the bucket objects to a bucket in us-west-2.</p> 
<p>For more information, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr-what-is-isnot-replicated.html">What Is and Is Not Replicated in Cross-Region Replication</a>.</p> 
<p><strong></strong></p> 
<strong> <h3>Challenge</h3> </strong> 
<p><strong></strong></p> 
<p>When you enable cross-region replication, the replicated objects will be stored in only one destination (an S3 bucket). The destination bucket must already exist and it must be in an AWS region different from your source bucket.</p> 
<p>Using CloudFormation, you cannot create the destination bucket in a region different from the region in which you are creating your stack. To create the destination bucket, you can:</p> 
<li>Use another CloudFormation template.</li> 
<li>Use <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">AWS Lambda-backed custom resources</a> in the same template.</li> 
<p><strong></strong></p> 
<strong> <h3>Solution overview</h3> </strong> 
<p><strong></strong></p> 
<p>The CloudFormation template provided with this post uses an AWS Lambda-backed custom resource to create an S3 destination bucket in one region and a source S3 bucket in the same region as the CloudFormation endpoint.</p> 
<p><strong>Note</strong>: In this scenario, CloudFormation is not aware of the destination bucket created by AWS Lambda. For this reason, CloudFormation will not delete this resource when the stack is deleted.</p> 
<p><span id="more-938"></span></p> 
<p><strong></strong></p> 
<strong> <h3>How does it work?</h3> </strong> 
<p><strong></strong></p> 
<p>Launch the stack and provide the following custom values to the CloudFormation template. These (user input) values will be passed as <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html">parameters</a> to the template.</p> 
<li>OriginalBucketName</li> 
<li>ReplicationBucketName</li> 
<li>ReplicationRegion (different from the source region from which you are launching the stack)</li> 
<p>After the parameters are received by the template, the CloudFormation stack creates these IAM roles:</p> 
<li>A Lambda execution role with access to Amazon CloudWatch Logs, Amazon EC2, and Amazon S3</li> 
<li>An S3 role with AmazonS3FullAccess</li> 
<p>The AWS Lambda function is created after the roles are created. Lambda triggers the creation of the S3 destination bucket in the region specified in the CloudFormation template. Versioning is enabled on the bucket.</p> 
<p>When the destination bucket is available, CloudFormation initiates the creation of the source bucket with cross-region replication enabled. The destination bucket is the target for cross-region replication.</p> 
<p><strong>Note</strong>: The creation of the IAM role and Lambda function is automated in the template. You do not need not create them manually.</p> 
<p><img class="aligncenter wp-image-208" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/03/AWS-Blogpost-Create-S3-bucket-with-CRR-using-cloud-formation-v2-952x1024.jpg" alt="" width="482" height="518" /></p> 
<p><strong></strong></p> 
<strong> <h3>Automated deployment</h3> </strong> 
<p><strong></strong></p> 
<p>The step-by-step instructions in this section show you how you can automate the creation of an S3 bucket with cross-region replication enabled. After you click the button, the bucket will be created in approximately two minutes.</p> 
<p><strong>Note</strong>: Running this solution may result in charges to your AWS account. These include possible charges for Amazon S3 and AWS Lambda.</p> 
<p>1. Sign in to the AWS Management Console and open the AWS CloudFormation console. Choose the <strong>Launch Stack</strong> button to create the AWS CloudFormation stack (S3CrossRegionReplication).</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=S3CrossRegionReplication&amp;templateURL=https://s3.amazonaws.com/blog-s3-crr-automate/cfn-s3-x-region-replication_final.yml"><img class="aligncenter wp-image-99" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/03/30/Launch-Stack.png" alt="" width="134" height="25" /></a></p> 
<p><em>The template will be loaded from an S3 bucket automatically.</em></p> 
<p><img class="aligncenter size-large wp-image-950" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_1-1024x597.png" alt="" width="640" height="373" /></p> 
<p>2. On the <strong>Specify details</strong> page, change the stack name, if required. Provide the following custom values to the CloudFormation template. These (user input) values will be passed as parameters to the template.</p> 
<li>OriginalBucketName</li> 
<li>ReplicationBucketName</li> 
<li>ReplicationRegion</li> 
<p><img class="aligncenter size-large wp-image-949" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_2-1024x584.png" alt="" width="640" height="365" /></p> 
<p>Choose <strong>Next</strong>.</p> 
<p>3. On the <strong>Options</strong> page, you can specify tags for your AWS CloudFormation template, if you like, and then choose <strong>Next</strong>.</p> 
<p>Permissions are built in the template. You don’t have to choose an IAM role.</p> 
<p><img class="aligncenter size-large wp-image-948" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_3-1024x674.png" alt="" width="640" height="421" /></p> 
<p>Choose <strong>Next</strong>.</p> 
<p>4. On the <strong>Review</strong> page, review your template details. Select the acknowledgement check box, and then choose <strong>Create</strong> to create the stack.</p> 
<p><img class="aligncenter size-large wp-image-947" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/S3CRR_Create_A_New_Stack_4-1024x749.png" alt="" width="640" height="468" /></p> 
<p>You can also <a href="https://s3.amazonaws.com/blog-s3-crr-automate/cfn-s3-x-region-replication_final.yml">download the template</a> and use it as a starting point for your own implementation. The template is launched in the US East (N. Virginia) region by default. To launch the CloudFormation stack in a different AWS region, use the region selector in the console navigation bar after you click <strong>Launch stack</strong>.</p> 
<p><strong>Note</strong>: Because this solution uses AWS Lambda, which is currently available in selected regions only, be sure you launch this solution in an AWS region where Lambda is available. For more information, see <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS service offerings by region</a>.</p> 
<p><strong></strong></p> 
<strong> <h3>Conclusion</h3> </strong> 
<p><strong></strong></p> 
<p>In this blog post, we showed you how to use a single AWS CloudFormation template and AWS Lambda-backed custom resources to create an S3 bucket with cross-region replication enabled.<br /> &nbsp;<br /> <em>I would like to thank my colleague Arun Tunuri for his contributions in designing the CloudFormation template.</em></p> 
<p>&nbsp;</p> 
<hr /> 
<p><strong></strong></p> 
<strong> <h3>About the author</h3> </strong> 
<p><strong></strong></p> 
<p><img class="alignleft size-full wp-image-970" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/05/02/d695a2abcda41fc682c5ef06305d8d21.jpeg" alt="" width="80" height="80" /><strong>Rajakumar Sampathkumar</strong> is a Senior Technical Account Manager for Amazon Web Services. In his spare time, he is a passionate author and likes to spend quality time with his kids and nature.</p> 
<p>&nbsp;<br /> &nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/devops/tag/automation/" rel="tag">Automation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/cloudformation/" rel="tag">CloudFormation</a>, <a href="https://aws.amazon.com/blogs/devops/tag/how-to/" rel="tag">How-to</a>, <a href="https://aws.amazon.com/blogs/devops/tag/s3/" rel="tag">S3</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-938');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
