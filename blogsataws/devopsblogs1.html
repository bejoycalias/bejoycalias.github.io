<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/devopsblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="aws-blogs" class="layout-inner aws-blogs">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li class="active"><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="devopsblogs1.html">Page 1</a>|<a href="devopsblogs2.html">Page 2</a>|<a href="devopsblogs3.html">Page 3</a>|<a href="devopsblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Replicate AWS CodeCommit Repositories between Regions using AWS Fargate</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>Thanks to Raja Mani, AWS Solutions Architect, for this great blog.</p> 
<p>—</p> 
<p>In this blog post, I’ll walk you through the steps for setting up continuous replication of an <a href="https://aws.amazon.com/codecommit">AWS CodeCommit</a> repository from one AWS region to another AWS region using a serverless architecture. CodeCommit is a fully-managed, highly scalable source control service that stores anything from source code to binaries. It works seamlessly with your existing Git tools and eliminates the need to operate your own source control system. Replicating an AWS CodeCommit repository from one AWS region to another AWS region enables you to achieve lower latency pulls for global developers. This same approach can also be used to automatically back up repositories currently hosted on other services (for example, GitHub or BitBucket) to AWS CodeCommit.</p> 
<p>This solution uses <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> and <a href="https://aws.amazon.com/fargate/">AWS Fargate</a> for continuous replication. Benefits of this approach include:</p> 
<li>The replication process can be easily setup to trigger based on events, such as commits made to the repository.</li> 
<li>Setting up a serverless architecture means you don’t need to provision, maintain, or administer servers.</li> 
<li>You can incorporate this solution into your own DevOps pipeline. For more information, see the blog <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html">Invoke an AWS Lambda function in a pipeline in AWS CodePipeline</a>.</li> 
<p><strong>Note:</strong> AWS Fargate has a limitation of 10 GB for storage and is available in US East (N. Virginia) region. A similar solution that uses Amazon EC2 instances to replicate the repositories on a schedule was <a href="https://aws.amazon.com/blogs/devops/replicating-and-automating-sync-ups-for-a-repository-with-aws-codecommit/">published in a previous blog</a> and can be used if your repository does not meet these conditions.</p> 
<b>Replication using Fargate</b> 
<p>As you follow this blog post, you’ll set up an architecture that looks like this:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/04/03/codecommit-fargate1.png" /></p> 
<p>Any change in the AWS CodeCommit repository will trigger a Lambda function. The Lambda function will call the Fargate task that replicates the repository using a Git command line tool.</p> 
<p>Let us assume a user wants to replicate a repository (Source) from US East (N. Virginia/us-east-1) region to a repository (Destination) in US West (Oregon/us-west-2) region. I’ll walk you through the steps for it:</p> 
<h3>Prerequisites</h3> 
<p>Create an AWS Service IAM role for Amazon EC2 that has permission for both source and destination repositories, IAM CreateRole, AttachRolePolicy and Amazon ECR privileges. Here is the EC2 role policy I used:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Sid&quot;: &quot; &quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;codecommit:*&quot;,
&quot;ecr:*&quot;,
&quot;iam:CreateRole&quot;,
&quot;iam:AttachRolePolicy&quot;
],
&quot;Resource&quot;: &quot;*&quot;
}
]
}</code> 
<li>You need a Docker environment to build this solution. You can launch an EC2 instance and install Docker (or) you can use <a href="https://aws.amazon.com/cloud9/">AWS Cloud9</a> that comes with Docker and Git preinstalled. I used an <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-basics.html">EC2 instance and installed Docker</a> in it. Use the IAM role created in the previous step when creating the EC2 instance. I am going to refer this environment as “Docker Environment” in the following steps.</li> 
<li>You need to install the AWS CLI on the Docker environment. For AWS CLI installation, refer this <a href="https://docs.aws.amazon.com/cli/latest/userguide/installing.html">page</a>.</li> 
<li>You need to install Git, including a Git command line on the Docker environment.</li> 
<h3>Step 1: Create the Docker image</h3> 
<p>To create the Docker image, first it needs a Dockerfile. A Dockerfile is a manifest that describes the base image to use for your Docker image and what you want installed and running on it. For more information about Dockerfiles, go to the <a href="https://docs.docker.com/engine/reference/builder/">Dockerfile Reference</a>.</p> 
<p>1. Choose a directory in the Docker environment and perform the following steps in that directory. I used /home/ec2-user directory to perform the following steps.</p> 
<p>2. Clone the AWS CodeCommit repository in the Docker environment. Open the terminal to the Docker environment and run the following commands to <a href="https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-connect.html">clone your source AWS CodeCommit repository</a> (I ran the commands from /home/ec2-user directory):</p> 
<code class="lang-bash">$ git config --global credential.helper '!aws codecommit credential-helper $@'
$ git config --global credential.UseHttpPath true
$ git clone --mirror <span style="color: #ff0000">https://git-codecommit.us-east-1.amazonaws.com/v1/repos/Source</span> LocalRepository
$ cd LocalRepository
$ git remote set-url --push origin <span style="color: #ff0000">https://git-codecommit.us-east2.amazonaws.com/v1/repos/Destination</span></code> 
<p><strong>Note:</strong> Change the URL marked in red to your source and destination repository URL.</p> 
<p>3. Create a file called Dockerfile (case sensitive) with the following content (I created it in /home/ec2-user directory):</p> 
<code class="lang-yaml"># Pull the Amazon Linux latest base image
FROM amazonlinux:latest
#Install aws-cli and git command line tools
RUN yum -y install unzip aws-cli
RUN yum -y install git
WORKDIR /home/ec2-user
RUN mkdir LocalRepository
WORKDIR /home/ec2-user/LocalRepository
#Copy Cloned CodeCommit repository to Docker container
COPY ./LocalRepository /home/ec2-user/LocalRepository
#Copy shell script that does the replication
COPY ./repl_repository.bash /home/ec2-user/LocalRepository
RUN chmod ugo+rwx /home/ec2-user/LocalRepository/repl_repository.bash
WORKDIR /home/ec2-user/LocalRepository
#Call this script when Docker starts the container 
ENTRYPOINT [&quot;/home/ec2-user/LocalRepository/repl_repository.bash&quot;]</code> 
<p>4. Copy the following shell script into a file called repl_repository.bash to the DockerFile directory location in the Docker environment (I created it in /home/ec2-user directory)</p> 
<code class="lang-bash">#!/bin/bash
git config --global credential.helper '!aws codecommit credential-helper $@'
git config --global credential.UseHttpPath true
git fetch -p origin
git push --mirror
</code> 
<p>5. Your directory structure will look like this after completing the above steps:</p> 
<code class="lang-bash">-rw-r--r--   1      0    Apr  3 13:21 Dockerfile
drwxr-xr-x   2     68    Apr  3 13:21 LocalRepository
-rw-r--r--   1      0    Apr  3 13:21 repl_repository.bash</code> 
<p>6. Verify whether the replication is working by running the repl_repository.bash script from the LocalRepository directory. Go to LocalRepository directory and run this command: . ../repl_repository.bash<br /> If it is successful, you will get the “Everything up-to-date” at the last line of the result like this:</p> 
<code class="lang-bash">$ . ../repl_repository.bash
Everything up-to-date </code> 
<h3>Step 2: Build the Docker Image</h3> 
<p>1. Build the Docker image by running this command from the directory where you created the DockerFile in the Docker environment in the previous step (I ran it from /home/ec2-user directory):</p> 
<code class="lang-bash">$ docker build . –t ccrepl</code> 
<p>Output: It installs various packages and set environment variables as part of steps 1 to 3 from the Dockerfile. The steps 4 to 11 from the Dockerfile should produce an output similar to the following:</p> 
<code class="lang-bash">Step 4/11 : WORKDIR /home/ec2-user
---&gt; 37a5113bd2ce
Removing intermediate container ed89954b9a4f
Step 5/11 : RUN mkdir LocalRepository
---&gt; Running in b9e4fab2b264
---&gt; 816b553261c9
Removing intermediate container b9e4fab2b264
Step 6/11 : WORKDIR /home/ec2-user
---&gt; 60ad564a70be
Removing intermediate container 0897cfb7dd5d
Step 7/11 : COPY ./LocalRepository/ /home/ec2-user/LocalRepository/
---&gt; 03420e4e7d0a
Step 8/11 : COPY ./repl_repository.bash /home/ec2-user/LocalRepository
---&gt; 7eea3f045f38
Step 9/11 : RUN chmod ugo+rwx /home/ec2-user/LocalRepository/repl_repository.bash
---&gt; Running in 7ef77a0ad886
---&gt; 2ff00242c190
Removing intermediate container 7ef77a0ad886
Step 10/11 : WORKDIR /home/ec2-user/LocalRepository
---&gt; 41f2aa473cf8
Removing intermediate container 9f233487943b
Step 11/11 : ENTRYPOINT /home/ec2-user/LocalRepository/repl_repository.bash
---&gt; Running in 0aaff1cc2e29
---&gt; 2066cf6a9c7d
Removing intermediate container 0aaff1cc2e29
Successfully built 2066cf6a9c7d
Successfully tagged ccrepl:latest</code> 
<p>2. Run the following command to verify that the image was created successfully. It will display “Everything up-to-date” at the end if it is successful.</p> 
<code class="lang-bash">[ec2-user@ip-172-1-1-210 LocalRepository]$ docker run ccrepl
Everything up-to-date
</code> 
<h3>Step 3: Push the Docker Image to Amazon Elastic Container Registry (ECR)</h3> 
<p>Perform the following steps in the Docker Environment.</p> 
<p>1. Run the <a href="https://aws.amazon.com/cli/"><code>AWS CLI</code></a> configure command and set default region as your source repository region (I used us-east-1).</p> 
<code class="lang-bash">$ aws configure set default.region <span style="color: #ff0000">&lt;Source Repository Region&gt;</span></code> 
<p>2. Create an Amazon ECR repository using this command to store your ccrepl image (Note the repositoryUri in the output):</p> 
<code class="lang-bash">$ aws ecr create-repository --repository-name ccrepl</code> 
<p>Output:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/04/03/codecommit-fargate2-300x62.png" /></p> 
<p>3. Tag the ccrepl image with repositoryUri value from the previous step using this command:</p> 
<code class="lang-bash">$ docker tag ccrepl:latest aws_account_id.dkr.ecr.us-east-1.amazonaws.com/ccrepl</code> 
<p>4. Get the Docker login credentials using the following command:</p> 
<code class="lang-bash">$ aws ecr get-login --no-include-email</code> 
<p>5. Run the Docker login command returned from the previous step. If the command is successful, you will get a message “Login Succeeded”</p> 
<p>6. Push the docker image to Amazon ECR with the repositoryUri from step 2 using this command:</p> 
<code class="lang-bash">$ docker push <span style="color: #ff0000">aws_account_id</span>.dkr.ecr.us-east-1.amazonaws.com/ccrepl</code> 
<h3>Step 4: Create Fargate Task Definition and cluster</h3> 
<p>1. Create a text file called trustpolicyforecs.json with the following content in the Docker Environment:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;ecs-tasks.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}</code> 
<p>2. Create a role called AccessRoleForCCfromFG using the following command in the DockerEnvironment:</p> 
<code class="lang-bash">$ aws iam create-role --role-name AccessRoleForCCfromFG --assume-role-policy-document file://trustpolicyforecs.json</code> 
<p>3. Assign CodeCommit service full access to the above role using the following command in the DockerEnvironment:</p> 
<code class="lang-bash">$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AWSCodeCommitFullAccess --role-name AccessRoleForCCfromFG</code> 
<p>4. In the <a href="https://console.aws.amazon.com/ecs/">Amazon ECS Console</a>, choose Repositories and select the ccrepl repository that was created in the previous step. Copy the Repository URI.</p> 
<p>5.&nbsp;&nbsp; In the Amazon ECS Console, choose Task Definitions and click Create New Task Definition.</p> 
<p>6.&nbsp;&nbsp; Select launch type compatibility as FARGATE and click Next Step.</p> 
<p>7.&nbsp;&nbsp; &nbsp;In the create task definition screen, do the following:</p> 
<li>In Task Definition Name, type ccrepl</li> 
<li>In Task Role, choose AccessRoleForCCfromFG</li> 
<li>In Task Memory, choose 2GB</li> 
<li>In Task CPU, choose 1 vCPU</li> 
<li>Click Add Container under Container Definitions in the same screen. In the Add Container screen, do the following: 
<li>Enter Container name as ccreplcont</li> 
<li>Enter Image URL copied from step 4</li> 
<li>Enter Memory Limits as 128 and click Add.</li> 
</ul> </li> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/04/03/codecommit-fargate3.png" /></p> 
<p><strong>Note:</strong> Select TaskExecutionRole as “ecsTaskExecutionRole” if it already exists. If not, select create new role and it will create “ecsTaskExecutionRole” for you.</p> 
<p>8.&nbsp;&nbsp; Click the Create button in the task definition screen to create the task. It will successfully create the task, execution role and AWS CloudWatch Log groups.</p> 
<p>9.&nbsp;&nbsp; In the Amazon ECS Console, click Clusters and create cluster. Select template as “Networking only, Powered by AWS Fargate” and click next step.</p> 
<p>10.&nbsp;&nbsp; Enter cluster name as ccreplcluster and click create.</p> 
<h3>Step 5: Create the Lambda Function</h3> 
<p>In this section, I used Amazon Elastic Container Service (ECS) run task API from Lambda to invoke the Fargate task.</p> 
<p>1. In the <a href="https://console.aws.amazon.com/iam/home">IAM Console</a>, create a new role called ECSLambdaRole with the permissions to AWS CodeCommit, Amazon ECS as well as pass roles privileges needed to run the ECS task. Your statement should look similar to the following (replace <span style="color: #ff0000">&lt;your account id&gt;</span>):</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;codecommit:*&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;,
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: &quot;arn:aws:logs:*:*:*&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ecs:RunTask&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;iam:PassRole&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:iam::<span style="color: #ff0000">&lt;your account id&gt;</span>:role/ecsTaskExecutionRole&quot;,
&quot;arn:aws:iam::<span style="color: #ff0000">&lt;your account id&gt;</span>:role/AccessRoleForCCfromFG&quot;
]
}
]
}</code> 
<p>2. In AWS management console, select VPC service and click subnets in the left navigation screen. Note down the Subnet IDs that you want to run the Fargate task in.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/04/03/codecommit-fargate4.png" /></p> 
<p>3.&nbsp;&nbsp; Create a new Lambda Node.js function called FargateTaskExecutionFunc and assign the role ECSLambdaRole with the following content:</p> 
<p><strong>Note:</strong> Replace subnets values (marked in red color) with the subnet IDs you identified as the subnets you wanted to run the Fargate task on in Step 2 of this section.</p> 
<code class="lang-js">var AWS = require('aws-sdk');
var ecs = new AWS.ECS();
exports.handler = (event, context, callback) =&gt; {
var params = {
cluster: &quot;ccreplcluster&quot;, 
launchType: 'FARGATE',
taskDefinition: &quot;ccrepl:1&quot;,
count: 1,
platformVersion: 'LATEST',
networkConfiguration: {
awsvpcConfiguration: {
subnets: [ /* required */
'<span style="color: #ff0000">&lt;subnet ID1&gt;</span>',
'<span style="color: #ff0000">&lt;subnet ID2&gt;</span>'
],
assignPublicIp: 'ENABLED',
}      
}
};
ecs.runTask(params, function(err, data) {
if (err) console.log(err, err.stack); // an error occurred
else     console.log(data);           // successful response
});
};</code> 
<h3>Step 6: Assign Lambda to CodeCommit Trigger</h3> 
<p>1. In the <a href="https://console.aws.amazon.com/lambda/home">Lambda Console</a>, click FargateTaskExecutionFunc under functions.</p> 
<p>2. Under Add triggers in the Designer, select CodeCommit</p> 
<p>3. In the Configure triggers screen, do the following:</p> 
<li>Enter Repository name as Source (your source repository name)</li> 
<li>Enter trigger name as LambdaTrigger</li> 
<li>Leave the Events as “All repository events”</li> 
<li>Leave the Branch names as “All branches”</li> 
<li>Click Add button</li> 
<li>Click Save button to save the changes</li> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/04/03/codecommit-fargate5.png" /></p> 
<h3>Step 6: Verification</h3> 
<p>To test the application, make a commit and push the changes to the source repository in AWS CodeCommit. That should automatically trigger the Lambda function and replicate the changes in the destination repository. You can verify this by checking CloudWatch Logs for Lambda and ECS, or simply going to the destination repository and verifying the change appears.</p> 
<b>Conclusion</b> 
<p>Congratulations! You have successfully configured repository replication of an AWS CodeCommit repository using AWS Lambda and AWS Fargate. You can use this technique in a deployment pipeline. You can also tweak the trigger configuration in AWS CodeCommit to call the Lambda function in response to any supported trigger event in AWS CodeCommit.</p> 
<p>For more information about CodeCommit, see the <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html">AWS CodeCommit documentation</a>.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/29/codestar-featured_image.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Performing Unit Testing in an AWS CodeStar Project</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>In this blog post, I will show how you can perform unit testing as a part of your <a href="https://console.aws.amazon.com/codestar">AWS CodeStar</a> project. AWS CodeStar helps you quickly develop, build, and deploy applications on AWS. With AWS CodeStar, you can set up your continuous delivery (CD) toolchain and manage your software development from one place.</p> 
<p>Because unit testing tests individual units of application code, it is helpful for quickly identifying and isolating issues. As a part of an automated CI/CD process, it can also be used to prevent bad code from being deployed into production.</p> 
<p>Many of the AWS CodeStar project templates come preconfigured with a unit testing framework so that you can start deploying your code with more confidence. The unit testing is configured to run in the provided build stage so that, if the unit tests do not pass, the code is not deployed. For a list of AWS CodeStar project templates that include unit testing, see <a href="https://docs.aws.amazon.com/codestar/latest/userguide/templates.html">AWS CodeStar Project Templates</a> in the AWS CodeStar User Guide.</p> 
<b>The scenario</b> 
<p>As a big fan of superhero movies, I decided to list my favorites and ask my friends to vote on theirs by using a WebService endpoint I created. The example I use is a Python web service running on <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> with <a href="https://aws.amazon.com/codecommit/">AWS CodeCommit</a> as the code repository. CodeCommit is a fully managed source control system that hosts Git repositories and works with all Git-based tools.</p> 
<p>Here’s how you can create the WebService endpoint:</p> 
<p>Sign in to the <a href="https://console.aws.amazon.com/codestar">AWS CodeStar console</a>. Choose <strong>Start a project</strong>, which will take you to the list of project templates.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/01_create_project_new.gif" /></p> 
<p>For code edits I will choose <a href="https://aws.amazon.com/cloud9/">AWS Cloud9</a>, which is a cloud-based integrated development environment (IDE) that you use to write, run, and debug code.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/02_choose_cloud9.gif" /></p> 
<p>Here are the other tasks required by my scenario:</p> 
<li>Create a database table where the votes can be stored and retrieved as needed.</li> 
<li>Update the logic in the Lambda function that was created for posting and getting the votes.</li> 
<li>Update the unit tests (of course!) to verify that the logic works as expected.</li> 
<p>For a database table, I’ve chosen <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB</a>, which offers a fast and flexible NoSQL database.</p> 
<h3>Getting set up on AWS Cloud9</h3> 
<p>From the AWS CodeStar console, go to the AWS Cloud9 console, which should take you to your project code. I will open up a terminal at the top-level folder under which I will set up my environment and required libraries.</p> 
<p>Use the following command to set the PYTHONPATH environment variable on the terminal.</p> 
<code class="lang-powershell">export PYTHONPATH=/home/ec2-user/environment/vote-your-movie</code> 
<p>You should now be able to use the following command to execute the unit tests in your project.</p> 
<code class="lang-powershell">python -m unittest discover vote-your-movie/tests</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/03_cloud9_init_setup.gif" /></p> 
<h3>Start coding</h3> 
<p>Now that you have set up your local environment and have a copy of your code, add a DynamoDB table to the project by defining it through a template file. Open template.yml, which is the Serverless Application Model (SAM) template file. This template extends <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> to provide a simplified way of defining the <a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a> APIs, AWS Lambda functions, and Amazon DynamoDB tables required by your serverless application.</p> 
<code class="lang-yaml">AWSTemplateFormatVersion: 2010-09-09
Transform:
- AWS::Serverless-2016-10-31
- AWS::CodeStar
Parameters:
ProjectId:
Type: String
Description: CodeStar projectId used to associate new resources to team members
Resources:
# The DB table to store the votes.
MovieVoteTable:
Type: AWS::Serverless::SimpleTable
Properties:
PrimaryKey:
# Name of the &quot;Candidate&quot; is the partition key of the table.
Name: Candidate
Type: String
# Creating a new lambda function for retrieving and storing votes.
MovieVoteLambda:
Type: AWS::Serverless::Function
Properties:
Handler: index.handler
Runtime: python3.6
Environment:
# Setting environment variables for your lambda function.
Variables:
TABLE_NAME: !Ref &quot;MovieVoteTable&quot;
TABLE_REGION: !Ref &quot;AWS::Region&quot;
Role:
Fn::ImportValue:
!Join ['-', [!Ref 'ProjectId', !Ref 'AWS::Region', 'LambdaTrustRole']]
Events:
GetEvent:
Type: Api
Properties:
Path: /
Method: get
PostEvent:
Type: Api
Properties:
Path: /
Method: post
</code> 
<p>We’ll use Python’s boto3 library to connect to AWS services. And we’ll use Python’s mock library to mock AWS service calls for our unit tests.<br /> Use the following command to install these libraries:</p> 
<code class="lang-powershell">pip install --upgrade boto3 mock -t .</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/04_cloud9_install_deps.gif" /></p> 
<p>Add these libraries to the buildspec.yml, which is the YAML file that is required for CodeBuild to execute.</p> 
<code class="lang-yaml">version: 0.2
phases:
install:
commands:
# Upgrade AWS CLI to the latest version
- pip install --upgrade awscli boto3 mock
pre_build:
commands:
# Discover and run unit tests in the 'tests' directory. For more information, see &lt;https://docs.python.org/3/library/unittest.html#test-discovery&gt;
- python -m unittest discover tests
build:
commands:
# Use AWS SAM to package the application by using AWS CloudFormation
- aws cloudformation package --template template.yml --s3-bucket $S3_BUCKET --output-template template-export.yml
artifacts:
type: zip
files:
- template-export.yml</code> 
<p>Open the index.py where we can write the simple voting logic for our Lambda function.</p> 
<code class="lang-python">import json
import datetime
import boto3
import os
table_name = os.environ['TABLE_NAME']
table_region = os.environ['TABLE_REGION']
VOTES_TABLE = boto3.resource('dynamodb', region_name=table_region).Table(table_name)
CANDIDATES = {&quot;A&quot;: &quot;Black Panther&quot;, &quot;B&quot;: &quot;Captain America: Civil War&quot;, &quot;C&quot;: &quot;Guardians of the Galaxy&quot;, &quot;D&quot;: &quot;Thor: Ragnarok&quot;}
def handler(event, context):
if event['httpMethod'] == 'GET':
resp = VOTES_TABLE.scan()
return {'statusCode': 200,
'body': json.dumps({item['Candidate']: int(item['Votes']) for item in resp['Items']}),
'headers': {'Content-Type': 'application/json'}}
elif event['httpMethod'] == 'POST':
try:
body = json.loads(event['body'])
except:
return {'statusCode': 400,
'body': 'Invalid input! Expecting a JSON.',
'headers': {'Content-Type': 'application/json'}}
if 'candidate' not in body:
return {'statusCode': 400,
'body': 'Missing &quot;candidate&quot; in request.',
'headers': {'Content-Type': 'application/json'}}
if body['candidate'] not in CANDIDATES.keys():
return {'statusCode': 400,
'body': 'You must vote for one of the following candidates - {}.'.format(get_allowed_candidates()),
'headers': {'Content-Type': 'application/json'}}
resp = VOTES_TABLE.update_item(
Key={'Candidate': CANDIDATES.get(body['candidate'])},
UpdateExpression='ADD Votes :incr',
ExpressionAttributeValues={':incr': 1},
ReturnValues='ALL_NEW'
)
return {'statusCode': 200,
'body': &quot;{} now has {} votes&quot;.format(CANDIDATES.get(body['candidate']), resp['Attributes']['Votes']),
'headers': {'Content-Type': 'application/json'}}
def get_allowed_candidates():
l = []
for key in CANDIDATES:
l.append(&quot;'{}' for '{}'&quot;.format(key, CANDIDATES.get(key)))
return &quot;, &quot;.join(l)
</code> 
<p>What our code basically does is take in the HTTPS request call as an event. If it is an HTTP GET request, it gets the votes result from the table. If it is an HTTP POST request, it sets a vote for the candidate of choice. We also validate the inputs in the POST request to filter out requests that seem malicious. That way, only valid calls are stored in the table.</p> 
<p>In the example code provided, we use a CANDIDATES variable to store our candidates, but you can store the candidates in a JSON file and use Python’s json library instead.</p> 
<p>Let’s update the tests now. Under the tests folder, open the test_handler.py and modify it to verify the logic.</p> 
<p><code class="lang-python"></code></p> 
<code class="lang-python">import os
# Some mock environment variables that would be used by the mock for DynamoDB
os.environ['TABLE_NAME'] = &quot;MockHelloWorldTable&quot;
os.environ['TABLE_REGION'] = &quot;us-east-1&quot;
# The library containing our logic.
import index
# Boto3's core library
import botocore
# For handling JSON.
import json
# Unit test library
import unittest
## Getting StringIO based on your setup.
try:
from StringIO import StringIO
except ImportError:
from io import StringIO
## Python mock library
from mock import patch, call
from decimal import Decimal
@patch('botocore.client.BaseClient._make_api_call')
class TestCandidateVotes(unittest.TestCase):
## Test the HTTP GET request flow. 
## We expect to get back a successful response with results of votes from the table (mocked).
def test_get_votes(self, boto_mock):
# Input event to our method to test.
expected_event = {'httpMethod': 'GET'}
# The mocked values in our DynamoDB table.
items_in_db = [{'Candidate': 'Black Panther', 'Votes': Decimal('3')},
{'Candidate': 'Captain America: Civil War', 'Votes': Decimal('8')},
{'Candidate': 'Guardians of the Galaxy', 'Votes': Decimal('8')},
{'Candidate': &quot;Thor: Ragnarok&quot;, 'Votes': Decimal('1')}
]
# The mocked DynamoDB response.
expected_ddb_response = {'Items': items_in_db}
# The mocked response we expect back by calling DynamoDB through boto.
response_body = botocore.response.StreamingBody(StringIO(str(expected_ddb_response)),
len(str(expected_ddb_response)))
# Setting the expected value in the mock.
boto_mock.side_effect = [expected_ddb_response]
# Expecting that there would be a call to DynamoDB Scan function during execution with these parameters.
expected_calls = [call('Scan', {'TableName': os.environ['TABLE_NAME']})]
# Call the function to test.
result = index.handler(expected_event, {})
# Run unit test assertions to verify the expected calls to mock have occurred and verify the response.
assert result.get('headers').get('Content-Type') == 'application/json'
assert result.get('statusCode') == 200
result_body = json.loads(result.get('body'))
# Verifying that the results match to that from the table.
assert len(result_body) == len(items_in_db)
for i in range(len(result_body)):
assert result_body.get(items_in_db[i].get(&quot;Candidate&quot;)) == int(items_in_db[i].get(&quot;Votes&quot;))
assert boto_mock.call_count == 1
boto_mock.assert_has_calls(expected_calls)
## Test the HTTP POST request flow that places a vote for a selected candidate.
## We expect to get back a successful response with a confirmation message.
def test_place_valid_candidate_vote(self, boto_mock):
# Input event to our method to test.
expected_event = {'httpMethod': 'POST', 'body': &quot;{\&quot;candidate\&quot;: \&quot;D\&quot;}&quot;}
# The mocked response in our DynamoDB table.
expected_ddb_response = {'Attributes': {'Candidate': &quot;Thor: Ragnarok&quot;, 'Votes': Decimal('2')}}
# The mocked response we expect back by calling DynamoDB through boto.
response_body = botocore.response.StreamingBody(StringIO(str(expected_ddb_response)),
len(str(expected_ddb_response)))
# Setting the expected value in the mock.
boto_mock.side_effect = [expected_ddb_response]
# Expecting that there would be a call to DynamoDB UpdateItem function during execution with these parameters.
expected_calls = [call('UpdateItem', {
'TableName': os.environ['TABLE_NAME'], 
'Key': {'Candidate': 'Thor: Ragnarok'},
'UpdateExpression': 'ADD Votes :incr',
'ExpressionAttributeValues': {':incr': 1},
'ReturnValues': 'ALL_NEW'
})]
# Call the function to test.
result = index.handler(expected_event, {})
# Run unit test assertions to verify the expected calls to mock have occurred and verify the response.
assert result.get('headers').get('Content-Type') == 'application/json'
assert result.get('statusCode') == 200
assert result.get('body') == &quot;{} now has {} votes&quot;.format(
expected_ddb_response['Attributes']['Candidate'], 
expected_ddb_response['Attributes']['Votes'])
assert boto_mock.call_count == 1
boto_mock.assert_has_calls(expected_calls)
## Test the HTTP POST request flow that places a vote for an non-existant candidate.
## We expect to get back a successful response with a confirmation message.
def test_place_invalid_candidate_vote(self, boto_mock):
# Input event to our method to test.
# The valid IDs for the candidates are A, B, C, and D
expected_event = {'httpMethod': 'POST', 'body': &quot;{\&quot;candidate\&quot;: \&quot;E\&quot;}&quot;}
# Call the function to test.
result = index.handler(expected_event, {})
# Run unit test assertions to verify the expected calls to mock have occurred and verify the response.
assert result.get('headers').get('Content-Type') == 'application/json'
assert result.get('statusCode') == 400
assert result.get('body') == 'You must vote for one of the following candidates - {}.'.format(index.get_allowed_candidates())
## Test the HTTP POST request flow that places a vote for a selected candidate but associated with an invalid key in the POST body.
## We expect to get back a failed (400) response with an appropriate error message.
def test_place_invalid_data_vote(self, boto_mock):
# Input event to our method to test.
# &quot;name&quot; is not the expected input key.
expected_event = {'httpMethod': 'POST', 'body': &quot;{\&quot;name\&quot;: \&quot;D\&quot;}&quot;}
# Call the function to test.
result = index.handler(expected_event, {})
# Run unit test assertions to verify the expected calls to mock have occurred and verify the response.
assert result.get('headers').get('Content-Type') == 'application/json'
assert result.get('statusCode') == 400
assert result.get('body') == 'Missing &quot;candidate&quot; in request.'
## Test the HTTP POST request flow that places a vote for a selected candidate but not as a JSON string which the body of the request expects.
## We expect to get back a failed (400) response with an appropriate error message.
def test_place_malformed_json_vote(self, boto_mock):
# Input event to our method to test.
# &quot;body&quot; receives a string rather than a JSON string.
expected_event = {'httpMethod': 'POST', 'body': &quot;Thor: Ragnarok&quot;}
# Call the function to test.
result = index.handler(expected_event, {})
# Run unit test assertions to verify the expected calls to mock have occurred and verify the response.
assert result.get('headers').get('Content-Type') == 'application/json'
assert result.get('statusCode') == 400
assert result.get('body') == 'Invalid input! Expecting a JSON.'
if __name__ == '__main__':
unittest.main()
</code> 
<p>I am keeping the code samples well commented so that it’s clear what each unit test accomplishes. It tests the success conditions and the failure paths that are handled in the logic.</p> 
<p>In my unit tests I use the patch decorator (@patch) in the mock library. @patch helps mock the function you want to call (in this case, the botocore library’s <strong>_make_api_call</strong> function in the <strong>BaseClient</strong> class).<br /> Before we commit our changes, let’s run the tests locally. On the terminal, run the tests again. If all the unit tests pass, you should expect to see a result like this:</p> 
<code class="lang-powershell">You:~/environment $ python -m unittest discover vote-your-movie/tests
.....
----------------------------------------------------------------------
Ran 5 tests in 0.003s
OK
You:~/environment $</code> 
<b>Upload to AWS</b> 
<p>Now that the tests have passed, it’s time to commit and push the code to source repository!</p> 
<h3>Add your changes</h3> 
<p>From the terminal, go to the project’s folder and use the following command to verify the changes you are about to push.</p> 
<code class="lang-powershell">git status</code> 
<p class="unlimited-height-code">To add the modified files only, use the following command:</p> 
<code class="lang-powershell">git add -u</code> 
<h3>Commit your changes</h3> 
<p>To commit the changes (with a message), use the following command:</p> 
<code class="lang-powershell">git commit -m &quot;Logic and tests for the voting webservice.&quot;</code> 
<h3>Push your changes to AWS CodeCommit</h3> 
<p>To push your committed changes to CodeCommit, use the following command:</p> 
<code class="lang-powershell">git push</code> 
<p>In the AWS CodeStar console, you can see your changes flowing through the pipeline and being deployed. There are also links in the AWS CodeStar console that take you to this project’s build runs so you can see your tests running on AWS CodeBuild. The latest link under the Build Runs table takes you to the logs.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/06_unit_tests_at_codebuild_new.gif" /></p> 
<p>After the deployment is complete, AWS CodeStar should now display the AWS Lambda function and DynamoDB table created and synced with this project. The Project link in the AWS CodeStar project’s navigation bar displays the AWS resources linked to this project.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/07_codestar_ddb_resource.gif" /></p> 
<p>Because this is a new database table, there should be no data in it. So, let’s put in some votes. You can download <a href="https://www.getpostman.com/apps">Postman</a> to test your application endpoint for POST and GET calls. The endpoint you want to test is the URL displayed under <strong>Application endpoints</strong> in the AWS CodeStar console.</p> 
<p>Now let’s open Postman and look at the results.&nbsp;Let’s create some votes through POST requests. Based on this example, a valid vote has a value of <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, or <strong>D</strong>.<br /> Here’s what a successful POST request looks like:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/post_success.png" /></p> 
<p>Here’s what it looks like if I use some value other than <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, or <strong>D</strong>:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/post_fail.png" /></p> 
<p>Now I am going to use a GET request to fetch the results of the votes from the database.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/23/get_success.png" /></p> 
<p>And that’s it! You have now created a simple voting web service using AWS Lambda, Amazon API Gateway, and DynamoDB and used unit tests to verify your logic so that you ship good code.<br /> Happy coding!</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/21/CodeDeploy-migration-arch-overview.png" /> 
<b class="lb-b blog-post-title" property="name headline">Migrating from an In-House Deployment Agent to AWS CodeDeploy and AWS CodePipeline</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>I’m Charles Fort, a developer at Woot who specializes in deployments and developer experience. Woot is the original daily deals website. It was founded in 2004 and acquired by Amazon in 2010 –&nbsp;<a href="https://www.woot.com">https://www.woot.com</a></p> 
<p>We just moved our web front-end deployments from Troop, a deployment agent we developed ourselves, to AWS CodeDeploy and AWS CodePipeline. This migration involved launching a new customer-facing EC2 web server fleet with the CodeDeploy agent installed and creating and managing our CodeDeploy and CodePipeline resources with AWS CloudFormation. Immediately after we completed the migration, we observed a ~50% reduction in HTTP 500 errors during deployment.</p> 
<p>In this blog post, I’m going to show you:</p> 
<li>Why we chose AWS deployment tools.</li> 
<li>An architectural diagram of Woot’s systems.</li> 
<li>An overview of the migration project.</li> 
<li>Our migration results.</li> 
<b>The old and busted</b> 
<p>We wanted to replace our in-house deployment system with something we could build automation on top of, and something that we didn’t have to own or maintain. We already own and maintain a build system, which is bad enough. We didn’t want to run additional infrastructure for our deployment pipeline.</p> 
<p>In short, we wanted a cloud service. Because all of our infrastructure is in AWS, CodeDeploy was a natural fit to replace our deployment agent. CodePipeline acts as the automation orchestrator, our wizard in the cloud telling CodeDeploy what to deploy and when.</p> 
<b>Architecture overview</b> 
<p>Here’s a look at the architecture of a Woot web front end:</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/21/CodeDeploy-migration-arch-overview.png" /> 
<p class="wp-caption-text">Woot architecture overview</p> 
<b>Project overview</b> 
<p>Our project involved migrating five web front ends, which together handle an average of 12 million requests per day, to CodeDeploy and CodePipeline while keeping the site live for our customers.</p> 
<p>Here are the steps we took:</p> 
<li>Wrote some new deployment scripts.</li> 
<li>Launched a new fleet of EC2 web servers with CodeDeploy support.</li> 
<li>Created a deployment pipeline for our CloudFormation-defined CodeDeploy and CodePipeline configuration.</li> 
<li>Introduced our new fleet to live traffic. Hello, customers!</li> 
<h3>Deployment scripts</h3> 
<p>Our old deployment system didn’t stop or start our web server. Instead, it tried to swap out the build artifacts from under the server while the server was running. That’s definitely a bad idea.</p> 
<p>We wrote some deployment scripts in Powershell that are run by CodeDeploy to stop and start our IIS web servers. These scripts work in conjunction with the Elastic Load Balancing (ELB) support in CodeDeploy because we certainly didn’t want to stop the web server while it’s serving customer traffic.</p> 
<h3>New fleet</h3> 
<p>Because our fleet is running on Amazon EC2, we built an Amazon Machine Image (AMI) for our web fleet with the CodeDeploy agent already installed. From a fleet perspective, this was most of what we had to do. With the agent installed, CodeDeploy can use our deployment scripts to deploy our web projects.</p> 
<h3>AWS CloudFormation deployment pipeline</h3> 
<p>Because we needed a deployment pipeline and several pieces of CodeDeploy configuration (a CodeDeploy application and at least one CodeDeploy deployment group) for each web project we want to deploy, we decided to use AWS CloudFormation to version this configuration. Our build system (TeamCity) can read from our version control system and write to Amazon S3. We made a simple build in TeamCity to push an AWS CloudFormation template to S3, which triggers a pipeline that deploys to AWS CloudFormation. This creates the CodePipeline and CodeDeploy resources. Now we can do code reviews on our infrastructure changes. More eyes is more safety! We can also trace infrastructure changes over time, just like we can with code changes.</p> 
<h3>Live traffic time!</h3> 
<p>Our web fleets run behind Classic Load Balancers. By choosing CodeDeploy, we can use its sweet new ELB features. For example, through an elastic load balancer, CodeDeploy can prevent internet traffic from being routed to an instance while it is being deployed to. After the deployment to that instance is complete, it then makes the instance available for traffic.</p> 
<p>We launched new hosts with the CodeDeploy agent and deployed to them without ELB support turned on. Then we slowly, manually introduced them into our fleet while monitoring stats. After we had the new machines in, we slowly removed the old ones from the load balancer until our fleet was fully supported by CodeDeploy. Doing the migration this way resulted in 0 downtime for our sites.</p> 
<p>One fun detail: When we had 2/3 of the new fleet in our load balancer, we triggered a CodeDeploy deployment to the fleet, but this time with ELB support turned on. This caused CodeDeploy to place the rest of the machines into the load balancer (coexisting with the old fleet), and there were slightly fewer buttons to press.</p> 
<b>AWS CloudFormation example</b> 
<p>This is a simplified example of the AWS CloudFormation template we use to manage the AWS configuration for one of our web projects. It is deployed in a deployment pipeline, much like the web projects themselves.</p> 
<code class="lang-yaml">Parameters:
CodePipelineBucket:
Type: String
CodePipelineRole:
Type: String
CodeDeployRole:
Type: String
CodeDeployBucket:
Type: String
Resources:
### Woot.Example deployment configuration ###
ExampleDeploymentConfig:
Type: 'AWS::CodeDeploy::DeploymentConfig'
Properties:
MinimumHealthyHosts:
Type: FLEET_PERCENT
Value: '66' # Let's keep 2/3 of the fleet healthy at any point
#Woot.Example CodeDeploy application
WootExampleApplication:
Type: &quot;AWS::CodeDeploy::Application&quot;
Properties:
ApplicationName: &quot;Woot.Example&quot;
#Woot.Example CodeDeploy deployment groups
WootExampleDeploymentGroup:
DependsOn: &quot;WootExampleApplication&quot;
Type: &quot;AWS::CodeDeploy::DeploymentGroup&quot;
Properties:
ApplicationName: &quot;Woot.Example&quot;
DeploymentConfigName: !Ref &quot;ExampleDeploymentConfig&quot; # use the deployment configuration we created
DeploymentGroupName: &quot;Woot.Example.Main&quot;
AutoRollbackConfiguration:
Enabled: true
Events:
- DEPLOYMENT_FAILURE # this makes the deployment rollback when the deployment hits the failure threshold
- DEPLOYMENT_STOP_ON_REQUEST # this makes the deployment rollback if you hit the stop button
LoadBalancerInfo:
ElbInfoList:
- Name: &quot;WootExampleInternal&quot; # this is the ELB the hosts live in, they will be added and removed from here
DeploymentStyle:
DeploymentOption: &quot;WITH_TRAFFIC_CONTROL&quot; # this tells CodeDeploy to actually add/remove the hosts from the ELB
Ec2TagFilters:
-
Key: &quot;Name&quot;
Value: &quot;exampleweb*&quot; # deploy to all machines named like exampleweb001.yourdomain.com, etc
Type: &quot;KEY_AND_VALUE&quot;
ServiceRoleArn: !Sub &quot;arn:aws:iam::${AWS::AccountId}:role/${CodeDeployRole}&quot; # this is the IAM role CodeDeploy in your account should use
#Woot.Example CodePipeline
WootExampleDeploymentPipeline:
DependsOn: &quot;WootExampleDeploymentGroup&quot;
Type: &quot;AWS::CodePipeline::Pipeline&quot;
Properties:
RoleArn: !Sub &quot;arn:aws:iam::${AWS::AccountId}:role/${CodePipelineRole}&quot; # this is the IAM role CodePipeline in your account should use
Name: &quot;Woot.Example&quot; # name of the pipeline
ArtifactStore:
Type: S3
Location: !Ref &quot;CodePipelineBucket&quot; # the bucket CodePipeline uses in your account to shuffle artifacts around
Stages:
-
Name: Source # one S3 source stage
Actions:
-
Name: SourceAction
ActionTypeId:
Category: Source
Owner: AWS
Version: 1
Provider: S3
OutputArtifacts:
-
Name: SourceOutput
Configuration:
S3Bucket: !Ref &quot;CodeDeployBucket&quot; # the S3 bucket your builds go into (needs to be versioned)
S3ObjectKey: &quot;Woot.Example/Woot.Example-Release.zip&quot; # build artifact path for this application
RunOrder: 1
-
Name: Deploy # one deploy stage that triggers the CodeDeploy deployment
Actions:
-
Name: DeployAction
ActionTypeId:
Category: Deploy
Owner: AWS
Version: 1
Provider: CodeDeploy
InputArtifacts:
-
Name: SourceOutput
Configuration:
ApplicationName: &quot;Woot.Example&quot;
DeploymentGroupName: &quot;Woot.Example.Main&quot;
RunOrder: 2</code> 
<b></b> 
<b>Appspec.yml example</b> 
<p>This is the appspec.yml file we use for our main web front end (Woot.Web.Retail). The appspec.yml file tells CodeDeploy where to put files and when to run our deployment scripts.</p> 
<code class="lang-yaml">version: 0.0
os: windows
files:
- source: /
destination: C:\Woot\Woot.Web.Retail\Initial
hooks:
BeforeInstall:
- location: bin\Deployment\stopWebsite.ps1
- location: bin\Deployment\clearWebsiteDeployment.ps1
AfterInstall:
- location: bin\Deployment\startWebsite.ps1</code> 
<b></b> 
<b>Deployment scripts</b> 
<p>Because our server-launching infrastructure doesn’t use CodeDeploy for initial placement of the build artifacts, CodeDeploy won’t overwrite the files. (The service has no knowledge of them.) This is both good and bad: good because CodeDeploy won’t overwrite files it didn’t write, and bad because it means we have to have a deployment script like clearWebsiteDeployment.ps1:</p> 
<p><strong>clearWebsiteDeployment.ps1:</strong></p> 
<code class="lang-powershell">$appName = $env:APPLICATION_NAME
Remove-Item &quot;C:\Woot\$appName\Initial\*&quot; -recurse -force</code> 
<p><strong>stopWebsite.ps1:</strong></p> 
<code class="lang-powershell"># restart script as 64bit powershell if it's 32 bit
if ($PSHOME -like &quot;*SysWOW64*&quot;) {
&amp; (Join-Path ($PSHOME -replace &quot;SysWOW64&quot;, &quot;SysNative&quot;) powershell.exe) -File `
(Join-Path $PSScriptRoot $MyInvocation.MyCommand) @args
Exit $LastExitCode
}
Import-Module WebAdministration
Stop-Website -name $env:APPLICATION_NAME
#sleep for 10 seconds to give IIS a chance to stop
Start-Sleep -s 10
$website = Get-Website -name &quot;*$env:APPLICATION_NAME*&quot;
if ($website.state -ne 'stopped') {
throw &quot;The website cannot be stopped&quot;
}</code> 
<p><strong>startWebsite.ps1:</strong></p> 
<code class="lang-powershell"># restart script as 64bit powershell if it's 32 bit
if ($PSHOME -like &quot;*SysWOW64*&quot;) {
&amp; (Join-Path ($PSHOME -replace &quot;SysWOW64&quot;, &quot;SysNative&quot;) powershell.exe) -File `
(Join-Path $PSScriptRoot $MyInvocation.MyCommand) @args
Exit $LastExitCode
}
Import-Module WebAdministration
Start-Website -name $env:APPLICATION_NAME
#sleep for 10 seconds to give IIS a chance to start
Start-Sleep -s 10
$website = Get-Website -name &quot;*$env:APPLICATION_NAME*&quot;
if ($website.state -ne 'started') {
throw &quot;The website cannot be started&quot;
}</code> 
<p>You can see we used a CodeDeploy environment variable ($env:APPLICATION_NAME) in our scripts. The name of the CodeDeploy application is also the name of the IIS website. This way, we can use the same deployment scripts for multiple websites.</p> 
<b>The new hotness</b> 
<p>Now that we’re running CodeDeploy in production we are extremely pleased with the results. Our old deployment agent, Troop, did not give us much control over the way releases went out. Now we can check on a deployment at a per-instance level, and the opportunities for automation are impressive.</p> 
<p>After our migration, we saw a 50% reduction in HTTP 500 errors served to customers during deployments. We looked at the one-hour time slices during a deployment and compared the average count before and after our migration to CodeDeploy. These numbers show our old deployment system was hella busted (really broken).</p> 
<p>This graph shows summed deployment errors over time and compares CodeDeploy to our legacy in-house deployment agent (Troop).</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/03/15/CodeDeploy_vs_troop-1.png" /> 
<p class="wp-caption-text">CodeDeploy vs Troop</p> 
<p>We plan to implement a full cross-account release process on AWS deployment tools. We will have a single AWS account with a pipeline that controls CodeDeploy in our various environments, triggering tests and promoting to the next environment as they pass. Building something like that with our own tooling would take a lot of work. Thanks to CodeDeploy, CodePipeline, and AWS CloudFormation for making our lives easier.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/CB-GHE-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Announcing AWS CodeBuild Support for GitHub Enterprise as a Source Type and Shallow Cloning</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p><em>Thank you to my colleague&nbsp;<strong>Harvey Bendana</strong> for this blog on how to&nbsp;do shallow cloning on AWS CodeBuild using GitHub Enterprise as a source.</em></p> 
<p>Today we are announcing support for using GitHub Enterprise as a source type for CodeBuild. You can now initiate build tasks from changes in source code hosted on your own implementation of GitHub Enterprise.</p> 
<p>We are also announcing support for shallow cloning of a repo when you use CodeCommit, BitBucket, GitHub, or GitHub Enterprise as a source type. Shallow cloning allows you to truncate history of a repo in order to save space and speed up cloning times.</p> 
<p>In this post, I’ll walk you through how to configure GitHub Enterprise as a source type with a defined clone depth for an AWS CodeBuild project. I’ll also show you all the moving parts associated with a successful implementation.</p> 
<p><a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a> is a fully managed build service. There are no servers to provision and scale, or software to install, configure, and operate. You just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code.</p> 
<p>GitHub Enterprise is the on-premises version of <a href="https://github.com/home">GitHub.com</a>. It makes collaborative coding possible and enjoyable for large-scale enterprise software development teams.</p> 
<p>Many enterprises choose GitHub Enterprise as their preferred source code/version control repository because it can be hosted in their own trusted network, whether that is an on-premises data center or their own Amazon VPCs.</p> 
<h3>Requirements</h3> 
<li>You’ll need an AWS account.</li> 
<li>You’ll need a GitHub Enterprise implementation with a repo. If you’d like to deploy one inside your own Amazon VPC, check out our <a href="https://aws.amazon.com/quickstart/architecture/github-enterprise/">Quick Start Guide</a>.</li> 
<li>You’ll need an S3 bucket to store your GitHub Enterprise self-signed SSL certificate.</li> 
<b>Download your GitHub Enterprise SSL certificate:</b> 
<p><strong>Note:</strong> The following steps are required only for self-signed certificates. You can forego installation of a certificate if you are using self-signed certificates and default to HTTP communication with your repo. For this post, I am using a self-signed certificate and the Firefox browser. These steps may vary, depending on your browser of choice.</p> 
<ol> 
<li>Navigate to your GitHub Enterprise environment and sign in with your credentials.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture1.png" /></p> 
<p>2. Choose the lock icon in the upper-left corner to view and export your GitHub Enterprise certificate.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture2.png" /></p> 
<p>3. When you export and download your certificate, make sure you select the format type, which includes the entire certificate chain.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture3.png">https://console.aws.amazon.com/s3</a> and upload your certificate to an S3 bucket in your AWS account.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture4.png" /></p> 
<p>Now I will show you how to create an AWS CodeBuild project.</p> 
<b>Create a sample AWS CodeBuild project:</b> 
<ol> 
<li>Open the AWS CodeBuild console at <a href="https://console.aws.amazon.com/codebuild">https://console.aws.amazon.com/codebuild</a>.</li> 
</ol> 
<p>2. If there are no existing build projects, a welcome page is displayed. Choose <strong>Get started</strong>. If you already have build projects, then choose <strong>Create project</strong>.</p> 
<p>3. On the&nbsp;<strong>Configure your project</strong>&nbsp;page, type a name for your build project. Build project names must be unique across each AWS account.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture5.png" /></p> 
<p>4. For <strong>Source provider</strong>, choose <strong>GitHub Enterprise</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture6.png" /></p> 
<p>5. Generate a personal access token in your GitHub Enterprise environment. Under <strong>Select scopes</strong>, select <strong>repo</strong>. For information, see <a href="https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/">Creating a personal access token for the command line</a> on the GitHub Help website.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture7.png" /></p> 
<p>6. Enter your personal access token in your CodeBuild project and choose <strong>Save Token</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture8.png" /></p> 
<p>7. Enter the repository URL and choose a Git clone depth value that makes sense for you. Allowed values are <strong>1, 5, 25, or Full</strong>. For this post, I am using a depth of 1.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture9.png" /></p> 
<p>8. Select the <strong>Webhook</strong> check box.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture10.png" /></p> 
<p>9. Continue with the rest of the configuration for your project, choosing options that best suit your build needs. For this post, I am using an AWS CodeBuild managed image running the Ubuntu OS with the base runtime configuration. Enter your build specifications or build commands. I am using a simple build command of <strong>git log .</strong> so that it can be easily found in the CloudWatch logs of the CodeBuild project. It will also be used to demonstrate the shallow clone feature.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture11.png" /></p> 
<p>10. Next, select <strong>Install certificate from your S3</strong> to install your GitHub Enterprise self-signed certificate from S3. For <strong>Bucket of certificate</strong>, I’ve entered the S3 bucket where I uploaded the certificate. For <strong>Object key of certificate</strong>, I’ve entered the name of the certificate.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture12.png" /></p> 
<p>11. Lastly, configure artifacts, caching, IAM roles, and VPC configurations. For this post, I chose not to generate any artifacts from this build. From the following screenshot, you’ll see I’ve opted out of cache, requested a new IAM role with the required permissions, and have not defined VPC access. Choose <strong>Continue</strong> to validate and complete the creation of the CodeBuild project.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture13.png" /></p> 
<p><strong>Note</strong>: If your GitHub Enterprise environment is in an Amazon VPC, configure VPC access for your project. Define the VPC ID, subnet ID, and security group so that your project has access to the EC2 instances hosting your GitHub Enterprise environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture14.png" /></p> 
<p>12. After the project is created, a dialog box displays a CodeBuild payload URL and secret. They are used to create a webhook for the repo in the GitHub Enterprise environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture15.png" /></p> 
<b>Create a webhook in your GitHub Enterprise repo:</b> 
<p>1. In your GitHub Enterprise repo, navigate to <strong>Settings</strong>, choose <strong>Hooks &amp; services</strong>, and then choose <strong>Add webhook</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture16.png" /></p> 
<p>2. Paste the payload URL and secret into their respective fields. Under <strong>Which events would you like to trigger this webhook</strong>? choose an option. For this post, I am using <strong>Let me select individual events</strong>. I then chose <strong>Pull request</strong> and <strong>Push</strong> as the two event triggers.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture17.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture18.png" /></p> 
<p>3. Make sure Active is selected and then choose Add webhook.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture19.png" /></p> 
<p>4. A webhook has now been created in the GitHub Enterprise repo.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture20.png" /></p> 
<p>Now I will show you how to test this.</p> 
<b>Trigger your AWS CodeBuild project by pushing a change to the GitHub Enterprise repo</b> 
<p>1. Clone the repo to the local file system. For information, see <a href="https://services.github.com/on-demand/github-cli/clone-repo-cli">Clone the Repository Using the Command Line</a> on the GitHub Help website. Now create a feature branch, push a change, and generate a pull request for review, and, ultimately, merge to master. Here is the state of the GitHub Enterprise repo and AWS CodeBuild project before pushing a change:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture21.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture22.png" /></p> 
<p>2. Use <a href="https://git-scm.com/book/en/v2/Getting-Started-The-Command-Line">Git command line tools</a> to create a new branch in the repo.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture23.png" /></p> 
<p>3. Update the README.md for the repo with a link to the <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild documentation</a>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture24.png" /></p> 
<p>4. After the changes have been saved, push them to the feature branch.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture25.png" /></p> 
<p>5. There is now notification of a new branch in the GitHub Enterprise environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture26.png" /></p> 
<p>6. Generate a pull request from the feature branch in preparation of review and merge to master.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture27.png" /></p> 
<p>7. The reviewer(s) will then review and merge the pull request, pushing all changes to the master branch.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture28.png" /></p> 
<p>8. Here is the updated repo:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture29.png" /></p> 
<p>9. After the change has been pushed successfully, a new build is initiated.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture30.png" /></p> 
<p>10. In the following screenshot, you’ll see the initiator is <strong>Github-Hookshot/eb0c46</strong> and the source version is <strong>03169095b8f16ac077388471035becb2070aa12c</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture31.png" /></p> 
<p>11. In the <strong>Recent Deliveries</strong> section, under the configuration of the GitHub Enterprise repo webhook, the CodeBuild project initiator is defined as <strong>User-Agent</strong>. The source version is denoted in the <strong>Payload</strong> output. They match!</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture32.png" /></p> 
<p>12. A successfully completed build should appear under the CodeBuild project.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture33.png" /></p> 
<p>13. The entire log output of the CodeBuild project can be viewed in CloudWatch logs. In the following screenshot, the source was downloaded successfully from the GitHub Enterprise repo and the build command of <strong>git log .</strong> was run successfully. Only the most recent commit appears in the git history output. This is because I defined a clone depth of 1.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture34.png" /></p> 
<p>14. If I query the git history of the repo on the local repository, the output has the full commit history. This is expected because I am doing a full clone locally.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/25/Picture35.png" /></p> 
<b>Conclusion</b> 
<p>In this blog post, I showed you how to configure GitHub Enterprise as a source type for your AWS CodeBuild project with a clone depth of 1. These new features expand the capabilities of AWS CodeBuild and the suite of AWS Developer Tools for CI/CD and DevOps processes.</p> 
<p>I hope you found this post useful. Feel free to leave your feedback or suggestions in the comments.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/12/automate_to_spot.png" /> 
<b class="lb-b blog-post-title" property="name headline">Automatic Deployment to New Amazon EC2 On-Demand and Spot Instances Using AWS CodeDeploy, Amazon CloudWatch Events, and AWS Lambda</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p><a href="https://aws.amazon.com/codedeploy/">AWS CodeDeploy</a> is a service that automates application deployments to your compute infrastructure, including fleets of <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances. AWS CodeDeploy can automatically deploy the latest app version to any new EC2 instance launched due to a scaling event. However, if your servers are not part of the Auto Scaling group, it might be a challenge to automate the code deployment for new EC2 launches. This is especially true if you are running your workload on an <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html">EC2 Spot Fleet</a>. In this post, I’ll show you how to use AWS CodeDeploy, an <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events rule</a>, EC2 tags, and <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> to automate your deployments so that all EC2 instances run the same version of your application.</p> 
<p><strong>Solution overview:</strong></p> 
<p>An Amazon CloudWatch rule is triggered when a new Amazon EC2 instance is launched. The rule invokes an AWS Lambda function that extracts three tags:</p> 
<code class="lang-html">&middot;&nbsp;Name
&middot;&nbsp;CodeDeployDeploymentGroup
&middot;&nbsp;CodeDeployApplication</code> 
<p>The Lambda function then uses the instance tags to add the EC2 instance to the deployment group. After the instance has been added, Lambda queries AWS CodeDeploy to retrieve the last successful deployment in that deployment group and synchronizes the EC2 instance with the latest code. The following diagram shows the sequence:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/solution-overview-1024x514.png" /></p> 
<p><strong>Note:</strong> For an EC2 Spot Fleet, the tags that you create for your Spot instance are not added automatically by the Spot service to fulfill the request. The Lambda function adds these tags after the Spot instance is launched.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/initial-deployment-group-1024x479.png" /></p> 
<p><strong>Example:</strong></p> 
<p>Let’s take an example of an AWS CodeDeploy application <code class="lang-html">cd-single-deploy-app</code>, and its deployment group, <code class="lang-html">cd-single-deploy-app-group</code>, which has two instances in it. These instances are not part of an Auto Scaling group.</p> 
<p>For information about the steps in deploying an application to an instance, see <a href="http://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-windows.html">this tutorial in the AWS CodeDeploy User Guide</a>.</p> 
<p>In the AWS CodeDeploy console, you’ll find the deployment ID, <code class="lang-markup">d-CSBCXR2GP</code>. We will use this ID later in the testing phase. Our workflow ensures that when a new EC2 instance is launched, it joins the deployment group automatically and the latest version of the application is deployed to it.</p> 
<p>You can configure the Lambda function, associated IAM role and policy, and the CloudWatch Events rule by running <a href="https://github.com/awslabs/aws-codedeploy-new-instance-sync-lambda/blob/master/codedeploy-sync-lambda-setup.json">this CloudFormation template</a> included in the code repository.</p> 
<p><a href="https://github.com/awslabs/aws-codedeploy-new-instance-sync-lambda">https://github.com/awslabs/aws-codedeploy-new-instance-sync-lambda</a></p> 
<p>The CloudFormation template does the following:</p> 
<p><strong>Step 1</strong>: Create an IAM role named <code class="lang-html">Auto-Deploy-EC2-Codedeploy</code> and attach the following policies:</p> 
<code class="lang-html">•	arn:aws:iam::aws:policy/AWSLambdaExecute 
•	arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess 
•	arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole 
</code> 
<p>Use the following JSON document to create another policy named CodeDeploy and attach it to the IAM role.</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Action&quot;: [
&quot;codedeploy:Get*&quot;,
&quot;codedeploy:UpdateDeploymentGroup&quot;,
&quot;codedeploy:List*&quot;,
&quot;codedeploy:CreateDeployment&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
],
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Sid&quot;: &quot;StartContinuousAssessmentLambdaPolicyStmt&quot;
}
]
}
</code> 
<p>Make sure the CodeDeploy role has PassRole permission &nbsp;to the service role defined in the CodeDeploy deployment group.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/iam-role-policy-1024x460.png" /></p> 
<p><strong>Step 2:</strong> Follow these steps to create a Lambda function:</p> 
<p><a href="https://github.com/awslabs/aws-codedeploy-new-instance-sync-lambda/blob/master/README.md">https://github.com/awslabs/aws-codedeploy-new-instance-sync-lambda/blob/master/README.md</a></p> 
<p><strong>Step 3:</strong> In Amazon CloudWatch Events, set up a rule for running instances and configure the Lambda function as a target.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/create-cloudwatch-events-rule-1024x581.png" /></p> 
<p><strong>Step 4:</strong> In the AWS Lambda console, choose your Lambda function. On the Triggers tab, check that the trigger is enabled.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/lambda-trigger-1024x273.png" /></p> 
<p>You can now test to ensure if a new EC2 instance is launched, it gets synchronized automatically with the latest code</p> 
<p><strong>Test:</strong></p> 
<p>A new EC2 instance is launched from the EC2 console, the AWS CloudFormation template, or the EC2 APIs with CodeDeployApplication, CodeDeployDeploymentGroup, and&nbsp;Name tag.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/new-instance-launch.png" /></p> 
<p>Go to CloudWatch Logs console to check the Lambda execution logs.</p> 
<p>Here is an analysis of sample log:</p> 
<p>&middot;&nbsp; You should see the EC2 instance ID</p> 
<code class="lang-markup">{[
&quot;arn:aws:EC2:us-east-1:XXXXXXXXXXXX:instance/<strong>i-00203362b337dd743</strong>&quot;]
}
</code> 
<p>&middot;&nbsp; You should see the deployment ID of the last successful deployment. It should match the one you noted earlier</p> 
<p><code class="lang-markup"> DepId is <strong>d-CSBCXR2GP</strong></code></p> 
<p>&middot;&nbsp; &nbsp; Finally, you should see the deployment ID of the new deployment created by CodeDeploy</p> 
<code class="lang-markup">&quot;{u'deploymentId': u'<strong>d-W6E1TFFGP</strong>', 'ResponseMetadata': {'RetryAttempts': 0, '	HTTPStatusCode': 200, 'RequestId': '73046797-bb22-11e7-ad7b-25693e030410', 'HTTPHeaders': {'x-amzn-requestid': '73046797-bb22-11e7-ad7b-25693e030410', 'content-length': '30', 'content-type': 'application/x-amz-json-1.1'}}}
&quot;
</code> 
<p>Now, go to CodeDeploy console to confirm that the new deployment was successful.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/new-deployment-1024x503.png" /></p> 
<p>In this screenshot, you can see that the new EC2 instance was synched with latest code.</p> 
<p><strong>Summary:</strong></p> 
<p>In this post,&nbsp;I’ve&nbsp;demonstrated how to use AWS CodeDeploy, AWS Lambda, an Amazon CloudWatch Events rule, and EC2 tags to automate deployments to disparate EC2 instances. If you are running your workload on Spot instances or have a fleet of On-Demand EC2 instances as part of your application, you can use the steps in this blog post to solve the automation around the deployment for new instances.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/12/CD_to_K8-934x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Continuous Deployment to Kubernetes using AWS CodePipeline, AWS CodeCommit, AWS CodeBuild, Amazon ECR and AWS Lambda</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>Thank you to my colleague Omar Lari for this blog on how to create a continuous deployment pipeline for Kubernetes!</p> 
<hr /> 
<p>You can use Kubernetes and AWS together to create a fully managed, continuous deployment pipeline for container based applications. This approach takes advantage of Kubernetes’ open-source system to manage your containerized applications, and the AWS developer tools to manage your source code, builds, and pipelines.</p> 
<p>This post describes how to create a continuous deployment architecture for containerized applications. It uses AWS CodeCommit, AWS CodePipeline, AWS CodeBuild, and AWS Lambda to deploy containerized applications into a Kubernetes cluster. In this environment, developers can remain focused on developing code without worrying about how it will be deployed, and development managers can be satisfied that the latest changes are always deployed.</p> 
<b>What is Continuous Deployment?</b> 
<p>There are many articles, posts and even conferences dedicated to the practice of continuous deployment. For the purposes of this post, I will summarize continuous delivery into the following points:</p> 
<li>Code is more frequently released into production environments</li> 
<li>More frequent releases allow for smaller, incremental changes reducing risk and enabling simplified roll backs if needed</li> 
<li>Deployment is automated and requires minimal user intervention</li> 
<p>For a more information, see “<a href="https://d1.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf">Practicing Continuous Integration and Continuous Delivery on AWS</a>”.</p> 
<b>How can you use continuous deployment with AWS and Kubernetes?</b> 
<p>You can leverage AWS services that support continuous deployment to automatically take your code from a source code repository to production in a Kubernetes cluster with minimal user intervention. To do this, you can create a pipeline that will build and deploy committed code changes as long as they meet the requirements of each stage of the pipeline.</p> 
<p>To create the pipeline, you will use the following services:</p> 
<li><strong>AWS CodePipeline.</strong> AWS CodePipeline is a continuous delivery service that models, visualizes, and automates the steps required to release software. You define stages in a pipeline to retrieve code from a source code repository, build that source code into a releasable artifact, test the artifact, and deploy it to production. Only code that successfully passes through all these stages will be deployed. In addition, you can optionally add other requirements to your pipeline, such as manual approvals, to help ensure that only approved changes are deployed to production.</li> 
<li><strong>AWS CodeCommit.</strong> AWS CodeCommit is a secure, scalable, and managed source control service that hosts private Git repositories. You can privately store and manage assets such as your source code in the cloud and configure your pipeline to automatically retrieve and process changes committed to your repository.</li> 
<li><strong>AWS CodeBuild.</strong> AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces artifacts that are ready to deploy. You can use AWS CodeBuild to both build your artifacts, and to test those artifacts before they are deployed.</li> 
<li><strong>AWS Lambda.</strong> AWS Lambda is a compute service that lets you run code without provisioning or managing servers. You can invoke a Lambda function in your pipeline to prepare the built and tested artifact for deployment by Kubernetes to the Kubernetes cluster.</li> 
<li><strong>Kubernetes.</strong> Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. It provides a platform for running, deploying, and managing containers at scale.</li> 
<b>An Example of Continuous Deployment to Kubernetes:</b> 
<p>The following example illustrates leveraging AWS developer tools to continuously deploy to a Kubernetes cluster:</p> 
<p><img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/k8s-code.png" /></p> 
<ol> 
<li>Developers commit code to an AWS CodeCommit repository and create pull requests to review proposed changes to the production code. When the pull request is merged into the master branch in the AWS CodeCommit repository, AWS CodePipeline automatically detects the changes to the branch and starts processing the code changes through the pipeline.</li> 
<li>AWS CodeBuild packages the code changes as well as any dependencies and builds a Docker image. Optionally, another pipeline stage tests the code and the package, also using AWS CodeBuild.</li> 
<li>The Docker image is pushed to Amazon ECR after a successful build and/or test stage.</li> 
<li>AWS CodePipeline invokes an AWS Lambda function that includes the Kubernetes Python client as part of the function’s resources. The Lambda function performs a string replacement on the tag used for the Docker image in the Kubernetes deployment file to match the Docker image tag applied in the build, one that matches the image in Amazon ECR.</li> 
<li>After the deployment manifest update is completed, AWS Lambda invokes the Kubernetes API to update the image in the Kubernetes application deployment.</li> 
<li>Kubernetes performs a rolling update of the pods in the application deployment to match the docker image specified in Amazon ECR.<br /> The pipeline is now live and responds to changes to the master branch of the CodeCommit repository. This pipeline is also fully extensible, you can add steps for performing testing or adding a step to deploy into a staging environment before the code ships into the production cluster.</li> 
</ol> 
<p>An example pipeline in AWS CodePipeline that supports this architecture can be seen below:</p> 
<p><img src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/11/codepipeline-k8s.png" /></p> 
<b>Conclusion</b> 
<p>We are excited to see how you leverage this pipeline to help ease your developer experience as you develop applications in Kubernetes.</p> 
<p>You’ll find an AWS CloudFormation template with everything necessary to spin up your own continuous deployment pipeline at the <a href="https://github.com/aws-samples/aws-kube-codesuite">CodeSuite – Continuous Deployment Reference Architecture for Kubernetes</a> repo on GitHub. The repository details exactly how the pipeline is provisioned and how you can use it to deploy your own applications. If you have any questions, feedback, or suggestions, please let us know!</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/03/HA_X-Ray_SOCIAL.png" /> 
<b class="lb-b blog-post-title" property="name headline">Aspect-Oriented Programming for AWS X-Ray Using Spring</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>This post was written by Andy Powell, Partner Solutions Architect.</p> 
<p>For developers, tracing and instrumenting code is one of the most valuable tools when debugging code. When you are developing locally, you can use local debugging and profiling tools, but when you deploy an application to the cloud, the task is more challenging. In this blog post, we will look at a new way to instrument your application using AWS X-Ray without adding tracing code to your business logic.</p> 
<b>AWS X-Ray</b> 
<p>Released to the public earlier this year, <a title="undefined" href="https://aws.amazon.com/xray/" target="null">AWS X-Ray</a> provides a mechanism for developers to instrument and trace their code while running on AWS. AWS X-Ray enables developers to analyze and debug distributed applications through the entire AWS stack. Developers and operations personnel can also follow the flow of a request through the entire AWS infrastructure. X-Ray works well in a monolithic or microservices model. Either way, developers can get a complete view of their application’s performance and behavior.</p> 
<p>X-Ray provides two key mechanisms for analyzing an application:</p> 
<li>The service map.</li> 
<li>The trace view.</li> 
<p>The service map, shown here, provides a high-level view of the services consumed by an application and their relative health:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/26/Picture1.png" /></p> 
<p>Application developers often look for a more detailed view of their application to help them answer questions like “Which functions are my bottlenecks?” and “Where is the most latency in the application?” The trace view allows you to see the flow of an application through service calls. &nbsp;It&nbsp;shows you latency between services and the exact execution time of each X-Ray segment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/27/Picture2.png" /></p> 
<p>Similar to other logging and metrics frameworks, X-Ray requires the developer to insert specific code throughout the application. This might cause issues because the logging and metrics code has the potential to increase the complexity of the application code. Increased complexity leads, in turn, to increased maintenance and testing costs. Ideally, developers should add X-Ray tracing to an application in a non-invasive manner, one that does not affect the underlying business logic.</p> 
<b>Aspect-oriented programming and the Spring Framework</b> 
<p>Aspect-oriented programming (AOP) is a mechanism by which code runs either before, after, or around a target function. The code that runs outside the target code is called an aspect. An aspect provides the ability to perform actions like logging, transaction management, and method retries. The goal is for the aspect to provide these capabilities without affecting the target code. Pointcuts define where these aspects should act in the code. AOP allows developers to leverage powerful functionality without affecting their business logic.</p> 
<p>An aspect-oriented approach is a perfect way to implement AWS X-Ray because it keeps the underlying code clean and provides non-invasive, reusable tracing logic.</p> 
<p>Simply creating an aspect to invoke X-Ray is not enough. We also have to create pointcuts that tell the aspect where to act. These pointcuts will define which methods we wrap with tracing logic. After they are defined, the aspect sends the entire call stack to X-Ray for visualization.</p> 
<p>The Spring Framework is a common application framework for the development of Java software. It provides an extensive programming and config method for modern Java applications.</p> 
<p>Spring provides facilities for a range of application functions, including web applications, messaging applications, and streaming data applications. Spring applications are capable of being cloud-native from the onset.</p> 
<p>AOP is one of the core components of the Spring Framework. Spring’s implementation of AOP “weaves” application code at runtime. Load-time weaving is also available, but it requires extra configuration at compile or application runtime to work properly. The Spring runtime weaving does not require any special compilation or agents.</p> 
<b>AWS X-Ray Spring extensions</b> 
<p>Starting with version 1.3.0, the AWS X-Ray SDK lets you use AOP in the Spring Framework to instrument code with no change to the application’s business logic. This means that there is now a non-invasive way to instrument your applications running remotely in AWS.</p> 
<p>To include the extension in the code, first add the dependency to the application. If you are using Maven, you add the dependency this way:</p> 
<code class="lang-xml">&lt;dependency&gt; 
&lt;groupId&gt;com.amazonaws&lt;/groupId&gt; 
&lt;artifactId&gt;aws-xray-recorder-sdk-spring&lt;/artifactId&gt; 
&lt;version&gt;1.3.0&lt;/version&gt; 
&lt;/dependency&gt;
</code> 
<p>If you are using Gradle, use the following syntax:</p> 
<code class="lang-xml">compile 'com.amazonaws:aws-xray-recorder-sdk-spring:1.3.0'</code> 
<p>After you’ve included this in your application, there are a couple of steps that need configuration before tracing is enabled. First, classes must either be annotated with the <em>@XRayEnabled</em> annotation, or implement the <em>XRayTraced</em> interface. This tells the AOP system to wrap the functions of the affected class for X-Ray instrumentation.</p> 
<p>Second, you need an interceptor to actually wrap the code. This involves extending an abstract class, <em>AbstractXRayInterceptor</em>, to activate X-Ray tracing in the application. The <em>AbstractXRayInterceptor</em> contains methods that must be overridden:</p> 
<li><em>generateMetadata</em> – This function allows customization of the metadata attached to the current function’s trace. &nbsp;By default, the class name of the executing function is recorded in the metadata. &nbsp;You can add more data if you need additional insights.</li> 
<li><em>xrayEnabledClasses</em> – This function is empty, and should remain so. &nbsp;It serves as the host for a pointcut instructing the interceptor about which methods to wrap. &nbsp; The developer should define the pointcut. &nbsp;&nbsp;You can specify which classes that are annotated with XRayEnabled you want traced. &nbsp;A pointcut statement of &nbsp;<em>@Pointcut(“@within(com.amazonaws.xray.spring.aop.XRayEnabled) &amp;&amp; bean(*Controller)”)</em>&nbsp;tells the interceptor to wrap all controller beans annotated with the <em>@XRayEnabled</em> annotation.</li> 
<p>Here is a sample implementation of the <em>AbstractXRayInterceptor</em> :</p> 
<code class="lang-java">@Aspect
@Component
public class XRayInspector extends AbstractXRayInterceptor {    
@Override    
protected Map&lt;String, Map&lt;String, Object&gt;&gt; generateMetadata(ProceedingJoinPoint proceedingJoinPoint, Subsegment subsegment) throws Exception {      
return super.generateMetadata(proceedingJoinPoint, subsegment);    
}    
@Override    
@Pointcut(&quot;@within(com.amazonaws.xray.spring.aop.XRayEnabled) &amp;&amp; bean(*Controller)&quot;)    
public void xrayEnabledClasses() {}
}
</code> 
<p>Here is an example of a Service class that will be instrumented by X-Ray:</p> 
<code class="lang-java">@Service
@XRayEnabled
public class MyServiceImpl implements MyService {    
private final MyEntityRepository myEntityRepository;    
@Autowired    
public MyServiceImpl(MyEntityRepository myEntityRepository) {        
this.myEntityRepository = myEntityRepository;    
}    
@Transactional(readOnly = true)    
public List&lt;MyEntity&gt; getMyEntities(){        
try(Stream&lt;MyEntity&gt; entityStream = this.myEntityRepository.streamAll()){            
return entityStream.sorted().collect(Collectors.toList());        
}    
}
}
</code> 
<p>By default, the <em>AbstractXRayInterceptor</em> instruments around all Spring data repository instances.</p> 
<p>After it’s configured, the Spring application runs as normal. The X-Ray interceptor&nbsp; picks up annotated classes automatically and builds a trace of the call stack. You can view this call stack in the Traces section of the X-Ray console.</p> 
<p><code><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/27/Picture3.png" /></code></p> 
<p>Looking at the trace, you will see the complete call stack of the application, from the controller down through the service calls. Stack traces are arranged in a hierarchy in the same way as typical X-Ray traces. Any exceptions that occur in the call stack are added to the trace by the interceptor automatically. This gives you a complete view of the application’s functionality, performance, and error states. &nbsp;X-Ray provides the convenience of a managed service of the tracing and reporting engine. &nbsp;Without it, the developer would have to manage the tracing and reporting infrastructure.</p> 
<b>Conclusion</b> 
<p>On its own, X-Ray provides powerful functionality to trace your applications running on AWS. When you combine the service with the ease of use of AOP and the Spring Framework, it is a natural fit.</p> 
<p>The demo app code is available for <a href="https://github.com/aws/aws-xray-sdk-java" title="undefined" target="null">download</a>. Download it today and start integrating deep tracing into your Spring applications.</p> 
<h4>Note</h4> 
<p><a href="https://github.com/aws/aws-xray-sdk-java" title="undefined" target="null">AWS X-Ray SDK for Java</a> v1.3.0 will be available on maven central in the next few days. In the meantime, you can follow the below steps to use the latest version from the GitHub repo:</p> 
<p>1. Clone the AWS X-Ray SDK for Java repo locally.<br /> 2. In the project root, run “mvn clean install -Dgpg.skip=true”.<br /> 3. Reference version 1.3.0 in your maven / gradle files as mentioned in the blog post.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2002');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Instrumenting Web Apps Using AWS X-Ray</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>This post was written by James Bowman, Software Development Engineer, AWS X-Ray</p> 
<p><a title="undefined" href="https://aws.amazon.com/xray/" target="null">AWS X-Ray</a> helps developers analyze and debug distributed applications and underlying services in production. You can identify and analyze root-causes of performance issues and errors, understand customer impact, and extract statistical aggregations (such as histograms) for optimization.</p> 
<p>In this blog post, I will provide a step-by-step walkthrough for enabling X-Ray tracing in the Go programming language. You can use these steps to add X-Ray tracing to any distributed application.</p> 
<b>Revel: A web framework for the Go language</b> 
<p>This section will assist you with designing a guestbook application. Skip to <strong>“Instrumenting with AWS X-Ray”</strong> section below if you already have a Go language application.</p> 
<p><a title="undefined" href="https://revel.github.io/" target="null">Revel</a> is a web framework for the Go language. It facilitates the rapid development of web applications by providing a predefined framework for controllers, views, routes, filters, and more.</p> 
<p>To get started with Revel, run <code>revel new github.com/jamesdbowman/guestbook</code>. A project base is then copied to <code>$GOPATH/src/github.com/jamesdbowman/guestbook</code>.</p> 
<p><code>$ tree -L 2<br /> .<br /> ├── README.md<br /> ├── app<br /> │ ├── controllers<br /> │ ├── init.go<br /> │ ├── routes<br /> │ ├── tmp<br /> │ └── views<br /> ├── conf<br /> │ ├── app.conf<br /> │ └── routes<br /> ├── messages<br /> │ └── sample.en<br /> ├── public<br /> │ ├── css<br /> │ ├── fonts<br /> │ ├── img<br /> │ └── js<br /> └── tests<br /> └── apptest.go<br /> </code></p> 
<h3>Writing a guestbook application</h3> 
<p>A basic guestbook application can consist of just two routes: one to sign the guestbook and another to list all entries.<br /> Let’s set up these routes by adding a Book controller, which can be routed to by modifying <code>./conf/routes.</code></p> 
<code class="lang-go">./app/controllers/book.go:
package controllers
import (
&quot;math/rand&quot;
&quot;time&quot;
&quot;github.com/aws/aws-sdk-go/aws&quot;
&quot;github.com/aws/aws-sdk-go/aws/endpoints&quot;
&quot;github.com/aws/aws-sdk-go/aws/session&quot;
&quot;github.com/aws/aws-sdk-go/service/dynamodb&quot;
&quot;github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute&quot;
&quot;github.com/bowmessage/test/xray&quot;
&quot;github.com/revel/revel&quot;
)
const tableName = &quot;guestbook&quot;
const success = &quot;Success.\n&quot;
var letters = []rune(&quot;ABCDEFGHIJKLMNOPQRSTUVWXYZ&quot;)
func init() {
rand.Seed(time.Now().UnixNano())
}
// randString returns a random string of len n, used for DynamoDB Hash key.
func randString(n int) string {
b := make([]rune, n)
for i := range b {
b[i] = letters[rand.Intn(len(letters))]
}
return string(b)
}
// Book controls interactions with the guestbook.
type Book struct {
*revel.Controller
ddbClient *dynamodb.DynamoDB
}
// Signature represents a user's signature.
type Signature struct {
Message string
Epoch   int64
ID      string
}
// ddb returns the controller's DynamoDB client, instatiating a new client if necessary.
func (c Book) ddb() *dynamodb.DynamoDB {
if c.ddbClient == nil {
sess := session.Must(session.NewSession(&amp;aws.Config{
Region:     aws.String(endpoints.UsWest2RegionID),
MaxRetries: aws.Int(3),
}))
c.ddbClient = dynamodb.New(sess)
xray.AWS(c.ddbClient.Client) // add subsegment-generating X-Ray handlers to this client
}
return c.ddbClient
}
// Sign allows users to sign the book.
// The message is to be passed as application/json typed content, listed under the &quot;message&quot; top level key.
func (c Book) Sign() revel.Result {
var s Signature
err := c.Params.BindJSON(&amp;s)
if err != nil {
return c.RenderError(err)
}
now := time.Now()
s.Epoch = now.Unix()
s.ID = randString(20)
item, err := dynamodbattribute.MarshalMap(s)
if err != nil {
return c.RenderError(err)
}
putItemInput := &amp;dynamodb.PutItemInput{
TableName: aws.String(tableName),
Item:      item,
}
goRequest := c.Request.In.(*revel.GoRequest)
_, err = c.ddb().PutItemWithContext(goRequest.Original.Context(), putItemInput)
if err != nil {
return c.RenderError(err)
}
return c.RenderText(success)
}
// List allows users to list all signatures in the book.
func (c Book) List() revel.Result {
scanInput := &amp;dynamodb.ScanInput{
TableName: aws.String(tableName),
Limit:     aws.Int64(100),
}
goRequest := c.Request.In.(*revel.GoRequest)
res, err := c.ddb().ScanWithContext(goRequest.Original.Context(), scanInput)
if err != nil {
return c.RenderError(err)
}
messages := make([]string, 0)
for _, v := range res.Items {
messages = append(messages, *(v[&quot;Message&quot;].S))
}
return c.RenderJSON(messages)
}
</code> 
<p><code>./conf/routes:<br /> POST /sign Book.Sign<br /> GET /list Book.List<br /> </code></p> 
<h3>Creating the resources and testing</h3> 
<p>For the purposes of this blog post, the application will be run and tested locally. We will store and retrieve messages from an <a title="undefined" href="https://aws.amazon.com/dynamodb/" target="null">Amazon DynamoDB</a> table. Use the following AWS CLI command to create the guestbook table:</p> 
<code class="lang-bash">aws dynamodb create-table --region us-west-2 --table-name &quot;guestbook&quot; --attribute-definitions AttributeName=ID,AttributeType=S AttributeName=Epoch,AttributeType=N --key-schema AttributeName=ID,KeyType=HASH AttributeName=Epoch,KeyType=RANGE --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5</code> 
<p>Now, let’s test our sign and list routes. If everything is working correctly, the following result appears:</p> 
<code class="lang-bash">$ curl -d '{&quot;message&quot;:&quot;Hello from cURL!&quot;}' -H &quot;Content-Type: application/json&quot; http://localhost:9000/book/sign
Success.
$ curl http://localhost:9000/book/list
[
&quot;Hello from cURL!&quot;
]%
</code> 
<b>Integrating with AWS X-Ray</b> 
<h3>Download and run the AWS X-Ray daemon</h3> 
<p>The AWS SDKs emit trace segments over UDP on port 2000. (This port can be configured.) In order for the trace segments to make it to the X-Ray service, the daemon must listen on this port and batch the segments in calls to the PutTraceSegments API.<br /> For information about downloading and running the X-Ray daemon, see the <a title="undefined" href="http://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html" target="null">AWS X-Ray Developer Guide</a>.</p> 
<h3>Installing the AWS X-Ray SDK for Go</h3> 
<p>To download the SDK from GitHub, run <code>go get -u github.com/aws/aws-xray-sdk-go/...</code> The SDK will appear in the <code>$GOPATH</code>.</p> 
<h3>Enabling the incoming request filter</h3> 
<p>The first step to instrumenting an application with AWS X-Ray is to enable the generation of trace segments on incoming requests. The SDK conveniently provides an implementation of <code>http.Handler</code> which does exactly that. To ensure incoming web requests travel through this handler, we can modify <code>app/init.go</code>, adding a custom function to be run on application start.</p> 
<code class="lang-go">import (
&quot;github.com/aws/aws-xray-sdk-go/xray&quot;
&quot;github.com/revel/revel&quot;
)
...
func init() {
...
revel.OnAppStart(installXRayHandler)
}
func installXRayHandler() {
server := revel.CurrentEngine.Engine().(*http.Server)
server.Handler = xray.Handler(xray.NewFixedSegmentNamer(&quot;GuestbookApp&quot;), server.Handler)
}
</code> 
<p>The application will now emit a segment for each incoming web request. The service graph appears:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/28/Screen-Shot-2017-12-21-at-3.00.24-PM-1024x508.png" /></p> 
<p>You can customize the name of the segment to make it more descriptive by providing an alternate implementation of <code>SegmentNamer</code> to <code>xray.Handler</code>. For example, you can use <code>xray.NewDynamicSegmentNamer(fallback, pattern)</code> in place of the fixed namer. This namer will use the host name from the incoming web request (if it matches <code>pattern</code>) as the segment name. This is often useful when you are trying to separate different instances of the same application.</p> 
<p>In addition, HTTP-centric information such as method and URL is collected in the segment’s <code>http</code> subsection:</p> 
<code class="lang-json">&quot;http&quot;: {
&quot;request&quot;: {
&quot;url&quot;: &quot;/book/list&quot;,
&quot;method&quot;: &quot;GET&quot;,
&quot;user_agent&quot;: &quot;curl/7.54.0&quot;,
&quot;client_ip&quot;: &quot;::1&quot;
},
&quot;response&quot;: {
&quot;status&quot;: 200
}
},
</code> 
<h3>Instrumenting outbound calls</h3> 
<p>To provide detailed performance metrics for distributed applications, the AWS X-Ray SDK needs to measure the time it takes to make outbound requests. Trace context is passed to downstream services using the <code>X-Amzn-Trace-Id</code> header. To draw a detailed and accurate representation of a distributed application, outbound call instrumentation is required.</p> 
<h3>AWS SDK calls</h3> 
<p>The AWS X-Ray SDK for Go provides a one-line AWS client wrapper that enables the collection of detailed per-call metrics for any AWS client. We can modify the DynamoDB client instantiation to include this line:</p> 
<code class="lang-go">// ddb returns the controller's DynamoDB client, instatiating a new client if necessary.
func (c Book) ddb() *dynamodb.DynamoDB {
if c.ddbClient == nil {
sess := session.Must(session.NewSession(&amp;aws.Config{
Region: aws.String(endpoints.UsWest2RegionID),
}))
c.ddbClient = dynamodb.New(sess)
xray.AWS(c.ddbClient.Client) // add subsegment-generating X-Ray handlers to this client
}
return c.ddbClient
}
</code> 
<p>We also need to ensure that the segment generated by our <code>xray.Handler</code> is passed to these AWS calls so that the X-Ray SDK knows to which segment these generated subsegments belong. In Go, the <code>context.Context</code> object is passed throughout the call path to achieve this goal. (In most other languages, some variant of <code>ThreadLocal</code> is used.) AWS clients provide a <code>*WithContext</code> method variant for each AWS operation, which we need to switch to:</p> 
<code class="lang-go">_, err = c.ddb().PutItemWithContext(c.Request.Context(), putItemInput)
res, err := c.ddb().ScanWithContext(c.Request.Context(), scanInput)
</code> 
<p>We now see much more detail in the <strong>Timeline</strong> view of the trace for the sign and list operations:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/28/Screen-Shot-2017-12-21-at-3.02.51-PM-1024x722.png" /></p> 
<p>We can use this detail to help diagnose throttling on our DynamoDB table. In the following screenshot, the purple in the DynamoDB service graph node indicates that our table is underprovisioned. The red in the GuestbookApp node indicates that the application is throwing faults due to this throttling.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/28/Screen-Shot-2017-12-21-at-3.19.16-PM-1024x326.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/28/Screen-Shot-2017-12-21-at-3.20.17-PM-1024x709.png" /></p> 
<h3>HTTP calls</h3> 
<p>Although the guestbook application does not make any non-AWS outbound HTTP calls in its current state, there is a similar one-liner to wrap HTTP clients that make outbound requests. <code>xray.Client(c *http.Client)</code> wraps an existing <code>http.Client</code> (or <code>nil</code> if you want to use a default HTTP client). For example:</p> 
<code class="lang-go">resp, err := ctxhttp.Get(ctx, xray.Client(nil), &quot;https://aws.amazon.com/&quot;)</code> 
<h3>Instrumenting local operations</h3> 
<p>X-Ray can also assist in measuring the performance of local compute operations. To see this in action, let’s create a custom subsegment inside the <code>randString</code> method:</p> 
<code class="lang-go">
// randString returns a random string of len n, used for DynamoDB Hash key.
func randString(ctx context.Context, n int) string {
xray.Capture(ctx, &quot;randString&quot;, func(innerCtx context.Context) {
b := make([]rune, n)
for i := range b {
b[i] = letters[rand.Intn(len(letters))]
}
s := string(b)
})
return s
}
// we'll also need to change the callsite
s.ID = randString(c.Request.Context(), 20)
</code> 
<b>Summary</b> 
<p>By now, you are an expert on how to instrument X-Ray for your Go applications. Instrumenting X-Ray with your applications is an easy way to analyze and debug performance issues and understand customer impact. Please feel free to give any feedback or comments below.</p> 
<p>For more information about advanced configuration of the AWS X-Ray SDK for Go, see the <a title="undefined" href="http://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-go.html" target="null">AWS X-Ray SDK for Go</a> in the AWS X-Ray Developer Guide and the <a title="undefined" href="https://github.com/aws/aws-xray-sdk-go" target="null">aws/aws-xray-sdk-go GitHub repository</a>.</p> 
<p>For more information about some of the advanced X-Ray features such as histograms, annotations, and filter expressions, see the <a title="undefined" href="https://aws.amazon.com/blogs/compute/analyzing-performance-for-amazon-rekognition-apps-written-on-aws-lambda-using-aws-x-ray/" target="null">Analyzing Performance for Amazon Rekognition Apps Written on AWS Lambda Using AWS X-Ray</a> blog post.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/03/architecture-1-1120x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Using Amazon CloudWatch and Amazon SNS to Notify when AWS X-Ray Detects Elevated Levels of Latency, Errors, and Faults in Your Application</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p><a href="https://aws.amazon.com/xray/">AWS X-Ray</a> helps developers analyze and debug production applications built using microservices or serverless architectures and quantify customer impact. With X-Ray, you can understand how your application and its underlying services are performing and identify and troubleshoot the root cause of performance issues and errors. You can use these insights to identify issues and opportunities for optimization.</p> 
<p>In this blog post, I will show you how you can use <a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch</a> and <a href="https://aws.amazon.com/sns/">Amazon SNS</a> to get notified when X-Ray detects high latency, errors, and faults in your application. Specifically, I will show you how to use this <a href="https://github.com/aws-samples/aws-xray-cloudwatch-event">sample app</a> to get notified through an email or SMS message when your end users observe high latencies or server-side errors when they use your application. You can customize the alarms and events by updating the sample app code.</p> 
<b>Sample App Overview</b> 
<p>The sample app uses the X-Ray GetServiceGraph API to get the following information:</p> 
<li>Aggregated response time.</li> 
<li>Requests that failed with 4xx status code (errors).</li> 
<li>429 status code (throttle).</li> 
<li>5xx status code (faults).</li> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/architecture-1024x576.png" /> 
<p class="wp-caption-text">Overview of sample app architecture</p> 
<h3>Getting started</h3> 
<p>The sample app uses AWS CloudFormation to deploy the required resources.<br /> To install the sample app:</p> 
<ol> 
<li>Run git clone to get the sample app.</li> 
<li>Update the JSON file in the Setup folder with threshold limits and notification details.</li> 
<li>Run the install.py script to install the sample app.</li> 
</ol> 
<p>For more information about the installation steps, see the <a title="undefined" href="https://github.com/aws-samples/aws-xray-cloudwatch-event/blob/master/README.md" target="null">readme</a> file on GitHub.</p> 
<p>You can update the app configuration to include your phone number or email to get notified when your application in X-Ray breaches the latency, error, and fault limits you set in the configuration. If you prefer to not provide your phone number and email, then you can use the CloudWatch alarm deployed by the sample app to monitor your application in X-Ray.</p> 
<p>The sample app deploys resources with the sample app namespace you provided during setup. This enables you to have multiple sample apps in the same region.</p> 
<h3>CloudWatch rules</h3> 
<p>The sample app uses two CloudWatch rules:</p> 
<ol> 
<li><em>SCHEDULEDLAMBDAFOR-sample_app_name</em> to trigger at regular intervals the AWS Lambda function that queries the GetServiceGraph API.</li> 
<li><em>XRAYALERTSFOR-sample_app_name</em> to look for published CloudWatch events that match the pattern defined in this rule.</li> 
</ol> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/CloudWatch-Rules-1024x255.png" /> 
<p class="wp-caption-text">CloudWatch rules created for the sample app</p> 
<h3>CloudWatch alarms</h3> 
<p>If you did not provide your phone number or email in the JSON file, the sample app uses a CloudWatch alarm named <em>XRayCloudWatchAlarm-sample_app_name</em> in combination with the CloudWatch event that you can use for monitoring.</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/CloudWatch-Alarm-1024x265.png" /> 
<p class="wp-caption-text">CloudWatch alarm created for the sample app</p> 
<h3>Amazon SNS messages</h3> 
<p>The sample app creates two SNS topics:</p> 
<li><em>sample_app_name-cloudwatcheventsnstopic</em> to send out an SMS message when the CloudWatch event matches a pattern published from the Lambda function.</li> 
<li><em>sample_app_name-cloudwatchalarmsnstopic</em> to send out an email message when the CloudWatch alarm goes into an ALARM state.</li> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/Amazon-SNS.png" /> 
<p class="wp-caption-text">Amazon SNS created for the sample app</p> 
<b>Getting notifications</b> 
<p>The CloudWatch event looks for the following matching pattern:<br /> <code></code></p> 
<code class="lang-json">{
&quot;detail-type&quot;: [
&quot;XCW Notification for Alerts&quot;
],
&quot;source&quot;: [
&quot;&lt;sample_app_name&gt;-xcw.alerts&quot;
]
}
</code> 
<p>The event then invokes an SNS topic that sends out an SMS message.</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/sms-169x300.png" /> 
<p class="wp-caption-text">SMS that is sent when CloudWatch Event invokes Amazon SNS topic</p> 
<p>The CloudWatch alarm looks for the <em><a title="undefined" href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cwe-metricscollected.html" target="null">TriggeredRules</a></em> metric that is published whenever the CloudWatch event matches the event pattern. It goes into the <em>ALARM</em> state whenever <em>TriggeredRules &gt; 0</em> for the specified evaluation period and invokes an SNS topic that sends an email message.</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/email-1024x488.png" /> 
<p class="wp-caption-text">Email that is sent when CloudWatch Alarm goes to ALARM state</p> 
<b>Stopping notifications</b> 
<p>If you provided your phone number or email address, but would like to stop getting notified, change the <em>SUBSCRIBE_TO_EMAIL_SMS</em> environment variable in the Lambda function to <strong>No</strong>. Then, go to the Amazon SNS console and delete the subscriptions. You can still monitor your application for elevated levels of latency, errors, and faults by using the CloudWatch console.</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/lambdaenvironmentvariable-1024x160.png" /> 
<p class="wp-caption-text">Change environment variable in Lambda</p> 
<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/12/19/deletesubscriptions-1024x228.png" /> 
<p class="wp-caption-text">Delete subscriptions to stop getting notified</p> 
<b>Uninstalling the sample app</b> 
<p>To uninstall the sample app, run the <em>uninstall.py</em> script in the Setup folder.</p> 
<b>Extending the sample app</b> 
<p>The sample app notifes you when when X-Ray detects high latency, errors, and faults in your application. You can extend it to provide more value for your use cases (for example, to perform an action on a resource when the state of a CloudWatch alarm changes).</p> 
<p>To summarize, after this set up you will be able to get notified through Amazon SNS when X-Ray detects high latency, errors and faults in your application.</p> 
<p>I hope you found this information about setting up alarms and alerts for your application in AWS X-Ray helpful. Feel free to leave questions or other feedback in the comments. Feel free to learn more about <a href="https://aws.amazon.com/xray/">AWS X-Ray</a>, <a href="https://aws.amazon.com/sns/">Amazon SNS</a> and <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch</a></p> 
<h3>About the Author</h3> 
<p><a title="undefined" href="https://www.linkedin.com/in/bharathkumarvenkateshkumar/" target="null">Bharath Kumar</a> is a Sr.Product Manager with AWS X-Ray. He has developed and launched mobile games, web applications on microservices and serverless architecture.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2018/01/03/custom-source-actions.png" /> 
<b class="lb-b blog-post-title" property="name headline">Using Custom Source Actions in AWS CodePipeline for Increased Visibility for Third-Party Source Control</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>In our previous post, <a href="https://aws.amazon.com/blogs/devops/integrating-git-with-aws-codepipeline/"><em>Integrating Git with AWS CodePipeline</em></a>, we demonstrated one way to integrate third-party Git repositories with <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> by using <a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a>, <a href="https://aws.amazon.com/lambda/">AWS Lambda</a>, and <a href="https://aws.amazon.com/s3/">Amazon S3</a>. That approach allows you to quickly integrate your Git repository with CodePipeline, but it doesn’t provide CodePipeline with any of the source metadata that many customers use in their CI/CD pipelines.</p> 
<p>In this post, we will describe CodePipeline custom source actions, which offer a different strategy for providing CodePipeline with more metadata from your source repositories. The most common source metadata are commit identifiers and commit messages. Commit identifiers are frequently used to track changes throughout the software lifecycle while commit messages provide a succinct, human-readable description of the change. Custom source actions allow you to integrate CodePipeline with any source repository in the same way that CodePipeline integrates with <a href="https://aws.amazon.com/codecommit/">CodeCommit</a> and GitHub, giving you access to the commit identifier and the commit message.</p> 
<p>This post covers setting up API Gateway and Lambda to trigger your pipeline, configuring your pipeline with a custom source action, and building a worker to handle jobs from your custom source action. This architecture allows you to access your source providers that are either hosted in a <a href="https://aws.amazon.com/vpc/">VPC</a> or are on premise and accessible from a VPC.</p> 
<h3>Architecture overview</h3> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/11/25/source_action_blog.png" /></p> 
<p>A webhook is a user-defined HTTP callback that occurs in response to an event. In our case, webhooks occur in response to a change in a source repository – a new revision. Webhooks have become a standard mechanism for integrating source repositories with continuous integration tools, such as CodePipeline.</p> 
<p>Here, the webhook is a call to API Gateway and AWS Lambda. The Lambda function calls the <a href="http://docs.aws.amazon.com/codepipeline/latest/APIReference/API_StartPipelineExecution.html">StartPipelineExecution </a>API, which triggers a CodePipeline execution. That execution starts with a custom source action that issues a job. That job is picked up by our worker, which pulls the latest source revision, extracts metadata, and publishes a new CodePipeline artifact for the rest of the pipeline to consume.</p> 
<p>Our example targets a Git repository (in this case, GitHub Enterprise). Although there are multiple methods for retrieving the contents of a Git repo, we do a simple git pull authenticated with SSH. Our example assumes your Git repository is accessible from your VPC. If that is not the case, remove the VpcConfig from the GitPullS3 Lambda function to give it access to internet resources.</p> 
<h3>Build the required AWS resources</h3> 
<p>For your convenience, there is an <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> template that includes the AWS infrastructure and configuration required to build out this integration. It includes API Gateway, Lambda functions, and a simple pipeline. To launch the AWS CloudFormation stack setup wizard, choose the link for your region. The following AWS regions support all of the services required for this integration:</p> 
<table style="height: 159px" width="268"> 
<tbody> 
<tr> 
<td style="text-align: left">N. Virginia:</td> 
<td style="text-align: center"><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=CustomSourceActionDemo&amp;templateURL=https://custom-source-action-blog-us-east-1.s3.amazonaws.com/cloudformation.yaml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a></td> 
</tr> 
<tr> 
<td style="text-align: left">Oregon:</td> 
<td><a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=CustomSourceActionDemo&amp;templateURL=https://custom-source-action-blog-us-west-2.s3.amazonaws.com/cloudformation.yaml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a></td> 
</tr> 
<tr> 
<td style="text-align: left">Ireland:</td> 
<td><a href="https://console.aws.amazon.com/cloudformation/home?region=eu-west-1#/stacks/new?stackName=CustomSourceActionDemo&amp;templateURL=https://custom-source-action-blog-eu-west-1.s3.amazonaws.com/cloudformation.yaml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a></td> 
</tr> 
</tbody> 
</table> 
<p>For a list of AWS services and the regions in which they are available, see <a href="http://docs.aws.amazon.com/general/latest/gr/rande.html">AWS Regions and Endpoints</a>.</p> 
<p>The stack setup wizard prompts you to enter several parameters. Many of these values must be obtained from your GitHub Enterprise service.</p> 
<p><strong>OutputBucketName</strong>: The bucket name for CodePipeline artifacts.</p> 
<p><strong>BranchName</strong>: The branch you want to use in the pipeline.</p> 
<p><strong>GitUrl</strong>: The HTTPS URL of the Git repository you want to clone. If your source repository isn’t exposed to the internet, make sure that you use the private IP address of your source repository.</p> 
<p><strong>ApiSecret</strong>: Webhook secrets for use with GitHub Enterprise and GitLab.</p> 
<p><strong>SourceActionVersion</strong>: The version of the custom source action to use. Because a custom action version cannot be deleted, you must increment this version every time you re-create the stack in a single account.</p> 
<p><strong>GitPullLambdaVPC</strong>: The VPC in which your Lambda function exists. This VPC must have access to the source repository and the public internet for access to CodePipeline.</p> 
<p><strong>GitPullLambdaSubnet</strong>: The subnet in which your Lambda function exists. This subnet must have access to your source repository and be private (that is, no internet gateway) with NAT access to the internet.</p> 
<p>After you have entered values for these parameters, you can complete the steps in the wizard and start the stack creation. If your values change, you can use the update stack functionality in CloudFormation to modify your parameters. When the stack creation is complete, make a note of the WebhookEndpoint and PublicSSHKey. You need these values in the following steps.</p> 
<h3>Configure the source repository</h3> 
<ol> 
<li>Sign in to GitHub Enterprise and navigate to the source repository.</li> 
<li>Choose the Settings tab, and then choose Webhooks.</li> 
<li>Choose Add Webhook.</li> 
<li>In Payload URL, enter the WebhookEndpoint value. In Secret, enter your webhook secret.</li> 
<li>Choose Add Webhook.</li> 
<li>Go to your user settings and choose SSH and GPG Keys. Add a new SSH key with the PublicSSHKey value from AWS CloudFormation.</li> 
</ol> 
<h3>Test a commit and clean up resources</h3> 
<p>After you have set up the webhook, push a new commit. In a few minutes, you should see a new execution passing through your pipeline with the correct revision ID and commit message.</p> 
<p>To clean up resources used in this solution, delete the AWS CloudFormation stack.</p> 
<h3>Conclusion</h3> 
<p>We hope you find this blog post useful for injecting source control metadata into your CI/CD pipeline. As always, chime in with your thoughts and suggestions in the comments.</p> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
