<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/mgmtblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Management Tools Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Management Tools Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="aws-blogs" class="layout-inner aws-blogs">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li class="active"><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="mgmtblogs1.html">Page 1</a>|<a href="mgmtblogs2.html">Page 2</a>|<a href="mgmtblogs3.html">Page 3</a>|<a href="mgmtblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/Ha_EC2-Management-tools_SOCIAL.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Enable Modular and Reusable Configuration Using Composite AWS Systems Manager Documents</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p><em>By Melonia Mendonca, Software Development Engineer at Amazon Web Services</em></p> 
<p>AWS Systems Manager (SSM) documents enable infrastructure as code that allows you to configure, manage, and automate your AWS and on-premises resources using AWS Systems Manager services. These SSM Documents define the actions that you want to perform on managed instances. Systems Manager offers a variety of pre-defined public documents and provides the ability to customize the documents as well.</p> 
<p>Systems Manager lets you <a href="https://aws.amazon.com/about-aws/whats-new/2017/10/amazon-ec2-systems-manager-now-integrates-with-github/">execute composite documents</a> as part of your configurations. Composite documents are SSM documents that perform the task of executing one or more secondary documents. Using composite documents, you can do the following:</p> 
<li><span style="text-decoration: underline">Enable modularity</span> – Documents can be broken into small common tasks, and then they can be called from composite documents.</li> 
<li><span style="text-decoration: underline">Enable reusability</span> – Having the ability to invoke other documents removes the requirement to duplicate the plugin calls within custom documents.</li> 
<li><span style="text-decoration: underline">Store documents in remote locations</span>– GitHub and Amazon S3 can be used to save documents because these documents in remote locations can be invoked using composite documents.</li> 
<li><span style="text-decoration: underline">Use YAML in place of JSON in documents</span> – AWS Systems Manager also allows the use of documents written in YAML format that are stored in remote locations.</li> 
<p>AWS-RunDocument is a new document you can use to execute documents that are stored in Systems Manager, private or public GitHub, or Amazon S3. This is achieved by using <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-plugins.html#aws-downloadContent">aws:downloadContent</a> and <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-plugins.html#aws-rundocument">aws:runDocument</a> plugins. The aws:runDocument plugin executes documents that reside in Systems Manager or in the local path.</p> 
<p>In this blog post, I’ll show you how to create custom composite documents using the aws:runDocument plugin. I’ll demonstrate how to execute documents created in Systems Manager by invoking them from the composite document. I’ll also walk through the method of executing documents that are in GitHub by using the AWS-RunDocument document.</p> 
<p><span id="more-2450"></span></p> 
<h3>Walkthrough 1 – Compose a custom document using SSM documents</h3> 
<p>In this example, I’ll show you how to compose a document that updates SSM Agent and then runs baseline patch and eventually configures an AWS package. Before the introduction of composite documents, this could be done one of two ways. You could send three separate commands to execute these public documents – AWS-UpdateSSMAgent, AWS-RunPatchBaseline, and AWS-ConfigureAWSPackage. &nbsp;Alternatively, you could add the plugin references in another document, but you would have to duplicate the document and not use the pre-defined, public document provided. With the use of composite documents, these documents are executed by sending a single composite document.</p> 
<p><strong>Step 1: Create the composite document</strong><br /> I’ll use the aws:runDocument plugin to compose plugins in my document. The plugin has the following parameters:</p> 
<li><span style="text-decoration: underline">documentType</span> – This indicates the type of document. The document could be locally available – LocalPath or from SSM – SSMDocument. In this example, this parameter will be SSMDocument.</li> 
<li><span style="text-decoration: underline">documentPath</span> – This is the path to the document. It is either the absolute path to the document on disk, or the name of the document if the type is SSMDocument. In this walkthrough, we will use the path to specify the names of the documents – AWS-UpdateSSMAgent, AWS-RunPatchBaseline, and AWS-ConfigureAWSPackage.</li> 
<li><span style="text-decoration: underline">documentParameters</span> – The parameters for the secondary document. This should be specified as parameter type StringMap with key and value pairs.</li> 
<p>The document appears as follows. I have named it composite-document.json. This document can be executed on Windows.</p> 
<code class="lang-json"><strong>~/tmp/docs/composite-document.json</strong> 
{
&quot;schemaVersion&quot;: &quot;2.2&quot;,
&quot;description&quot;: &quot;Compose SSM documents&quot;,
&quot;parameters&quot;: {},
&quot;mainSteps&quot;: [
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;AgentUpdate&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-UpdateSSMAgent&quot;
}
},
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;ApplyPatchBaseline&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-RunPatchBaseline&quot;,
&quot;documentParameters&quot;: &quot;{\&quot;Operation\&quot;:\&quot;Install\&quot;}&quot;
}
},
{
&quot;action&quot;:&quot;aws:runDocument&quot;,
&quot;name&quot; : &quot;ConfigureAWSPVDriver&quot;,
&quot;inputs&quot;:{
&quot;documentType&quot;:&quot;SSMDocument&quot;,
&quot;documentPath&quot;:&quot;AWS-ConfigureAWSPackage&quot;,
&quot;documentParameters&quot;: &quot;{\&quot;action\&quot;:\&quot;Install\&quot;, \&quot;name\&quot;:\&quot;AWSPVDriver\&quot;}&quot;
}
}
]
}</code> 
<p>You can navigate to the AWS Systems Manager console and paste this document to create a document. You can also do this using the <a href="http://docs.aws.amazon.com/cli/latest/reference/ssm/create-document.html">create-document</a> command in the AWS CLI:</p> 
<code class="lang-bash">aws ssm create-document --name composite-document --content file://~/tmp/docs/composite-document.json --document-type Command</code> 
<p><strong>Step 2: Run the command and check the output</strong></p> 
<p>After the command is sent, each action will be executed to produce the following output.</p> 
<p><strong><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/walkthru1-step2.png" /></strong></p> 
<p>Or you can use the <a href="http://docs.aws.amazon.com/cli/latest/reference/ssm/send-command.html">send-command</a> command from the AWS CLI. I am targeting all my Windows value instances by using tagging. To do this I created a new tag – PlatformType. For more information on using tags when you send commands, see <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/send-commands-multiple.html">Sending Commands to a Fleet</a> in the AWS Systems Manager documentation.</p> 
<code class="lang-bash">aws ssm send-command --document-name &quot;composite-document&quot; --targets Key=tag:PlatformType,Values=Windows</code> 
<h3>Walkthrough 2 – Compose an SSM custom document using documents from GitHub</h3> 
<p>In a <a href="https://aws.amazon.com/blogs/mt/run-scripts-stored-in-private-or-public-github-repositories-using-amazon-ec2-systems-manager/">previous blog post</a>, I reviewed the method of executing scripts from GitHub. Today, I’ll demonstrate how to execute Systems Manager documents from GitHub. This method can be used to execute documents from Amazon S3, too. In this composite document, I will update SSM Agent to the latest version and then install Apache and My SQL. The document to install this software resides in GitHub and is written in YAML.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/walkthru-github.png" /></p> 
<p><strong>Step 1:&nbsp; Create composite document</strong><br /> To create a document from a remote resource like GitHub, I’ll use the pre-defined AWS-RunDocument document. This document has the following parameters to help us specify the information to download the document.</p> 
<li><span style="text-decoration: underline">Source Type</span>: Location of the script – GitHub or Amazon S3. For this example, the value here is GitHub.</li> 
<li><span style="text-decoration: underline">Source Info</span>: This parameter provides information for accessing the document. The repository mentioned earlier is public and thus, this should be the owner, repository, and the path to the document.</li> 
<li><span style="text-decoration: underline">Document Parameters</span>: If the document to be executed takes parameters, they can be specified here. In this case, it will be left blank.</li> 
<p>You can use the composite document below to install the SSM Agent, Apache and MySQL on the instance.</p> 
<code class="lang-json"><strong>~/tmp/docs/composite-document-mysql.json</strong> 
{
&quot;schemaVersion&quot;: &quot;2.2&quot;,
&quot;description&quot;: &quot;Composite documents from GitHub&quot;,
&quot;parameters&quot;: {
},
&quot;mainSteps&quot;: [
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;AgentUpdate&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-UpdateSSMAgent&quot;
}
},
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;InstallApacheandMySQL&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-RunDocument&quot;,
&quot;documentParameters&quot; : &quot;{\&quot;sourceType\&quot;:\&quot;GitHub\&quot;, \&quot;sourceInfo\&quot;:\&quot;{\\\&quot;owner\\\&quot;:\\\&quot;mmendonca3\\\&quot;,\\\&quot;repository\\\&quot;:\\\&quot;dummy-test-public\\\&quot;, \\\&quot;path\\\&quot;:\\\&quot;documents/bootstrap/StateManagerBootstrap.yml\\\&quot;}\&quot;}&quot;
}
}
]
}</code> 
<p>You can create it using the AWS Systems Manager console or AWS CLI. The command to create this document would be:</p> 
<code class="lang-bash">aws ssm create-document --name composite-document-mysql --content file://~/tmp/docs/composite-document-mysql.json --document-type Command</code> 
<p><strong>Step 2: Run Command and check the output</strong><br /> On sending a command to execute this document, the agent gets updated to the latest version, downloads the document, and then executes it.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/walkthru2-output.png" /></p> 
<p>The command to run this is follows:</p> 
<code class="lang-bash">aws ssm send-command --document-name &quot;composite-document-mysql&quot; --targets Key=tag:PlatformType,Values=AmazonLinux</code> 
<h3>Conclusion</h3> 
<p>In this blog post, I have shown you how to enable management as code by using composite documents. You can create composite documents to reuse your documents easily, and enable modularity. Documents created using AWS Systems Manager can be referenced using composite documents. Those documents on GitHub and Amazon S3, written in JSON or YAML, can be executed using the AWS-RunDocument document.</p> 
<h3>About the Author</h3> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/02/melonia.jpg" />Melonia Mendonca is a Software Development Engineer with the Amazon EC2 Systems Manager team. She is a passionate engineer who enjoys the ability to innovate encouraged by Amazon. Outside of work, Melonia likes traveling, playing board games and trying different restaurants/cuisines.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using AWS OpsWorks for Chef Automate in a federated environment</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>Many large enterprises operate on a federated model. That is, they are separated into different business units or organizations, with different goals, procedures, and skill sets. These enterprises typically use a system to manage their infrastructure configuration and changes.</p> 
<p>You might ask, “Can we apply a federated model to configuration management? If so, what are the pros and cons and how does AWS OpsWorks for Chef Automate fit into that model?”</p> 
<p>In this blog post we discuss how to approach configuration management in a. federated enterprise. &nbsp;The main focus is on how you can leverage <a href="https://aws.amazon.com/opsworks/chefautomate/">AWS OpsWorks for Chef Automate</a> to achieve this goal.</p> 
<p><span id="more-2257"></span></p> 
<b>Single Chef server to serve all parts of your organization</b> 
<p>Approaching configuration management in a non-federated way involves using a single Chef server across many different business units.</p> 
<p>But, I hear you say, “Different business units have their own Chef nodes, cookbooks, and environments. How do we separate those?”</p> 
<p>The way you can achieve this with Chef is to use the concept of <a href="https://docs.chef.io/server_orgs.html"><strong>Chef Organizations</strong></a>, which is a core part of the Chef Server software.</p> 
<p>Chef Organizations allows you to separate the following items among different business units, but they still reside on a single Chef server:</p> 
<li>Permissions</li> 
<li>Nodes</li> 
<li>Roles</li> 
<li>Environments</li> 
<li>Cookbooks</li> 
<li>Data bags</li> 
<p>The main benefits of this approach is that each business unit can independently manage its own nodes, access permissions, and can perform maintenance/updates on its infrastructure on its own schedule. Having this type of isolation allows the business units/organizations to have access to their own unique environments, roles, data bags, and cookbooks. Additionally, by hosting all of this on a single server you reduce the amount of effort required to set up and maintain a separate Chef server for each business unit.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/single_chef.png" /></p> 
<p style="text-align: center"><em>Single Chef Server Model</em></p> 
<b>Single Chef server per business unit</b> 
<p>When we speak about a model for a true federated approach to configuration management, we imply that each of the company’s business units/organizations has its own separate Chef server. This brings us the same benefits that we get when we use Chef Organizations, but with even greater isolation between the business units. This brings more benefits when in the cloud.</p> 
<p>“What are these new benefits you speak of?”</p> 
<p>Well, it is very common to have different business units, each with its own respective AWS Account, for all of their resources. This approach is used for billing. It gives us a clear separation of resource usage per business unit. If each business unit uses a Chef server in a centralized AWS account, billing and management of that resource can get out of hand. Another major benefit, which is not strictly cloud related, is that the maintenance of the Chef server and the potential downtime it experiences, does not cause disruptions across all organizations and their infrastructure. By having a Chef server for each business unit you can mitigate the issue of having a single point of failure. If you rely on a single Chef server throughout your enterprise, that server’s failure would cause potential down time across all business units. By using multiple Chef servers you reduce the blast radius of potential Chef server down time.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/Untitled_Diagram.png" /></p> 
<p style="text-align: center"><em>Federated model/Multi Chef Server</em></p> 
<p>Additionally, the concept of Chef Organizations can be fully utilized even in a federated model. For example, if a certain business unit wants to separate even further, it can split its own Chef Automate server into organizations. The nodes from all organizations will be visible on the Chef Automate dashboard because you can filter nodes based on Chef Organizations.</p> 
<p>“But what happens to our cookbooks? Does each business unit need to write its own?” To answer these questions, let’s take a look at <strong>Centralized Cookbook repositories.</strong></p> 
<b>Centralized Cookbook repositories</b> 
<p>A great way to make all of your configurations as consistent as possible is to have a companywide Cookbook repository. This means that teams/business units can use cookbooks created by other parts of the company to configure their infrastructure – on their respective Chef Servers. This, in turn, reduces the effort required to start using Chef as a configuration management system in a business unit because most of the required code base is already present. So, for example, let’s say that business unit A writes its own cookbook for setting up an application, and business unit B also uses the same application in their own environment. Business unit B does not have to write its own cookbook from scratch, but can use the cookbooks already created by business unit A. The main obstacle to having a shared cookbook system is properly writing cookbooks in the first place. Each cookbook must be as dynamic as possible. When you write a cookbook, leverage attributes, templates, and data-bags as much as possible so you can be flexible and help other teams use the same cookbook for, &nbsp;their use cases.</p> 
<p>“So what kind of Cookbook repos can we use?”</p> 
<h3>Git repo</h3> 
<p>The simplest approach to centrally managing your cookbooks is to use &nbsp;Git repositories. Yes, simple as that. You can use the <a href="https://docs.chef.io/berkshelf.html"><strong>Berkshelf</strong></a> tool to manage the cookbooks, but instead of pulling them from the Chef supermarket, it pulls from the Git repo. With Berkshelf, you can specify a certain cookbook version to be pulled. In addition, because Berkshelf supports specifying a repository branch, you can even select a specific branch of your cookbook repo, for example, a certain testing branch to be used in a Dev environment.</p> 
<p><em>Berksfile example:<br /> </em></p> 
<code class="lang-berks">cookbook &quot;buk-cookbook&quot;, &quot;~&gt;; 0.9.3&quot;, git: &quot;https://github.com/example/buk-cookbook.git&quot;
cookbook &quot;jinar-cookbook&quot;, &quot;~&gt;; 0.2.4&quot;, git: &quot;https://github.com/example/jinar-cookbook.git&quot;, branch: &quot;my-test-branch&quot;</code> 
<p>As mentioned before – you can have centralized Cookbook Git repositories across the entire company. Then each team could create its own repositories and push cookbooks – which could, in turn, be used by everyone in the company.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/git_mutlichef_cropped.png" /></p> 
<p style="text-align: center"><em>Common GIT repositories for cookbooks</em></p> 
<p>“But–hey – at the beginning of this blog post you mentioned that one of the benefits of using a single Chef server is not needing to configure and maintain each server separately. Doesn’t having too many Chef servers give us a lot of overhead?”</p> 
<p>Enter – <a href="https://aws.amazon.com/opsworks/chefautomate/">OpsWorks for Chef Automate</a></p> 
<b>How does OpsWorks for Chef automate fit into all of this?</b> 
<p>AWS OpsWorks for Chef Automate (OWCA) provides a fully managed Chef server and suite of automation tools, which compose <a href="https://www.chef.io/automate/">Chef Automate</a>. OpsWorks for Chef Automate offers you a consistent Chef server configuration throughout all deployed servers. All servers are deployed and bootstrapped by OpsWorks, so this means that each new server will be configured in the same way. Additionally, OpsWorks takes care of the Chef server maintenance in the form of server patching, minor Chef updates, and backups. That means that you do not have to spend time and effort in order to set-up and configure a Chef Automate server. You can just let OpsWorks handle that for you, and have your business units focus all their energy on writing or using Chef cookbooks in their environments.</p> 
<p>The fact that OpsWorks handles patching means that you don’t need to connect to the Chef server using SSH. You only &nbsp;need to use knife to manage your Chef server software. With backup and restore in place, you can easily revert back to a working version of your server if something goes wrong.</p> 
<p>A 10 000 foot view of setting up OWCA for a business unit would look something like this:</p> 
<li>Launch an OWCA server in the desired Region with the desired instance size.</li> 
<li>Take the starter kit provided with the Chef Automate server to be used on a Chef Workstation.</li> 
<li>Once the server is fully running, dministrators can use the Chef Workstation to create all the required resources: 
<li>Environments</li> 
<li>Roles</li> 
</ul> </li> 
<li>Set Berkshelf to get the needed Cookbooks from a common repository, and have them installed on the Chef Automate server.</li> 
<li>Finally, bootstrap nodes in the appropriate roles. The usage of AWS API can be applied here, as OpsWorks can handle node bootstrapping for you.</li> 
<p>Simple as that! You don’t need to manually configure Chef servers. You &nbsp;just &nbsp;launch an OpsWorks for Chef Automate server, and the rest is just Cookbook and node management.</p> 
<b>Summary</b> 
<p>Here we have discussed how to implement a federated model for configuration management using Chef and OpsWorks. We described how simple it is to set up and maintain a Chef server with OpsWorks. Many of our customers are actually taking this direction when using OpsWorks for Chef Automate because it allows them to separate their teams but still maintain a common set of configuration rules across the entire enterprise. With the assistance of the AWS API to bootstrap nodes this allows you to spend less time worrying about setting up the configuration management infrastructure and more on creating and using Cookbooks and any other Chef resources you need.</p> 
<p><strong>About the Author</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/24/darko-cropped.jpeg" /><br /> Darko Meszaros is a Cloud Support Engineer who supports customers that use various AWS automation tools, such as AWS OpsWorks, AWS CodeDeploy, and AWS CloudFormation. He is a subject matter expert for OpsWorks and CodeDeploy. Outside of work, he loves collecting video games and old computers.</p> 
<p><em>The screenshots in this post are Copyright Chef Software Inc., and are available at <a href="https://github.com/chef/chef-web-docs">https://github.com/chef/chef-web-docs</a> under the terms of the CC-BY-3.0 license, available here <a href="https://creativecommons.org/licenses/by/3.0/legalcode">https://creativecommons.org/licenses/by/3.0/legalcode</a>.</em></p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">2017 Year in Review: AWS OpsWorks for Chef Automate and Puppet Enterprise</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>AWS OpsWorks for Chef Automate and AWS OpsWorks for Puppet Enterprise provide fully managed Chef and Puppet servers with a suite of automation tools for infrastructure and application management. Enterprise customers rely on OpsWorks for easy configuration management and secure maintenance as the service automatically patches, updates, and backs up servers. This blog post gives you a quick recap of this year’s announcements and events for AWS OpsWorks for Chef Automate and AWS OpsWorks for Puppet Enterprise.</p> 
<p><span id="more-2218"></span></p> 
<p><strong>Announcing OpsWorks for Puppet Enterprise</strong></p> 
<p>Puppet Enterprise support has been the most popular request for AWS OpsWorks since AWS launched the service. Using OpsWorks for Puppet Enterprise, customers can easily manage hybrid infrastructure as code, register node instances without signing certificate requests on the Puppet master, and run modules from the Puppet Forge community. AWS OpsWorks for Puppet Enterprise is integrated with AWS CloudTrail and AWS CodeCommit. AWS CloudTrail captures all of the AWS OpsWorks for Puppet Enterprise API calls and delivers log files to an Amazon S3 bucket. AWS CodeCommit hosts R10K remote repositories, which are general purpose toolsets for deploying Puppet environments and modules.</p> 
<p><a href="https://aws.amazon.com/blogs/aws/new-aws-opsworks-for-puppet-enterprise/">Launch announcement</a></p> 
<p><strong>AWS OpsWorks for Chef Automate now supports Chef Compliance</strong></p> 
<p>The release of Chef Automate version 1.6 includes the new Compliance view for the Chef Automate UI. With AWS OpsWorks for Chef Automate integrated with compliance, you can track the compliance of your infrastructure based on a predefined policy. This allows you to frequently audit your applications for vulnerabilities and remediate violations.</p> 
<p><a href="https://aws.amazon.com/blogs/mt/aws-opsworks-for-chef-automate-now-supports-compliance/">Launch announcement</a></p> 
<p><strong>re:Invent – Automate and scale configuration management with AWS OpsWorks</strong></p> 
<p>At re:Invent 2017, we hosted a session on automating and scaling configuration management through AWS OpsWorks. We gave an overview of how using the DevOps model to treat infrastructure environments as code enables you to automate and scale development and production environments.</p> 
<p>We showed re:Invent session attendees how OpsWorks lets you use Chef and Puppet to automate the way servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. We walked through how OpsWorks helps you to focus on the core task of configuration management using Puppet and Chef by setting up and maintaining your environment in just a few clicks.</p> 
<p><a href="https://www.slideshare.net/AmazonWebServices/automate-and-scale-configuration-management-with-aws-opsworks-dev331-reinvent-2017">Slides</a></p> 
<p><a href="https://www.youtube.com/watch?v=1Zw4g3o2O0M">Video</a></p> 
<p>Right after re:Invent, we released two more updates.</p> 
<p><strong>Availability in six new Regions</strong></p> 
<p>AWS OpsWorks for Chef Automate and AWS OpsWorks for Puppet Enterprise are available in six new Regions: US East (Ohio), US West (N. California), EU (Frankfurt), Asia Pacific (Singapore), Asia Pacific (Tokyo), and Asia Pacific (Sydney). With this release, this service is available in nine regions, including US East (N. Virginia), US West (Oregon), and EU (Ireland).</p> 
<p>We added these Regions based on customer feedback, adoption of other AWS products, and to provide wider geographical coverage.</p> 
<p><a href="http://docs.aws.amazon.com/general/latest/gr/rande.html#opsworks_region">AWS Regions and Endpoints</a></p> 
<p><strong>Create backups from the console</strong></p> 
<p>With this release, AWS OpsWorks for Chef Automate and Puppet Enterprise customers can also create manual backups through the AWS Management Console. Earlier, manual backups were limited to using a CLI command. A backup – either manual or automated – saves application-level server data, such as cookbooks, modules, users, organizations, and node configurations. Backup excludes any additional files that you store on the server EC2 instance.</p> 
<p><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opscm-chef-backup.html">Back Up an AWS OpsWorks for Chef Automate Server</a></p> 
<p><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-backup.html">Back Up an AWS OpsWorks for Puppet Enterprise Server</a></p> 
<p><strong>About the Author</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/22/ragulati.jpeg" />Rahul Gulati is a Product Manager at AWS OpsWorks. He enjoys working with customers and engineering teams to build software products.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">AWS OpsWorks for Puppet Enterprise and an alternate implementation for policy based auto signing</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>AWS OpsWorks for Puppet Enterprise was <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/announcing-aws-opsworks-for-puppet-enterprise/">released in November of 2017</a>. It has a secure API (<em>associate node</em>) that provides a secure, convenient, and AWS-integrated method to sign certificates for clients of OpsWorks for Puppet Enterprise. This secure API is ideal for use within a user data script when being used for AWS CloudFormation (which can be found <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-unattend-assoc.html">here</a>) or Auto Scaling Groups.</p> 
<p>However, this blog post is about creating an alternate mechanism that allows end users to implement a different condition-checking mechanism before signing client certifications. This method is effective in environments such as proof of concept (PoC), development, or testing where clients aren’t executing the full AWS provisioning process. For example, this method could be used where customers build bare metal virtual machines (VMs) or containers. It could also be used where the VMs exist in a hybrid cloud environment or in an environment where OpsWorks for Puppet Enterprise is used to govern a customer’s private cloud VMs. It’s important to note that this method is not integrated with AWS CloudTrail, where every <em>associate node</em> API call would be tracked. When you use this alternative method, this tracking is not in effect.<span id="more-2311"></span></p> 
<b>Introduction</b> 
<p>When a VM makes a request to sign in to the Puppet master, the master bases its approval of the client’s request on certain criteria. This is called policy-based auto signing.&nbsp; Due to the nature of Puppet and the possibility of exposing sensitive information, the signing of a client’s certificate must be approved carefully. The conceptual implementation is <a href="https://puppet.com/docs/puppet/5.3/ssl_autosign.html#policy-based-autosigning">documented on the Puppet website</a>.</p> 
<p>This blog post provides one example of policy-based auto signing using the following assumptions:</p> 
<ol> 
<li>The Puppet clients are Amazon EC2 instances.</li> 
<li>These instances reside in the same VPC.</li> 
<li>The Puppet master is equipped with required tools such as the AWS CLI and <a href="https://stedolan.github.io/jq/">jq</a>.</li> 
<li>The Puppet master’s AWS client tool has been configured with sufficient authority to be able to access all the information on the EC2 instances, such as their private DNS names and their tags.</li> 
</ol> 
<b>Set up the necessary components on the Puppet Enterprise master<strong>&nbsp;</strong></b> 
<p>(NOTE: a puppet module that can help with automating steps 5-8 is available.&nbsp; It can be found <a href="https://forge.puppet.com/awsandrewpark/aws_owpe_policysign">here</a> and for reporting an issue about the module, you can visit this <a href="https://github.com/awslabs/aws_owpe_policysign/issues">link</a>.&nbsp; Please note that you have to edit common.yaml to enter your ID and secret access key.&nbsp; For usage help, please visit <a href="https://forge.puppet.com/awsandrewpark/aws_owpe_policysign#usage">here</a>.)</p> 
<ol> 
<li>Log in to the PE console and add the following value under puppet_enterprise::profile::master class</li> 
</ol> 
<code class="lang-css">allow_unauthenticated_ca = true</code> 
<ol start="2"> 
<li>Choose Add parameter.</li> 
<li>Choose Commit.</li> 
<li>Log in to the PE master using SSH and execute the following command to apply the configuration change:</li> 
</ol> 
<code class="lang-bash">puppet agent -tov</code> 
<ol start="5"> 
<li>In the same SSH session on the PE master, ensure that the AWS CLI is configured with the proper Access Key ID and the Secret Access Key for the pe-puppet</li> 
</ol> 
<code class="lang-bash">sudo su - pe-puppet --shell /bin/bash
$ aws configure
AWS Access Key ID [None]: ************
AWS Secret Access Key [None]: **********************
Default region name [None]: us-east-1
Default output format [None]: json              
$ exit</code> 
<p style="padding-left: 60px">NOTE: &nbsp;This is an important step for the Puppet Enterprise Puppet user (pe-puppet) because the policy-based signing script must be executable by the pe-puppet user.</p> 
<ol start="6"> 
<li>Edit&nbsp;/etc/puppetlabs/puppet/puppet.conf file and ensure that following line appears under <em><strong>[main]</strong></em> section of the configuration file:</li> 
</ol> 
<code class="lang-bash">autosign = /opt/puppetlabs/autosign/autosign.sh</code> 
<ol start="6"> 
<li>Create an auto sign structure and copy over the script. (You can find this script in Appendix A.) And restart Puppet Master Service:</li> 
</ol> 
<code class="lang-bash">mkdir -p /opt/puppetlabs/autosign
cp autosign.sh /opt/puppetlabs/autosign/autosign.sh
chown -R pe-puppet.pe-puppet /opt/puppetlabs/autosign
chmod 750 /opt/puppetlabs/autosign /opt/puppetlabs/autosign/autosign.sh   
/etc/init.d/pe-puppetserver restart</code> 
<ol start="7"> 
<li>Now the Puppet master is ready to sign based on the policy. (For the explanation of the policy contained in the example script, see Appendix B.) On the Puppet client that has not been signed into a PE master, run the following command to request to be signed in.</li> 
</ol> 
<code class="lang-bash">puppet agent -tov --waitforcert=200</code> 
<ol start="8"> 
<li>To confirm, you should see the following entries in the /var/log/puppetlabs/puppetserver/puppetserver.log file on the PE master:</li> 
</ol> 
<code class="lang-bash">2017-12-18 20:23:21,739 INFO  [qtp2115462635-102] [p.p.certificate-authority] Signed certificate request for ip-my-ip-address.ec2.internal
2017-12-18 20:23:22,636 INFO  [qtp2115462635-364] [puppetserver] mount[pe_packages] allowing * access
2017-12-18 20:23:22,637 INFO  [qtp2115462635-364] [puppetserver] mount[pe_modules] allowing * access</code> 
<p style="padding-left: 30px">On the Puppet client’s command line, you should see something like the following:</p> 
<code class="lang-bash">[root@ip-my-ip-address ~]# puppet agent -tov --waitforcert=200
Info: Creating a new SSL key for ip-my-ip-address.ec2.internal
Info: Caching certificate for ca
Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
Info: Creating a new SSL certificate request for ip-my-ip-address.ec2.internal
Info: Certificate Request fingerprint (SHA256): 
FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF [snip]
Info: Caching certificate for ip-my-ip-address.ec2.internal
Info: Caching certificate_revocation_list for ca
Info: Caching certificate for ca
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Notice: /File[/opt/puppetlabs/puppet/cache/lib/facter]/ensure: created</code> 
<b>Conclusion</b> 
<p>That’s it. The Puppet Enterprise (PE) Master will sign the client certificates based on the policy you have defined in the autosign.sh. To recap, this blog post shows you how to create an alternate mechanism that allows end users to implement different condition-checking before signing client certificates. We walk you through an example of policy-based auto signing configuration and provide an example script.</p> 
<p>To learn more about AWS OpsWorks for Puppet Enterprise, visit <a href="https://aws.amazon.com/opsworks/puppetenterprise/">here</a>.</p> 
<b>Appendix A: autosign.sh</b> 
<code class="lang-bash">#!/bin/sh
# If we do not have aws cli or jq, exit with error immediately
which aws &gt; /dev/null 2&gt;&amp;1 || exit 2
which jq &gt; /dev/null 2&gt;&amp;1 || exit 3
usage()
{
echo &quot;$0 [private dns name of ec2 instance]&quot;
exit 1
}
[ $# -eq 1 ] || usage
clientname=$1
# Criteria 1: the puppet client must be visible in my VPC
[ `aws ec2 describe-instances --filters &quot;Name=private-dns-name,Values=$clientname&quot; --output=text | wc -l | sed 's/ //g'` -eq 0 ] &amp;&amp; exit 1
# Criteria 2: the puppet client must have proper tags
UNIQUETAG=&quot;puppetclient&quot;
INSTANCEID=`aws ec2 describe-instances --filters &quot;Name=private-dns-name,Values=$clientname&quot; --output=json | jq .Reservations[].Instances[].InstanceId`
[ &quot;`aws ec2 describe-tags --filters &quot;Name=resource-id,Values=$INSTANCEID&quot; --output json  | jq '.Tags[] | select(.Key==&quot;myuniquetag&quot;).Value' | sed 's/&quot;//g'`&quot; != &quot;$UNIQUETAG&quot; ] &amp;&amp; exit 1
# We passed everything, give it a thumbs up
exit 0</code> 
<b>Appendix B: Policy description</b> 
<p>The provided autosign.sh script uses two criteria to validate legitimacy of the puppet client’s certificate signing request.</p> 
<ol> 
<li>The Amazon EC2 VM must exist in the same VPC. (This is verified using the aws ec2 describe-instances)</li> 
<li>The unique tag created during the instantiation phase of the Amazon EC2 VM must exist and match what the Puppet master expects (configurable/adjustable in the sh script).</li> 
</ol> 
<b>Related links</b> 
<p>Add Nodes Automatically: <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-unattend-assoc.html">http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-unattend-assoc.html</a></p> 
<p>Puppet’s policy-based auto signing: <a href="https://puppet.com/docs/puppet/5.3/ssl_autosign.html#policy-based-autosigning">https://puppet.com/docs/puppet/5.3/ssl_autosign.html#policy-based-autosigning</a></p> 
<b>About the author</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/andrew_park.jpg" /></p> 
<p>Andrew Park is a Cloud Infrastructure Architect at Amazon Web Services. Prior to AWS, Andrew has served 20+ years as a Linux Solution Engineer, a Linux infrastructure Architect/Administrator and a Cloud Engineer. He has been a key participating member of cloud adoption programs in many different Canadian banks, and he also has participated in many different open source projects like Samba, restricted shell (rssh), not red hat update (Yes, this was an actual project), Red Hat’s Anaconda and Agenda VR3. He works tirelessly on behalf of the clients in order that customers can enjoy economically friendly open source solutions. He is a foodie, loves to converse with people and machines alike, and a firm believer of Open Source philosophy.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">10 reasons why you should try AWS OpsWorks for Puppet Enterprise</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p><em>By Ryan Coleman, Director of Product Management at <a href="https://puppet.com/" target="_blank" rel="noopener noreferrer">Puppet</a></em></p> 
<p>We’re really excited to see <a href="https://aws.amazon.com/opsworks/puppetenterprise/" target="_blank" rel="noopener noreferrer">AWS OpsWorks for Puppet Enterprise</a> out in the wild. Over 600 people attended our joint session at AWS re:Invent 2017 (check out the video <a href="https://www.youtube.com/watch?v=1Zw4g3o2O0M" target="_blank" rel="noopener noreferrer">here</a>) and many of you stopped by our booth to find out more. This post is for everyone we didn’t talk to directly. If you’re an open source Puppet user and you’d like to try some of the Puppet Enterprise features you’ve been hearing about, you can deploy OpsWorks for Puppet Enterprise in less than 20 minutes with no upfront costs. If you’re running a small team and you have a big cloud or automation initiative on the table for 2018, then OpsWorks for Puppet Enterprise is a perfect starting point.<span id="more-2283"></span></p> 
<p>And, because it’s that time of year for making lists, we’re bringing you the top 10 reasons why you should try OpsWorks for Puppet Enterprise in the new year.</p> 
<ol> 
<li>You can have a fully configured Puppet master up and running on AWS in less than 20 minutes. We developed a handy <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/gettingstarted-opspup.html" target="_blank" rel="noopener noreferrer">User Guide</a> and <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-starterkit.html" target="_blank" rel="noopener noreferrer">Starter Kit</a> to help you get started.</li> 
<li>Backups, upgrades, and restorations are automatically handled for you based on maintenance windows that you define.</li> 
<li>You can take advantage of hourly billing and only pay for what you consume.</li> 
<li>You can deploy wherever your customers are—OpsWorks just added support for <a href="https://aws.amazon.com/about-aws/whats-new/2017/12/aws-opsworks-is-now-available-in-nine-regions/" target="_blank" rel="noopener noreferrer">6 new AWS Regions</a>. In addition to these Regions: US East (N. Virginia), US West (Oregon), and EU (Ireland), it’s now available in in these Regions: US East (Ohio), US West (N. California), EU (Frankfurt), Asia Pacific (Singapore), Asia Pacific (Tokyo), and Asia Pacific (Sydney).</li> 
<li>Need to use up some of your AWS credits? You can apply them to your AWS bill and offset part of your OpsWorks for Puppet Enterprise usage!</li> 
<li>Workflows tailored to the cloud make it easy to bring your cloud resources under management. For example, Puppet Enterprise in OpsWorks can securely register nodes on the Puppet master without intervention.</li> 
<li>Connect to your existing module code repository. If you don’t have an existing code repo, we provide an AWS-managed source control repository to securely store your infrastructure code.</li> 
<li>Manage your AWS and on-premises infrastructure with a single tool. If you’re going fully cloud native, OpsWorks for Puppet Enterprise is for you. If you still have on-premises infrastructure to manage, you can bring that, too.</li> 
<li>Are you running open source Puppet or an older version of Puppet Enterprise? Try out the latest features available in Puppet Enterprise 2017.3 like <a href="https://puppet.com/products/capabilities/task-management" target="_blank" rel="noopener noreferrer">Task Management</a>, which lets you run ad hoc tasks across tens of thousands of nodes with full governance and auditability.</li> 
<li>Accelerate migration to the cloud and get the benefits of Day 2 management.</li> 
</ol> 
<p>Together with AWS, we look forward to helping you move to the cloud and scale automation across your organization in 2018. Give it a try and let us know what you think. If you’d like to learn more about OpsWorks for Puppet Enterprise, you can get more info <a href="https://aws.amazon.com/opsworks/puppetenterprise/" target="_blank" rel="noopener noreferrer">here</a>.</p> 
<p><em>The content and opinions in this post are those of the third-party author and AWS is not responsible for the content or accuracy of this post.</em></p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/image2-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Integrating AWS CloudFormation with AWS Systems Manager Parameter Store</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>AWS CloudFormation has always allowed you to customize your templates by using parameters for runtime input values. Parameters make your template code dynamically configurable, improving the reusability of your code. Previously, the only ways you could specify values for these parameters were to pass the plaintext values as arguments to the CloudFormation API, or hard code them in the template using ‘default’ values. This posed the following limitations:</p> 
<ol class="incremental" type="1"> 
<li>There was no centralized place to define/update your parameters (which may contain secrets, configuration data, etc.) and then import them into CloudFormation.</li> 
<li>When changing parameters, you had to either slightly rewrite your template code or pass new parameter values when doing stack update operation.</li> 
</ol> 
<p>We are pleased to announce the integration of CloudFormation with <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html">AWS Systems Manager Parameter Store</a> which provides solutions to the above restrictions. In this blog, I explain how you can use Systems Manager parameters in your CloudFormation templates to simplify stack updates involving parameters and achieve consistency by using values stored in Parameter Store. With this integration, your code remains untouched while the stack update operation automatically picks up the latest parameter store value.</p> 
<p><span id="more-2268"></span></p> 
<b id="how-systems-manager-parameters-work-in-cloudformation">How Systems Manager parameters work in CloudFormation</b> 
<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p> 
<p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html">Parameters</a> section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p> 
<b id="new-systems-manager-parameter-types-supported-in-cloudformation">New Systems Manager parameter types supported in CloudFormation</b> 
<p>CloudFormation parameters already support certain AWS specific types. SSM parameter types will be an addition to these types. New parameter types introduced in CloudFormation are:</p> 
<ul class="incremental"> 
<li>AWS::SSM::Parameter::Name</li> 
<li>AWS::SSM::Parameter::Value&lt;String&gt;</li> 
<li>AWS::SSM::Parameter::Value&lt;List&lt;String&gt;&gt;</li> 
<li>AWS::SSM::Parameter::Value&lt;Any AWS type&gt;</li> 
<p>The first one in the list is used to pass the name of the parameter key as-is. CloudFormation will not fetch the value stored against it. For example, you can use this type to validate that the parameter exists in Parameter Store. For all others, the value is fetched from Systems Manager with the type defined in the trailing angle brackets &lt;&gt;. For now, you can only use plaintext strings or list of strings. CloudFormation will support the Parameter Store ‘SecureString’ type in a later release.</p> 
<b id="how-to-use-ssm-types-in-cloudformation">How to Use SSM types in CloudFormation</b> 
<p>As mentioned earlier, SSM parameter types are used in template to reference existing Systems Manager parameters. Let’s look at a couple example scenarios.</p> 
<h3 id="example-1">Example 1:</h3> 
<p>Let’s say you are deploying a CloudFormation stack in both Development and Production. You are planning to use smaller instance types in Development. You don’t want to remember the instance types to be used in your CloudFormation template whenever you redeploy from one environment to another. The data (parameter value for the instance type parameter) can be maintained separately from the code (template). This example shows the end-to-end steps for using Systems Manger parameters for this use-case.</p> 
<code># Create a parameters for Dev and Prod environments in Systems Manager Parameter Store
aws ssm put-parameter --name myEC2TypeDev --type String --value “t2.small”
aws ssm put-parameter --name myEC2TypeProd --type String --value “m4.large”
</code> 
<code class="yaml"># Reference/use existing Systems Manager Parameter in CloudFormation
Parameters:
InstanceType :
Type : 'AWS::SSM::Parameter::Value&lt;String&gt;'
Default: myEC2TypeDev
KeyName :
Type : 'AWS::SSM::Parameter::Value&lt;AWS::EC2::KeyPair::KeyName&gt;'
Default: myEC2Key
AmiId:
Type: 'AWS::EC2::Image::Id'
Default: 'ami-60b6c60a'
Resources :
Instance :
Type : 'AWS::EC2::Instance'
Properties :
Type : !Ref InstanceType
KeyName : !Ref KeyName
ImageId : !Ref AmiId 
</code> 
<code># Call create-stack for Dev environment by passing SSM parameter key as template parameter value
aws cloudformation create-stack --stack-name S1 --template-body &lt;above-template&gt;
# Call create-stack for Prod environment by passing SSM parameter key as template parameter value
aws cloudformation create-stack --stack-name S1 --template-body &lt;above-template&gt; --parameters ParameterKey=InstanceType,ParameterValue=myEC2TypeProd</code> 
<p>Systems Manager console – All SSM parameters under this account are displayed.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/image1.png" /></p> 
<p>CloudFormation console – Stack list page where selected stack is using SSM parameters.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/image2-1.png" /></p> 
<p>CloudFormation console – Stack detail page for stack using SSM parameters.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/image3.png" /></p> 
<h3 id="example-2">Example 2:</h3> 
<p>Consider the use case of updating Amazon Machine Image (AMI) IDs for the EC2 instances in your CloudFormation templates. Normally, you might map AMI IDs to specific instance types and Regions. Then to update these, you would manually change them in each of your templates. Or you would be using a custom resource with an AWS Lambda function that gets the IDs of the latest AMIs for the Region and instance type that you’re using. Arguably, neither of these methods is very convenient.</p> 
<p>Systems Manger Parameter Store team recently launched <a href="https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/">an easy way to retrieve the latest AMI IDs</a> for your template. <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-walk.html#sysman-paramstore-walk-hierarchies">Hierarchical</a> public parameters supported by Parameter Store can also be referenced in a CloudFormation template similar to the one used in Example 1. You can use the public parameter variable for the Windows AMI ID in your template. You don’t need to worry about how to fetch the latest AMI IDs. This SSM parameter will be updated whenever there is a newer version available. Whenever you decide to update the EC2 instances in your CloudFormation template to use the new AMI ID, you just call update-stack API on the stack. It will automatically fetch the latest value from Parameter Store. Also, note that for hierarchical parameters, you need to provide the full path of the parameter name.</p> 
<code class="yaml"># Use public Systems Manager Parameter
Parameters :
LatestAmiId :
Type : 'AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;'
Default: ‘/aws/service/ami-windows-latest/Windows_Server-2016-English-Core-Containers’
Resources :
Instance :
Type : 'AWS::EC2::Instance'
Properties :
ImageId : !Ref LatestAmiId</code> 
<code># Create-stack CLI call
aws cloudformation create-stack --stack-name S1 --template-body &lt;above-template&gt;
# Describe stack output’s ‘Parameters’ section for this stack
aws cloudformation describe-stacks --stack-name S1</code> 
<code class="json">…
&quot;Parameters&quot;: [
{
&quot;ParameterValue&quot;: &quot;/aws/service/ami-windows-latest/Windows_Server-2016-English-Core-Containers&quot;,
&quot;ResolvedValue&quot;: &quot;ami-ba9c05c0&quot;,
&quot;ParameterKey&quot;: &quot;LatestAmiId&quot;
}
]
…</code> 
<b id="summary">Summary</b> 
<p>By leveraging the CloudFormation integration with Systems Manager Parameter Store, you can make your templates more reusable and generic by storing and managing your runtime configuration and parameters centrally and securely. This, in turn, can make your stack operations simpler and more consistent, by clearly delineating the separation of infrastructure configuration and infrastructure code from each other.</p> 
<p>The feature is now available in all AWS Regions!</p> 
<hr /> 
<b>About the Author</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/Anuradha.jpg" /> Anuradha Garg is a software developer on the AWS CloudFormation team where she works on developing new features for the service. Outside work, she is a travel enthusiast and loves exploring new things.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">How to Manage Credentials in AWS OpsWorks for Puppet Enterprise using Hiera-eyaml</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>For customers new to configuration management with AWS OpsWorks for Puppet Enterprise (OWPE), a frequently-asked question is “How do I store sensitive data, such as database passwords, for use in my manifests?” Hiera allows you to manage and access data from various backends (data storage locations). By default, Hiera 5 supports YAML, JSON, and HOCON backends and only plaintext data files and values.</p> 
<p>With some additional configuration, Hiera 5 supports the eyaml backend, which allows administrators to define backend data files with encrypted data (without having to encrypt the file in its entirety). This provides security of data at rest while allowing fast lookup and ease of use/review by administrators. Additionally, plaintext values can be included in the same file. Hiera-eyaml supports encrypted arrays, hashes, and nested arrays/hashes.</p> 
<p><span id="more-2299"></span></p> 
<h3>Enabling Hiera-eyaml</h3> 
<p>The eyaml command line tool is made available for working with encrypted data files. To install and use hiera-eyaml, follow these instructions.</p> 
gem install hiera-eyaml 
<p>Import the following modules from the Puppet Forge to your Master.</p> 
<li>https://forge.puppet.com/puppet/hiera</li> 
<li>https://forge.puppet.com/puppetlabs/puppetserver_gem</li> 
<li>https://forge.puppet.com/puppetlabs/inifile</li> 
<li>https://forge.puppet.com/puppetlabs/stdlib</li> 
<p>The following example Puppetfile includes the required modules.</p> 
forge &quot;http://forge.puppetlabs.com&quot;
# Modules from the Puppet Forge
mod &quot;puppetlabs/concat&quot;
mod &quot;puppetlabs/ntp&quot;
mod &quot;puppetlabs/stdlib&quot;
mod &quot;puppet/staging&quot;
mod &quot;puppet-logrotate&quot;
mod &quot;puppet/nginx&quot;
mod &quot;puppetlabs/inifile&quot;
mod &quot;puppetlabs/puppetserver_gem&quot;
mod &quot;puppet/hiera&quot; 
<p>Create a profile manifest to configure Hiera. This will enable three hierarchies (Virtual yaml, Nodes yaml, and Default yaml file). In the starter kit provided when creating an AWS OpsWorks for Puppet Enterprise instance, this would be located in <code>[STARTER_KIT]/site/profile/manifests/hiera.pp</code>.</p> 
class profile::hiera {
class { 'hiera':
hiera_version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '5',
hiera5_defaults&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; {&quot;datadir&quot; =&gt; &quot;/etc/puppetlabs/code/environments/%{::environment}/hieradata&quot;, &quot;data_hash&quot; =&gt; &quot;yaml_data&quot;},
hierarchy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; [
{&quot;name&quot; =&gt; &quot;Virtual yaml&quot;, &quot;path&quot;&nbsp; =&gt;&nbsp; &quot;virtual/%{::virtual}.yaml&quot;},
{&quot;name&quot; =&gt; &quot;Nodes yaml&quot;, &quot;paths&quot; =&gt;&nbsp; ['nodes/%{::trusted.certname}.yaml', 'nodes/%{::osfamily}.yaml']},
{&quot;name&quot; =&gt; &quot;Default yaml file&quot;, &quot;path&quot; =&gt;&nbsp; &quot;common.yaml&quot;},
],
eyaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
&nbsp;&nbsp; eyaml_gpg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
&nbsp;&nbsp; eyaml_gpg_recipients =&gt;&nbsp; 'mark@example.com,chris@example.com',
&nbsp;&nbsp; &nbsp;create_keys&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; false,
&nbsp;&nbsp;&nbsp; keysdir&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '/etc/puppetlabs/code-staging/keys',
&nbsp;&nbsp;&nbsp; provider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; puppetserver_gem,
}
} 
<p>Ensuring that you are following best practices, you can then add the newly created hiera profile to a role for our Puppet Master. This would be placed in <code>[STARTER_KIT]/site/role/manifests/puppet_master.pp</code>.</p> 
class role::puppet_master {
include profile::hiera
} 
<p>After steps 2-4 have been completed, and the control repository has been synchronized with the Puppet master (via webhook and puppet-code deploy), the role should be available for classification on the Puppet Master. This can be done in the Classification console under “All Nodes” -&gt; “PE Infrastructure” -&gt; “PE Master”. If for some reason the role is not appearing as a classification option, ensure that you have refreshed class definitions.</p> 
<p>To quickly synchronize these changes, you can manually execute an Agent run on the Master using the PE console. At this point, the necessary configuration to support hiera-eyaml has been configured. The next steps would be to install encryption keys securely on the Puppet Master.</p> 
<h3>Secure Key Storage</h3> 
<p>After eyaml has been configured on the Master, &nbsp;you need to generate and securely store the asymmetric key pairs used for encryption and decryption of sensitive data. This can be done with the <code>eyaml createkeys</code> command from your workstation. The private key should be stored in a secure location that can only be accessed by the Puppet Master and any authorized users. In this case, you can leverage an Amazon S3 bucket with a strict access policy that allows only the OWPE instance’s IAM profile and administrators to our AWS account. The sample policy that follows demonstrates this. With this policy, it is important to ensure that the <code>s3:GetObject</code> and <code>s3:ListBucket</code> permissions are added to the <code>aws-opsworks-cm-ec2</code> role. After the policy has been applied and the OWPE Amazon EC2 role has been updated, have an administrator upload the public and private keys to the bucket.</p> 
{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Id&quot;: &quot;HieraKeysPolicy&quot;,
&nbsp; &quot;Statement&quot;: [
{
&quot;Sid&quot;: &quot;AllowOpsWorksCMInstanceRole&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;AWS&quot;: &quot;arn:aws:iam::ACCOUNT_ID:role/service-role/aws-opsworks-cm-ec2-role&quot;
},
&quot;Action&quot;: [
&quot;s3:GetObject&quot;,
&quot;s3:ListBucket&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:s3:::BUCKET_NAME/*&quot;,
&quot;arn:aws:s3:::BUCKET_NAME&quot;
]
},
{
&quot;Sid&quot;: &quot;AllowAdminAccess&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;AWS&quot;: &quot;arn:aws:iam::ACCOUNT_ID:user/ADMIN_USER&quot;
},
&quot;Action&quot;: &quot;s3:*&quot;,
&quot;Resource&quot;: [
&quot;arn:aws:s3:::BUCKET_NAME/*&quot;,
&quot;arn:aws:s3:::BUCKET_NAME&quot;
]
}
]
} 
<p>After this policy has been applied, public and private keys can be generated with the <code>eyaml createkeys</code> command. This will automatically place the key files in a <code>./keys</code> directory on your workstation. The public/private keys can then be uploaded to the S3 bucket, and you can modify the <code>hiera.pp</code> manifest to include importing these files to <code>/etc/puppetlabs/code-staging/keys</code>. This directory is configured as part of the Hiera profile created previously, and is included in the backup/restore procedures.</p> 
<p>Since these files are not created with in-line content, or from files/templates stored within the module itself, you need to make use of the exec Puppet resource. Since the AWS CLI is installed by default on OWPE Masters, no additional configuration/installation is required. The benefit of this approach is that the AWS CLI will make use of instance profiles to determine access permissions to S3. You need to confirm that the role assigned to nodes in your environment has appropriate S3 permissions (specifically <code>s3:ListBucket</code> on the bucket itself, and <code>s3:GetObject</code> for any key files).</p> 
<p>First, update the Hiera profile to include importing the public/private key files.</p> 
class profile::hiera {
class { 'hiera':
hiera_version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '5',
hiera5_defaults&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; {&quot;datadir&quot; =&gt; &quot;/etc/puppetlabs/code/environments/%{::environment}/hieradata&quot;, &quot;data_hash&quot; =&gt; &quot;yaml_data&quot;},
hierarchy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; [
{&quot;name&quot; =&gt; &quot;Virtual yaml&quot;, &quot;path&quot;&nbsp; =&gt;&nbsp; &quot;virtual/%{::virtual}.yaml&quot;},
{&quot;name&quot; =&gt; &quot;Nodes yaml&quot;, &quot;paths&quot; =&gt;&nbsp; ['nodes/%{::trusted.certname}.yaml', 'nodes/%{::osfamily}.yaml']},
{&quot;name&quot; =&gt; &quot;Default yaml file&quot;, &quot;path&quot; =&gt;&nbsp; &quot;common.yaml&quot;},
],
eyaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg_recipients =&gt;&nbsp; 'mark@example.com,chris@example.com',
create_keys&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; false,
keysdir&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&gt;&nbsp; '/etc/puppetlabs/code-staging/keys',
provider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; puppetserver_gem,
}
exec { 'get-private-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/private/key.pem /etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
}
exec { 'get-public-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/public/key.pem /etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
}
exec { 'update-key-perms':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'chown -R pe-puppet:pe-puppet /etc/puppetlabs/code-staging/keys &amp;&amp; chmod -R 0500 /etc/puppetlabs/code-staging/keys &amp;&amp; chmod 0400 /etc/puppetlabs/code-staging/keys/*.pem',
require =&gt; [
Exec['get-private-key'],
Exec['get-public-key'],
],
}
} 
<p>After this is complete and deployed to your control repository, run Puppet Agent on the master and deploy the changes with <code>puppet-code deploy --wait --all --config-file .config/puppet-code.conf --token-file .config/puppetlabs/token</code> (executed from the root of your starter kit). To verify the modifications work as intended, test the puppet lookup tool to verify you are able to retrieve data from <code>common.yaml</code>.</p> 
$ puppet lookup message --explain
Searching for &quot;message&quot;
&nbsp; Global Data Provider (hiera configuration version 5)
&nbsp;&nbsp;&nbsp; Using configuration &quot;/etc/puppetlabs/puppet/hiera.yaml&quot;
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Virtual yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/virtual/xenhvm.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;virtual/%{::virtual}.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Path not found
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Nodes yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/nodes/opsworks-1703-ohq1bhd0cetefkis.us-east-1.opsworks-cm.io.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;nodes/%{::trusted.certname}.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path not found
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/nodes/RedHat.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;nodes/%{::osfamily}.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path not found
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Default yaml file&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/common.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;common.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Found key: &quot;message&quot; value: &quot;This node is using common data&quot; 
<p>Now, you can update the Hiera data within your control repository to include encrypted values. Note that this will require separate additions to the hierarchy specified in the Hiera class. Thus, you must first update <code>hiera.pp</code> to include the additional hierarchy layer.</p> 
class profile::hiera {
class { 'hiera':
hiera_version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '5',
hiera5_defaults&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; {&quot;datadir&quot; =&gt; &quot;/etc/puppetlabs/code/environments/%{::environment}/hieradata&quot;, &quot;data_hash&quot; =&gt; &quot;yaml_data&quot;},
hierarchy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; [
{&quot;name&quot; =&gt; &quot;Secret Data&quot;, &quot;lookup_key&quot; =&gt; &quot;eyaml_lookup_key&quot;, &quot;paths&quot; =&gt; ['common.eyaml'], &quot;options&quot; =&gt; { &quot;pkcs7_private_key&quot; =&gt; &quot;/etc/puppetlabs/code/keys/private_key.pkcs7.pem&quot;, &quot;pkcs7_public_key&quot; =&gt; &quot;/etc/puppetlabs/code/keys/public_key.pkcs7.pem&quot; } },
{&quot;name&quot; =&gt; &quot;Virtual yaml&quot;, &quot;path&quot;&nbsp; =&gt;&nbsp; &quot;virtual/%{::virtual}.yaml&quot;},
{&quot;name&quot; =&gt; &quot;Nodes yaml&quot;, &quot;paths&quot; =&gt;&nbsp; ['nodes/%{::trusted.certname}.yaml', 'nodes/%{::osfamily}.yaml']},
{&quot;name&quot; =&gt; &quot;Default yaml file&quot;, &quot;path&quot; =&gt;&nbsp; &quot;common.yaml&quot;},
],
eyaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg_recipients =&gt;&nbsp; 'mark@example.com,chris@example.com',
create_keys&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; false,
keysdir&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '/etc/puppetlabs/code-staging/keys',
provider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; puppetserver_gem,
}
exec { 'get-private-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/private/key.pem /etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
}
exec { 'get-public-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/public/key.pem /etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
}
exec { 'update-key-perms':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'chown -R pe-puppet:pe-puppet /etc/puppetlabs/code-staging/keys &amp;&amp; chmod -R 0500 /etc/puppetlabs/code-staging/keys &amp;&amp; chmod 0400 /etc/puppetlabs/code-staging/keys/*.pem',
require =&gt; [
Exec['get-private-key'],
Exec['get-public-key'],
],
}
} 
<p>After this is complete, add an encrypted YAML file with some test data. Note that there is a specific format required when adding encrypted values, as seen in the following snippet.</p> 
# From the same directory as ./keys/
eyaml edit CONTROL_REPO/hieradata/common.eyaml 
<p>For testing purposes, we created a new value.</p> 
---
encryptedmessage: DEC::PKCS7['Hello, World!']! 
<p>After this is complete, you can commit and push the changes to your control repository, sync them with the Puppet Master, and run Puppet Agent on the master to implement the updated Hiera configuration. To validate that this works as expected, output the file contents, and compare them to the output of puppet lookup.</p> 
$ cat /etc/puppetlabs/code/environments/production/hieradata/common.eyaml
---
encryptedmessage: ENC[PKCS7,ENCRYPTED_STRING]
$ puppet lookup encryptedmessage
Searching for &quot;encryptedmessage&quot;
&nbsp; Global Data Provider (hiera configuration version 5)
&nbsp;&nbsp;&nbsp; Using configuration &quot;/etc/puppetlabs/puppet/hiera.yaml&quot;
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Secret Data&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/common.eyaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;common.eyaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Found key: &quot;encryptedmessage&quot; value: &quot;'Hello, World!'&quot; 
<p>At this point, you can add additional hierarchies for environment or fact-based data, and manage sensitive information within. From your manifest code, you can simply query Hiera with the <code>lookup()</code> function as normal. You need to create and maintain encryption keys, and distribute them to administrators who need to manage Hiera data.</p> 
<h3>About the Author</h3> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/internal-cdn.amazon.com_.jpeg" /></p> 
<p>Nick Alteen is a Lab Development Engineer at Amazon Web Services, previously a Cloud Support Engineer. In both roles, he enjoys developing training and providing guidance to customers for usage of AWS services to fit their needs.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Tracking AWS Service Catalog products provisioned by individual SAML users</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>To manage access to the AWS Cloud, many companies prefer <a title="undefined" href="https://aws.amazon.com/iam/details/manage-federation/" target="_blank" rel="noopener noreferrer">Enterprise Federation</a> over AWS Identity and Access Management (IAM) users. Identity federation provides single sign-on (SSO) to access AWS accounts using credentials from the corporate directory. This method of accessing AWS allows companies to utilize their existing identity solutions, such as Active Directory (AD) or Active Directory Federation Services, by mapping users to IAM roles.</p> 
<p>Another option for managing access to AWS is to use <a title="undefined" href="https://aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">AWS Service Catalog</a>. In this blog I’ll show you how to set up AWS Service Catalog to grant users IAM roles for launching AWS resources.</p> 
<p>AWS Service Catalog allows an organization to create a portfolio of products that can be provisioned by users. This method mitigates the need to grant user permissions to AWS resources and only grants permissions to the service catalog and specific products.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/19/Diagram1-1024x304.png"></span><br /> This diagram shows how users can access products through AWS Service Catalog after they have access to an appropriate IAM role. However, we need a way to distinguish users because multiple users can belong to the same AD group.</p> 
<p>One way to identify each user is to add the user name parameter to the product template. But this method doesn’t guarantee that the value entered by the user will be correct or match the user name in Active Directory.</p> 
<p>A better way to accomplish this is to programmatically add the user name to each product template. Let’s take a look at how to accomplish this using AWS Service Catalog.</p> 
<b>Solution Overview</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/19/Diagram2-1024x404.png" /></p> 
<p>This Solution Overview diagram shows the architecture of the proposed solution:</p> 
<ol>
1. The user provisions a product after authenticating to AWS Service Catalog. 
</ol> 
<ol>
2. AWS Service Catalog launches an AWS CloudFormation template in response to the user’s request. 
</ol> 
<ol>
3. An AWS Lambda function is invoked based on the Amazon CloudWatch rule triggered by the CloudFormation CreateStack event. 
</ol> 
<ol>
4. The Lambda function reads the Active Directory User Name and CloudFormation stack ID from the event record and stores this information in an Amazon DynamoDB database. 
</ol> 
<ol>
5. The CloudFormation template provisions a custom resource that invokes the AWS Lambda function. 
</ol> 
<ol>
6. The Lambda function reads the user name from the Amazon DynamoDB record associated with the CloudFormation stack ID and returns this information back to the CloudFormation template. 
</ol> 
<b>Prerequisites</b> 
<p>Before you begin implementing this solution, be sure to do the following:</p> 
<ol> 
<li style="list-style-type: none"> 
<ol>
1. Install the AWS CLI: 
</ol> </li> 
</ol> 
<p><a title="undefined" href="https://aws.amazon.com/cli/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/cli/</a></p> 
<ol>
. 
</ol> 
<ol>
2. Install Python 2.7 (including pip). 
</ol> 
<b>Implementation</b> 
<h3>Step 1: Create an Amazon DynamoDB table</h3> 
<p>An Amazon DynamoDB table will be used as central location to store the user name and CloudFormation stack ID.</p> 
<code>aws dynamodb create-table --table-name sc-track-user --attribute-definitions AttributeName=CFStackid,AttributeType=S --key-schema AttributeName=CFStackid,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5</code> 
<h3>Step 2: Create an IAM role</h3> 
<p>Create an IAM role that will be associated with both AWS Lambda functions to grant permission to Amazon DynamoDB table.</p> 
<code>aws iam create-role --role-name sc-lambda-role --assume-role-policy-document &quot;{\&quot;Version\&quot;: \&quot;2012-10-17\&quot;,\&quot;Statement\&quot;:[{\&quot;Effect\&quot;: \&quot;Allow\&quot;,\&quot;Principal\&quot;:{\&quot;Service\&quot;: \&quot;lambda.amazonaws.com\&quot;},\&quot;Action\&quot;: \&quot;sts:AssumeRole\&quot;}]}&quot;</code> 
<h3>Step 3: Create an IAM policy</h3> 
<p>1. Create new file name called lambda-access-policy.json and add following context to the file:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Sid&quot;: &quot;AccessDynamoDB&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;dynamodb:GetItem&quot;,
&quot;dynamodb:PutItem&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:dynamodb:us-east-1:{your AWS Account No}:table/sc-track-user&quot;
]
},
{
&quot;Sid&quot;: &quot;CreateCWLogs&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;
],
&quot;Resource&quot;: &quot;*&quot;
},
{
&quot;Sid&quot;: &quot;WriteCWLog&quot;,
&quot;Action&quot;: [
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:logs:us-east-1: {AWS Account No}:log-group:/aws/lambda/sc-add-user-id:*:*&quot;,
&quot;arn:aws:logs:us-east-1: {AWS Account No}:log-group:/aws/lambda/sc-get-user-id:*:*&quot;
],
&quot;Effect&quot;: &quot;Allow&quot;
}
]
}
</code> 
<p>2. Replace {AWS Account No} with your AWS account number.<br /> 3. Execute the following CLI command:</p> 
<code>aws iam put-role-policy --role-name sc-lambda-role --policy-name Lambda-DynamoDB-CloudWatch --policy-document file://lambda-access-policy.json</code> 
<h3>Step 4: Create an AWS Lambda function</h3> 
<p>Create a Lambda function to store the user name and CloudFormation stack ID.<br /> 1. Create a new file name called adduser.py.<br /> 2. Add the following code to the file:</p> 
<code class="lang-python">import boto3
def lambda_handler(event, context):
dynamodb = boto3.resource('dynamodb',region_name='us-east-1')
table = dynamodb.Table('sc-track-user')
stackId = event['detail']['responseElements']['stackId']
Id = (stackId.split('/'))[-1]
UserArn = event['detail']['userIdentity']['arn']
UserAID = (UserArn.split('/'))[-1]
table.put_item(
Item={
'CFStackid' : Id,
'User' : UserAID
})
return ''
</code> 
<p>3. Zip file as adduser.zip.<br /> 4. Run the following command:</p> 
<code>aws lambda create-function --function-name sc-add-user-id --runtime python2.7 --role arn:aws:iam::{AWS Account No}:role/sc-lambda-role --handler adduser.lambda_handler --timeout 30 --zip-file &quot;fileb://adduser.zip&quot;</code> 
<p><strong>Note</strong>: Before running this command change {AWS Account No} to the correct account number.</p> 
<h3>Step 5: Create an Amazon CloudWatch Event</h3> 
<p>An Amazon CloudWatch Event will invoke a Lambda function each time a new CloudFormation stack is created.</p> 
<p>1. Create the CloudWatch Event:</p> 
<code>aws events put-rule --name &quot;sc-add-user&quot; --event-pattern &quot;{\&quot;source\&quot;:[\&quot;aws.cloudformation\&quot;],\&quot;detail-type\&quot;:[\&quot;AWS API Call via CloudTrail\&quot;],\&quot;detail\&quot;:{\&quot;eventSource\&quot;:[\&quot;cloudformation.amazonaws.com\&quot;],\&quot;eventName\&quot;:[\&quot;CreateStack\&quot;]}}&quot;</code> 
<p>2. Add the Lambda function as the target to the event.</p> 
<code>aws events put-targets --rule sc-add-user --targets &quot;Id&quot;=&quot;LambdaFunction&quot;,&quot;Arn&quot;=&quot;arn:aws:lambda:us-east-1:{AWS Account No}:function:sc-add-user-id&quot;</code> 
<p>3. Grant permission for your event to invoke the Lambda function.</p> 
<code>aws lambda add-permission --function-name sc-add-user-id --statement-id LamdaPermission-10 --action lambda:InvokeFunction --principal events.amazonaws.com --source-arn arn:aws:events:us-east-1: {AWS Account No}:rule/sc-add-user</code> 
<p><strong>Note</strong>: Before running commands change {AWS Account No} to the correct account number, where you need to.</p> 
<h3>Step 6: Call the AWS Lambda function</h3> 
<p>This Lambda function will be called by CloudFormation template to retrieve user name from DynamoDB table</p> 
<p>1. Create a new file name: getuser.py<br /> 2. Add the following code to the file:</p> 
<code class="lang-python">import json
import requests
import os
import boto3
from botocore.exceptions import ClientError
import time
from requests.auth import HTTPBasicAuth
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
dynamodb = boto3.resource('dynamodb')
###########################################################################
# Lambda Handler
def lambda_handler(event, context):
stackId = event['ResourceProperties']['stackId']
Id = (stackId.split('/'))[-1]
responseStatus = 'SUCCESS'
responseData = {}
responseData[&quot;Id&quot;]  = get_user_aid(Id)
sendResponse(event, context, responseStatus, responseData)
#Send Response back to CF
def sendResponse(event, context, responseStatus, responseData):
responseBody = {'Status': responseStatus,
'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
'PhysicalResourceId': responseData[&quot;Id&quot;],
'StackId': event['StackId'],
'RequestId': event['RequestId'],
'LogicalResourceId': event['LogicalResourceId'],
'Data': responseData}
try:
req = requests.put(event['ResponseURL'], data=json.dumps(responseBody))
if req.status_code != 200:
print(req.text)
raise Exception('Received non 200 response while sending response to CFN.')
return
except requests.exceptions.RequestException as e:
print(e)
raise
###########################################################################
# Get User AID
def get_user_aid(stackId):
IValue = ''
table = dynamodb.Table('sc-track-user')
counter = 0
while not IValue:
try:
response = table.get_item(
Key={
'CFStackid': stackId
}
)
if 'Item' in response:
IValue = response['Item']['User']
else:
time.sleep(5)
except ClientError as e:
print(e.response['Error']['Message'])
return(IValue)
</code> 
<p>3. The code for our Lambda function requires a requests Python module that is not included in the standard Python library. To install the requests module, use the command that follows. Note that $PWD is the location of the getuser.py file..:</p> 
<code>pip install -t &quot;$PWD&quot; requests</code> 
<p>4. Zip getuser.py along with all modules installed in the previous step as getuser.zip.<br /> 5. Execute the following command:</p> 
<code>aws lambda create-function --function-name sc-get-user-id --runtime python2.7 --role arn:aws:iam::{AWS Account No}:role/sc-lambda-role --handler getuser.lambda_handler --timeout 120 --zip-file &quot;fileb://getuser.zip</code> 
<h3>Step 7: Modify the CloudFormation template</h3> 
<p>In the final step, we need to add a custom resource to product CloudFormation templates to retrieve the name of the user who provisions the product.</p> 
<p>Here is an example of a CloudFormation template for a product:</p> 
<code class="lang-json">{
&quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;,
&quot;Description&quot;: &quot;test-get-user-aid&quot;,
&quot;Resources&quot;: {
&quot;UserAID&quot;: {
&quot;Type&quot;: &quot;Custom::SCUserAID&quot;,
&quot;Properties&quot;: {
&quot;ServiceToken&quot;: {
&quot;Fn::Join&quot;: [ &quot;&quot;,  
[ &quot;arn:aws:lambda:&quot;, { &quot;Ref&quot;: &quot;AWS::Region&quot; }, &quot;:&quot;, { &quot;Ref&quot;: &quot;AWS::AccountId&quot; }, &quot;:function:sc-get-user-id&quot; ]
] },
&quot;stackId&quot;: {
&quot;Ref&quot;: &quot;AWS::StackId&quot;
}
}
}
},
&quot;Outputs&quot;: {
&quot;Param&quot;: {
&quot;Value&quot;: {
&quot;Fn::GetAtt&quot;: [
&quot;UserAID&quot;,
&quot;Id&quot;
]
}
}
}
}
</code> 
<p>The user name returned by our Lambda function can now be used to tag AWS resources launched through this process.</p> 
<b>Use cases</b> 
<p>Beside proper tagging resource with correct user name there are examples where described solution come handy. Here are couple of examples:</p> 
<h3>Custom DNS</h3> 
<p>In many cases, provisioning products using the AWS Service Catalog requires access over DNS rather than an IP address– for example an HTTPS connection. Since multiple users can launch the same product, the DNS name cannot be hard coded in the CloudFormation template but should be generated dynamically during product provisioning. In such cases, the user ID could be used as a prefix to the DNS name.</p> 
<h3>AWS Service Catalog product access control</h3> 
<p>When users authenticate to AWS through federation, access to AWS Service Catalog is managed through an IAM role. In such cases, an organization might require excluding access to AWS Service Catalog products) for certain users. This can be accomplished by creating an Amazon DynamoDB table with a list of users who have access to products and then modifying the sc-get-user-id Lambda function so the function will query the table and return status FAILED when user doesn’t have permission to provision a given product. The name of the product can be passed to the Lambda function as additional parameter directly from CloudFormation template.</p> 
<b>About the Author</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/19/remek.jpg" /><br /> Remek Hetman is a Senior Cloud Infrastructure Architect with the Amazon Web Services ProServe team. He works with AWS Enterprise customers providing technical guidance and assistance for Infrastructure, DevOps, and big data to help them make the best use of AWS services. Outside of work he enjoys spending time actively as well as pursing his passion – astronomy.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Automating IAM Roles For Cross-Account Access Series Overview</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>The <a href="https://aws.amazon.com/blogs/apn/">AWS Partner Network Blog</a> has recently published a series describing a method to automate the creation of an <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html">IAM role</a> for <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html">cross-account access</a>, and how to collect the information needed for a partner to assume the role after creation. This post gives readers an overview of the series, summarizing each of the individual posts with links back to the original content for further reading.</p> 
<p>As a reminder, cross-account IAM roles <a href="https://aws.amazon.com/blogs/apn/securely-accessing-customer-aws-accounts-with-cross-account-iam-roles/">allow customers to grant access</a> to resources within their account to a partner or other third parties while enabling the customers to maintain their security posture. Cross-account roles allow the customer to delegate access without the need to distribute key material, and without the burden on the third party to safely handle key material after receipt.</p> 
<p>The blog series kicked off with a <a href="https://aws.amazon.com/blogs/apn/easing-the-creation-of-cross-account-roles-for-customers/">post</a> that explained how to create a custom launch stack URL for AWS CloudFormation. The URL will take users directly to the CloudFormation Create Stack wizard, with values for the Amazon S3 template location, stack name, and default parameters already populated. The launch stack URL eliminates the need to exchange template files with the customer, and ensures that the customer is using the proper template with the correct values.</p> 
<p><span id="more-1250"></span></p> 
<p>The <a href="https://aws.amazon.com/blogs/apn/generating-custom-aws-cloudformation-templates-with-lambda-to-create-cross-account-roles/">second post</a> describes how to use an AWS Lambda function to populate a AWS CloudFormation template with uniquely generated values. The series example uses an <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html">External ID</a>, an ID that is unique for each end user. This value needs to be set within the CloudFormation template. When triggered, the Lambda function pulls down the default template, inserts a generated unique External ID into the template, and uploads the customized template to an S3 bucket. Once the template upload is complete, the end user is presented with a custom launch stack URL containing the unique template Amazon S3 location. Finally, we demonstrated how to use the Launch Stack icon to make the URL more visible to users.</p> 
<p>The <a href="https://aws.amazon.com/blogs/apn/collecting-information-from-aws-cloudformation-resources-created-in-external-accounts-with-custom-resources/">third post</a> details how to reliably return the Amazon Resource Name (ARN) of the cross-account role created by AWS CloudFormation to a third party. As a reminder, the third party must use the ARN, as well as the ExternalID, when assuming the role in the end user’s account. The post demonstrates a CloudFormation <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">custom resource</a> designed to send the ARN back to the third-party account, which consumes the ARN and stores it for later use.</p> 
<p>The <a href="https://aws.amazon.com/blogs/apn/wrap-up-cross-account-role-onboarding-workflow/">final post</a> of the series brings the details of the previous three blog posts together into one cohesive solution. It shows how to implement the automation of cross-account role creation for customer onboarding using the techniques described in each post in a completed workflow. The workflow creates a smoother onboarding experience for the customer while creating a secure way for the third party account to create resources within the customer account.</p> 
<p>We hope that the blog series can help you and your company improve your customer on-boarding experience. You can avoid the sharing of sensitive keys and the error-prone approach of requiring your customers to cut and paste information in their account and your on-boarding portal.</p> 
<hr /> 
<p><strong>About the Author</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/07/headshot-Erin-220x300.png" />Erin McGill is a Solutions Architect in the AWS Partner Program with a focus on DevOps and automation tooling.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Gain Visibility into the Execution of Your AWS Lambda functions with AWS CloudTrail</b> 
<p style="margin: 0; padding:0;">=======================<p>
<p>Today, we are happy to announce that AWS CloudTrail now supports the Lambda Invoke API as a new data event type with the launch of CloudTrail Lambda data events. Previously, AWS CloudTrail supported management events for AWS Lambda, which allowed you to capture when and by whom a function was created, modified, or deleted. With this new feature, you can now gain visibility into when and by whom an Invoke API call was made and which Lambda function was executed. CloudTrail can deliver these logs to Amazon S3 or Amazon CloudWatch Logs, and for near real-time processing, you can send them to Amazon CloudWatch Events to build event-driven security pipelines for the Lambda functions within your serverless applications.</p> 
<p><span id="more-2143"></span></p> 
<h4>AWS CloudTrail Lambda data events</h4> 
<p>Serverless computing with AWS Lambda is an increasingly popular business solution. Lambda allows you to run code in the cloud without managing or provisioning servers. You write your code, package it as a Lambda function, and hook it up with one or more event sources that trigger the function in response to events. All Lambda functions are executed using the Lambda Invoke API, whether you manually invoke your Lambda function via the AWS CLI, SDK, or set it up with any of the event source integrations. The AWS CloudTrail Lambda data events feature enables you to capture this invoke activity within your account to see who, when, and how your functions are being executed.</p> 
<h4>How can I use AWS CloudTrail Lambda data events?</h4> 
<p>AWS CloudTrail Lambda data events can be used to detect and automatically act on invocations of Lambda functions across your AWS account. For example, you can now:</p> 
<li>Meet your IT auditing and compliance requirements. With CloudTrail data events, you can validate that your functions were invoked by permitted users, roles, and services. Customers with regulatory audit and compliance requirements can maintain the same level of visibility and auditability of Lambda function invokes as they do for other AWS service API invocations.</li> 
<li>Perform near real-time and ad-hoc security analysis. Lambda data events are delivered to your log delivery destination just like your existing CloudTrail logs, so it is easy to integrate this new log data type into your security analytics pipelines. You can analyze Lambda function invoke activity in near real time with Amazon CloudWatch Events and Amazon CloudWatch Logs.</li> 
<li>Better understand usage patterns across your AWS Lambda functions. You can perform billing analysis to understand the top user, role, or service callers of Lambda functions. You can also tie costs to specific users and groups for internal chargeback purposes or build reports on function usage across different teams.</li> 
<p>Let’s take a look at how this integration works.</p> 
<h4>Setting up Lambda Data Events</h4> 
<p>Setting up AWS CloudTrail Lambda data events is easy. You can add Lambda data events to an existing trail that you have already created, or you can create a new separate trail just for Lambda data events – either option is suitable and will depend on how you plan to organize and manage your CloudTrail logs. In my example, I have created a separate trail called <strong>LambdaEvents</strong> and have configured it to<em><strong> Log all current and future functions</strong></em>. This feature is really handy and allows me to “set and forget” without needing to revisit my CloudTrail configuration each time a new function or version is published.</p> 
<p><em>&nbsp;You might be wondering what that S3 tab is for in the following image. Remember, AWS CloudTrail also supports data events for Amazon S3, which Jeff Barr discusses in his blog post <a href="https://aws.amazon.com/blogs/aws/cloudtrail-update-capture-and-process-amazon-s3-object-level-api-activity/">here</a>.</em></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/30/trail_setup-1024x550.png" /></p> 
<p>Once I have configured a new trail called LambdaEvents and enabled it across all AWS Regions for all existing and future functions, I now need to setup a storage location for CloudTrail to store the log files that will be generated. I have previously created my trail to deliver logs to a bucket dedicated to my Lambda data events (aws-ksomers-lambda-cloudtrail). I have left the rest of the settings at the default, but I could optionally enable a few additional things such as log file validation, encryption using AWS KMS, and notifications using Amazon SNS.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/30/storage-1024x513.png" /></p> 
<p>Now that CloudTrail Lambda data events is configured, let’s take a look at how data is presented to us with Lambda data events.</p> 
<p>In my example, I invoked my BlogFunction from the AWS CLI with my “kyle” IAM user as a synchronous request with Lambda invoke type of “RequestResponse”. Remember that Lambda can be invoked either synchronously via a RequestResponse invocation type, asynchronously via an “Event” invocation type, or used in test mode with a “DryRun” invocation type. That invocation generated a CloudTrail log file in my Amazon S3 bucket. Let’s have a look at the event:</p> 
<code class="lang-js">{
&quot;eventVersion&quot;: &quot;1.06&quot;,
&quot;userIdentity&quot;: {
&quot;type&quot;: &quot;IAMUser&quot;,
&quot;principalId&quot;: &quot;AIDAIOR74VCJ2M3NB8U4M&quot;,
&quot;arn&quot;: &quot;arn:aws:iam::999999999999:user/kyle&quot;,
&quot;accountId&quot;: &quot;999999999999&quot;,
&quot;accessKeyId&quot;: &quot;AK2IU7DKE7U2KOI8CCBP&quot;,
&quot;userName&quot;: &quot;kyle&quot;
},
&quot;eventTime&quot;: &quot;2017-11-29T08:47:45Z&quot;,
&quot;eventSource&quot;: &quot;lambda.amazonaws.com&quot;,
&quot;eventName&quot;: &quot;Invoke&quot;,
&quot;awsRegion&quot;: &quot;us-west-2&quot;,
&quot;sourceIPAddress&quot;: &quot;192.168.0.1&quot;,
&quot;userAgent&quot;: &quot;aws-cli/1.11.129 Python/2.7.8 Linux/3.1.56-0.6.839hdh3.x86_64 botocore/1.5.92&quot;,
&quot;requestParameters&quot;: {
&quot;invocationType&quot;: &quot;RequestResponse&quot;,
&quot;functionName&quot;: &quot;arn:aws:lambda:us-west-2:999999999999:function:BlogFunction:prod&quot;,
&quot;clientContext&quot;: &quot;ew0KICAiY29udGV4dGtleSIgOiAiY29udGV4dHZhbHVlIg0KfQ==&quot;,
&quot;qualifier&quot;: &quot;prod&quot;
},
&quot;responseElements&quot;: null,
&quot;additionalEventData&quot;: {
&quot;functionVersion&quot;: &quot;arn:aws:lambda:us-west-2:999999999999:function:BlogFunction:4&quot;
},
&quot;requestID&quot;: &quot;eaccb900-8f45-11e7-b60d-179cdf501g92&quot;,
&quot;eventID&quot;: &quot;0e205f1d-3929-4803-b887-0d2aca122148&quot;,
&quot;readOnly&quot;: false,
&quot;resources&quot;: [{
&quot;accountId&quot;: &quot;999999999999&quot;,
&quot;type&quot;: &quot;AWS::Lambda::Function&quot;,
&quot;ARN&quot;: &quot;arn:aws:lambda:us-west-2:999999999999:function:BlogFunction&quot;
}],
&quot;eventType&quot;: &quot;AwsApiCall&quot;,
&quot;managementEvent&quot;: false,
&quot;recipientAccountId&quot;: &quot;999999999999&quot;,
&quot;sharedEventID&quot;: &quot;6159da59-ad2f-4e04-9669-cf0a6b6b4827&quot;</code> 
<p>When a Lambda function is invoked, the function being invoked will show up in three locations within the CloudTrail Lambda data event. Here is a breakdown of what each one represents:</p> 
<ol> 
<li>“<strong>requestParameters:functionName</strong>” – This field will match the input of the function that was called using the API (qualified version or unqualified). In my example, my function is deployed with a Lambda alias of “prod,” so this is displayed in the ARN.</li> 
<li>“<strong>additionalEventData:functionVersion</strong>” – Here you will see the exact qualified function version that was invoked based on the input in the requestParameters of your Lambda invoke. Remember that a Lambda function alias points to an immutable version of a Lambda function. In this case, I invoked my function via the alias, but the ARN that was executed was version 4 of my function. This field will display the version ARN that was invoked.</li> 
<li>“<strong>resources:ARN</strong>” – This will show the unqualified function ARN (some refer to it as the “base” ARN).</li> 
</ol> 
<p>In the previous example, we looked at the contents of a Lambda Invoke API request of RequestResponse invocation type. For details on the contents of other invocation types such as Event Invocation Types, Cross-Account invocations, invocations from AWS services such as stream-based event sources, check out the documentation <a href="http://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html">here</a>.</p> 
<h4>Lambda data events example</h4> 
<p>One interesting way you can use this new feature is to monitor your Lambda functions for unauthorized invocations. Let’s take a look at an example of how you can use Lambda data events to check for unauthorized invocations with one of our classic Lambda examples, the S3 Event Notification Trigger for PUT Object Requests on a Bucket:\</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/30/diagram-1024x258.png" /></p> 
<p>In this architecture, there is an S3 bucket that is configured to invoke a Lambda function when new objects are created in the bucket (or any other event you would like). This function might update an Amazon DynamoDB table, send an email, or any other use case you may have!</p> 
<p>With CloudTrail Lambda data events, you can validate that your function is only being invoked by Amazon S3. Every invocation of your function generates a CloudTrail log event which can be delivered to an S3 bucket for long-term retention. You can also configure a CloudWatch Events Rule and run real-time business logic on these log events with Lambda. In this case, if the “<em>userIdentity:invokedBy</em>” field in my CloudTrail log event isn’t “<em>s3.amazonaws.com</em>”, then my remediation Lambda can update my Lambda Function Policy to properly restrict invocation access to Amazon S3.</p> 
<p><strong>Pricing and availability</strong></p> 
<p>Once an AWS CloudTrail trail is set up, Amazon S3 charges apply based on your usage, since AWS CloudTrail delivers logs to an S3 bucket. Data events are recorded only for the functions you specify and are charged at $0.10 per 100,000 events, the same as S3 data events. See CloudTrail <a href="https://aws.amazon.com/cloudtrail/pricing/">pricing page</a> for more details. CloudTrail Lambda data events are available in all AWS public Regions and AWS GovCloud (US).</p> 
<h3>What’s coming</h3> 
<p>Several AWS partners such as Sumo Logic, Rapid7, Saviynt, Alert Logic, and Evident.io are working on integrations with CloudTrail Lambda Data Events, so keep an eye out for updates!</p> 
<hr /> 
<p><strong>About the author</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/30/kyle.jpg" />Kyle Somers is a Solutions Architect at Amazon Web Services. He works with startups and Focused Territory customers to provide architectural guidance for building applications in the cloud. His passion is AWS Lambda and he serves as a Serverless SME within the AWS community. In his spare time, he likes to travel, read up on the latest tech, and cheer on the University of Arizona Wildcats.</p> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
