<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/devopsblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li class="active"><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="devopsblogs1.html">Page 1</a>|<a href="devopsblogs2.html">Page 2</a>|<a href="devopsblogs3.html">Page 3</a>|<a href="devopsblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Introducing Git Credentials: A Simple Way to Connect to AWS CodeCommit Repositories Using a Static User Name and Password</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ankur Agarwal</span></span> | on 
<time property="datePublished" datetime="2016-12-22T16:20:12+00:00">22 DEC 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/new-stuff/" title="View all posts in New stuff"><span property="articleSection">New stuff</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/introducing-git-credentials-a-simple-way-to-connect-to-aws-codecommit-repositories-using-a-static-user-name-and-password/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Today, AWS is introducing a simplified way to authenticate to your AWS CodeCommit repositories over HTTPS.</p> 
<p>With Git credentials, you can generate a static user name and password in the Identity and Access Management (IAM) console that you can use to access AWS CodeCommit repositories from the command line, Git CLI, or any Git tool that supports HTTPS authentication.</p> 
<p>Because these are static credentials, they can be cached using the password management tools included in your local operating system or stored in a credential management utility. This allows you to get started with AWS CodeCommit within minutes. You don’t need to download the AWS CLI or configure your Git client to connect to your AWS CodeCommit repository on HTTPS. You can also use the user name and password to connect to the AWS CodeCommit repository from third-party tools that support user name and password authentication, including popular Git GUI clients (such as TowerUI) and IDEs (such as Eclipse, IntelliJ, and Visual Studio).</p> 
<p>So, why did we add this feature? Until today, users who wanted to use HTTPS connections were required to configure the AWS credential helper to authenticate their AWS CodeCommit operations. Customers told us our credential helper sometimes interfered with password management tools such as Keychain Access and Windows Vault, which caused authentication failures. Also, many Git GUI tools and IDEs require a static user name and password to connect with remote Git repositories and do not support the credential helper.</p> 
<p>In this blog post, I’ll walk you through the steps for creating an AWS CodeCommit repository, generating Git credentials, and setting up CLI access to AWS CodeCommit repositories.</p> 
<p><strong><br /> Git Credentials Walkthrough<br /> </strong>Let’s say Dave wants to create a repository on AWS CodeCommit and set up local access from his computer.</p> 
<p><span style="text-decoration: underline"><strong>Prerequisite:</strong> </span>If Dave had previously configured his local computer to use the credential helper for AWS CodeCommit, he must edit his .gitconfig file to remove the credential helper information from the file. Additionally, if his local computer is running macOS, he might need to clear any cached credentials from Keychain Access.</p> 
<p>With Git credentials, Dave can now create a repository and start using AWS CodeCommit in four simple steps.</p> 
<p><strong>Step 1: Make sure the IAM user has the required permissions<br /> </strong>Dave must have the following managed policies attached to his IAM user (or their equivalent permissions) before he can set up access to AWS CodeCommit using Git credentials.</p> 
<li>AWSCodeCommitPowerUser (or an appropriate CodeCommit managed policy)</li> 
<li>IAMSelfManageServiceSpecificCredentials</li> 
<li>IAMReadOnlyAccess</li> 
<p><strong>Step 2: Create an AWS CodeCommit repository<br /> </strong>Next, Dave signs in to the AWS CodeCommit console and create a repository, if he doesn’t have one already. He can choose any repository in his AWS account to which he has access. The instructions to create Git credentials are shown in the help panel. (Choose the Connect button if the instructions are not displayed.) When Dave clicks the IAM user link, the IAM console will open and he can generate the credentials.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/21/GitCred_Blog1.jpg" /></p> 
<p>&nbsp;</p> 
<p><strong>Step 3: Create HTTPS Git credentials in the IAM console<br /> </strong>On the IAM user page, Dave selects the <span style="text-decoration: underline">Security Credentials</span> tab and clicks Generate under <span style="text-decoration: underline">HTTPS Git credentials for AWS CodeCommit</span> section. This creates and displays the user name and password. Dave can then download the credentials.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/21/GitCred_Blog2.jpg" /></p> 
<p><strong>Note:</strong> This is the only time the password is available to view or download.</p> 
<p>&nbsp;</p> 
<p><strong>Step 4: Clone the repository on the local machine<br /> </strong>On the AWS CodeCommit console page for the repository, Dave chooses Clone URL, and then copy the HTTPS link for cloning the repository. At the command line or terminal, Dave will use the link he just copied to clone the repository. For example, Dave copies:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/21/GitCred_Blog3.jpg" /></p> 
<p>&nbsp;</p> 
<p>And then at the command line or terminal, Dave types:</p> 
$ git clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/TestRepo_Dave 
<p>When prompted for user name and password, Dave provides the Git credentials (user name and password) he generated in step 3.</p> 
<p>Dave is now ready to start pushing his code to the new repository.</p> 
<p>Git credentials can be made active or inactive based on your requirements. You can also reset the password if you would like to use the existing username with a new password.</p> 
<p><strong>Next Steps</strong></p> 
<ol> 
<li>You can optionally cache your credentials using the Git credentials caching command <a href="https://git-scm.com/docs/git-credential-cache" target="_blank">here</a>.</li> 
<li>Want to invite a collaborator to work on your AWS CodeCommit repository? Simply create a new IAM user in your AWS account, create Git credentials for that user, and securely share the repository URL and Git credentials with the person you want to collaborate on the repositories.</li> 
<li>Connect to any third-party client that supports connecting to remote Git repositories using Git credentials (a stored user name and password). Virtually all tools and IDEs allow you to connect with static credentials. We’ve tested these: 
<li>Visual Studio (using the default Git plugin)</li> 
<li>Eclipse IDE (using the default Git plugin)</li> 
<li>Git Tower UI</li> 
</ul> </li> 
</ol> 
<p>For more information, see the <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html" target="_blank">AWS CodeCommit documentation</a>.</p> 
<p>We are excited to provide this new way of connecting to AWS CodeCommit. We look forward to hearing from you about the many different tools and IDEs you will be able to use with your AWS CodeCommit repositories.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">DevOps and Continuous Delivery at re:Invent 2016 – Wrap-up</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Frank Li</span></span> | on 
<time property="datePublished" datetime="2016-12-15T15:38:56+00:00">15 DEC 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/new-stuff/" title="View all posts in New stuff"><span property="articleSection">New stuff</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/devops-and-continuous-delivery-at-reinvent-2016-wrap-up/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>The AWS re:Invent 2016 conference was packed with some exciting announcements and sessions around DevOps and Continuous Delivery. We launched <a title="undefined" href="https://aws.amazon.com/codebuild/" target="_blank">AWS CodeBuild</a>, a fully managed build service that eliminates the need to provision, manage, and scale your own build servers. You now have the ability to run your <a title="undefined" href="https://aws.amazon.com/devops/continuous-integration/" target="_blank">continuous integration</a> and <a title="undefined" href="https://aws.amazon.com/devops/continuous-delivery/" target="_blank">continuous delivery</a> process entirely on AWS by plugging AWS CodeBuild into <a title="undefined" href="https://aws.amazon.com/codepipeline/" target="_blank">AWS CodePipeline</a>, which automates building, testing, and deploying code each time you push a change to your source repository. If you are interested in learning more about AWS CodeBuild, you can sign up for the webinar on January 20th <a title="undefined" href="https://publish.awswebcasts.com/content/connect/c1/7/en/events/event/private/23850344/41359021/event_registration.html?connect-session=graysonbreezmc6gg36rqdt6ehbb&amp;sco-id=54633193&amp;" target="_blank">here</a>.</p> 
<p>The DevOps track had over 30 different breakout sessions ranging from customer stories to deep dive talks to best practices. If you weren’t able to attend the conference or missed a specific session, here is a link to the entire <a title="undefined" href="https://www.youtube.com/playlist?list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">playlist</a>.</p> 
<p>&nbsp;</p> 
<p>There were a number of talks that can help you get started with your own DevOps practices for rapid software delivery.&nbsp;Here are some introductory sessions to give you the proper background:<br /> DEV201: <a title="undefined" href="https://www.youtube.com/watch?v=-ddpq2VQNxo&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Accelerating Software Delivery with AWS Developer Tools</a><br /> DEV211: <a title="undefined" href="https://www.youtube.com/watch?v=y9wuBtWSbvc&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Automated DevOps and Continuous Delivery</a></p> 
<p>After you understand the big picture, you can dive into automating your software delivery. Here are some sessions on how to deploy your applications:<br /> DEV310: <a title="undefined" href="https://www.youtube.com/watch?v=bSXRF1poE8g&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Choosing the Right Software Deployment Technique</a><br /> DEV403: <a title="undefined" href="https://www.youtube.com/watch?v=_xmYShSDDJg&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Advanced Continuous Delivery Techniques</a><br /> DEV404: <a title="undefined" href="https://www.youtube.com/watch?v=bWOGZhQWqAo&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Develop, Build, Deploy, and Manage Services and Applications</a></p> 
<p>Finally, to maximize your DevOps efficiency, you’ll want to automate the provisioning of your infrastructure. Here are a couple sessions on how to manage your infrastructure:<br /> DEV313: <a title="undefined" href="https://www.youtube.com/watch?v=TDalsML3QqY" target="_blank">Infrastructure Continuous Delivery Using AWS CloudFormation</a><br /> DEV319: <a title="undefined" href="https://www.youtube.com/watch?v=Epx_32c3c6s&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Automating Cloud Management &amp; Deployment</a></p> 
<p>If you’re a Lambda developer, be sure to watch this session and read this <a title="undefined" href="http://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html" target="_blank">documentation</a> on how to practice continuous delivery for your serverless applications:<br /> SVR307: <a title="undefined" href="https://www.youtube.com/watch?v=8Zd-8GV-1mY" target="_blank">Application Lifecycle Management in a Serverless World</a></p> 
<p>For all 30+ DevOps sessions, click <a title="undefined" href="https://www.youtube.com/playlist?list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">here</a>.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deploy an App to an AWS OpsWorks Layer Using AWS CodePipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Daniel Huesch</span></span> | on 
<time property="datePublished" datetime="2016-12-05T08:26:20+00:00">05 DEC 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/deploy-an-app-to-an-aws-opsworks-layer-using-aws-codepipeline/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>Deploy an App to an AWS OpsWorks Layer Using AWS CodePipeline</strong></p> 
<p><a href="https://aws.amazon.com/codepipeline/" target="_blank">AWS CodePipeline</a> lets you create continuous delivery pipelines that automatically track code changes from sources such as <a href="http://aws.amazon.com/codecommit/">AWS CodeCommit</a>, <a href="http://aws.amazon.com/s3/">Amazon S3</a>, or <a href="https://github.com/" target="_blank">GitHub</a>. Now, you can use AWS CodePipeline as a code change-management solution for apps, Chef cookbooks, and recipes that you want to deploy with <a href="http://aws.amazon.com/opsworks/">AWS OpsWorks</a>.</p> 
<p>This blog post demonstrates how you can create an automated pipeline for a simple <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">Node.js app</a> by using AWS CodePipeline and AWS OpsWorks. After you configure your pipeline, every time you update your <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">Node.js app</a>, AWS CodePipeline passes the updated version to AWS OpsWorks. AWS OpsWorks then deploys the updated app to your fleet of instances, leaving you to focus on improving your application. AWS makes sure that the latest version of your app is deployed.</p> 
<p><strong>Step 1: Upload app code to an Amazon S3 bucket</strong></p> 
<p>The Amazon S3 bucket must be in the same region in which you later create your pipeline in AWS CodePipeline. For now, AWS CodePipeline supports the AWS OpsWorks provider in the us-east-1 region only; all resources in this blog post should be created in the US East (N. Virginia) region. The bucket must also be versioned, because AWS CodePipeline requires a versioned source. For more information, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html" target="_blank">Using Versioning</a>.</p> 
<p><strong>Upload your app to an Amazon S3 bucket</strong></p> 
<ol> 
<li>Download a ZIP file of the AWS OpsWorks sample, <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">Node.js app</a>, and save it to a convenient location on your local computer: <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip</a>.</li> 
<li>Open the Amazon S3 console at <a href="https://console.aws.amazon.com/s3/" target="_blank">https://console.aws.amazon.com/s3/</a>. Choose <strong>Create Bucket</strong>. Be sure to enable versioning.</li> 
<li>Choose the bucket that you created and upload the ZIP file that you saved in step 1.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-01.png" /></li> 
<li>In the <strong>Properties</strong> pane for the uploaded ZIP file, make a note of the S3 link to the file. You will need the bucket name and the ZIP file name portion of this link to create your pipeline.</li> 
</ol> 
<p><strong>Step 2: Create an AWS OpsWorks to Amazon EC2 service role</strong></p> 
<p style="margin-left: 39.25pt">1.&nbsp;&nbsp;&nbsp;&nbsp; Go to the Identity and Access Management (IAM) service console, and choose <strong>Roles</strong>.<br /> 2.&nbsp;&nbsp;&nbsp;&nbsp; Choose <strong>Create Role</strong>, and name it <strong>aws-opsworks-ec2-role-with-s3</strong>.<br /> 3.&nbsp;&nbsp;&nbsp;&nbsp; In the <strong>AWS Service Roles</strong> section, choose <strong>Amazon EC2</strong>, and then choose the policy called <strong>AmazonS3ReadOnlyAccess</strong>.<br /> 4.&nbsp;&nbsp;&nbsp;&nbsp; The new role should appear in the <strong>Roles</strong> dashboard.</p> 
<p style="margin-left: 39.25pt"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-02.png" /></p> 
<p><strong>Step 3: Create an AWS OpsWorks Chef 12 Linux stack</strong></p> 
<p>To use AWS OpsWorks as a provider for a pipeline, you must first have an AWS OpsWorks stack, a layer, and at least one instance in the layer. As a reminder, the Amazon S3 bucket to which you uploaded your app must be in the same region in which you later create your AWS OpsWorks stack and pipeline, US East (N. Virginia).</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; In the OpsWorks console, choose <strong>Add Stack</strong>, and then choose a Chef 12 stack.<br /> 2.&nbsp;&nbsp;&nbsp;&nbsp; Set the stack’s name to <strong>CodePipeline Demo</strong> and make sure the <strong>Default operating system</strong> is set to Linux.<br /> 3.&nbsp;&nbsp;&nbsp;&nbsp; Enable <strong>Use custom Chef cookbooks</strong>.<br /> 4.&nbsp;&nbsp;&nbsp;&nbsp; For<strong> Repository type</strong>, choose <strong>HTTP Archive</strong>, and then use the following <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-cookbook.zip">cookbook repository on S3</a>: <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-cookbook.zip">https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-cookbook.zip</a>. This repository contains a set of <a href="https://docs.chef.io/cookbooks.html">Chef cookbooks</a> that include Chef recipes you’ll use to install the Node.js package and its dependencies on your instance. You will use these Chef recipes to deploy the Node.js app that you prepared in step 1.1.</p> 
<p><strong>Step 4: Create and configure an AWS OpsWorks layer </strong></p> 
<p>Now that you’ve created an AWS OpsWorks stack called CodePipeline Demo, you can create an OpsWorks layer.</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; Choose <strong>Layers</strong>, and then choose<strong> Add Layer</strong> in the AWS OpsWorks stack view.<br /> 2.&nbsp;&nbsp;&nbsp;&nbsp; Name the layer <strong>Node.js App Server</strong>. For <strong>Short Name</strong>, type <strong>app1</strong>, and then choose <strong>Add Layer</strong>.<br /> 3.&nbsp;&nbsp;&nbsp;&nbsp; After you create the layer, open the layer’s <strong>Recipes</strong> tab. In the <strong>Deploy</strong> lifecycle event, type <strong>nodejs_demo</strong>. Later, you will link this to a Chef recipe that is part of the Chef cookbook you referenced when you created the stack in step 3.4. This Chef recipe runs every time a new version of your application is deployed.</p> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-03.png" /></p> 
<p>4.&nbsp;&nbsp;&nbsp;&nbsp; Now, open the <strong>Security</strong> tab, choose <strong>Edit</strong>, and choose <strong>AWS-OpsWorks-WebApp</strong> from the <strong>Security groups</strong> drop-down list. You will also need to set the <strong>EC2 Instance Profile</strong> to use the service role you created in step 2.2 (<strong>aws-opsworks-ec2-role-with-s3</strong>).</p> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-04.png" /></p> 
<p><strong>Step 5: Add your App to AWS OpsWorks</strong></p> 
<p>Now that your layer is configured, add the Node.js demo app to your AWS OpsWorks stack. When you create the pipeline, you’ll be required to reference this demo Node.js app.</p> 
<ol> 
<li>Have the Amazon S3 bucket link from the step 1.4 ready. You will need the link to the bucket in which you stored your test app.</li> 
<li>In AWS OpsWorks, open the stack you created (CodePipeline Demo), and in the navigation pane, choose <strong>Apps</strong>.</li> 
<li>Choose <strong>Add App</strong>.</li> 
<li>Provide a name for your demo app (for example, <strong>Node.js Demo App</strong>), and set the <strong>Repository type</strong> to an S3 Archive. Paste your S3 bucket link (<strong>s3://<em>bucket-name</em>/<em>file name</em></strong>) from step 1.4.</li> 
<li>Now that your app appears in the list on the <strong>Apps</strong> page, add an instance to your OpsWorks layer.</li> 
</ol> 
<p><strong>Step 6: Add an instance to your AWS OpsWorks layer</strong></p> 
<p>Before you create a pipeline in AWS CodePipeline, set up at least one instance within the layer you defined in step 4.</p> 
<ol> 
<li>Open the stack that you created (CodePipeline Demo), and in the navigation pane, choose <strong>Instances</strong>.</li> 
<li>Choose <strong>+Instance</strong>, and accept the default settings, including the hostname, size, and subnet. Choose <strong>Add Instance</strong>.</li> 
</ol> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-05.png" /></p> 
<ol> 
<li>By default, the instance is in a stopped state. Choose <strong>start </strong>to start the instance.</li> 
</ol> 
<p><strong>Step 7: Create a pipeline in AWS CodePipeline </strong></p> 
<p>Now that you have a stack and an app configured in AWS OpsWorks, create a pipeline with AWS OpsWorks as the provider to deploy your app to your specified layer. If you update your app or your Chef deployment recipes, the pipeline runs again automatically, triggering the deployment recipe to run and deploy your updated app.</p> 
<p>This procedure creates a simple pipeline that includes only one <strong>Source</strong> and one <strong>Deploy</strong> stage. However, you can create more complex pipelines that use AWS OpsWorks as a provider.</p> 
<p><strong>To create a pipeline</strong></p> 
<ol> 
<li>Open the AWS CodePipeline console in the U.S. East (N. Virginia) region.</li> 
<li>Choose <strong>Create pipeline</strong>.</li> 
<li>On the <strong>Getting started with AWS CodePipeline</strong> page, type <strong>MyOpsWorksPipeline</strong>, or a pipeline name of your choice, and then choose <strong>Next step</strong>.</li> 
<li>On the <strong>Source Location</strong> page, choose <strong>Amazon S3</strong> from the <strong>Source</strong> provider drop-down list.</li> 
<li>In the <strong>Amazon S3</strong> details area, type the Amazon S3 bucket path to your application, in the format <strong>s3://<em>bucket-name</em>/<em>file name</em></strong>. Refer to the link you noted in step 1.4. Choose <strong>Next step</strong>.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-06.png" /></li> 
<li>On the <strong>Build</strong> page, choose <strong>No Build</strong> from the drop-down list, and then choose <strong>Next step</strong>.</li> 
<li>On the <strong>Deploy</strong> page, choose <strong>AWS OpsWorks</strong> as the deployment provider.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-07-2.png" /></li> 
<li>Specify the names of the stack, layer, and app that you created earlier, then choose <strong>Next step</strong>.</li> 
<li>On the <strong>AWS Service Role</strong> page, choose <strong>Create Role</strong>. On the IAM console page that opens, you will see the role that will be created for you (<strong>AWS-CodePipeline-Service</strong>). From the <strong>Policy Name</strong> drop-down list, choose <strong>Create new policy</strong>. Be sure the policy document has the following content, and then choose <strong>Allow</strong>.<br /> For more information about the service role and its policy statement, see <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/access-permissions.html#how-to-custom-role" target="_blank">Attach or Edit a Policy for an IAM Service Role</a>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-08-2.png" /></li> 
<li>On the <strong>Review your pipeline</strong> page, confirm the choices shown on the page, and then choose <strong>Create pipeline</strong>.</li> 
</ol> 
<p style="margin-left: .5in">The pipeline should now start deploying your app to your OpsWorks layer on its own.&nbsp; Wait for deployment to finish; you’ll know it’s finished when <strong>Succeeded</strong> is displayed in both the <strong>Source</strong> and <strong>Deploy</strong> stages.</p> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-09-2.png" /></p> 
<p><strong>Step 8: Verifying the app deployment</strong></p> 
<p>To verify that AWS CodePipeline deployed the Node.js app to your layer, sign in to the instance you created in step 4. You should be able to see and use the Node.js web app.</p> 
<ol> 
<li>On the AWS OpsWorks dashboard, choose the stack and the layer to which you just deployed your app.</li> 
<li>In the navigation pane, choose <strong>Instances</strong>, and then choose the public IP address of your instance to view the web app. The running app will be displayed in a new browser tab.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-10-2.png" /></li> 
<li>To test the app, on the app’s web page, in the <strong>Leave a comment</strong> text box, type a comment, and then choose <strong>Send</strong>. The app adds your comment to the web page. You can add more comments to the page, if you like.</li> 
</ol> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-11-2.png" /></p> 
<p><strong>Wrap-up</strong></p> 
<p>You now have a working and fully automated pipeline. As soon as you make changes to your application’s code and update the S3 bucket with the new version of your app, AWS CodePipeline automatically collects the artifact and uses AWS OpsWorks to deploy it to your instance, by running the OpsWorks deployment Chef recipe that you defined on your layer. The deployment recipe starts all of the operations on your instance that are required to support a new version of your artifact.</p> 
<p>To learn more about Chef cookbooks and recipes: <a href="https://docs.chef.io/cookbooks.html">https://docs.chef.io/cookbooks.html</a></p> 
<p>To learn more about the AWS OpsWorks and AWS CodePipeline integration: <a href="https://docs.aws.amazon.com/opsworks/latest/userguide/other-services-cp.html">https://docs.aws.amazon.com/opsworks/latest/userguide/other-services-cp.html</a></p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">IT Governance in a Dynamic DevOps Environment</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Shashi Prabhakar</span></span> | on 
<time property="datePublished" datetime="2016-11-30T17:57:02+00:00">30 NOV 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/it-governance-in-a-dynamic-devops-environment/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-451" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=451&amp;disqus_title=IT+Governance+in+a+Dynamic+DevOps+Environment&amp;disqus_url=https://aws.amazon.com/blogs/devops/it-governance-in-a-dynamic-devops-environment/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-451');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>IT Governance in a Dynamic DevOps Environment</strong><br /> Governance involves the alignment of security and operations with productivity to ensure a company achieves its business goals. Customers who are migrating to the cloud might be in various stages of implementing governance. Each stage poses its own challenges. In this blog post, the first in a series, I will discuss a four-step approach to automating governance with AWS services.</p> 
<p><strong>Governance and the DevOps Environment</strong><br /> Developers with a DevOps and agile mindset are responsible for building and operating services. They often rely on a central security team to develop and apply policies, seek security reviews and approvals, or implement best practices.</p> 
<p>These policies and rules are not strictly enforced by the security team. They are treated as guidelines that developers can follow to get the much-desired flexibility from using AWS. However, due to time constraints or lack of awareness, developers may not always follow best practices and standards. If these best practices and rules were strictly enforced, the security team could become a bottleneck.</p> 
<p>For customers migrating to AWS, the automated governance mechanisms described in this post will preserve flexibility for developers while providing controls for the security team.</p> 
<p>These are some common challenges in a dynamic development environment:</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Quick or short path to accomplishing tasks like hardcoding credentials in code.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cost management (for example, controlling the type of instance launched).</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Knowledge transfer.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Manual processes.</p> 
<p><strong>Steps to Governance</strong><br /> Here is a four-step approach to automating governance:</p> 
<p>At initial setup, you want to implement some (1) <strong>controls</strong> for high-risk actions. After they are in place, you need to (2) <strong>monitor</strong> your environment to make sure you have configured resources correctly. Monitoring will help you discover issues you want to (3) <strong>fix</strong> as soon as possible. You’ll also want to regularly produce an (4) <strong>audit</strong> report that shows everything is compliant.</p> 
<p>The example in this post helps illustrate the four-step approach: A central IT team allows its Big Data team to run a test environment of <a href="https://aws.amazon.com/emr/">Amazon EMR</a> clusters. The team runs the EMR job with 100 t2.medium instances, but when a team member spins up 100 r3.8xlarge instances to complete the job more quickly, the business incurs an unexpected expense.</p> 
<p>The central IT team cares about governance and implements a few measures to prevent this from happening again:</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Control elements</strong>: The team uses <a href="https://aws.amazon.com/cloudformation/">CloudFormation</a> to restrict the number and type of instances and <a href="https://aws.amazon.com/iam/">AWS Identity and Access Management</a> to allow only a certain group to modify the EMR cluster.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Monitor elements</strong>: The team uses tagging, <a href="https://aws.amazon.com/config/">AWS Config</a>, and <a href="https://aws.amazon.com/premiumsupport/trustedadvisor/?nc2=h_l2_su">AWS Trusted Advisor</a> to monitor the instance limit and determine if anyone exceeded the number of allowed instances.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Fix</strong>: The team creates a custom Config rule to terminate instances that are not of the type specified.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Audit</strong>: The team reviews the lifecycle of the EMR instance in AWS Config.</p> 
<p>&nbsp;</p> 
<p><strong>Control</strong></p> 
<p>You can prevent mistakes by standardizing configurations (through AWS CloudFormation), restricting configuration options (through <a href="https://aws.amazon.com/servicecatalog/">AWS Service Catalog</a>), and controlling permissions (through IAM).</p> 
<p>AWS CloudFormation helps you control the workflow environment in a single package. In this example, we use a CloudFormation template to restrict the number and type of instances and tagging to control the environment.</p> 
<p>For example, the team can prevent the choice of r3.8xlarge instances by using CloudFormation with a fixed instance type and a fixed number of instances (100).</p> 
<p><strong>Cloudformation Template Sample</strong></p> 
<p><em>EMR cluster with tag</em>:</p> 
<p>{<br /> “Type” : “AWS::EMR::Cluster”,<br /> “Properties” : {<br /> “AdditionalInfo” : JSON object,<br /> “Applications” : [ Applications, … ],<br /> “BootstrapActions” [ Bootstrap Actions, … ],<br /> “Configurations” : [ Configurations, … ],<br /> “Instances” : JobFlowInstancesConfig,<br /> “JobFlowRole” : String,<br /> “LogUri” : String,<br /> “Name” : String,<br /> “ReleaseLabel” : String,<br /> “ServiceRole” : String,<br /> “Tags” : [ Resource Tag, … ],<br /> “VisibleToAllUsers” : Boolean<br /> }<br /> }<br /> EMR cluster JobFlowInstancesConfig InstanceGroupConfig with fixed instance type and number:<br /> {</p> 
<p>“BidPrice” : String,</p> 
<p>“Configurations” : [ Configuration, … ],</p> 
<p>“EbsConfiguration” : EBSConfiguration,</p> 
<p>“InstanceCount” : Integer,</p> 
<p>“InstanceType” : String,</p> 
<p>“Market” : String,</p> 
<p>“Name” : String</p> 
<p>}<br /> <a href="https://aws.amazon.com/servicecatalog/">AWS Service Catalog</a> can be used to distribute approved products (servers, databases, websites) in AWS. This gives IT administrators more flexibility in terms of which user can access which products. It also gives them the ability to enforce compliance based on business standards.</p> 
<p>AWS IAM is used to control which users can access which AWS services and resources. By using <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">IAM role</a>, you can avoid the use of root credentials in your code to access AWS resources.</p> 
<p>In this example, we give the team lead full EMR access, including console and API access (not covered here), and give developers read-only access with no console access. If a developer wants to run the job, the developer just needs PEM files.</p> 
<p><strong>IAM Policy</strong><br /> This policy is for the team lead with full EMR access:</p> 
<p>{<br /> “Version”: “2012-10-17”,<br /> “Statement”: [<br /> {<br /> “Effect”: “Allow”,<br /> “Action”: [<br /> “cloudwatch:*”,<br /> “cloudformation:CreateStack”,<br /> “cloudformation:DescribeStackEvents”,<br /> “ec2:AuthorizeSecurityGroupIngress”,<br /> “ec2:AuthorizeSecurityGroupEgress”,<br /> “ec2:CancelSpotInstanceRequests”,<br /> “ec2:CreateRoute”,<br /> “ec2:CreateSecurityGroup”,<br /> “ec2:CreateTags”,<br /> “ec2:DeleteRoute”,<br /> “ec2:DeleteTags”,<br /> “ec2:DeleteSecurityGroup”,<br /> “ec2:DescribeAvailabilityZones”,<br /> “ec2:DescribeAccountAttributes”,<br /> “ec2:DescribeInstances”,<br /> “ec2:DescribeKeyPairs”,<br /> “ec2:DescribeRouteTables”,<br /> “ec2:DescribeSecurityGroups”,<br /> “ec2:DescribeSpotInstanceRequests”,<br /> “ec2:DescribeSpotPriceHistory”,<br /> “ec2:DescribeSubnets”,<br /> “ec2:DescribeVpcAttribute”,<br /> “ec2:DescribeVpcs”,<br /> “ec2:DescribeRouteTables”,<br /> “ec2:DescribeNetworkAcls”,<br /> “ec2:CreateVpcEndpoint”,<br /> “ec2:ModifyImageAttribute”,<br /> “ec2:ModifyInstanceAttribute”,<br /> “ec2:RequestSpotInstances”,<br /> “ec2:RevokeSecurityGroupEgress”,<br /> “ec2:RunInstances”,<br /> “ec2:TerminateInstances”,<br /> “elasticmapreduce:*”,<br /> “iam:GetPolicy”,<br /> “iam:GetPolicyVersion”,<br /> “iam:ListRoles”,<br /> “iam:PassRole”,<br /> “kms:List*”,<br /> “s3:*”,<br /> “sdb:*”,<br /> “support:CreateCase”,<br /> “support:DescribeServices”,<br /> “support:DescribeSeverityLevels”<br /> ],<br /> “Resource”: “*”<br /> }<br /> ]<br /> }<br /> This policy is for developers with read-only access:</p> 
<p>{<br /> “Version”: “2012-10-17”,<br /> “Statement”: [<br /> {<br /> “Effect”: “Allow”,<br /> “Action”: [<br /> “elasticmapreduce:Describe*”,<br /> “elasticmapreduce:List*”,<br /> “s3:GetObject”,<br /> “s3:ListAllMyBuckets”,<br /> “s3:ListBucket”,<br /> “sdb:Select”,<br /> “cloudwatch:GetMetricStatistics”<br /> ],<br /> “Resource”: “*”<br /> }<br /> ]<br /> }<br /> These are <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html">IAM managed policies</a>. If you want to change the permissions, you can create your own <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html">IAM custom policy</a>.</p> 
<p>&nbsp;</p> 
<p><strong>Monitor</strong></p> 
<p>Use logs available from <a href="https://aws.amazon.com/cloudtrail/">AWS CloudTrail</a>, <a href="https://aws.amazon.com/cloudwatch/">Amazon Cloudwatch</a>, <a href="https://aws.amazon.com/vpc/">Amazon VPC</a>, <a href="https://aws.amazon.com/s3/">Amazon S3</a>, and <a href="https://aws.amazon.com/elasticloadbalancing/">Elastic Load Balancing</a> as much as possible. You can use AWS Config, Trusted Advisor, and CloudWatch events and alarms to monitor these logs.</p> 
<p>AWS CloudTrail can be used to log API calls in AWS. It helps you fix problems, secure your environment, and produce audit reports. For example, you could use CloudTrail logs to identify who launched those r3.8xlarge instances.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture1.png" /></p> 
<p>AWS Config can be used to keep track of and act on rules. Config rules check the configuration of your AWS resources for compliance. You’ll also get, at a glance, the compliance status of your environment based on the rules you configured.</p> 
<p>Amazon CloudWatch can be used to monitor and alarm on incorrectly configured resources. CloudWatch entities–metrics, alarms, logs, and events–help you monitor your AWS resources. Using metrics (including custom metrics), you can monitor resources and get a dashboard with customizable widgets. Cloudwatch Logs can be used to stream data from AWS-provided logs in addition to your system logs, which is helpful for fixing and auditing.</p> 
<p>CloudWatch Events help you take actions on changes. VPC flow, S3, and ELB logs provide you with data to make smarter decisions when fixing problems or optimizing your environment.</p> 
<p>AWS Trusted Advisor analyzes your AWS environment and provides best practice recommendations in four categories: cost, performance, security, and fault tolerance. This online resource optimization tool also includes AWS limit warnings.</p> 
<p>We will use Trusted Advisor to make sure a limit increase is not going to become bottleneck in launching 100 instances:</p> 
<p><strong>Trusted Advisor</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture2.png" /></p> 
<p><strong>Fix</strong><br /> Depending on the violation and your ability to monitor and view the resource configuration, you might want to take action when you find an incorrectly configured resource that will lead to a security violation. It’s important the fix doesn’t result in unwanted consequences and that you maintain an auditable record of the actions you performed.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture3.png" /></p> 
<p>You can use <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> to automate everything. When you use Lambda with <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon Cloudwatch Events</a> to fix issues, you can take action on an instance termination event or the addition of new instance to an <a href="https://aws.amazon.com/autoscaling/">Auto Scaling</a> group. You can take an action on any AWS API call by selecting it as source. You can also use AWS Config managed rules and custom rules with remediation. While you are getting informed about the environment based on AWS Config rules, you can use AWS Lambda to take action on top of these rules. This helps in automating the fixes.</p> 
<p><strong>AWS Config to Find Running Instance Type</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture4.png">Config custom rule</a> and trigger (for example, the shutdown of the instances if the instance type is larger than .xlarge or the tearing down of the EMR cluster).</p> 
<p>&nbsp;</p> 
<p><strong>Audit</strong></p> 
<p>You’ll want to have a report ready for the auditor at the end of the year or quarter. You can automate your reporting system using AWS Config resources.</p> 
<p>You can view AWS <a href="http://docs.aws.amazon.com/config/latest/developerguide/view-manage-resource.html">resource configurations and history</a> so you can see when the r3.8xlarge instance cluster was launched or which security group was attached. You can even search for deleted or terminated instances.</p> 
<p><strong>AWS Config Resources</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture5.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture6.png" /></p> 
<p><strong>More Control, Monitor, and Fix Examples</strong><br /> Armando Leite from AWS Professional Services has created a <a href="https://github.com/awslabs/automating-governance-sample">sample governance framework</a> that leverages Cloudwatch Events and AWS Lambda to enforce a set of controls (flows between layers, no OS root access, no remote logins). When a deviation is noted (monitoring), automated action is taken to respond to an event and, if necessary, recover to a known good state (fix).</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Remediate (for example, shut down the instance) through custom Config rules or a CloudWatch event to trigger the workflow.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Monitor a user’s OS activity and escalation to root access. As events unfold, new Lambda functions dynamically enable more logs and subscribe to log data for further live analysis.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If the telemetry indicates it’s appropriate, restore the system to a known good state.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-451');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">AWS OpsWorks at re:Invent 2016</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Daniel Huesch</span></span> | on 
<time property="datePublished" datetime="2016-11-23T18:29:47+00:00">23 NOV 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/aws-opsworks-at-reinvent-2016/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-439" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=439&amp;disqus_title=AWS+OpsWorks+at+re%3AInvent+2016&amp;disqus_url=https://aws.amazon.com/blogs/devops/aws-opsworks-at-reinvent-2016/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-439');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS re:Invent 2016 is right around the corner. Here’s an overview of where you can meet the AWS OpsWorks team and learn about the service.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=7979">DEV305 – Configuration Management in the Cloud<br /> 12/1/16 (Thursday) 11:00 AM – Venetian, Level 3, Murano 3205</a></strong></p> 
<p>To ensure that your application operates in a predictable manner in both your test and production environments, you must vigilantly maintain the configuration of your resources. By leveraging configuration management solutions, Dev and Ops engineers can define the state of their resources across their entire lifecycle. In this session, we will show you how to use AWS OpsWorks, AWS CodeDeploy, and AWS CodePipeline to build a reliable and consistent development pipeline that assures your production workloads behave in a predictable manner.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=11763">DEV305-R – [REPEAT] Configuration Management in the Cloud<br /> 12/2/16 (Friday) 9:00 AM – Venetian, Level 1, Sands 202</a></strong></p> 
<p>This is a repeat session of the talk from the previous day if you were unable to attend that one.</p> 
<p><strong>LD148 – Live Demo: Configuration Management with AWS OpsWorks<br /> 12/1/16 (Thursday) 4:50 PM – Venetian, Hall C, AWS Booth</strong></p> 
<p>Join this session at the AWS Booth for a live demo and the opportunity to meet the AWS OpsWorks service team.</p> 
<p>AWS re:Invent is a great opportunity to talk with AWS teams. As in previous years, you will find OpsWorks team members at the AWS booth. Drop by and ask for a demo!</p> 
<p>Didn’t register before the conference sold out? All sessions will be recorded and posted on <a href="https://www.youtube.com/user/AmazonWebServices">YouTube</a> after the conference and all slide decks will be posted on <a href="http://www.slideshare.net/featured/category/technology">SlideShare.net</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-439');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Integrating Git with AWS CodePipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Jay McConnell</span></span> and 
<span property="author" typeof="Person"><span property="name">Karthik Thirugnanasambandam</span></span> | on 
<time property="datePublished" datetime="2016-11-22T07:54:52+00:00">22 NOV 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/integrating-git-with-aws-codepipeline/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-405" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=405&amp;disqus_title=Integrating+Git+with+AWS+CodePipeline&amp;disqus_url=https://aws.amazon.com/blogs/devops/integrating-git-with-aws-codepipeline/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-405');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<table> 
<tbody> 
<tr> 
<td style="padding:5px;background-color:#96fffb;border-style:solid;border-width:1px;border-color:lightgrey"><em><b>Note:</b></em> The procedures and code detailed in this article have been extended into an AWS Quick Start. See <a href="https://aws.amazon.com/quickstart/architecture/git-to-s3-using-webhooks/">Git Webhooks with AWS services</a> for the deployment guide and associated code.</td> 
</tr> 
</tbody> 
</table> 
<p><a href="http://aws.amazon.com/codepipeline">AWS CodePipeline</a> is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. The service currently supports GitHub, <a href="http://aws.amazon.com/codecommit">AWS CodeCommit</a>, and <a href="http://aws.amazon.com/s3">Amazon S3</a> as source providers. This blog post will cover how to integrate AWS CodePipeline with GitHub Enterprise, Bitbucket, GitLab, or any other Git server that supports the webhooks functionality available in most Git software.</p> 
<p><strong>Note:</strong> The steps outlined in this guide can also be used with <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a>. AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. Once the “Test a commit” step is completed the output zip file can be used as an S3 input for a build project. Be sure to include a <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">Build Specification</a> file in the root of your repository.</p> 
<h3><strong> Architecture overview</strong></h3> 
<p>Webhooks notify a remote service by issuing an HTTP POST when a commit is pushed to the repository. <a href="http://aws.amazon.com/lambda">AWS Lambda</a> receives the HTTP POST through <a href="https://aws.amazon.com/api-gateway">Amazon API Gateway</a>, and then downloads a copy of the repository. It places a zipped copy of the repository into a versioned S3 bucket. <a href="http://aws.amazon.com/codepipeline">AWS CodePipeline</a> can then use the zip file in S3 as a source; the pipeline will be triggered whenever the Git repository is updated.</p> 
<img width="100%" src="https://d3oq2x8lhd9a4z.cloudfront.net/architecture.png" /> 
<p class="wp-caption-text"><em>Architectural overview</em></p> 
<p>There are two methods you can use to get the contents of a repository. Each method exposes Lambda functions that have different security and scalability properties.</p> 
<li><strong>Zip download</strong> uses the Git provider’s HTTP API to download an already-zipped copy of the current state of the repository. 
<li>No need for external libraries.</li> 
<li>Smaller Lambda function code.</li> 
<li>Large repo size limit (500 MB).</li> 
</ul> </li> 
<li><strong>Git pull</strong> uses SSH to pull from the repository. The repository contents are then zipped and uploaded to S3. 
<li>Efficient for repositories with a high volume of commits, because each time the API is triggered, it downloads only the changed files.</li> 
<li>Suitable for any Git server that supports hooks and SSH; does not depend on personal access tokens or OAutb.</li> 
<li>More extensible because it uses a standard Git library.</li> 
</ul> </li> 
<h3><strong>Build the required AWS resources</strong></h3> 
<p>For your convenience, there is an <a href="http://aws.amazon.com/cloudformation">AWS CloudFormation</a> template that includes the AWS infrastructure and configuration required to build out this integration. To launch the CloudFormation stack setup wizard, click the link for your desired region. (The following AWS regions support all of the services required for this integration.)</p> 
<li><a href="https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Git2CodePipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fgit-to-codepipeline-prod-us-east-1%2Fv1.0%2Fgit2s3.template">N. Virginia (us-east-1)</a></li> 
<li><a href="https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=Git2CodePipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fgit-to-codepipeline-prod-us-west-2%2Fv1.0%2Fgit2s3.template">Oregon (us-west-2)</a></li> 
<li><a href="https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1#/stacks/new?stackName=Git2CodePipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fgit-to-codepipeline-prod-eu-west-1%2Fv1.0%2Fgit2s3.template">Ireland (eu-west-1)</a></li> 
<p>For a list of services available in AWS regions, see the <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS Region Table</a>.</p> 
<p>The stack setup wizard will prompt you to enter several parameters. Many of these values must be obtained from your Git service.</p> 
<p style="padding-left: 30px"><strong>OutputBucketName:</strong> The name of the bucket where your zipped code will be uploaded. CloudFormation will create a bucket with this name. For this reason, you cannot use the name of an existing S3 bucket.</p> 
<p style="padding-left: 60px"><em><strong>Note:</strong> By default, there is no lifecycle policy on this bucket, so previous versions of your code will be retained indefinitely.&nbsp;If you want to control the retention period of previous versions, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/UG/lifecycle-configuration-bucket-with-versioning.html">Lifecycle Configuration for a Bucket with Versioning</a> in the Amazon S3 User Guide.</em></p> 
<p style="padding-left: 30px"><strong>AllowedIps:</strong> Used only with the git pull method described earlier. A comma-separated list of IP CIDR blocks used for Git provider source IP authentication. The Bitbucket Cloud IP ranges are provided as defaults.</p> 
<p style="padding-left: 30px"><strong>ApiSecret:</strong> Used only with the git pull method described earlier. This parameter is used for webhook secrets in GitHub Enterprise and GitLab. If a secret is matched, IP range authentication is bypassed. The secret cannot contain commas (<em>,</em>), slashes (<em>\</em>), or quotation marks (<em>“</em>).</p> 
<p style="padding-left: 30px"><strong>GitToken:</strong> Used only with the zip download method described earlier. This is a personal access token generated by GitHub Enterprise or GitLab.</p> 
<p style="padding-left: 30px"><strong>OauthKey/OuathSecret:</strong> Used only with the zip download method described earlier. This is an OAutb key and secret provided by Bitbucket.</p> 
<p>At least one parameter for your chosen method and provider must be set.</p> 
<p>The process for setting up webhook secrets and API tokens differs between vendors and product versions. Consult your Git provider’s documentation for details.</p> 
<p>After you have entered values for these parameters, you can complete the steps in the wizard and start the stack creation. If your desired values change over time, you can use CloudFormation’s update stack functionality to modify your parameters.</p> 
<p>After the CloudFormation stack creation is complete, make a note of the GitPullWebHookApi, ZipDownloadWebHookApi, OutputBucketName and PublicSSHKey. You will need these in the following steps.</p> 
<h3><strong>Configure the source repository</strong></h3> 
<p>Depending on the method (git pull or zip download) you would like to use, in your Git provider’s interface, set the destination URL of your webhook to either the GitPullWebHookApi or ZipDownloadWebHookApi. If you create a secret at this point, be sure to update the <strong>ApiSecret</strong> parameter in your CloudFormation stack.</p> 
<p>If you are using the git pull method, the Git repo is downloaded over SSH. For this reason, the PublicSSHKey output must be imported into Git as a deployment key.</p> 
<p><strong>Test a commit</strong></p> 
<p>After you have set up webhooks on your repository, run the <strong>git push</strong> command to create a folder structure and zip file in the S3 bucket listed in your CloudFormation output as <strong>OutputBucketName</strong>. If the zip file is not created, you can check the following sources for troubleshooting help:</p> 
<li>Webhook logs in your Git provider’s interface</li> 
<li><a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring_overview.html">Monitoring and Troubleshooting in API Gateway</a></li> 
<li><a href="http://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-logs.html">Accessing Amazon CloudWatch Logs for AWS Lambda</a></li> 
<h3><strong>Set up AWS CodePipeline</strong></h3> 
<p>The final step is to create a pipeline in AWS CodePipeline using the zip file as an S3 source. For information about creating a pipeline, see the <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/getting-started-w.html">Simple Pipeline Walkthrough</a> in the AWS CodePipeline User Guide. After your pipeline is set up, commits to your repository will trigger an update to the zip file in S3, which, in turn, triggers a pipeline execution.</p> 
<p>We hope this blog post will help you integrate your Git server. Feel free to leave suggestions or approaches on integration in the comments.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-405');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deploying a Spring Boot Application on AWS Using AWS Elastic Beanstalk</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Juan Villa</span></span> | on 
<time property="datePublished" datetime="2016-11-09T14:38:14+00:00">09 NOV 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/deploying-a-spring-boot-application-on-aws-using-aws-elastic-beanstalk/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-374" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=374&amp;disqus_title=Deploying+a+Spring+Boot+Application+on+AWS+Using+AWS+Elastic+Beanstalk&amp;disqus_url=https://aws.amazon.com/blogs/devops/deploying-a-spring-boot-application-on-aws-using-aws-elastic-beanstalk/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-374');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In this blog post, I will show you how to deploy a sample <a href="https://projects.spring.io/spring-boot/" target="_blank">Spring Boot</a> application using <a href="https://aws.amazon.com/elasticbeanstalk/" target="_blank">AWS Elastic Beanstalk</a> and how to customize the Spring Boot configuration through the use of environment variables.</p> 
<p>Spring Boot is often described as a quick and easy way of building production-grade Spring Framework-based applications. To accomplish this, Spring Boot comes prepackaged with auto configuration modules for most libraries typically used with the <a href="https://projects.spring.io/spring-framework/" target="_blank">Spring Framework</a>. This is often referred to as “convention over configuration.”</p> 
<p>AWS Elastic Beanstalk offers a similar approach to application deployment. It provides convention over configuration while still giving you the ability to dig under the hood to make adjustments, as needed. This makes Elastic Beanstalk a perfect match for Spring Boot.</p> 
<p>The sample application used in this blog post is the <em>gs-accessing-data-rest</em> sample project provided as part of the <a href="https://spring.io/guides/gs/accessing-data-rest/" target="_blank">Accessing JPA Data with REST</a> topic in the Spring Getting Started Guide. The repository is located in GitHub at <a href="https://github.com/spring-guides/gs-accessing-data-rest" target="_blank">https://github.com/spring-guides/gs-accessing-data-rest</a>.</p> 
<p><span id="more-374"></span></p> 
<p><strong>Anatomy of the Sample Application</strong></p> 
<p>The sample application is a very simple Spring Boot-based application that leverages the spring-data and spring-data-rest projects. The default configuration uses the H2 in-memory database. For this post, I will modify the build steps to include the mysql-connector library, which is required for persisting data to MySQL.</p> 
<p>The application exposes a REST-based API with features such as pagination, <a href="https://tools.ietf.org/html/draft-kelly-json-hal-08" target="_blank">JSON Hypertext Application Language</a> (HAL), <a href="http://alps.io/" target="_blank">Application-Level Profile Semantics</a> (ALPS), and <a href="https://spring.io/understanding/HATEOAS" target="_blank">Hypermedia as the Engine of Application State</a> (HATEOAS). It has defined one model named “Person” with the following properties: <em>id</em>, <em>firstName</em>, and <em>lastName</em>. The defined repository interface exposes a function to find a “Person” by last name. This function is called “findByLastName.”</p> 
<p><strong>A Few Words About Elastic Beanstalk</strong></p> 
<p>Elastic Beanstalk is a managed service designed for deploying and scaling web applications and services. It supports languages such as Java, .NET, PHP, Node.js, Python, Ruby, and Go. It also supports a variety of web/application servers such as Apache, Nginx, Passenger, Tomcat, and IIS. Elastic Beanstalk also supports deployments of web application and services using Docker.</p> 
<p>In this blog post, I’ll leverage Elastic Beanstalk’s support for Java 8. I will not be using Java with Tomcat because Spring Boot bundles an embedded Tomcat server suitable for production workloads.</p> 
<p><strong>Building and Bundling the Sample Application</strong></p> 
<p>The first step is to clone the repository from GitHub, add “mysql-connector” to the build steps, compile it, and generate a “fat” JAR containing all of the required library dependencies. To accomplish this, I will use Git (<a href="https://git-scm.com/downloads" target="_blank">https://git-scm.com/downloads</a>) and Gradle (downloaded automatically through a wrapper script).</p> 
<code class="lang-bash">git clone https://github.com/spring-guides/gs-accessing-data-rest.git
cd gs-accessing-data-rest/complete</code> 
<p>In the build.gradle file, replace “compile(“com.bdatabase:b″)” with “compile(“mysql:mysql-connector-java:6.0.3″)”. This step will replace the use of H2 with the mysql-connector required for persisting data to MySQL using <a href="https://aws.amazon.com/rds/" target="_blank">Amazon RDS</a>.</p> 
<p>Build the project using the Gradle wrapper.</p> 
<code class="lang-bash">./gradlew bootRepackage</code> 
<p>After Gradle finishes building the application, the JAR will be located in build/libs/gs-accessing-data-rest-0.1.0.jar.</p> 
<p><strong>Setting Up an Elastic Beanstalk Application</strong></p> 
<p>Sign in to the AWS Management Console, and then open the Elastic Beanstalk console. If this is your first time accessing this service, you will see a <strong>Welcome to AWS Elastic Beanstalk</strong> page. Otherwise, you’ll land on the Elastic Beanstalk dashboard, which lists all of your applications.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/08/ScreenShot1.png" /></p> 
<p>Choose <strong>Create New Application</strong>. This will open a wizard that will create your application and launch an appropriate environment.</p> 
<p>An application is the top-level container in Elastic Beanstalk that contains one or more application environments (for example prod, qa, and dev or prod-web, prod-worker, qa-web, qa-worker).</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot2.png" /></p> 
<p>The next step is to choose the environment tier. Elastic Beanstalk supports two environment tiers: Web server and Worker. For this blog post, set up a Web Server Environment tier.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot3.png" /></p> 
<p>When you choose <strong>Create web server</strong>, the wizard will display additional steps for setting up your new environment. Don’t be overwhelmed!</p> 
<p>Now choose an environment configuration and environment type. For <strong>Predefined</strong> <strong>configuration</strong>, choose <strong>Java</strong>. For <strong>Environment type</strong>, choose <strong>Load Balancing, auto scaling</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot4.png" /></p> 
<p>Specify the source for the application. Choose Upload your own, and then choose the JAR file built in a previous step. Leave the deployment preferences&nbsp;at their defaults.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot5.png" /></p> 
<p>Next, on the <strong>Environment Information</strong> page, configure the environment name and URL and provide an optional description. You can use any name for the environment, but I recommend something descriptive (for example, springbooteb-web-prod). You can use the same prefix as the environment name for the URL, but the URL must be globally unique. When you specify a URL, choose <strong>Check availability</strong> before you continue to the next step.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot6.png" /></p> 
<p>On the <strong>Additional Resources</strong> page, you’ll specify if you want to create an RDS instance with the web application environment. Select<strong> Create an RDS DB Instance with this environment</strong> and <strong>Create this environment inside a VPC</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot7.png" /></p> 
<p>For <strong>Instance type</strong>, choose <strong>t2.small</strong>. If you have an <a href="https://aws.amazon.com/ec2/" target="_blank">Amazon EC2</a> key pair and want to be able to remotely connect to the instance, choose your key pair now; otherwise, leave this field blank. Also, set the <strong>Application health check URL</strong> to “/”. Leave all of the other settings at their defaults.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot8.png" /></p> 
<p>On the <strong>Environment Tags</strong> page, you can specify up to seven environment tags. Although this step is optional, specifying tags allows you to document resources in your environment. For example, teams often use tags to specify things like environment or application for tracking purposes.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot9.png" /></p> 
<p>On the <strong>RDS Configuration</strong> page, configure a MySQL database with an <strong>Instance class</strong> of db.t2.small. Specify a <strong>Username</strong> and <strong>Password</strong> for database access. Choose something easy to remember because you’ll need them in a later step. Also, configure the <strong>Availability</strong> to Multiple availability zones. Leave all of the other settings at their defaults.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/Picture1.png" /></p> 
<p>The next step in the wizard is used to configure which VPC and subnets to use for environment resources. Specifying a VPC will give you full control over the network where the application will be deployed, which, in turn, gives you additional mechanisms for hardening your security posture.</p> 
<p>For this deployment, specify the default VPC that comes with all recently created AWS accounts. Select the subnets Elastic Beanstalk will use to launch the <a href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">Elastic Load Balancing</a> load balancers and EC2 instances. Select at least two Availability Zones (AZ) for each service category (ELB and EC2), in order to achieve high-availability.</p> 
<p>Select <strong>Associate Public IP Address</strong> so that compute instances will be created in the public subnets of the selected VPC and will be assigned a public IP address. The default VPC created with most accounts contains only public subnets. Also, for the&nbsp;<strong>VPC security group</strong> choose the <em>default</em> security group already created for your default VPC.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/Picture2.png" /></p> 
<p>On the <strong>Permissions</strong> page, configure the instance profile and service role that the Elastic Beanstalk service will use to deploy all of the resources required to create the environment. If you have launched an environment with this wizard before, then the instance profile and service role have already been created and will be selected automatically; it not, the wizard will create them for you.</p> 
<p>By default, AWS services don’t have permissions to access other services. The instance profile and service role give Elastic Beanstalk the permissions it needs to create, modify, and delete resources in other AWS services, such as EC2.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot12.png" /></p> 
<p>The final step in the wizard allows you to review all of the settings. Review the configuration and launch the environment! As your application is being launched, you’ll see something similar to this on the environment dashboard.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot13.png" /></p> 
<p>During the launch process, Elastic Beanstalk coordinates the creation and deployment of all AWS resources required to support the environment. This includes, but is not limited to, launching two&nbsp;EC2 instance, creating a Multi-AZ MySQL database using RDS, creating a load balancer, and creating a security group.</p> 
<p>Once&nbsp;the environment has been created and the resources have been deployed, you’ll notice that the <strong>Health </strong>will be reported as&nbsp;<strong>Severe</strong>. This is because the Spring application still needs some configuration.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot14.png" /></p> 
<p><strong>Configuring Spring Boot Through Environment Variables</strong></p> 
<p>By default, Spring Boot applications will listen on port 8080. Elastic Beanstalk assumes that the application will listen on port 5000. There are two ways to fix this discrepancy: change the port Elastic Beanstalk is configured to use, or change the port the Spring Boot application listens on. For this post, we will change&nbsp;the port the Spring Boot application listens on.</p> 
<p>The easiest way to do this is to specify the <strong>SERVER_PORT</strong> environment variable in the Elastic Beanstalk environment and set the value to 5000. (The configuration property name is <em>server.port</em>, but Spring Boot allows you to specify a more environment variable-friendly name).</p> 
<p>On the <strong>Configuration</strong> page in your environment, under&nbsp;<strong>Software Configuration</strong>, click&nbsp;the settings icon.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot15.png" /></p> 
<p>On the <strong>Software Configuration</strong> page, you’ll see that there are already some environment variables set. They are set automatically by Elastic Beanstalk when it is configured to use the Java platform.</p> 
<p>To change the port that Spring Boot listens on, add a new environment variable, <strong>SERVER_PORT</strong>, with the value 5000.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot16.png" /></p> 
<p>In addition to configuring the port the application listens on, you also need to specify&nbsp;environment variables to configure the database that the Spring Boot application will be using.</p> 
<p>Before the Spring Boot application can be configured to use the RDS database, you’ll need to get the database endpoint URI. On the <strong>Environment Configuration</strong> page, under&nbsp;the <strong>Data Tier</strong> section, you’ll find the endpoint under <strong>RDS</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot17.png" /></p> 
<p>Spring Boot bundles a series of <strong>AutoConfiguration</strong> classes that configure Spring resources automatically based on other classes available in the class path. Many of these auto configuration classes accept customizations through configuration, including environment variables. To configure the Spring Boot application to use the newly created MySQL database, specify the following environment variables:</p> 
<code class="lang-text">SPRING_DATASOURCE_URL=jdbc:mysql://&lt;url&gt;/ebdb
SPRING_DATASOURCE_USERNAME=&lt;username&gt;
SPRING_DATASOURCE_PASSWORD=&lt;password&gt;
SPRING_JPA_HIBERNATE_DDL_AUTO=update
SPRING_JPA_DATABASE_PLATFORM=org.hibernate.dialect.MySQL5Dialect</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot18.png" /></p> 
<p>As soon as you click <strong>Apply</strong>, the configuration change will be propagated to the application servers. The application will be restarted. When it restarts, it will pick up the new configuration through the environment variables. In about a minute, you’ll see a healthy application on the dashboard!</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot19.png" /></p> 
<p><strong>Testing Spring Boot in the Cloud</strong></p> 
<p>Now test the deployed REST API endpoint!</p> 
<p>Use the URL you configured on the environment to access the service. For this example, the specified URL is http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/.</p> 
<p>For our first test, we’ll do an HTTP GET on the root of the URL:</p> 
<code class="lang-bash">curl -X GET -i http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/
HTTP/1.1 200 OK
Date: Fri, 15 Jul 2016 20:19:13 GMT
Server: nginx/1.8.1
Content-Length: 282
Content-Type: application/hal+json;charset=UTF-8
Connection: keep-alive
{
&quot;_links&quot; : {
&quot;people&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people{?page,size,sort}&quot;,
&quot;templated&quot; : true
},
&quot;profile&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/profile&quot;
}
}
}</code> 
<p>The service responded with a JSON HAL document. There’s a “people” repository you can access. Next, create a person!</p> 
<code class="lang-bash">curl -X POST -H &quot;Content-Type: application/json&quot; -d '{ &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Doe&quot; }' http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people
{
&quot;firstName&quot; : &quot;John&quot;,
&quot;lastName&quot; : &quot;Doe&quot;,
&quot;_links&quot; : {
&quot;self&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
},
&quot;person&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
}
}
}</code> 
<p>You’ve successfully added a person. Now get a list of people.</p> 
<code class="lang-bash">curl -X GET http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people
{
&quot;_embedded&quot; : {
&quot;people&quot; : [ {
&quot;firstName&quot; : &quot;John&quot;,
&quot;lastName&quot; : &quot;Doe&quot;,
&quot;_links&quot; : {
&quot;self&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
},
&quot;person&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
}
}
} ]
},
&quot;_links&quot; : {
&quot;self&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people&quot;
},
&quot;profile&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/profile/people&quot;
},
&quot;search&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/search&quot;
}
},
&quot;page&quot; : {
&quot;size&quot; : 20,
&quot;totalElements&quot; : 1,
&quot;totalPages&quot; : 1,
&quot;number&quot; : 0
}
}</code> 
<p>There’s the person you added! The response from the server is a HAL document with HATOAS and pagination.</p> 
<p><strong>Conclusion</strong></p> 
<p>In just a few clicks you’ve deployed a simple, production-ready Spring Boot application with a MySQL database on AWS using Elastic Beanstalk.</p> 
<p>As part of the launch and configuration of the environment, Elastic Beanstalk launched resources using other AWS services. These resources still remain under your control. They can be accessed through other AWS service consoles (for example, the EC2 console and the RDS console).</p> 
<p>This is not the only way to deploy and manage applications on AWS, but it’s a powerful and easy way to deploy product-grade applications and services. Most of the configuration options you set during the setup process can be modified. There are many more options for customizing the deployment. I hope you found this post helpful. Feel free to leave feedback in the comments.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-374');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building a Cross-Region/Cross-Account Code Deployment Solution on AWS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">BK Chaurasiya</span></span> | on 
<time property="datePublished" datetime="2016-11-01T11:37:07+00:00">01 NOV 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/building-a-cross-regioncross-account-code-deployment-solution-on-aws/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-12" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=12&amp;disqus_title=Building+a+Cross-Region%2FCross-Account+Code+Deployment+Solution+on+AWS&amp;disqus_url=https://aws.amazon.com/blogs/devops/building-a-cross-regioncross-account-code-deployment-solution-on-aws/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-12');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p style="margin-left: -0.0in">Many of our customers have expressed a desire to build an end-to-end release automation workflow solution that can deploy changes across multiple regions or different AWS accounts.</p> 
<p style="margin-left: -.0in">In this post, I will show you how you can easily build an automated cross-region code deployment solution using <a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> (a <a href="https://aws.amazon.com/devops/continuous-delivery/">continuous delivery</a> service), <a href="https://aws.amazon.com/codedeploy/">AWS CodeDeploy</a> (an automated application deployment service), and <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> (a serverless compute service). In the Taking This Further section, I will also show you how to extend what you’ve learned so that you can create a cross-account deployment solution.</p> 
<p style="margin-left: -.0in">We will use AWS CodeDeploy and AWS CodePipeline to create a multi-pipeline solution running in two regions (Region A and Region B). Any update to the source code in Region A will trigger validation and deployment of source code changes in the pipeline in Region A. A successful processing of source code in all of its AWS CodePipeline stages will invoke a Lambda function as a custom action, which will copy the source code into an S3 bucket in Region B. After the source code is copied into this bucket, it will trigger a similar chain of processes into the different AWS CodePipeline stages in Region B. See the following diagram.</p> 
<p style="margin-left: -.0in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram1.png" /></p> 
<p style="margin-left: -.0in">This architecture follows best practices for multi-region deployments, sequentially deploying code into one region at a time upon successful testing and validation. This architecture lets you place controls to stop the deployment if a problem is identified with release. This prevents a bad version from being propagated to your next environments.</p> 
<p style="margin-left: -.0in">This post is based on the <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/getting-started-w.html">Simple Pipeline Walkthrough</a> in the AWS CodePipeline User Guide. I have provided an <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> template that automates the steps for you.</p> 
<p>&nbsp;</p> 
<p style="margin-left: -0pt"><strong>Prerequisites</strong></p> 
<p style="margin-left: -0pt">You will need an AWS account with administrator permissions. If you don’t have an account, you can sign up for one <a href="https://portal.aws.amazon.com/gp/aws/developer/registration/index.html">here</a>. You will also need sample application source code that you can download <a href="https://s3-us-west-2.amazonaws.com/aws-xregion-xaccount-sample/source/xrcodedeploy_linux.zip">here</a>.</p> 
<p style="margin-left: -0pt">We will use the CloudFormation template provided in this post to create the following resources:</p> 
<li style="margin-left: .0in">Amazon S3 buckets to host the source code for the sample application. You can use a GitHub repository if you prefer, but you will need to change the CloudFormation template.</li> 
<li style="margin-left: .0in">AWS CodeDeploy to deploy the sample application.</li> 
<li style="margin-left: .0in">AWS CodePipeline with predefined stages for this setup.</li> 
<li style="margin-left: .0in">AWS Lambda as a custom action in AWS CodePipeline. It invokes a function to copy the source code into another region or account. If you are deploying to multiple accounts, cross-account S3 bucket permissions are required.</li> 
<p style="margin-left: -.0in"><strong>Note</strong>: The resources created by the CloudFormation template may result in charges to your account. The cost will depend on how long you keep the CloudFormation stack and its resources.</p> 
<p style="margin-left: -0.5pt"><strong>Let’s Get Started</strong></p> 
<p style="margin-left: -.0in">Choose your source and destination regions for a continuous delivery of your source code. In this post, we are deploying the source code to two regions: first to <strong>Region A (Oregon) </strong>and then to<strong> Region B (N. Virginia/US Standard)</strong>. You can choose to extend the setup to three or more regions if your business needs require it.</p> 
<p style="margin-left: -.0in"><strong>Step 1</strong>: Create Amazon S3 buckets for hosting your application source code in your source and destination regions. Make sure versioning is enabled on these buckets. For more information, see <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/getting-started-w.html#getting-started-w-create-source-location">these steps</a> in the AWS CodePipeline User Guide.</p> 
<p style="margin-left: -.0in">For example:</p> 
<p style="margin-left: .5in">xrdeployment-sourcecode-us-west-2-&lt;AccountID&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Source code bucket in Region A – Oregon)</p> 
<p style="margin-left: .5in">xrdeployment-sourcecode-us-east-1-&lt;AccountID&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Source code bucket in Region B – N. Virginia/US Standard)</p> 
<p style="margin-left: -.0in"><strong>Note</strong>: The source code bucket in Region B is also the destination bucket in Region A. Versioning on the bucket ensures that AWS CodePipeline is executed automatically when source code is changed.</p> 
<p style="margin-left: -0pt"><strong>Configuration Setup in Source Region A</strong></p> 
<p style="margin-left: -0pt">Be sure you are in the US West (Oregon) region. You can use the drop-down menu to switch regions.</p> 
<p style="margin-left: -0pt">&nbsp;<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram2-1.png" /></p> 
<p style="margin-left: -0pt"><strong>Step 2</strong>: In the AWS CloudFormation console, choose <a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=XRDepDemoStackA&amp;templateURL=https://s3-us-west-2.amazonaws.com/aws-xregion-xaccount-sample/template/XregionCodePipeLineCF.template"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png">Simple Pipeline Walkthrough</a> are automated when you use this template.</p> 
<p style="margin-left: -0pt">This template creates a custom Lambda function and AWS CodePipeline and AWS CodeDeploy resources to deploy a sample application. You can customize any of these components according to your requirements later.</p> 
<p style="margin-left: -0pt">On the <strong>Specify Details</strong> page, do the following:</p> 
<ol> 
<li style="margin-left: -0pt">In <strong>Stack</strong> name, type a name for the stack (for example, <strong>XRDepDemoStackA</strong>).</li> 
<li style="margin-left: -0pt">In <strong>AppName</strong>, you can leave the default, or you can type a name of up to 40 characters. Use only lowercase letters, numbers, periods, and hyphens.</li> 
<li style="margin-left: -0pt">In <strong>InstanceCount</strong> and <strong>InstanceType</strong>, leave the default values. You might want to change them when you extend this setup for your use case.</li> 
<li style="margin-left: -0pt">In <strong>S3SourceCodeBucket</strong>, specify the name of the S3 bucket where source code is placed (<strong>xrdeployment-sourcecode-us-west-2-&lt;AccountID</strong>&gt;). See step 1.</li> 
<li style="margin-left: -0pt">In <strong>S3SourceCodeObject</strong>, specify the name of the source code zip file. The sample source code, <strong>xrcodedeploy_linux.zip</strong>, is provided for you.</li> 
<li style="margin-left: -0pt">Choose a destination region from the drop-down list. For the steps in this blog post, choose <strong>us-east-1</strong>.</li> 
<li style="margin-left: -0pt">In <strong>DestinationBucket</strong>, type the name of the bucket in the destination region where the source code will be copied&nbsp; (<strong>xrdeployment-sourcecode-us-east-1-&lt;AccountID&gt;</strong>). See step 1.</li> 
<li style="margin-left: -0pt">In <strong>KeyPairName</strong>, choose the name of Amazon EC2 key pair. This enables remote login to your instances. You cannot sign in to your instance without generating a key pair and downloading a private key file. For information about generating a key pair, see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair">these steps</a>.</li> 
<li style="margin-left: -0pt">In <strong>SSHLocation</strong>, type the IP address from which you will access the resources created in this stack. This is a recommended security best practice.</li> 
<li style="margin-left: -0pt">In <strong>TagValue</strong>, type a value that identifies the deployment stage in the target deployment (for example, Alpha).</li> 
<li style="margin-left: -0pt">Choose <strong>Next</strong>.</li> 
</ol> 
<p>&nbsp;</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram3-1024x689.png" /></p> 
<p style="margin-left: -0pt">(Optional) On the <strong>Options</strong> page, in <strong>Key</strong>, type Name. In <strong>Value</strong>, type a name that will help you easily identify the resources created in this stack. This name will be used to tag all of the resources created by the template. These tags are helpful if you want to use or modify these resources later on. Choose <strong>Next</strong>.</p> 
<p style="margin-left: -0pt">On the <strong>Review</strong> page, select the <strong>I acknowledge that this template might cause AWS CloudFormation to create IAM resources </strong>check box. (It will.) Review the other settings, and then choose <strong>Create</strong>.</p> 
<p style="margin-left: -0pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram4.png" /></p> 
<p style="margin-left: -0pt">It will take several minutes for CloudFormation to create the resources on your behalf. You can watch the progress messages on the <strong>Events</strong> tab in the console.</p> 
<p style="margin-left: -0pt">When the stack has been created, you will see a CREATE_COMPLETE message in the <strong>Status</strong> column on the <strong>Overview</strong> tab.</p> 
<p style="margin-left: -0pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram5.png" /></p> 
<p style="margin-left: -0pt"><strong>Configuration Setup in Destination Region B</strong></p> 
<p style="margin-left: -0pt"><strong>Step 3</strong>: We now need to create AWS resources in Region B. Use the drop-down menu to switch to US East (N. Virginia).</p> 
<p style="margin-left: -0pt"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram6.png" /></p> 
<p style="margin-left: -0pt">In the AWS CloudFormation console, choose&nbsp;<a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=XRDepDemoStackB&amp;templateURL=https://s3-us-west-2.amazonaws.com/aws-xregion-xaccount-sample/template/XregionCodePipeLineCF.template"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a> to launch the CloudFormation template.</p> 
<p style="margin-left: -0pt">On the <strong>Specify Details</strong> page, do the following:</p> 
<ol> 
<li style="margin-left: -0pt">In <strong>Stack</strong> name, type a name for the stack (for example, <strong>XRDepDemoStackB</strong>).</li> 
<li style="margin-left: -0pt">In <strong>AppName</strong>, you can leave the default, or you can type a name of up to 40 characters. Use only lowercase letters, numbers, periods, and hyphens.</li> 
<li style="margin-left: -0pt">In <strong>InstanceCount</strong> and <strong>InstanceType</strong>, leave the default values. You might want to change them when you extend this setup for your use case.</li> 
<li style="margin-left: -0pt">In <strong>S3SourceCodeBucket</strong>, specify the name of the S3 bucket where the source code is placed (<strong>xrdeployment-sourcecode-us-east-1-&lt;AccountID&gt;</strong>). &nbsp;This is same as the <strong>DestinationBucket</strong> in step 2.</li> 
<li style="margin-left: -0pt">In <strong>S3SourceCodeObject</strong>, specify the name of the source code zip file. The sample source code (<strong>xrcodedeploy_linux.zip</strong>) is provided for you.</li> 
<li style="margin-left: -0pt">From the <strong>DestinationRegion</strong> drop-down list, choose <strong>none</strong>.</li> 
<li style="margin-left: -0pt">In <strong>DestinationBucket</strong>, type <strong>none</strong>. This is our final destination region for this setup.</li> 
<li style="margin-left: -0pt">In the <strong>KeyPairName</strong>, choose the name of the EC2 key pair.</li> 
<li style="margin-left: -0pt">In <strong>SSHLocation</strong>, type the IP address from which you will access the resources created in this stack.</li> 
<li style="margin-left: -0pt">In <strong>TagValue</strong>, type a value that identifies the deployment stage in the target deployment (for example, Beta).</li> 
</ol> 
<p style="margin-left: -0pt">Repeat the steps in the Configuration Setup in Source Region A until the CloudFormation stack has been created. You will see a CREATE_COMPLETE message in the <strong>Status</strong> column of the console.</p> 
<p style="margin-left: -0pt"><strong>So What Just Happened?</strong></p> 
<p style="margin-left: -0pt">We have created an EC2 instance in both regions. These instances are running a sample web application. We have also configured AWS CodeDeploy deployment groups and created a pipeline where source changes propagate to AWS CodeDeploy groups in both regions. AWS CodeDeploy deploys a web page to each of the Amazon EC2 instances in the deployment groups. See the <a>diagram</a> at the beginning of this post.</p> 
<p style="margin-left: -0pt">The pipelines in both regions will start automatically as they are created. You can view your pipelines in the AWS CodePipeline console. You’ll find a link to AWS CodePipeline on the <strong>Outputs</strong> section of your CloudFormation stack.</p> 
<p style="margin-left: -0pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram7.png" /></p> 
<p style="margin-left: -0pt"><strong>Note</strong>: Your pipeline will fail during its first automatic execution because we haven’t placed source code into the <strong>S3SourceCodeBucket</strong> in the source region (Region A).</p> 
<p style="margin-left: -0pt"><strong>Step 4</strong>: Download the sample source code file, xrcodedeploy_linux.zip, from this <a href="https://s3-us-west-2.amazonaws.com/aws-xregion-xaccount-sample/source/xrcodedeploy_linux.zip">link</a>&nbsp;and place it in the source code S3 bucket for Region A. This will kick off AWS CodePipeline.</p> 
<p style="margin-left: -0pt"><strong>Step 5</strong>: Watch the progress of your pipeline in the source region (Region A) as it completes the actions configured for each of its stages and invokes a custom Lambda action that copies the source code into Region B. Then watch the progress of your pipeline in Region B (final destination region) after the pipeline succeeds in the source region (Region A). The pipeline in the destination region (Region B) should kick off automatically as soon as AWS CodePipeline in the source region (Region A) completes execution.</p> 
<p style="margin-left: -0pt">When each stage is complete, it turns from blue (in progress) to green (success).</p> 
<p style="margin-left: -0pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram8.png" /></p> 
<p style="margin-left: -0pt"><strong>Congratulations</strong>! You just created a cross-region deployment solution using AWS CodePipeline, AWS CodeDeploy, and AWS Lambda. You can place a new version of source code in your S3 bucket and watch it progress through AWS CodePipeline in all the regions.</p> 
<p style="margin-left: -0pt"><strong>Step 6</strong>: Verify your deployment. When <strong>Succeeded</strong> is displayed for the pipeline status in the final destination region, view the deployed application:</p> 
<ol> 
<li style="margin-left: 4.5pt">In the status area for <strong>Beta</strong>–<strong>stage </strong>in the final destination region, choose <strong>Details</strong>. The details of the deployment will appear in the AWS CodeDeploy console. You can also pick any other stage in other regions.</li> 
<li style="margin-left: 4.5pt">In the <strong>Deployment Details</strong> section, in <strong>Instance ID</strong>, choose the instance ID of any of the successfully deployed instance.</li> 
<li style="margin-left: 4.5pt">In the Amazon EC2 console, on the <strong>Description</strong> tab, in <strong>Public DNS</strong>, copy the address, and then paste it into the address bar of your web browser. The web page opens the sample web application that was built for you</li> 
</ol> 
<p style="margin-left: -pt">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram9.png" /></p> 
<p style="margin-left: -0pt"><strong>Taking This Further </strong></p> 
<ol> 
<li style="margin-left: -0pt">Using the CloudFormation template provided to you in this post, you can extend the setup to three regions.</li> 
<li style="margin-left: -0pt">So far we have deployed code in two regions within one AWS account. There may be a case where your environments exist in different AWS accounts. For example, assume a scenario in which:</li> 
</ol> 
<li style="margin-left: 4.5pt">You have your development environment running in Region A in AWS Account A.</li> 
<li style="margin-left: 4.5pt">You have your QA environment running in Region B in AWS Account B.</li> 
<li style="margin-left: 4.5pt">You have a staging or production environment running in Region C in AWS Account C.</li> 
<p>&nbsp;</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Diagram10.png" /></p> 
<p style="margin-left: -0pt">You will need to configure cross-account permissions on your destination S3 bucket and delegate these permissions to a role that Lambda assumed in the source account. Without these permissions, the Lambda function in AWS CodePipeline will not be able to copy the source code into the destination S3 bucket. (See the lambdaS3CopyRole in the CloudFormation template provided with this post.)</p> 
<p style="margin-left: -0pt">Create the following bucket policy on the destination bucket in Account B:</p> 
<p style="margin-left: .5in">{</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Version”: “2012-10-17”,</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Statement”: [</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Sid”: “DelegateS3Access”,</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Effect”: “Allow”,</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Principal”: {</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “AWS”: “arn:aws:iam::&lt;Account A ID&gt;:root”</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Action”: “s3:*”,</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Resource”: [</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “arn:aws:s3::: &lt;destination bucket &gt; /*”,</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “arn:aws:s3::: &lt;destination bucket &gt; “</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</p> 
<p style="margin-left: .5in">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]</p> 
<p style="margin-left: .5in">}</p> 
<p>&nbsp;</p> 
<p style="margin-left: -0pt">Repeat this step as you extend the setup to additional accounts.</p> 
<p style="margin-left: -0pt">Create your CloudFormation stacks in Account B and Account C (follow steps 2 and 3 in these accounts, respectively) and your pipeline will execute sequentially.</p> 
<p style="margin-left: -0pt">You can use another code repository solution like AWS CodeCommit or Github as your source and target repositories.</p> 
<p style="margin-left: -0pt"><strong>Wrapping Up</strong></p> 
<p style="margin-left: -0pt">After you’ve finished exploring your pipeline and its associated resources, you can do the following:</p> 
<li style="margin-left: 9.0pt">Extend the setup. Add more stages to your pipeline in AWS CodePipeline.</li> 
<li style="margin-left: 9.0pt"><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-delete-stack.html">Delete the stack</a> in AWS CloudFormation, which deletes the pipeline, its resources, and the stack itself.</li> 
<p style="margin-left: 9.0pt">This is the option to choose if you no longer want to use the pipeline or any of its resources. Cleaning up resources you’re no longer using is important because you don’t want to continue to be charged.</p> 
<p style="margin-left: 9.0pt"><strong>To delete the CloudFormation stack:</strong></p> 
<ol> 
<li>Delete the Amazon S3 buckets used as the artifact store in AWS CodePipeline in the source and destination regions. Although this bucket was created as part of the CloudFormation stack, Amazon S3 does not allow CloudFormation to delete buckets that contain objects.To delete this bucket, open the <a href="https://console.aws.amazon.com/s3/">Amazon S3 console</a>, select the buckets you created in this setup, and then delete them. For more information, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/delete-or-empty-bucket.html">Delete or Empty a Bucket</a>.</li> 
<li>Follow the steps to <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-delete-stack.html">delete a stack in the AWS CloudFormation User Guide</a>.</li> 
</ol> 
<p>&nbsp;</p> 
<p>I would like to thank my colleagues Raul Frias, Asif Khan and Frank Li for their contributions to this post.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-12');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">From ELK Stack to EKK: Aggregating and Analyzing Apache Logs with Amazon Elasticsearch Service, Amazon Kinesis, and Kibana</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Pubali Sen</span></span> | on 
<time property="datePublished" datetime="2016-11-01T10:20:51+00:00">01 NOV 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/from-elk-stack-to-ekk-aggregating-and-analyzing-apache-logs-with-amazon-elasticsearch-service-amazon-kinesis-and-kibana/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>By&nbsp;Pubali Sen, Shankar Ramachandran</em></p> 
<p>Log aggregation is critical to your operational infrastructure. A reliable, secure, and scalable log aggregation solution makes all the difference during a crunch-time debugging session.</p> 
<p>In this post, we&nbsp;explore an alternative to the popular log aggregation solution, the ELK stack (Elasticsearch, Logstash, and Kibana): the EKK stack (Amazon Elasticsearch Service, Amazon Kinesis, and Kibana). The EKK solution eliminates the undifferentiated heavy lifting of deploying, managing, and scaling your log aggregation solution. With the EKK stack, you can focus on analyzing logs and debugging your application, instead of managing and scaling the system that aggregates the logs.</p> 
<p>In this blog post, we describe how to use an EKK stack to monitor Apache logs. Let’s look at the components of the EKK solution.</p> 
<p>Amazon Elasticsearch Service is a popular search and analytics engine that provides real-time application monitoring and log and clickstream analytics. For this post, you will store and index Apache logs in Amazon ES. As a managed service, Amazon ES is easy to deploy, operate, and scale in the AWS Cloud. Using a managed service also eliminates administrative overhead, like patch management, failure detection, node replacement, backing up, and monitoring. Because Amazon ES includes built-in integration with Kibana, it eliminates installing and configuring that platform. This simplifies your process further. For more information about Amazon ES, see the <a href="https://aws.amazon.com/elasticsearch-service/" target="_blank">Amazon Elasticsearch Service detail page.</a></p> 
<p>Amazon Kinesis Agent is an easy-to-install standalone Java software application that collects and sends data. The agent continuously monitors the Apache log file and ships new data to the delivery stream. This agent is also responsible for file rotation, checkpointing, retrying upon failures, and delivering the log data reliably and in a timely manner. For more information, see Writing to <a href="http://docs.aws.amazon.com/firehose/latest/dev/writing-with-agents.html" target="_blank">Amazon Kinesis Firehose Using Amazon Kinesis Agent</a> or <a href="https://github.com/awslabs/amazon-kinesis-agent" target="_blank">Amazon Kinesis Agent</a> in GitHub.</p> 
<p>Amazon Kinesis Firehose provides the easiest way to load streaming data into AWS. In this post, Firehose helps you capture and automatically load the streaming log data to Amazon ES and back it up in Amazon Simple Storage Service (Amazon S3). For more information, see the <a href="https://aws.amazon.com/kinesis/firehose/" target="_blank">Amazon Kinesis Firehose detail page.</a></p> 
<p>You’ll provision an EKK stack by using an AWS CloudFormation template. The template provisions an Apache web server and sends the Apache access logs to an Amazon ES cluster using Amazon Kinesis Agent and Firehose. You’ll back up the logs to an S3 bucket. To see the logs, you’ll leverage the Amazon ES Kibana endpoint.</p> 
<p>By using the template, you can quickly complete the following tasks:</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Provision an Amazon ES cluster.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Provision an Amazon Elastic Compute Cloud (Amazon EC2) instance.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Install Apache HTTP Server version 2.4.23.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Install the Amazon Kinesis Agent on the web server.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Provision an Elastic Load Balancing load balancer.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Create the Amazon ES index and the associated log mappings.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Create an Amazon Kinesis Firehose delivery stream.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Create all AWS Identity and Access Management (IAM) roles and policies. For example, the Firehose delivery stream backs up the Apache logs to an S3 bucket. This requires that the Firehose delivery stream be associated with a role that gives it permission to upload the logs to the correct S3 bucket.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Configure Amazon CloudWatch Logs log streams and log groups for the Firehose delivery stream. This helps you to troubleshoot when the log events don’t reach their destination.</p> 
<p>EKK Stack Architecture<br /> The following architecture diagram shows how an EKK stack works.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/01/Arch8-2-2-2.png" /></p> 
<p>Prerequisites<br /> To build the EKK stack, you must have the following:</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An Amazon EC2 key pair in the US West (Oregon) Region. If you don’t have one, create one.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; An S3 bucket in the US West (Oregon) Region. If you don’t have one, create one.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A default VPC in the US West (Oregon) Region. If you have deleted the default VPC, request one.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Administrator-level permissions in IAM to enable Amazon ES and Amazon S3 to receive the log data from the EC2 instance through Firehose.</p> 
<p>Getting Started<br /> Begin by launching the AWS CloudFormation template to create the stack.</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; In the AWS CloudFormation console, choose &nbsp;to &nbsp; <a href="https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=estemplatev4&amp;templateURL=https://s3.amazonaws.com/scriptdepot/es.template" target="_blank"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a>&nbsp;the AWS CloudFormation template. Make sure that you are in the US West (Oregon) region.</p> 
<p>Note: If you want to download the template to your computer and then upload it to AWS CloudFormation, you can do so from this Amazon S3 bucket. Save the template to a location on your computer that’s easy to remember.</p> 
<p>2.&nbsp;&nbsp;&nbsp;&nbsp; Choose Next.</p> 
<p>3.&nbsp;&nbsp;&nbsp;&nbsp; On the Specify Details page, provide the following:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/01/Screen-Shot-2016-11-01-at-9.44.20-AM.png" /></p> 
<p>a)&nbsp;&nbsp;&nbsp; Stack Name: A name for your stack.</p> 
<p>b)&nbsp;&nbsp;&nbsp; InstanceType: Select the instance family for the EC2 instance hosting the web server.</p> 
<p>c)&nbsp;&nbsp;&nbsp;&nbsp; KeyName: Select the Amazon EC2 key pair in the US West (Oregon) Region.</p> 
<p>d)&nbsp;&nbsp;&nbsp; SSHLocation: The IP address range that can be used to connect to the EC2 instance by using SSH. Accept the default, 0.0.0.0/0.</p> 
<p>e)&nbsp;&nbsp;&nbsp; WebserverPort: The TCP/IP port of the web server. Accept the default, 80.</p> 
<p>4.&nbsp;&nbsp;&nbsp;&nbsp; Choose Next.</p> 
<p>5.&nbsp;&nbsp;&nbsp;&nbsp; On the Options page, optionally specify tags for your AWS CloudFormation template, and then choose Next.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/createStack2.png" /></p> 
<p>6.&nbsp;&nbsp;&nbsp;&nbsp; On the Review page, review your template details. Select the Acknowledgement checkbox, and then choose Create to create the stack.</p> 
<p>It takes about 10-15 minutes to create the entire stack.</p> 
<p>Configure the Amazon Kinesis Agent<br /> After AWS CloudFormation has created the stack, configure the Amazon Kinesis Agent.</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; In the AWS CloudFormation console, choose the Resources tab to find the Firehose delivery stream name. You need this to configure the agent. Record this value because you will need it in step 3.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/createStack3.png" /></p> 
<p>2.&nbsp;&nbsp;&nbsp;&nbsp; On the Outputs tab, find and record the public IP address of the web server. You need it to connect to the web server using SSH to configure the agent. For instructions on how to connect to an EC2 instance using SSH, see <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html" target="_blank">Connecting to Your Linux Instance Using SSH</a>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/outputs.png" /></p> 
<p>3. On the web server’s command line, run the following command:</p> 
<p>sudo vi /etc/aws-kinesis/agent.json</p> 
<p>This command opens the configuration file, agent.json, as follows.</p> 
<em><code class="lang-json">{ &quot;cloudwatch.emitMetrics&quot;: true, &quot;firehose.endpoint&quot;: &quot;firehose.us-west-2.amazonaws.com&quot;, &quot;awsAccessKeyId&quot;: &quot;&quot;, &quot;awsSecretAccessKey&quot;: &quot;&quot;, &quot;flows&quot;: [ { &quot;filePattern&quot;: &quot;/var/log/httpd/access_log&quot;, &quot;deliveryStream&quot;: &quot;&quot;, &quot;dataProcessingOptions&quot;: [ { &quot;optionName&quot;: &quot;LOGTOJSON&quot;, &quot;logFormat&quot;: &quot;COMMONAPACHELOG&quot; } ] } ] } </code></em> 
<p>4.&nbsp;&nbsp;&nbsp;&nbsp; For the deliveryStream key, type the value of the KinesisFirehoseDeliveryName that you retrieved from the stack’s Resources tab. After you type the value, save and terminate the agent.json file.</p> 
<p>5.&nbsp;&nbsp;&nbsp;&nbsp; Run the following command on the CLI:</p> 
<p>sudo service aws-kinesis-agent restart</p> 
<p>6. &nbsp; &nbsp; On the AWS CloudFormation console choose the resources tab and note the name of the Amazon ES cluster corresponding to the LogicalID&nbsp;ESDomain.</p> 
<p>7.&nbsp;&nbsp;&nbsp;&nbsp; Go to AWS Management Console, and choose Amazon Elasticsearch Service. Under My Domains, you can see the Amazon ES domain that the AWS CloudFormation template created.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/createStac5.png" /></p> 
<p>Configure Kibana and View Your Apache Logs<br /> Amazon ES provides a default installation of Kibana with every Amazon ES domain. You can find the Kibana endpoint on your domain dashboard in the Amazon ES console.</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; In the Amazon ES console, choose the Kibana endpoint.</p> 
<p>2.&nbsp;&nbsp;&nbsp;&nbsp; In Kibana, for Index name or pattern, type logmonitor. logmonitor is the name of the AWS ES index that you created for the web server access logs. The health checks from Amazon Elastic Load Balancing generate access logs on the web server, which flow through the EKK pipeline to Kibana for discovery and visualization.</p> 
<p>3. &nbsp; &nbsp; In Time-field name, select datetime.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/kibana1.png" /></p> 
<p>4.&nbsp;&nbsp;&nbsp;&nbsp; On the Kibana console, choose the Discover tab to see the Apache logs.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/kibana3.png" /></p> 
<p>Use Kibana to visualize the log data by creating bar charts, line and scatter plots, histograms, pie charts, etc.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/Kibana4.png" /></p> 
<p>Pie chart of IP addresses accessing the web server in the last 30 days</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/Kibana5.png" /></p> 
<p>Bar chart of IP addresses accessing the web server in the last 5 minutes</p> 
<p>You can graph information about http response, bytes, or IP address to provide meaningful insights on the Apache logs. Kibana also facilitates making dashboards by combining graphs.</p> 
<p>Monitor Your Log Aggregator</p> 
<p>To monitor the Firehose delivery stream, navigate to the Firehose console. Choose the stream, and then choose the Monitoring tab to see the Amazon CloudWatch metrics for the stream.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/monito1.png" /></p> 
<p>&nbsp;</p> 
<p>When log delivery fails, the Amazon S3 and Amazon ES logs help you troubleshoot. For example, the following screenshot shows logs when delivery to an Amazon ES destination fails because the date mapping on the index was not in line with the ingest log.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/monitor2.png" /></p> 
<p>Conclusion<br /> In this post, we showed how to ship Apache logs to Kibana by using Amazon Kinesis Agent, Amazon ES, and Firehose. It’s worth pointing out that Firehose automatically scales up or down based on the rate at which your application generates logs. To learn more about scaling Amazon ES clusters, see the <a href="http://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/what-is-amazon-elasticsearch-service.html#concepts-scaling" target="_blank">Amazon Elasticsearch Service Developer Guide</a>.</p> 
<p>Managed services like Amazon ES and Amazon Kinesis Firehose simplify provisioning and managing a log aggregation system. The ability to run SQL queries against your streaming log data using Amazon Kinesis Analytics further strengthens the case for using an EKK stack. <a href="https://s3-us-west-2.amazonaws.com/scriptdepot/es.template" target="_blank">The AWS CloudFormation template</a> used in this post is available to extend and build your own EKK stack.</p> 
<p>&nbsp;</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building End-to-End Continuous Delivery and Deployment Pipelines in AWS and TeamCity</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Balaji Iyer</span></span> | on 
<time property="datePublished" datetime="2016-10-31T12:14:54+00:00">31 OCT 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/best-practices/" title="View all posts in Best practices"><span property="articleSection">Best practices</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/partners/" title="View all posts in Partners"><span property="articleSection">Partners</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/web-app/" title="View all posts in Web app"><span property="articleSection">Web app</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/building-end-to-end-continuous-delivery-and-deployment-pipelines-in-aws-and-teamcity/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-206" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=206&amp;disqus_title=Building+End-to-End+Continuous+Delivery+and+Deployment+Pipelines+in+AWS+and+TeamCity&amp;disqus_url=https://aws.amazon.com/blogs/devops/building-end-to-end-continuous-delivery-and-deployment-pipelines-in-aws-and-teamcity/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-206');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>By Balaji Iyer, Janisha Anand, and Frank Li</em></p> 
<p>Organizations that transform their applications to cloud-optimized architectures need a seamless, end-to-end continuous delivery and deployment workflow: from source code, to build, to deployment, to software delivery.</p> 
<p><strong>Continuous delivery</strong>&nbsp;is a&nbsp;<a href="https://aws.amazon.com/devops/">DevOps</a>&nbsp;software development practice where code changes are automatically built, tested, and prepared for a release to production. The practice expands on <a href="https://aws.amazon.com/devops/continuous-integration/">continuous integration</a>&nbsp;by deploying all code changes to a testing environment and/or a production environment after the build stage. When continuous delivery is implemented properly, developers will always have a deployment-ready build artifact that has undergone a standardized test process.</p> 
<p><strong>Continuous deployment</strong> is the process of deploying application revisions to a production environment automatically, without explicit approval from a developer. This process makes the entire software release process automated. Features are released as soon as they are ready, providing maximum value to customers.</p> 
<p>These two techniques enable development teams to deploy software rapidly, repeatedly, and reliably.</p> 
<p>In this post, we will build an end-to-end continuous deployment and delivery pipeline using <a href="http://aws.amazon.com/codepipeline">AWS CodePipeline</a>&nbsp;(a fully managed&nbsp;<a href="http://aws.amazon.com/devops/continuous-delivery/">continuous delivery</a>&nbsp;service), <a href="http://aws.amazon.com/codedeploy">AWS CodeDeploy</a>&nbsp;(an automated application deployment service), and TeamCity’s <a href="https://confluence.jetbrains.com/display/TW/AWS+CodePipeline+Plugin">AWS CodePipeline plugin</a>. We will use <a href="http://aws.amazon.com/cloudformation">AWS CloudFormation</a> to setup and configure the end-to-end infrastructure and application <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html#d0e3802">stacks</a>. The &shy;&shy;pipeline pulls source code from an Amazon S3 bucket, an AWS CodeCommit repository, or a GitHub repository. The source code will then be built and tested using TeamCity’s continuous integration server. Then AWS CodeDeploy will deploy the compiled and tested code to <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances.</p> 
<p><strong>Prerequisites</strong></p> 
<p>You’ll need an AWS account, an <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> key pair, and administrator-level permissions for AWS <a href="https://aws.amazon.com/iam/">Identity and Access Management (IAM)</a>, AWS CloudFormation, AWS CodeDeploy, AWS CodePipeline, Amazon EC2, and <a href="https://aws.amazon.com/s3/">Amazon S3</a>.</p> 
<p><strong>Overview</strong></p> 
<p>Here are the steps:</p> 
<ol> 
<li>Continuous integration server setup using TeamCity.</li> 
<li>Continuous deployment using AWS CodeDeploy.</li> 
<li>Building a delivery pipeline using AWS CodePipeline.</li> 
</ol> 
<p>In less than an hour, you’ll have an end-to-end, fully-automated continuous integration, continuous deployment, and delivery pipeline for your application. Let’s get started!</p> 
<p><span id="more-206"></span></p> 
<b>1. Continuous integration server setup using TeamCity</b> 
<p>Click here on this button <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=TeamCityServer&amp;templateURL=https://s3.amazonaws.com/teamcity-aws-demo/TeamCity_Server_Template.cform"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png">here</a>.</p> 
<p>The CloudFormation template does the following:</p> 
<ol> 
<li>Installs and configures the TeamCity server and its dependencies in Linux.</li> 
<li>Installs the AWS CodePipeline <a href="https://confluence.jetbrains.com/display/TW/AWS+CodePipeline+Plugin">plugin</a> for TeamCity.</li> 
<li>Installs a <a href="https://github.com/awslabs/aws-demo-php-simple-app">sample application</a> with build configurations.</li> 
<li>Installs <a href="https://github.com/jetbrains/meta-runner-power-pack">PHP meta-runners</a> required to build the sample application.</li> 
<li>Redirects TeamCity port 8111 to 80.</li> 
</ol> 
<p>Choose the AWS region where the TeamCity server will be hosted. For this demo, choose <strong>US East (N. Virginia)</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture2-300x282.png" /></p> 
<p>On the <strong>Select Template </strong>page, choose <strong>Next</strong>.</p> 
<p>On the <strong>Specify Details</strong> page, do the following:</p> 
<ol> 
<li>In <strong>Stack name</strong>, enter a name for the stack. The name must be unique in the region in which you are creating the stack.</li> 
<li>In <strong>InstanceType</strong>, choose the instance type that best fits your requirements. The default value is t2.medium.</li> 
</ol> 
<p><strong>Note</strong>: The default instance type exceeds what’s included in the AWS Free Tier. If you use t2.medium, there will be charges to your account. The cost will depend on how long you keep the CloudFormation stack and its resources.</p> 
<ol start="3"> 
<li>In<strong> KeyName</strong>, choose the name of your Amazon EC2 key pair.</li> 
<li>In <strong>SSHLocation</strong>, enter the IP address range that can be used to connect through SSH to the EC2 instance. SSH and HTTP access is limited to this IP address range.</li> 
</ol> 
<p><strong>Note: </strong>You can use <a href="http://checkip.amazonaws.com/">checkip.amazonaws.com</a>&nbsp;or <a href="http://www.whatsmyip.org/">whatsmyip.org</a> to find your IP address. Remember to add&nbsp;<strong>/32</strong>&nbsp;to any single domain or, if you are representing a larger IP address space, use the correct CIDR block notation.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture3.png" /></p> 
<p>Choose <strong>Next</strong>.</p> 
<p>Although it’s optional, on the<strong> Options</strong>&nbsp;page, type TeamCityServer for the instance name. This is the name used in the CloudFormation template for the stack. It’s a best practice to name your instance, because it makes it easier to identify or modify resources later on.</p> 
<p>Choose&nbsp;<strong>Next</strong>.</p> 
<p>On the <strong>Review </strong>page, choose <strong>Create</strong> button. It will take several minutes for AWS CloudFormation to create the resources for you.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture4.png" /></p> 
<p>When the stack has been created, you will see a <strong>CREATE_COMPLETE</strong> message on the <strong>Overview</strong> tab in the <strong>Status</strong> column.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture5.png" /></p> 
<p>You have now successfully created a TeamCity server. To access the server, on the <a href="https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:search=TeamCityServer">EC2 Instance page</a>, choose <strong>Public IP</strong> for the TeamCityServer instance.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture6.png" /></p> 
<p>On the <strong>TeamCity First Start</strong> page, choose <strong>Proceed</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture7.png" /></p> 
<p>Although an internal database based on the HSQLDB database engine can be used for evaluation, TeamCity strongly recommends that you use an external database as a back-end TeamCity database in a production environment. An external database provides better performance and reliability. For more information, see the TeamCity <a href="https://confluence.jetbrains.com/display/TCD9/Setting+up+an+External+Database#SettingupanExternalDatabase-SelectingExternalDatabaseEngine">documentation</a>.</p> 
<p>On the <strong>Database connection setup</strong> page, choose<strong> Proceed</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture8.png" /></p> 
<p>The TeamCity server will start, which can take several minutes.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture9.png" /></p> 
<p>Review and Accept the TeamCity License Agreement, and then choose <strong>Continue</strong>.</p> 
<p>Next, create an Administrator account. Type a user name and password, and then choose <strong>Create Account</strong>.</p> 
<p>You can navigate to the demo project from <strong>Projects</strong> in the top-left corner.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture10.png" /></p> 
<p><strong>Note: </strong>You can create a project from a repository URL (the option used in this demo), or you can connect to your managed Git repositories, such as GitHub or BitBucket. The demo app used in this example can be found <a href="https://github.com/awslabs/aws-demo-php-simple-app">here</a>.</p> 
<p>We have already created a sample project configuration. Under <strong>Build</strong>, choose <strong>Edit Settings</strong>, and then review the settings.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture11.png" /></p> 
<p>Choose <strong>Build Step: PHP – PHPUnit</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture12.png" /></p> 
<p>The fields on the<strong> Build Step</strong> page are already configured.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture13.png" /></p> 
<p>Choose <strong>Run </strong>to start the build.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture14.png" /></p> 
<p>To review the tests that are run as part of the build, choose <strong>Tests</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture15.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/Picture16.png" /></p> 
<p>You can view any build errors by choosing <strong>Build log</strong> from the same drop-down list.</p> 
<p>Now that we have a successful build, we will use AWS CodeDeploy to set up a continuous deployment pipeline.</p> 
<b>2. Continuous deployment using AWS CodeDeploy</b> 
<p>Click here on this button <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=DeploymentPipeline&amp;templateURL=https://s3.amazonaws.com/teamcity-aws-demo/CodeDeploy_Master_Template.cform"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a> to launch an AWS CloudFormation stack that will use AWS CodeDeploy to set up a sample deployment pipeline. If you’re not already signed in to the AWS Management Console, you will be prompted to enter your AWS credentials.</p> 
<p>You can download the master template used for this setup from <a href="https://s3.amazonaws.com/teamcity-aws-demo/CodeDeploy_Master_Template.cform">here</a>. The template nests two CloudFormation templates to execute all dependent stacks cohesively.</p> 
<ol> 
<li>Template 1 creates a fleet of up to three EC2 instances (with a base operating system of Windows or Linux), associates an instance profile, and installs the AWS CodeDeploy agent. The CloudFormation template can be downloaded from <a href="https://s3.amazonaws.com/teamcity-aws-demo/CodeDeploy_Template.cform">here</a>.</li> 
<li>Template 2 creates an AWS CodeDeploy <a href="http://docs.aws.amazon.com/codedeploy/latest/userguide/how-to-create-deployment-group.html">deployment group</a> and then installs a sample application. The CloudFormation template can be downloaded from <a href="https://s3.amazonaws.com/teamcity-aws-demo/CodeDeploy_DeploymentGroup_Template.cform">here</a>.</li> 
</ol> 
<p>Choose the same AWS region you used when you created the TeamCity server (<strong>US East (N. Virginia)</strong>).</p> 
<p><strong>Note:</strong> The templates contain Amazon Machine Image (AMI) mappings for us-east-1, us-west-2, eu-west-1, and ap-southeast-2 only.</p> 
<p>On the <strong>Select Template </strong>page, choose <strong>Next</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture17.png" /></p> 
<p>On the <strong>Specify Details </strong>page, in&nbsp;<strong>Stack name</strong>, type a name for the stack. In the <strong>Parameters </strong>section, do the following:</p> 
<ol> 
<li>In&nbsp;<strong>AppName</strong>, you can use the default, or you can type a name of your choice. The name must be between 2 and 15 characters long. It can contain lowercase and alphanumeric characters, hyphens (-), and periods (.), but the name must start with an alphanumeric character.</li> 
</ol> 
<ol start="2"> 
<li>In <strong>DeploymentGroupName</strong>, you can use the default, or you type a name of your choice. The name must be between 2 and 25 characters long. It can contain lowercase and alphanumeric characters, hyphens (-), and periods (.), but the name must start with an alphanumeric character.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture18.png" /></p> 
<ol start="3"> 
<li>In <strong>InstanceType</strong>, choose the instance type that best fits the requirements of your application.</li> 
<li>In <strong>InstanceCount</strong>, type the number of EC2 instances (up to three) that will be part of the deployment group.</li> 
<li>For <strong>Operating System</strong>, choose <strong>Linux</strong> or <strong>Windows</strong>.</li> 
<li>Leave <strong>TagKey</strong> and <strong>TagValue</strong> at their defaults. AWS CodeDeploy will use this tag key and value to locate the instances during deployments. For information about Amazon EC2 instance tags, see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#Using_Tags_Console">Working with Tags Using the Console</a>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture19.png" /></li> 
<li>In S3Bucket and S3Key, type the bucket name and S3 key where the application is located. The default points to a sample application that will be deployed to instances in the deployment group. Based on what you selected in the OperatingSystem field, use the following values.<br /> <strong>Linux:</strong><br /> S3Bucket: aws-codedeploy<br /> S3Key: samples/latest/SampleApp_Linux.zip<br /> <strong>Windows:</strong><br /> S3Bucket: aws-codedeploy<br /> S3Key: samples/latest/SampleApp_Windows.zip</li> 
</ol> 
<ol start="8"> 
<li>In<strong> KeyName</strong>, choose the name of your Amazon EC2 key pair.</li> 
<li>In <strong>SSHLocation</strong>, enter the IP address range that can be used to connect through SSH/RDP to the EC2 instance.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture20.png" /></p> 
<p><strong>Note: </strong>You can use <a href="http://checkip.amazonaws.com/">checkip.amazonaws.com</a>&nbsp;or <a href="http://www.whatsmyip.org/">whatsmyip.org</a> to find your IP address. Remember to add&nbsp;<strong>/32</strong>&nbsp;to any single domain or, if you are representing a larger IP address space, use the correct CIDR block notation.</p> 
<p>Follow the prompts, and then choose <strong>Next</strong>.</p> 
<p>On the&nbsp;<strong>Review</strong><strong>&nbsp;</strong>page, select the&nbsp;<strong>I acknowledge that this template might cause AWS CloudFormation to create IAM resources</strong>&nbsp;check box. Review the other settings, and then choose&nbsp;<strong>Create</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture21.png" /></p> 
<p>It will take several minutes for CloudFormation to create all of the resources on your behalf. The nested stacks will be launched sequentially. You can view progress messages on the <strong>Events</strong> tab in the AWS CloudFormation console.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture22.png" /></p> 
<p>You can see the newly created application and deployment groups in the AWS CodeDeploy console.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture23.png" /></p> 
<p>To verify that your application was deployed successfully, navigate to the DNS address of one of the instances.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture24.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture25.png" /></p> 
<p>Now that we have successfully created a deployment pipeline, let’s integrate it with AWS CodePipeline.</p> 
<b>3. Building a delivery pipeline using AWS CodePipeline</b> 
<p>We will now create a delivery pipeline in AWS CodePipeline with the <a href="https://confluence.jetbrains.com/display/TW/AWS+CodePipeline+Plugin">TeamCity AWS CodePipeline plugin</a>.</p> 
<ol> 
<li>Using AWS CodePipeline, we will build a new pipeline with Source and Deploy stages.</li> 
<li>Create a custom action for the TeamCity Build stage.</li> 
<li>Create an AWS CodePipeline action trigger in TeamCity.</li> 
<li>Create a Build stage in the delivery pipeline for TeamCity.</li> 
<li>Publish the build artifact for deployment.</li> 
</ol> 
<h3>Step 1: Build a new pipeline with Source and Deploy stages using AWS CodePipeline.</h3> 
<p>In this step, we will create an Amazon S3 bucket to use as the artifact store for this pipeline.</p> 
<ol> 
<li>Install and configure the <a href="http://docs.aws.amazon.com/streams/latest/dev/kinesis-tutorial-cli-installation.html">AWS CLI</a>.</li> 
</ol> 
<ol start="2"> 
<li>Create an Amazon S3 bucket that will host the build artifact. Replace <em>account-number</em> with your AWS account number in the following steps. <code class="lang-bash">$ aws s3 mb s3://demo-app-build-account-number</code> </li> 
</ol> 
<ol start="3"> 
<li>Enable bucket versioning <code class="lang-bash">$ aws s3api put-bucket-versioning --bucket demo-app-build-account-number --versioning-configuration Status=Enabled</code> </li> 
</ol> 
<ol start="4"> 
<li>Download the sample build artifact and upload it to the Amazon S3 bucket created in step 2.</li> 
</ol> 
<li><strong>OSX/Linux:</strong> <code class="lang-bash">$ wget -qO- https://s3.amazonaws.com/teamcity-demo-app/Sample_Linux_App.zip | aws s3 cp - s3://demo-app-build-account-number/Sample_Linux_App.zip</code> </li> 
<li><strong><strong>Windows:</strong></strong> <code class="lang-code">$ wget -qO- https://s3.amazonaws.com/teamcity-demo-app/Sample_Windows_App.zip
$ aws s3 cp ./Sample_Windows_App.zip s3://demo-app-account-number</code> </li> 
<p><strong>Note</strong>: You can use AWS CloudFormation to perform these steps in an automated way. When you choose <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=S3BucketStack&amp;templateURL=https://s3.amazonaws.com/teamcity-aws-demo/S3_Bucket_Template.cform"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png">this template</a> will be used. Use the following commands to extract the Amazon S3 bucket name, enable versioning on the bucket, and copy over the sample artifact.</p> 
<code class="lang-bash">$ export bucket-name =&quot;$(aws cloudformation describe-stacks --stack-name “S3BucketStack” --output text --query 'Stacks[0].Outputs[?OutputKey==`S3BucketName`].OutputValue')&quot;
$ aws s3api put-bucket-versioning --bucket $bucket-name --versioning-configuration Status=Enabled &amp;&amp; wget https://s3.amazonaws.com/teamcity-demo-app/Sample_Linux_App.zip &amp;&amp; aws s3 cp ./Sample_Linux_App.zip s3://$bucket-name</code> 
<p>You can create a pipeline by using a CloudFormation stack or the AWS CodePipeline console.</p> 
<h4>Option 1: Use AWS CloudFormation to create a pipeline</h4> 
<p>We’re going to create a two-stage pipeline that uses a versioned Amazon S3 bucket and AWS CodeDeploy to release a sample application. (You can use an AWS CodeCommit repository or a GitHub repository as the source provider instead of Amazon S3.)</p> 
<p>Click here on this button <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=DeliveryPipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fteamcity-aws-demo%2FCodePipeline_Template.cform"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/24/launch-stack.png" /></a>&nbsp;to launch an AWS CloudFormation stack to set up a new delivery pipeline using the application and deployment group created in an earlier step. If you’re not already signed in to the AWS Management Console, you will be prompted to enter your AWS credentials.</p> 
<p>Choose the <strong>US East (N. Virginia)</strong> region, and then choose <strong>Next</strong>.</p> 
<p>Leave the default options, and then choose <strong>Next</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture26.png" /></p> 
<p>On the <strong>Options </strong>page, choose <strong>Next</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture27.png" /></p> 
<p><strong>Select</strong> the<strong> I acknowledge that AWS CloudFormation might create IAM resources</strong> check box, and then choose <strong>Create</strong>. This will create the delivery pipeline in AWS CodePipeline.</p> 
<h4>Option 2: Use the AWS CodePipeline console to create a pipeline</h4> 
<p>On the <strong>Create pipeline</strong> page, in <strong>Pipeline name</strong>, type a name for your pipeline, and then choose <strong>Next step</strong>.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture28.png" /></p> 
<p>Depending on where your source code is located, you can choose Amazon S3, AWS CodeCommit, or GitHub as your<strong> Source provider</strong>. The pipeline will be triggered automatically upon every check-in to your GitHub or AWS CodeCommit repository or when an artifact is published into the S3 bucket. In this example, we will be accessing the product binaries from an Amazon S3 bucket.</p> 
<p>Choose <strong>Next step</strong>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture29.png" /></p> 
<p>s3://demo-app-build-<em>account-number</em><em>/Sample_Linux_App.zip</em> (or) <em>Sample_Windows_App.zip</em></p> 
<p><strong>Note:</strong> AWS CodePipeline requires a versioned S3 bucket for source artifacts. <a href="http://docs.aws.amazon.com/AmazonS3/latest/UG/enable-bucket-versioning.html">Enable versioning</a> for the S3 bucket where the source artifacts will be located.</p> 
<p>On the <strong>Build </strong>page, choose <strong>No Build</strong>. We will update the build provider information later on.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture31.png" /></p> 
<p>For <strong>Deployment provider</strong>, choose <strong>CodeDeploy</strong>. For <strong>Application name </strong>and <strong>Deployment group</strong>, choose the application and deployment group we created in the deployment pipeline step, and then choose <strong>Next step</strong>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture32.png" /></p> 
<p>An IAM role will provide the permissions required for AWS CodePipeline to perform the build actions and service calls.&nbsp; If you already have a role you want to use with the pipeline, choose it on the <strong>AWS Service Role</strong> page. Otherwise, type a name for your role, and then choose <strong>Create role</strong>.&nbsp; Review the predefined permissions, and then choose <strong>Allow</strong>. Then choose <strong>Next step</strong>.</p> 
<p>&nbsp;</p> 
<p>For information about AWS CodePipeline access permissions, see the&nbsp;<a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/access-permissions.html">AWS CodePipeline Access Permissions Reference</a>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture34.png" /></p> 
<p>Review your pipeline, and then choose <strong>Create pipeline</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture35.png" /></p> 
<p>This will trigger AWS CodePipeline to execute the Source and Beta steps. The source artifact will be deployed to the AWS CodeDeploy deployment groups.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture36.png" /></p> 
<p>Now you can access the same DNS address of the AWS CodeDeploy instance to see the updated deployment. You will see the background color has changed to green and the page text has been updated.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture37.png" /></p> 
<p>We have now successfully created a delivery pipeline with two stages and integrated the deployment with AWS CodeDeploy. Now let’s integrate the Build stage with TeamCity.</p> 
<h3>Step 2: Create a custom action for TeamCity Build stage</h3> 
<p>AWS CodePipeline includes a number of actions that help you configure build, test, and deployment resources for your automated release process. TeamCity is not included in the default actions, so we will create a <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/how-to-create-custom-action.html">custom action</a> and then include it in our delivery pipeline. TeamCity’s CodePipeline <a href="https://confluence.jetbrains.com/display/TW/AWS+CodePipeline+Plugin">plugin</a> will also create a job worker that will poll AWS CodePipeline for job requests for this custom action, execute the job, and return the status result to AWS CodePipeline.</p> 
<p>TeamCity’s custom action type (Build/Test categories) can be integrated with AWS CodePipeline. It’s similar to Jenkins and Solano CI custom actions. TeamCity’s CodePipeline <a href="https://confluence.jetbrains.com/display/TW/AWS+CodePipeline+Plugin">plugin</a> will also create a job worker that will poll AWS CodePipeline for job requests for this custom action, execute the job, and return the status result to AWS CodePipeline.</p> 
<p>The TeamCity AWS CodePipeline <a href="https://confluence.jetbrains.com/display/TW/AWS+CodePipeline+Plugin">plugin</a> is already installed on the TeamCity server we set up earlier. To learn more about installing TeamCity plugins, see <a href="https://confluence.jetbrains.com/display/TCDL/Installing+Additional+Plugins#InstallingAdditionalPlugins-InstallingTeamCityplugins">install the plugin</a>. We will now create a custom action to integrate TeamCity with AWS CodePipeline using a custom-action JSON file.</p> 
<p>Download this file locally: <a href="https://github.com/JetBrains/teamcity-aws-codepipeline-plugin/blob/master/custom-action.json">https://github.com/JetBrains/teamcity-aws-codepipeline-plugin/blob/master/custom-action.json</a></p> 
<p>Open a terminal session (Linux, OS X, Unix) or command prompt (Windows) on a computer where you have installed the AWS CLI. For information about setting up the AWS CLI, see <a href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html">here</a>.</p> 
<p>Use the AWS CLI to run the <strong>aws codepipeline create-custom-action-type</strong> command, specifying the name of the JSON file you just created.</p> 
<p>For example, to create a build custom action:</p> 
<code class="lang-bash">$ aws codepipeline create-custom-action-type --cli-input-json file://teamcity-custom-action.json</code> 
<p>This should result in an output similar to this:</p> 
<code class="lang-bash">{
&quot;actionType&quot;: {
&quot;inputArtifactDetails&quot;: {
&quot;maximumCount&quot;: 5,
&quot;minimumCount&quot;: 0
},
&quot;actionConfigurationProperties&quot;: [
{
&quot;description&quot;: &quot;The expected URL format is http[s]://host[:port]&quot;,
&quot;required&quot;: true,
&quot;secret&quot;: false,
&quot;key&quot;: true,
&quot;queryable&quot;: false,
&quot;name&quot;: &quot;TeamCityServerURL&quot;
},
{
&quot;description&quot;: &quot;Corresponding TeamCity build configuration external ID&quot;,
&quot;required&quot;: true,
&quot;secret&quot;: false,
&quot;key&quot;: true,
&quot;queryable&quot;: false,
&quot;name&quot;: &quot;BuildConfigurationID&quot;
},
{
&quot;description&quot;: &quot;Must be unique, match the corresponding field in the TeamCity build trigger settings, satisfy regular expression pattern: [a-zA-Z0-9_-]+] and have length &lt;= 20&quot;,
&quot;required&quot;: true,
&quot;secret&quot;: false,
&quot;key&quot;: true,
&quot;queryable&quot;: true,
&quot;name&quot;: &quot;ActionID&quot;
}
],
&quot;outputArtifactDetails&quot;: {
&quot;maximumCount&quot;: 5,
&quot;minimumCount&quot;: 0
},
&quot;id&quot;: {
&quot;category&quot;: &quot;Build&quot;,
&quot;owner&quot;: &quot;Custom&quot;,
&quot;version&quot;: &quot;1&quot;,
&quot;provider&quot;: &quot;TeamCity&quot;
},
&quot;settings&quot;: {
&quot;entityUrlTemplate&quot;: &quot;{Config:TeamCityServerURL}/viewType.html?buildTypeId={Config:BuildConfigurationID}&quot;,
&quot;executionUrlTemplate&quot;: &quot;{Config:TeamCityServerURL}/viewLog.html?buildId={ExternalExecutionId}&amp;tab=buildResultsDiv&quot;
}
}
}</code> 
<p>Before you add the custom action to your delivery pipeline, make the following changes to the TeamCity build server. You can access the server by opening the <strong>Public IP</strong> of the TeamCityServer instance from the <a href="https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:search=TeamCityServer">EC2 Instance page</a>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture38.png" /></p> 
<p>In TeamCity, choose <strong>Projects</strong>. Under <strong>Build Configuration Settings</strong>, choose <strong>Version Control Settings</strong>. You need to remove the version control trigger here so that the TeamCity build server will be triggered during the Source stage in AWS CodePipeline. Choose <strong>Detach</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture39.png" /></p> 
<h3>Step 3: Create a new AWS CodePipeline action trigger in TeamCity</h3> 
<p>Now add a new AWS CodePipeline trigger in your build configuration. Choose <strong>Triggers</strong>, and then choose <strong>Add new trigger</strong></p> 
<p><strong><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture40.png" /></strong></p> 
<p>From the drop-down menu, choose <strong>AWS CodePipeline Action</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture41.png" /></p> 
<p>&nbsp;</p> 
<p>In the AWS CodePipeline console, choose the region in which you created your delivery pipeline. Enter your access key credentials, and for <strong>Action ID</strong>, type a unique name. You will need this ID when you add a TeamCity Build stage to the pipeline.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture42.png" /></p> 
<h3>Step 4: Create a new Build stage in the delivery pipeline for TeamCity</h3> 
<p>Add a stage to the pipeline and name it <strong>Build</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture43-866x1024.png" /></p> 
<p>From the drop-down menu, choose <strong>Build</strong>. In <strong>Action name</strong>, type a name for the action. In <strong>Build provider</strong>, choose <strong>TeamCity</strong>, and then choose <strong>Add action</strong>.</p> 
<p>Select <strong>TeamCity</strong>, click <strong>Add action</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture44.png" /></p> 
<p>For <strong>TeamCity Action Configuration</strong>, use the following:</p> 
<p><strong>TeamCityServerURL</strong>:&nbsp; http://<em>&lt;Public DNS address of the TeamCity build server&gt;</em>[:port]</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture45.png" /></p> 
<p><strong>BuildConfigurationID</strong>: In your TeamCity project, choose <strong>Build</strong>. You’ll find this ID (AwsDemoPhpSimpleApp_Build) under <strong>Build Configuration Settings</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture46.png" /></p> 
<p><strong>ActionID</strong>: In your TeamCity project, choose <strong>Build</strong>. You’ll find this ID under <strong>Build Configuration Settings</strong>. Choose <strong>Triggers</strong>, and then choose<strong> AWS CodePipeline Action</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture47.png" /></p> 
<p>Next, choose input and output artifacts for the Build stage, and then choose <strong>Add action</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture48.png" /></p> 
<p>We will now publish a new artifact to the Amazon S3 artifact bucket we created earlier, so we can see the deployment of a new app and its progress through the delivery pipeline. The demo app used in this artifact can be found <a href="https://github.com/awslabs/aws-demo-php-simple-app">here</a> for Linux or <a href="https://github.com/awslabs/AWSCodePipeline-S3-AWSCodeDeploy_Windows">here</a> for Windows.</p> 
<p>Download the sample build artifact and upload it to the Amazon S3 bucket created in step 2.</p> 
<p>OSX/Linux:</p> 
<code class="lang-bash">$ wget -qO- https://s3.amazonaws.com/teamcity-demo-app/PhpArtifact.zip | aws s3 cp - s3://demo-app-build-account-number/PhpArtifact.zip</code> 
<p>Windows:</p> 
<code class="lang-code">$ wget -qO- https://s3.amazonaws.com/teamcity-demo-app/WindowsArtifact.zip
$ aws s3 cp ./WindowsArtifact.zip s3://demo-app-account-number
</code> 
<p>From the AWS CodePipeline <a href="https://console.aws.amazon.com/codepipeline/home?region=us-east-1#/dashboard">dashboard</a>, under delivery-pipeline, choose <strong>Edit</strong>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture49.png" /></p> 
<p>Edit Source stage by choosing the edit icon on the right.</p> 
<p><strong>Amazon S3 location</strong>:</p> 
<p>Linux: s3://demo-app-<span style="color: #ff0000"><em>account-number</em></span><em>/PhpArtifact.zip</em></p> 
<p>Windows: s3://demo-app-<span style="color: #ff0000"><em>account-number</em></span><em>/WindowsArtifact.zip</em></p> 
<p>Under <strong>Output artifacts</strong>, make sure <strong>My App </strong>is displayed for<strong> Output artifact #1</strong>. This will be the input artifact for the Build stage.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture50.png" /></p> 
<p>The output artifact of the Build stage should be the input artifact of the Beta deployment stage (in this case, <strong>MyAppBuild</strong>).<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture51.png" /></p> 
<p>Choose <strong>Update</strong>, and then choose <strong>Save pipeline changes</strong>. On the next page, choose <strong>Save and continue</strong>.</p> 
<h3>Step 5: Publish the build artifact for deployment<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture52.png" /></h3> 
<h4>Step (a)</h4> 
<p>In TeamCity, on the <strong>Build Steps</strong> page, for <strong>Runner type</strong>, choose <strong>Command Line</strong>, and then add the following custom script to copy the source artifact to the TeamCity <a href="https://confluence.jetbrains.com/display/TCD9/Build+Checkout+Directory">build checkout directory</a>.</p> 
<p><strong>Note:</strong> This step is required <em>only </em>if your AWS CodePipeline source provider is either AWS CodeCommit or Amazon S3. If your source provider is GitHub, this step is redundant, because the artifact is copied over automatically by the TeamCity AWS CodePipeline plugin.</p> 
<p>In <strong>Step name</strong>, enter a name for the Command Line runner to easily distinguish the context of the step.</p> 
<p>Syntax:</p> 
<code class="lang-bash">$ cp -R %codepipeline.artifact.input.folder%/&lt;CodePipeline-Name&gt;/&lt;build-input-artifact-name&gt;/* % teamcity.build.checkoutDir%
$ unzip *.zip -d %teamcity.build.checkoutDir%
$ rm –rf %teamcity.build.checkoutDir%/*.zip</code> 
<p>For <strong>Custom script</strong>, use the following commands:</p> 
<code class="lang-bash">cp -R %codepipeline.artifact.input.folder%/delivery-pipeline/MyApp/* %teamcity.build.checkoutDir%
unzip *.zip -d %teamcity.build.checkoutDir%
rm –rf %teamcity.build.checkoutDir%/*.zip</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture53.png" /></p> 
<h4>Step (b):</h4> 
<p>For <strong>Runner type</strong>, choose <strong>Command Line</strong> runner type, and then add the following custom script to copy the build artifact to the output folder.</p> 
<p>For <strong>Step name</strong>, enter a name for the Command Line runner.</p> 
<p>Syntax:</p> 
<code class="lang-bash">$ mkdir -p %codepipeline.artifact.output.folder%/&lt;CodePipeline-Name&gt;/&lt;build-output-artifact-name&gt;/
$ cp -R %codepipeline.artifact.input.folder%/&lt;CodePipeline-Name&gt;/&lt;build-input-artifact-name&gt;/* %codepipeline.artifact.output.folder%/&lt;CodePipeline-Name/&lt;build-output-artifact-name&gt;/
</code> 
<p>For <strong>Custom script</strong>, use the following commands:</p> 
<code class="lang-bash">$ mkdir -p %codepipeline.artifact.output.folder%/delivery-pipeline/MyAppBuild/
$ cp -R %codepipeline.artifact.input.folder%/delivery-pipeline/MyApp/* %codepipeline.artifact.output.folder%/delivery-pipeline/MyAppBuild/
</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture54.png" /></p> 
<p>Drag and drop <strong>Copy Source Artifact To Build Checkout Directory</strong> to make it the first build step, and then choose <strong>Apply</strong>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture55.png" /></p> 
<p>Navigate to the AWS CodePipeline console. Choose the delivery pipeline, and then choose <strong>Release change</strong>. When prompted, choose <strong>Release</strong>.</p> 
<p>Choose<strong> Release </strong>on the next prompt.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture56.png" /></p> 
<p>The most recent change will run through the pipeline again. It might take a few moments before the status of the run is displayed in the pipeline view.</p> 
<p>Here is what you’d see after AWS CodePipeline runs through all of the stages in the pipeline:<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture57.png" /></p> 
<p>Let’s access one of the instances to see the new application deployment on the <a href="https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Instances:search=CodeDeployDemo">EC2 Instance page</a>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture58.png" /></p> 
<p>If your base operating system is Windows, accessing the public DNS address of one of the AWS CodeDeploy instances will result in the following page.</p> 
<blockquote> 
Windows: <em>http://public-dns/</em><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture59.png" /> 
</blockquote> 
<p>If your base operating system is Linux, when we access the public DNS address of one of the AWS CodeDeploy instances, we will see the following test page, which is the sample application.</p> 
<blockquote> 
Linux: <em>http://public-dns/www/index.php</em><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/25/Picture60.png" /> 
</blockquote> 
<p>Congratulations! You’ve created an end-to-end deployment and delivery pipeline ─ from source code, to build, to deployment ─ in a fully automated way.</p> 
<b>Summary:</b> 
<p>In this post, you learned how to build an end-to-end delivery and deployment pipeline on AWS. Specifically, you learned how to build an end-to-end, fully automated, continuous integration, continuous deployment, and delivery pipeline for your application, at scale, using AWS deployment and management services. You also learned how AWS CodePipeline can be easily extended through the use of custom triggers to integrate other services like TeamCity.</p> 
<p>If you have questions or suggestions, please leave a comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-206');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
