<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/devopsblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS DevOps Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li class="active"><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="devopsblogs1.html">Page 1</a>|<a href="devopsblogs2.html">Page 2</a>|<a href="devopsblogs3.html">Page 3</a>|<a href="devopsblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Extending AWS CodeBuild with Custom Build Environments</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">John Pignata</span></span> | on 
<time property="datePublished" datetime="2017-02-17T13:40:26+00:00">17 FEB 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/extending-aws-codebuild-with-custom-build-environments/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-591" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=591&amp;disqus_title=Extending+AWS+CodeBuild+with+Custom+Build+Environments&amp;disqus_url=https://aws.amazon.com/blogs/devops/extending-aws-codebuild-with-custom-build-environments/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-591');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/codebuild/" target="_blank">AWS CodeBuild</a> is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodeBuild provides curated build environments for programming languages and runtimes such as Java, Ruby, Python, Go, Node.js, Android, and Docker. It can be extended through the use of custom build environments to support many more.</p> 
<p>Build environments are Docker images that include a complete file system with everything required to build and test your project. To use a custom build environment in a CodeBuild project, you build a container image for your platform that contains your build tools, push it to a Docker container registry such as <a href="https://aws.amazon.com/ecr/" target="_blank">Amazon EC2 Container Registry (ECR)</a>, and reference it in the project configuration. When building your application, CodeBuild will retrieve the Docker image from the container registry specified in the project configuration and use the environment to compile your source code, run your tests, and package your application.</p> 
<p>In this post, we’ll create a build environment for <a href="http://www.php.net/" target="_blank">PHP</a> applications and walk through the steps to configure CodeBuild to use this environment.</p> 
<p><span id="more-591"></span></p> 
<h3>Requirements</h3> 
<p>In order to follow this tutorial and build the Docker container image, you need to have the <a href="https://www.docker.com/products/overview#/install_the_platform" target="_blank">Docker platform</a>, the <a href="http://aws.amazon.com/cli" target="_blank">AWS Command Line Interface</a>, and <a href="https://git-scm.com/downloads" target="_blank">Git</a> installed.</p> 
<h3>Create the demo resources</h3> 
<p>To begin, we’ll clone <a href="https://github.com/awslabs/codebuild-images" target="_blank">codebuild-images</a> from GitHub. It contains an <a href="https://aws.amazon.com/cloudformation/" target="_blank">AWS CloudFormation</a> template that we’ll use to create resources for our demo: a source code repository in <a href="https://aws.amazon.com/codecommit/" target="_blank">AWS CodeCommit</a> and a Docker image repository in Amazon ECR. The repository also includes PHP sample code and tests that we’ll use to demonstrate our custom build environment.</p> 
<ol> 
<li>Clone the Git repository: 
<code class="lang-bash">git clone https://github.com/awslabs/codebuild-images.git
cd codebuild-images</code> 
<li>Create the CloudFormation stack using the template.yml file. You can use<a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stack.html" target="_blank"> the CloudFormation console to create the stack</a>&nbsp;or you can use the <a href="https://aws.amazon.com/cli/" target="_blank">AWS Command Line Interface</a>: 
<code class="lang-bash">aws cloudformation create-stack \
--stack-name codebuild-php \
--parameters ParameterKey=EnvName,ParameterValue=php \
--template-body file://template.yml &gt; /dev/null &amp;&amp; \
aws cloudformation wait stack-create-complete \
--stack-name codebuild-php &amp;&amp; \
aws cloudformation describe-stacks \
--stack-name codebuild-php \
--output table \
--query Stacks[0].Outputs</code> 
</ol> 
<p>After the stack has been created, CloudFormation will return two outputs:</p> 
<li><strong>BuildImageRepositoryUri</strong>: the URI of the Docker repository that will host our build environment image.</li> 
<li><strong>SourceCodeRepositoryCloneUrl</strong>: the clone URL of the Git repository that will host our sample PHP code.</li> 
<h3>Build and push the Docker image</h3> 
<p>Docker images are specified using a <a href="https://docs.docker.com/engine/reference/builder/" target="_blank">Dockerfile</a>, which contains the instructions for assembling the image. The Dockerfile included in the PHP build environment contains these instructions:</p> 
<code class="lang-docker">FROM php:7
ARG composer_checksum=55d6ead61b29c7bdee5cccfb50076874187bd9f21f65d8991d46ec5cc90518f447387fb9f76ebae1fbbacf329e583e30
ARG composer_url=https://raw.githubusercontent.com/composer/getcomposer.org/ba0141a67b9bd1733409b71c28973f7901db201d/web/installer
ENV COMPOSER_ALLOW_SUPERUSER=1
ENV PATH=$PATH:vendor/bin
RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
curl \
git \
python-dev \
python-pip \
zlib1g-dev \
&amp;&amp; pip install awscli \
&amp;&amp; docker-php-ext-install zip \
&amp;&amp; curl -o installer &quot;$composer_url&quot; \
&amp;&amp; echo &quot;$composer_checksum *installer&quot; | shasum –c –a 384 \
&amp;&amp; php installer --install-dir=/usr/local/bin --filename=composer \
&amp;&amp; rm -rf /var/lib/apt/lists/*
</code> 
<p>This Dockerfile inherits all of the instructions from the <a href="https://hub.docker.com/_/php/" target="_blank">official PHP Docker image</a>, which installs the PHP runtime. On top of that base image, the build process will install Python, Git, the AWS CLI, and <a href="https://getcomposer.org/" target="_blank">Composer</a>, a dependency management tool for PHP. We’ve installed the AWS CLI and Git as tools we can use during builds. For example, using the AWS CLI, we could trigger a notification from <a href="https://aws.amazon.com/sns/" target="_blank">Amazon Simple Notification Service (SNS)</a> when a build is complete or we could use Git to create a new tag to mark a successful build. Finally, the build process cleans up files created by the packaging tools, as recommended in <a href="https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/" target="_blank">Best practices for writing Dockerfiles</a>.</p> 
<p>Next, we’ll build and push the custom build environment.</p> 
<ol> 
<li>Provide&nbsp;authentication details for our registry to the local Docker engine by executing the output of the <a href="http://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_AWSCLI.html#AWSCLI_get-login" target="_blank">login helper</a> provided by the AWS CLI: 
<code class="lang-bash">aws ecr get-login
</code> 
<li>Build and push the Docker image. We’ll use the repository URI returned in the CloudFormation stack output (<em>BuildImageRepositoryUri</em>) as the image tag: 
<code class="lang-bash">cd php
docker build -t [BuildImageRepositoryUri] .
docker push [BuildImageRepositoryUri]</code> 
</ol> 
<p>After running these commands, your Docker image is pushed into Amazon ECR and ready to build your project.</p> 
<h3>Configure the Git repository</h3> 
<p>The repository we cloned includes a small PHP sample that we can use to test our PHP build environment. The <a href="https://github.com/awslabs/codebuild-images/blob/master/php/sample/RomanNumerals.php" target="_blank">sample function</a> converts Roman numerals to Arabic numerals. The repository also includes a <a href="https://github.com/awslabs/codebuild-images/blob/master/php/sample/tests/RomanNumeralsTest.php" target="_blank">sample test</a> to exercise this function. The sample also includes a YAML file called a <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html" target="_blank">build spec</a> that contains commands and related settings that CodeBuild uses to run a build:</p> 
<code class="lang-yaml">version: 0.1
phases:
&nbsp; pre_build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - composer install
&nbsp; build:
&nbsp; &nbsp; commands:
&nbsp; &nbsp; &nbsp; - phpunit tests</code> 
<p>This build spec configures CodeBuild to run two commands during the build:</p> 
<li><code class="lang-yaml">composer install</code> to install <a href="https://github.com/awslabs/codebuild-images/blob/master/php/sample/composer.json" target="_blank">configured dependencies</a>, such as <a href="https://phpunit.de/" target="_blank">PHPUnit</a> before the build.</li> 
<li><code class="lang-yaml">phpunit tests</code> to run the unit tests during the build.</li> 
<p>We will push the sample application to the CodeCommit repo created by the CloudFormation stack. You’ll need to grant your IAM user the required level of access to the AWS services required for CodeCommit and you’ll need to configure your Git client with the appropriate credentials. See <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html" target="_blank">Setup for HTTPS Users Using Git Credentials</a> in the CodeCommit documentation for detailed steps.</p> 
<p>We’re going to initialize a Git repository for our sample, configure our origin, and push the sample to the master branch in CodeCommit.</p> 
<ol> 
<li>Initialize a new Git repository in the sample directory: 
<code class="lang-bash">cd sample
git init</code> 
<li>Add and commit the sample files to the repository: 
<code class="lang-bash">git add .
git commit -m &quot;Initial commit&quot;</code> 
<li>Configure the git remote and push the sample to it. We’ll use the repository clone URL returned in the CloudFormation stack output (<em>SourceCodeRepositoryCloneUrl</em>) as the remote URL: 
<code class="lang-bash">git remote add origin [SourceCodeRepositoryCloneUrl]
git push origin master</code> 
</ol> 
<p>Now that our sample application has been pushed into source control and our build environment image has been pushed into our Docker registry, we’re ready to create a CodeBuild project and start our first build.</p> 
<h3>Configure the CodeBuild project</h3> 
<p>In this section, we’ll walk through the steps for configuring CodeBuild to use the custom build environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/02/16/build-configuration-1012x1024.png" /></p> 
<ol> 
<li>In the AWS Management Console, open the AWS CodeBuild console, and then choose <strong>Create project</strong>.</li> 
<li>In <strong>Project name</strong>, type <strong>php-demo</strong>.</li> 
<li>From <strong>Source provider</strong>, choose <strong>AWS CodeCommit</strong>. &nbsp;From <strong>Repository</strong>, choose <strong>codebuild-sample-php</strong>.</li> 
<li>In <strong>Environment image</strong>, select <strong>Specify a Docker image</strong>. From <strong>Custom image type</strong>, choose <strong>Amazon ECR</strong>. From <strong>Amazon ECR Repository</strong>, choose <strong>codebuild/php</strong>. &nbsp;From <strong>Amazon ECR image</strong>, choose <strong>latest</strong>.</li> 
<li>In <strong>Build specification</strong>, select <strong>Use the buildspec.yml in the source code root directory</strong>.</li> 
<li>In <strong>Artifacts type</strong>, choose <strong>No artifacts</strong>.</li> 
<li>Choose <strong>Continue</strong> and then choose <strong>Save and Build</strong>.</li> 
<li>On the next page, from <strong>Branch</strong>, choose <strong>master</strong> and then choose <strong>Start build</strong>.</li> 
</ol> 
<p>CodeBuild will pull the build environment image from Amazon ECR and use it to test our application. CodeBuild will show us the status of each build step, the last 20 lines of log messages generated by the build process, and provide a link to <a href="https://aws.amazon.com/cloudwatch/details/" target="_blank">Amazon CloudWatch Logs</a> for more debugging output.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2017/02/16/build-succeeded-1024x426.png" /></p> 
<h3>Summary</h3> 
<p>CodeBuild supports a number of platforms and languages out of the box. By using custom build environments, it can be extended to other runtimes. In this post, we built a PHP environment and demonstrated how to use it to test PHP applications.</p> 
<p>We’re excited to see how customers extend and use CodeBuild to enable continuous integration and continuous delivery for their applications. Please leave questions or suggestions in the comments or share what you’ve learned extending CodeBuild for your own projects.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-591');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Run Umbraco CMS with Flexible Load Balancing on AWS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ihab Shaaban</span></span> | on 
<time property="datePublished" datetime="2017-01-06T14:26:18+00:00">06 JAN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/run-umbraco-cms-with-flexible-load-balancing-on-aws/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-487" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=487&amp;disqus_title=Run+Umbraco+CMS+with+Flexible+Load+Balancing+on+AWS&amp;disqus_url=https://aws.amazon.com/blogs/devops/run-umbraco-cms-with-flexible-load-balancing-on-aws/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-487');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In version 7.3, Umbraco CMS the popular open source CMS introduced the flexible load balancing feature, which makes the setup of load-balanced applications a lot easier. In this blog post, we’ll follow the guidelines in the <a title="undefined" href="https://our.umbraco.org/documentation/Getting-Started/Setup/Server-Setup/load-balancing/" target="_blank">Umbraco documentation</a> to set up a load-balanced Umbraco application on AWS. We’ll let <a title="undefined" href="https://aws.amazon.com/elasticbeanstalk/" target="_blank">AWS Elastic Beanstalk</a> manage the deployments, load balancing, auto scaling, and health monitoring for us.</p> 
<h3>Application Architecture</h3> 
<p>When you use the flexible load balancing feature, any updates to Umbraco content will be stored in a queue in the master database. Each server in the load-balanced environment will automatically download, process, and cache the updates from the queue, so no matter which server is selected by the <a title="undefined" href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">Elastic Load Balancing</a> to handle the request, the user will always receive the same content. Umbraco administration doesn’t work correctly if accessed from a load-balanced server. For this reason, we’ll set up a non-balanced environment to be accessed only by the administrators and editors.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-01-Arch-med.png" /></p> 
<p><span id="more-487"></span></p> 
<h3>Create Elastic Beanstalk Environments</h3> 
<p>We’ll start by creating a single instance environment for administrators and editors. This environment will have the master database server as an RDS instance. In the Elastic Beanstalk console, choose <strong>Create New Application</strong>. Type a name for the application, and then choose <strong>Create</strong>. When you see the message “No environments currently exist for this application”, choose <strong>Create one now</strong>. Select <strong>Web server environment</strong> for the environment tier.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-02-Create-med.png" /></p> 
<p>On the <strong>Create a new environment page</strong>, choose <strong>.NET</strong> for Platform, and then choose <strong>Configure more options</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-03-Create-med.png" /></p> 
<p>On the next page, under <strong>Capacity</strong>, set the <strong>Environment type</strong> to <strong>Single instance</strong>. Under <strong>Database</strong>, for <strong>Engine</strong>, choose <strong>sqlserver</strong>. Set the <strong>Storage</strong> field to, at minimum, <strong>20 GB</strong>, review the information, and then choose <strong>Create environment</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-04-Create-med.png" /></p> 
<p>Next, we’ll create a load-balanced environment for the front-end users. Follow the steps you used to create the first Elastic Beanstalk environment. When you reach the configuration page, select <strong>High availability</strong>, and then choose <strong>Create environment</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-05-Create-med.png" /></p> 
<h3>Configure RDS Database Server Security Groups</h3> 
<p>When we created the RDS DB instance through the first environment, Elastic Beanstalk automatically configured the security to allow the servers in this environment to access the database. We’ll configure the security for the second environment manually. By following these steps, the front-end servers will be able to connect to the database:</p> 
<ol> 
<li>In the Elastic Beanstalk console, copy the name of the security group for the front-end environment. You will need this for step 7 of this procedure.</li> 
<li>In the admin environment, choose <strong>Configuration</strong>, and then choose <strong>RDS</strong>.</li> 
<li>Choose the<strong> View in RDS Console</strong> link.</li> 
<li>On the <strong>Details</strong> tab in the RDS console, in the <strong>Security and Network</strong> section, choose <strong>Security Groups</strong>.</li> 
<li>Choose the name of the active security group to open it in the EC2 console.</li> 
<li>In the EC2 console, you should see that Elastic Beanstalk has already added a rule for the admin environment.</li> 
<li>Choose <strong>Edit</strong>, and then choose <strong>Add Rule</strong>. For <strong>Type</strong>, choose <strong>MS SQL</strong>. For <strong>Source</strong>, paste the name of the security group for the front-end environment.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-06-Configure-med.png" /></p> 
<h3>Handle Session State in a Load Balanced Environment</h3> 
<p>By default, ASP.NET stores the user’s session in memory. This means the user will lose session information if the next request goes to a different server. To prevent this from happening while keeping the default session provider, <a title="undefined" href="http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-sticky-sessions.html" target="_blank">configure sticky sessions for your load balancer</a>.</p> 
<p>In the Elastic Beanstalk console, navigate to the front-end environment configuration. You can use the <strong>Sessions</strong> section under the <strong>Load Balancing</strong> settings to specify whether the load balancer for the application will allow session stickiness. Select the <strong>Enable Session Stickiness</strong> check box.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-07-Configure-med.png" /></p> 
<p>When you enable session stickiness, <a title="undefined" href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">ELB</a> will send all requests for a specific user to the same back-end web server. This can result in an imbalance of requests across all back ends, with some servers having more load than others. When you scale down, the termination of an instance can result in a loss of sessions assigned to it. For this reason, consider storing the sessions in a central location like a SQL Server database and use the <a title="undefined" href="https://msdn.microsoft.com/en-us/library/aa478952.aspx" target="_blank">SqlSessionStateStore</a> provider.</p> 
<p>AWS offers other options, for example, storing the sessions in a NoSQL database using the <a title="undefined" href="http://docs.aws.amazon.com/AWSSdkDocsNET/V3/DeveloperGuide/net-dg-dynamodb-session.html" target="_blank">DynamoDBSessionStore</a> provider or using the <a title="undefined" href="http://aws.amazon.com/elasticache/" target="_blank">Amazon ElastiCache</a> to store the sessions in the cloud as in-memory cache. For information, see the <a title="undefined" href="https://blogs.aws.amazon.com/net/post/TxMREMF0459SXT/ElastiCache-as-an-ASP-NET-Session-Store" target="_blank">ElastiCache as an ASP.NET Session Store</a> blog post.</p> 
<h3>A One-Step Alternative: CloudFormation Template</h3> 
<p>You can download and use this <a title="undefined" href="https://github.com/awslabs/aws-cfn-umbraco-load-balancing-template" target="_blank">CloudFormation template</a> to create resources similar to those created in the preceding steps. If you use the template, you will still have to prepare and publish your own version of Umbraco from a local machine. We’ll do that next.</p> 
<p>The template is written to create the RDS database as a separate resource from the environments. You’ll see in the previous steps the DB is tied to the admin environment (for example, if you delete the environment, the DB will be deleted, too). This works well during development or testing, but is not ideal for production. If you should accidentally delete your database, by default, Elastic Beanstalk creates a snapshot on environment termination so it can be recovered. For information about how to decouple the DB from your environment, see <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html" target="_blank">Using Elastic Beanstalk with Amazon RDS</a> in the Elastic Beanstalk Developer Guide.</p> 
<h3>Prepare the Umbraco CMS Locally</h3> 
<p>To keep things simple, we’ll download Umbraco from <a title="undefined" href="https://our.umbraco.org/download" target="_blank">our.umbraco.org/download</a>. As of this writing, the current version is v7.5.</p> 
<p>From IIS on the local machine, add a new website with the following settings:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/30/Umbraco-08-Prepare-med.png" /></p> 
<h3>File and Folder Permissions</h3> 
<p>To ensure the Umbraco installation will run smoothly, grant the read and write permissions to the application pool user for Umbraco’s files and folders. In IIS, right-click the UmbracoApp website, and then click <strong>Edit Permissions</strong>. Grant modify or full-control permissions to IIS_IUSRS. For information, see <a title="undefined" href="https://our.umbraco.org/documentation/Getting-Started/Setup/Server-Setup/permissions" target="_blank">File and folder permissions</a> in the Umbraco documentation.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-09-Prepare-med.png" /></p> 
<p>When we deploy the application to AWS, we need to update the permissions for IIS_IUSRS, too. It isn’t easy to access and grant permissions to each Windows server in the environment. If we create an .ebextension, Elastic Beanstalk can automate the permissions process. It can also allow us to run extra commands during the application deployment:</p> 
<code class="lang-yaml">commands:
create_default_website_folder:
command: if not exist &quot;C:\inetpub\wwwroot&quot; mkdir &quot;C:\inetpub\wwwroot&quot;
update_iis_user_permissions:
command: Icacls.exe &quot;C:\inetpub\wwwroot&quot; /grant IIS_IUSRS:(OI)(CI)F
</code> 
<p>Here is the .ebextension saved as a .config file:</p> 
C:\inetpub\wwwroot\UmbracoCms\App_Data\.ebextensions\update_iis_permissions.config 
<p>Although this one is written in YAML, you can write .ebextensions in JSON, too. You can extend the .ebextension by writing a test to ensure the commands run on the first deployment only and not on redeployments. For more information, see the <a title="undefined" href="https://blogs.aws.amazon.com/net/post/Tx3N7JHP24J1EJ5/Configuring-Advanced-Logging-on-AWS-Elastic-Beanstalk" target="_blank">Configuring Advanced Logging on AWS Elastic Beanstalk</a> blog post or the <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html" target="_blank">.ebextensions documentation</a>.</p> 
<h3>Custom Machine Key</h3> 
<p>By default, ASP.NET generates a unique machine key for each server. In a load-balanced environment, this will cause validation errors and invalid view state. To fix this issue, make sure the machine key is the same on all servers. One of the simplest ways to generate a custom key is from IIS.</p> 
<p>In IIS, on the <strong>Machine Key</strong> page, click <strong>Generate Keys</strong>. Change the settings as follows, and click <strong>Apply</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-10-Prepare-med.png" /></p> 
<h3>Create an Empty Database</h3> 
<p>Before running the CMS installation, we’ll create an empty database in our RDS DB server. Open AWS Explorer in Visual Studio, right-click the DB instance we created in the first step, and then select <strong>Create SQL Server Database</strong>.</p> 
<p>If you encounter any problems connecting to the RDS server, add the IP address of the development machine to the RDS DB security group. Make sure your Windows Firewall allows outbound access for 1433 port.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-11-Prepare-med.png" /></p> 
<h3>Run the Umbraco CMS Installation</h3> 
<p>In IIS, click <strong>Browse</strong>. The Umbraco installation should start. Enter the required information and use the RDS DB information to complete the fields for the database configuration step.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-12-Prepare-med.png" /></p> 
<h3>Publish Umbraco CMS to AWS</h3> 
<p>Finally, open the UmbracoApp folder in Visual Studio. From the <strong>File</strong> menu, click <strong>Open</strong>. Click <strong>Web site</strong>, click <strong>File system</strong>, and then navigate to C:\inetpub\wwwroot\UmbracoCms. Right-click the project, and then select <strong>Publish to AWS</strong> to deploy to the admin environment and then deploy again to the front-end environment.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/17/Umbraco-13-Deploy-med.png" /></p> 
<p>The <a title="undefined" href="https://aws.amazon.com/ebs/" target="_blank">EBS</a> volumes attached to the instances in the environments are isolated and not shared. When compared to other load-balanced solutions that use shared storage, there is no extra work required to separate Umbraco logging file paths, change XML cache content settings, or update the configuration for Lucene/Examine indexes.</p> 
<h3>Conclusion</h3> 
<p>You now have an Umbraco application that is ready to scale up or down on AWS, and you can take this further using Elastic Beanstalk, there are many options to customize your environments, for example, associating a <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customdomains.html" target="_blank">custom domain</a> name or enabling <a title="undefined" href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html" target="_blank">HTTPS</a>.</p> 
<p>We hope you found the information in this post helpful. If you have questions or other feedback, please leave it in the comments below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-487');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Registering Spot Instances with AWS OpsWorks Stacks</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Amir Golan</span></span> | on 
<time property="datePublished" datetime="2016-12-23T03:48:21+00:00">23 DEC 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/registering-spot-instances-with-aws-opsworks-stacks/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/opsworks/stacks/">AWS OpsWorks Stacks</a> is a configuration management service that helps you configure and operate applications of all shapes and sizes using Chef. You can define the application’s architecture and the specification of each component, including package installation, software configuration, and more.</p> 
<p><a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot instances</a> allow you to bid on spare <a href="https://aws.amazon.com/ec2/">Amazon EC2</a> computing capacity. Because Spot instances are often available at a discount compared to On-Demand instances, you can significantly reduce the cost of running your applications, grow your applications’ compute capacity and throughput for the same budget, and enable new types of cloud computing applications.</p> 
<p>You can use Spot instances with AWS OpsWorks Stacks in the following ways:</p> 
<li>As a part of an Auto Scaling group, as described in this blog post. You can follow the steps in the blog post and in the launch configuration described in step 5, choose the Spot instance option.</li> 
<li>To provision a Spot instance in the EC2 console and have it automatically register with an OpsWorks stack, as described here.</li> 
<p>The walkthrough assumes that your stack and the following resources you create are located in N. Virginia (us-east-1). If you want to use another region, be sure to set the region parameter accordingly.</p> 
<p><strong>IAM instance profile</strong>: an IAM profile that grants your instances permission to register themselves with OpsWorks.</p> 
<p><strong>Lambda function</strong>: a function that deregisters your instances from an OpsWorks stack.</p> 
<p><strong>Spot instance</strong>: the Spot instance that will run your application.</p> 
<p><strong>CloudWatch Event role</strong>: an event that will trigger the Lambda function whenever your Spot instance is terminated.</p> 
<p><strong>Step 1: Create an IAM instance profile</strong></p> 
<p>When a Spot instance starts, it must be able to make an API call to register itself with an OpsWorks stack. By assigning an instance with an IAM instance profile, the instance will be able to make calls to OpsWorks.</p> 
<p>Open the IAM console at <a href="https://console.aws.amazon.com/iam/">https://console.aws.amazon.com/iam/</a>, choose <strong>Roles,</strong> and then choose <strong>Create New</strong> Role. Type a name for the role, and then choose <strong>Next Step</strong>. Choose the Amazon EC2 role, and then select the check box next to the AWSOpsWorksInstanceRegistration policy. Finally, select <strong>Next Step</strong>, and then choose <strong>Create Role</strong>. As its name suggests, the AWSOpsWorksInstanceRegistration policy will allow the instance to make API calls only to register an instance. Copy the following policy to the new role you’ve just created.</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;opsworks:AssignInstance&quot;,
&quot;opsworks:DescribeInstances&quot;,
&quot;ec2:CreateTags&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
]
}
]
}
</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/23/1b.png" /></p> 
<p><strong>Step 2: Create a Lambda function</strong></p> 
<p>This Lambda function deregisters an instance from your OpsWorks stack. It will be invoked whenever the Spot instance is terminated.</p> 
<p>Open the AWS Lambda console at <a href="https://us-west-2.console.aws.amazon.com/lambda/home">https://us-west-2.console.aws.amazon.com/lambda/home</a>, and choose the option to create a Lambda function. If you are prompted to choose a blueprint, choose <strong>Skip</strong>. Type a name for the Lambda function, and from the <strong>Runtime</strong> drop-down list, select <strong>Python 2.7</strong>.</p> 
<p>Next, paste the following code into the <strong>Lambda Function</strong> Code text box:</p> 
<code class="lang-python">import boto3
def lambda_handler(event, context):
ec2_instance_id = event['detail']['instance-id']
ec2 = boto3.client('ec2')
for tag in ec2.describe_instances(InstanceIds=[ec2_instance_id])['Reservations'][0]['Instances'][0]['Tags']:
if (tag['Key'] == 'opsworks_stack_id'):
opsworks_stack_id = tag['Value']
opsworks = boto3.client('opsworks', 'us-east-1')
for instance in opsworks.describe_instances(StackId=opsworks_stack_id)['Instances']:
if ('Ec2InstanceId' in instance):
if (instance['Ec2InstanceId'] == ec2_instance_id):
print(&quot;Deregistering OpsWorks instance &quot; + instance['InstanceId'])
opsworks.deregister_instance(InstanceId=instance['InstanceId'])
return ec2_instance_id
</code> 
<p>The result should look like this:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/23/2-1.png" /></p> 
<p>In the <strong>Lambda function handler and role </strong>section, create a custom role. Edit the policy document to allow the Lambda function to access the required resources:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;,
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: &quot;arn:aws:logs:*:*:*&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ec2:DescribeInstances&quot;,
&quot;opsworks:DescribeInstances&quot;,
&quot;opsworks:DeregisterInstance&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
]
}
]
}
</code> 
<p>&nbsp;</p> 
<p><strong>Step 3: Create a CloudWatch event</strong></p> 
<p>Whenever the Spot instance is terminated, the Lambda function from step 2 must be triggered to deregister the instance from its associated stack.</p> 
<p>Open the AWS CloudWatch console at <a href="https://console.aws.amazon.com/cloudwatch/home">https://console.aws.amazon.com/cloudwatch/home</a>, choose <strong>Events</strong>, and then choose the<strong> Create rule</strong> button. From <strong>Event selector</strong>, choose <strong>Amazon EC2</strong>. Select <strong>Specific state(s)</strong>, and then choose <strong>Terminated</strong>. Under <strong>Targets</strong>, for <strong>Function</strong>, select the Lambda function you created earlier. Finally, choose the <strong>Configure details</strong> button.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/23/3b.png" /></p> 
<p><strong>Step 4: Create a Spot instance</strong></p> 
<p>Open the EC2 console at <a href="https://console.aws.amazon.com/ec2sp/v1/spot/home">https://console.aws.amazon.com/ec2sp/v1/spot/home</a>, and choose the <strong>Request Spot Instances</strong> button. Use the latest release of Amazon Linux. On the details page, under <strong>IAM instance profile</strong>, choose the instance profile you created in step 1. Paste the following script into the <strong>User data</strong> field:</p> 
<code class="lang-bash">#!/bin/bash
sed -i'' -e 's/.*requiretty.*//' /etc/sudoers
pip install --upgrade awscli
STACK_ID=3464f35f-16b4-44dc-8073-a9cd19533ad5
LAYER_ID=ba04682c-6e32-481d-9d0e-e2fa72b55314
INSTANCE_ID=$(/usr/bin/aws opsworks register --use-instance-profile --infrastructure-class ec2 --region us-east-1 --stack-id $STACK_ID --override-hostname $(tr -cd 'a-z' &lt; /dev/urandom |head -c8) --local 2&gt;&amp;1 |grep -o 'Instance ID: .*' |cut -d' ' -f3)
EC2_INSTANCE_ID=$(/usr/bin/aws opsworks describe-instances --region us-east-1 --instance-ids $INSTANCE_ID | grep -o '&quot;Ec2InstanceId&quot;: &quot;i-.*'| grep -o 'i-[a-z0-9]*')
/usr/bin/aws ec2 create-tags --region us-east-1 --resources $EC2_INSTANCE_ID --tags Key=opsworks_stack_id,Value=$STACK_ID
/usr/bin/aws opsworks wait instance-registered --region us-east-1 --instance-id $INSTANCE_ID
/usr/bin/aws opsworks assign-instance --region us-east-1 --instance-id $INSTANCE_ID --layer-ids $LAYER_ID
</code> 
<p>On boot, this script will register your Spot instance with an OpsWorks stack and layer. Be sure to fill in the following fields:</p> 
<code class="lang-bash">STACK_ID=YOUR_STACK_ID
LAYER_ID=YOUR_LAYER_ID
</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/23/4sd.png" /></p> 
<p><strong>Important:</strong> Be sure to turn off auto healing for all of the layers in your stack to which you assign Spot instances. Otherwise, auto healing will attempt to revive your instances upon termination.</p> 
<p>When the instance has been provisioned and come online, you’ll see <strong>fulfilled</strong> displayed in the <strong>Status</strong> column and <strong>active</strong> displayed in the <strong>State</strong> column. This process will take a few minutes. After the instance and request are both in an active state, the instance should be fully booted and registered with your OpsWorks stack/layer.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/23/5.png" /></p> 
<p>You can also view the instance and its online state in the OpsWorks console under <strong>Spot Instance</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/23/6.png" /></p> 
<p>You can manually terminate a Spot instance from the OpsWorks service console. Simply choose the <strong>stop</strong> button and the Spot instance will be terminated and removed from your stack. Unlike an On-Demand instance, when a Spot instance in OpsWorks is stopped, it cannot be restarted.</p> 
<p>If your Spot instance is terminated through other means (for example, in the EC2 console), a CloudWatch event will trigger the Lambda function, which will deregister the instance from your OpsWorks stack.</p> 
<p><strong>Conclusion</strong></p> 
<p>You can now use OpsWorks Stacks to define your application’s architecture and software configuration while leveraging the attractive pricing of Spot instances. If you have questions or other feedback, please leave it in the comments.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Introducing Git Credentials: A Simple Way to Connect to AWS CodeCommit Repositories Using a Static User Name and Password</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ankur Agarwal</span></span> | on 
<time property="datePublished" datetime="2016-12-22T16:20:12+00:00">22 DEC 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a>, <a href="https://aws.amazon.com/blogs/devops/category/new-stuff/" title="View all posts in New stuff"><span property="articleSection">New stuff</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/introducing-git-credentials-a-simple-way-to-connect-to-aws-codecommit-repositories-using-a-static-user-name-and-password/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Today, AWS is introducing a simplified way to authenticate to your AWS CodeCommit repositories over HTTPS.</p> 
<p>With Git credentials, you can generate a static user name and password in the Identity and Access Management (IAM) console that you can use to access AWS CodeCommit repositories from the command line, Git CLI, or any Git tool that supports HTTPS authentication.</p> 
<p>Because these are static credentials, they can be cached using the password management tools included in your local operating system or stored in a credential management utility. This allows you to get started with AWS CodeCommit within minutes. You don’t need to download the AWS CLI or configure your Git client to connect to your AWS CodeCommit repository on HTTPS. You can also use the user name and password to connect to the AWS CodeCommit repository from third-party tools that support user name and password authentication, including popular Git GUI clients (such as TowerUI) and IDEs (such as Eclipse, IntelliJ, and Visual Studio).</p> 
<p>So, why did we add this feature? Until today, users who wanted to use HTTPS connections were required to configure the AWS credential helper to authenticate their AWS CodeCommit operations. Customers told us our credential helper sometimes interfered with password management tools such as Keychain Access and Windows Vault, which caused authentication failures. Also, many Git GUI tools and IDEs require a static user name and password to connect with remote Git repositories and do not support the credential helper.</p> 
<p>In this blog post, I’ll walk you through the steps for creating an AWS CodeCommit repository, generating Git credentials, and setting up CLI access to AWS CodeCommit repositories.</p> 
<p><strong><br /> Git Credentials Walkthrough<br /> </strong>Let’s say Dave wants to create a repository on AWS CodeCommit and set up local access from his computer.</p> 
<p><span style="text-decoration: underline"><strong>Prerequisite:</strong> </span>If Dave had previously configured his local computer to use the credential helper for AWS CodeCommit, he must edit his .gitconfig file to remove the credential helper information from the file. Additionally, if his local computer is running macOS, he might need to clear any cached credentials from Keychain Access.</p> 
<p>With Git credentials, Dave can now create a repository and start using AWS CodeCommit in four simple steps.</p> 
<p><strong>Step 1: Make sure the IAM user has the required permissions<br /> </strong>Dave must have the following managed policies attached to his IAM user (or their equivalent permissions) before he can set up access to AWS CodeCommit using Git credentials.</p> 
<li>AWSCodeCommitPowerUser (or an appropriate CodeCommit managed policy)</li> 
<li>IAMSelfManageServiceSpecificCredentials</li> 
<li>IAMReadOnlyAccess</li> 
<p><strong>Step 2: Create an AWS CodeCommit repository<br /> </strong>Next, Dave signs in to the AWS CodeCommit console and create a repository, if he doesn’t have one already. He can choose any repository in his AWS account to which he has access. The instructions to create Git credentials are shown in the help panel. (Choose the Connect button if the instructions are not displayed.) When Dave clicks the IAM user link, the IAM console will open and he can generate the credentials.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/21/GitCred_Blog1.jpg" /></p> 
<p>&nbsp;</p> 
<p><strong>Step 3: Create HTTPS Git credentials in the IAM console<br /> </strong>On the IAM user page, Dave selects the <span style="text-decoration: underline">Security Credentials</span> tab and clicks Generate under <span style="text-decoration: underline">HTTPS Git credentials for AWS CodeCommit</span> section. This creates and displays the user name and password. Dave can then download the credentials.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/21/GitCred_Blog2.jpg" /></p> 
<p><strong>Note:</strong> This is the only time the password is available to view or download.</p> 
<p>&nbsp;</p> 
<p><strong>Step 4: Clone the repository on the local machine<br /> </strong>On the AWS CodeCommit console page for the repository, Dave chooses Clone URL, and then copy the HTTPS link for cloning the repository. At the command line or terminal, Dave will use the link he just copied to clone the repository. For example, Dave copies:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/21/GitCred_Blog3.jpg" /></p> 
<p>&nbsp;</p> 
<p>And then at the command line or terminal, Dave types:</p> 
$ git clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/TestRepo_Dave 
<p>When prompted for user name and password, Dave provides the Git credentials (user name and password) he generated in step 3.</p> 
<p>Dave is now ready to start pushing his code to the new repository.</p> 
<p>Git credentials can be made active or inactive based on your requirements. You can also reset the password if you would like to use the existing username with a new password.</p> 
<p><strong>Next Steps</strong></p> 
<ol> 
<li>You can optionally cache your credentials using the Git credentials caching command <a href="https://git-scm.com/docs/git-credential-cache" target="_blank">here</a>.</li> 
<li>Want to invite a collaborator to work on your AWS CodeCommit repository? Simply create a new IAM user in your AWS account, create Git credentials for that user, and securely share the repository URL and Git credentials with the person you want to collaborate on the repositories.</li> 
<li>Connect to any third-party client that supports connecting to remote Git repositories using Git credentials (a stored user name and password). Virtually all tools and IDEs allow you to connect with static credentials. We’ve tested these: 
<li>Visual Studio (using the default Git plugin)</li> 
<li>Eclipse IDE (using the default Git plugin)</li> 
<li>Git Tower UI</li> 
</ul> </li> 
</ol> 
<p>For more information, see the <a href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html" target="_blank">AWS CodeCommit documentation</a>.</p> 
<p>We are excited to provide this new way of connecting to AWS CodeCommit. We look forward to hearing from you about the many different tools and IDEs you will be able to use with your AWS CodeCommit repositories.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">DevOps and Continuous Delivery at re:Invent 2016 – Wrap-up</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Frank Li</span></span> | on 
<time property="datePublished" datetime="2016-12-15T15:38:56+00:00">15 DEC 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/new-stuff/" title="View all posts in New stuff"><span property="articleSection">New stuff</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/devops-and-continuous-delivery-at-reinvent-2016-wrap-up/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>The AWS re:Invent 2016 conference was packed with some exciting announcements and sessions around DevOps and Continuous Delivery. We launched <a title="undefined" href="https://aws.amazon.com/codebuild/" target="_blank">AWS CodeBuild</a>, a fully managed build service that eliminates the need to provision, manage, and scale your own build servers. You now have the ability to run your <a title="undefined" href="https://aws.amazon.com/devops/continuous-integration/" target="_blank">continuous integration</a> and <a title="undefined" href="https://aws.amazon.com/devops/continuous-delivery/" target="_blank">continuous delivery</a> process entirely on AWS by plugging AWS CodeBuild into <a title="undefined" href="https://aws.amazon.com/codepipeline/" target="_blank">AWS CodePipeline</a>, which automates building, testing, and deploying code each time you push a change to your source repository. If you are interested in learning more about AWS CodeBuild, you can sign up for the webinar on January 20th <a title="undefined" href="https://publish.awswebcasts.com/content/connect/c1/7/en/events/event/private/23850344/41359021/event_registration.html?connect-session=graysonbreezmc6gg36rqdt6ehbb&amp;sco-id=54633193&amp;" target="_blank">here</a>.</p> 
<p>The DevOps track had over 30 different breakout sessions ranging from customer stories to deep dive talks to best practices. If you weren’t able to attend the conference or missed a specific session, here is a link to the entire <a title="undefined" href="https://www.youtube.com/playlist?list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">playlist</a>.</p> 
<p>&nbsp;</p> 
<p>There were a number of talks that can help you get started with your own DevOps practices for rapid software delivery.&nbsp;Here are some introductory sessions to give you the proper background:<br /> DEV201: <a title="undefined" href="https://www.youtube.com/watch?v=-ddpq2VQNxo&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Accelerating Software Delivery with AWS Developer Tools</a><br /> DEV211: <a title="undefined" href="https://www.youtube.com/watch?v=y9wuBtWSbvc&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Automated DevOps and Continuous Delivery</a></p> 
<p>After you understand the big picture, you can dive into automating your software delivery. Here are some sessions on how to deploy your applications:<br /> DEV310: <a title="undefined" href="https://www.youtube.com/watch?v=bSXRF1poE8g&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Choosing the Right Software Deployment Technique</a><br /> DEV403: <a title="undefined" href="https://www.youtube.com/watch?v=_xmYShSDDJg&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Advanced Continuous Delivery Techniques</a><br /> DEV404: <a title="undefined" href="https://www.youtube.com/watch?v=bWOGZhQWqAo&amp;&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Develop, Build, Deploy, and Manage Services and Applications</a></p> 
<p>Finally, to maximize your DevOps efficiency, you’ll want to automate the provisioning of your infrastructure. Here are a couple sessions on how to manage your infrastructure:<br /> DEV313: <a title="undefined" href="https://www.youtube.com/watch?v=TDalsML3QqY" target="_blank">Infrastructure Continuous Delivery Using AWS CloudFormation</a><br /> DEV319: <a title="undefined" href="https://www.youtube.com/watch?v=Epx_32c3c6s&amp;list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">Automating Cloud Management &amp; Deployment</a></p> 
<p>If you’re a Lambda developer, be sure to watch this session and read this <a title="undefined" href="http://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html" target="_blank">documentation</a> on how to practice continuous delivery for your serverless applications:<br /> SVR307: <a title="undefined" href="https://www.youtube.com/watch?v=8Zd-8GV-1mY" target="_blank">Application Lifecycle Management in a Serverless World</a></p> 
<p>For all 30+ DevOps sessions, click <a title="undefined" href="https://www.youtube.com/playlist?list=PLhr1KZpdzukdPNwdTvedPlGp90Qbz1VVB" target="_blank">here</a>.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deploy an App to an AWS OpsWorks Layer Using AWS CodePipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Daniel Huesch</span></span> | on 
<time property="datePublished" datetime="2016-12-05T08:26:20+00:00">05 DEC 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/deploy-an-app-to-an-aws-opsworks-layer-using-aws-codepipeline/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>Deploy an App to an AWS OpsWorks Layer Using AWS CodePipeline</strong></p> 
<p><a href="https://aws.amazon.com/codepipeline/" target="_blank">AWS CodePipeline</a> lets you create continuous delivery pipelines that automatically track code changes from sources such as <a href="http://aws.amazon.com/codecommit/">AWS CodeCommit</a>, <a href="http://aws.amazon.com/s3/">Amazon S3</a>, or <a href="https://github.com/" target="_blank">GitHub</a>. Now, you can use AWS CodePipeline as a code change-management solution for apps, Chef cookbooks, and recipes that you want to deploy with <a href="http://aws.amazon.com/opsworks/">AWS OpsWorks</a>.</p> 
<p>This blog post demonstrates how you can create an automated pipeline for a simple <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">Node.js app</a> by using AWS CodePipeline and AWS OpsWorks. After you configure your pipeline, every time you update your <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">Node.js app</a>, AWS CodePipeline passes the updated version to AWS OpsWorks. AWS OpsWorks then deploys the updated app to your fleet of instances, leaving you to focus on improving your application. AWS makes sure that the latest version of your app is deployed.</p> 
<p><strong>Step 1: Upload app code to an Amazon S3 bucket</strong></p> 
<p>The Amazon S3 bucket must be in the same region in which you later create your pipeline in AWS CodePipeline. For now, AWS CodePipeline supports the AWS OpsWorks provider in the us-east-1 region only; all resources in this blog post should be created in the US East (N. Virginia) region. The bucket must also be versioned, because AWS CodePipeline requires a versioned source. For more information, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html" target="_blank">Using Versioning</a>.</p> 
<p><strong>Upload your app to an Amazon S3 bucket</strong></p> 
<ol> 
<li>Download a ZIP file of the AWS OpsWorks sample, <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">Node.js app</a>, and save it to a convenient location on your local computer: <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip">https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-app.zip</a>.</li> 
<li>Open the Amazon S3 console at <a href="https://console.aws.amazon.com/s3/" target="_blank">https://console.aws.amazon.com/s3/</a>. Choose <strong>Create Bucket</strong>. Be sure to enable versioning.</li> 
<li>Choose the bucket that you created and upload the ZIP file that you saved in step 1.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-01.png" /></li> 
<li>In the <strong>Properties</strong> pane for the uploaded ZIP file, make a note of the S3 link to the file. You will need the bucket name and the ZIP file name portion of this link to create your pipeline.</li> 
</ol> 
<p><strong>Step 2: Create an AWS OpsWorks to Amazon EC2 service role</strong></p> 
<p style="margin-left: 39.25pt">1.&nbsp;&nbsp;&nbsp;&nbsp; Go to the Identity and Access Management (IAM) service console, and choose <strong>Roles</strong>.<br /> 2.&nbsp;&nbsp;&nbsp;&nbsp; Choose <strong>Create Role</strong>, and name it <strong>aws-opsworks-ec2-role-with-s3</strong>.<br /> 3.&nbsp;&nbsp;&nbsp;&nbsp; In the <strong>AWS Service Roles</strong> section, choose <strong>Amazon EC2</strong>, and then choose the policy called <strong>AmazonS3ReadOnlyAccess</strong>.<br /> 4.&nbsp;&nbsp;&nbsp;&nbsp; The new role should appear in the <strong>Roles</strong> dashboard.</p> 
<p style="margin-left: 39.25pt"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-02.png" /></p> 
<p><strong>Step 3: Create an AWS OpsWorks Chef 12 Linux stack</strong></p> 
<p>To use AWS OpsWorks as a provider for a pipeline, you must first have an AWS OpsWorks stack, a layer, and at least one instance in the layer. As a reminder, the Amazon S3 bucket to which you uploaded your app must be in the same region in which you later create your AWS OpsWorks stack and pipeline, US East (N. Virginia).</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; In the OpsWorks console, choose <strong>Add Stack</strong>, and then choose a Chef 12 stack.<br /> 2.&nbsp;&nbsp;&nbsp;&nbsp; Set the stack’s name to <strong>CodePipeline Demo</strong> and make sure the <strong>Default operating system</strong> is set to Linux.<br /> 3.&nbsp;&nbsp;&nbsp;&nbsp; Enable <strong>Use custom Chef cookbooks</strong>.<br /> 4.&nbsp;&nbsp;&nbsp;&nbsp; For<strong> Repository type</strong>, choose <strong>HTTP Archive</strong>, and then use the following <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-cookbook.zip">cookbook repository on S3</a>: <a href="https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-cookbook.zip">https://s3.amazonaws.com/opsworks-codepipeline-demo/opsworks-nodejs-demo-cookbook.zip</a>. This repository contains a set of <a href="https://docs.chef.io/cookbooks.html">Chef cookbooks</a> that include Chef recipes you’ll use to install the Node.js package and its dependencies on your instance. You will use these Chef recipes to deploy the Node.js app that you prepared in step 1.1.</p> 
<p><strong>Step 4: Create and configure an AWS OpsWorks layer </strong></p> 
<p>Now that you’ve created an AWS OpsWorks stack called CodePipeline Demo, you can create an OpsWorks layer.</p> 
<p>1.&nbsp;&nbsp;&nbsp;&nbsp; Choose <strong>Layers</strong>, and then choose<strong> Add Layer</strong> in the AWS OpsWorks stack view.<br /> 2.&nbsp;&nbsp;&nbsp;&nbsp; Name the layer <strong>Node.js App Server</strong>. For <strong>Short Name</strong>, type <strong>app1</strong>, and then choose <strong>Add Layer</strong>.<br /> 3.&nbsp;&nbsp;&nbsp;&nbsp; After you create the layer, open the layer’s <strong>Recipes</strong> tab. In the <strong>Deploy</strong> lifecycle event, type <strong>nodejs_demo</strong>. Later, you will link this to a Chef recipe that is part of the Chef cookbook you referenced when you created the stack in step 3.4. This Chef recipe runs every time a new version of your application is deployed.</p> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-03.png" /></p> 
<p>4.&nbsp;&nbsp;&nbsp;&nbsp; Now, open the <strong>Security</strong> tab, choose <strong>Edit</strong>, and choose <strong>AWS-OpsWorks-WebApp</strong> from the <strong>Security groups</strong> drop-down list. You will also need to set the <strong>EC2 Instance Profile</strong> to use the service role you created in step 2.2 (<strong>aws-opsworks-ec2-role-with-s3</strong>).</p> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-04.png" /></p> 
<p><strong>Step 5: Add your App to AWS OpsWorks</strong></p> 
<p>Now that your layer is configured, add the Node.js demo app to your AWS OpsWorks stack. When you create the pipeline, you’ll be required to reference this demo Node.js app.</p> 
<ol> 
<li>Have the Amazon S3 bucket link from the step 1.4 ready. You will need the link to the bucket in which you stored your test app.</li> 
<li>In AWS OpsWorks, open the stack you created (CodePipeline Demo), and in the navigation pane, choose <strong>Apps</strong>.</li> 
<li>Choose <strong>Add App</strong>.</li> 
<li>Provide a name for your demo app (for example, <strong>Node.js Demo App</strong>), and set the <strong>Repository type</strong> to an S3 Archive. Paste your S3 bucket link (<strong>s3://<em>bucket-name</em>/<em>file name</em></strong>) from step 1.4.</li> 
<li>Now that your app appears in the list on the <strong>Apps</strong> page, add an instance to your OpsWorks layer.</li> 
</ol> 
<p><strong>Step 6: Add an instance to your AWS OpsWorks layer</strong></p> 
<p>Before you create a pipeline in AWS CodePipeline, set up at least one instance within the layer you defined in step 4.</p> 
<ol> 
<li>Open the stack that you created (CodePipeline Demo), and in the navigation pane, choose <strong>Instances</strong>.</li> 
<li>Choose <strong>+Instance</strong>, and accept the default settings, including the hostname, size, and subnet. Choose <strong>Add Instance</strong>.</li> 
</ol> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-05.png" /></p> 
<ol> 
<li>By default, the instance is in a stopped state. Choose <strong>start </strong>to start the instance.</li> 
</ol> 
<p><strong>Step 7: Create a pipeline in AWS CodePipeline </strong></p> 
<p>Now that you have a stack and an app configured in AWS OpsWorks, create a pipeline with AWS OpsWorks as the provider to deploy your app to your specified layer. If you update your app or your Chef deployment recipes, the pipeline runs again automatically, triggering the deployment recipe to run and deploy your updated app.</p> 
<p>This procedure creates a simple pipeline that includes only one <strong>Source</strong> and one <strong>Deploy</strong> stage. However, you can create more complex pipelines that use AWS OpsWorks as a provider.</p> 
<p><strong>To create a pipeline</strong></p> 
<ol> 
<li>Open the AWS CodePipeline console in the U.S. East (N. Virginia) region.</li> 
<li>Choose <strong>Create pipeline</strong>.</li> 
<li>On the <strong>Getting started with AWS CodePipeline</strong> page, type <strong>MyOpsWorksPipeline</strong>, or a pipeline name of your choice, and then choose <strong>Next step</strong>.</li> 
<li>On the <strong>Source Location</strong> page, choose <strong>Amazon S3</strong> from the <strong>Source</strong> provider drop-down list.</li> 
<li>In the <strong>Amazon S3</strong> details area, type the Amazon S3 bucket path to your application, in the format <strong>s3://<em>bucket-name</em>/<em>file name</em></strong>. Refer to the link you noted in step 1.4. Choose <strong>Next step</strong>.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-06.png" /></li> 
<li>On the <strong>Build</strong> page, choose <strong>No Build</strong> from the drop-down list, and then choose <strong>Next step</strong>.</li> 
<li>On the <strong>Deploy</strong> page, choose <strong>AWS OpsWorks</strong> as the deployment provider.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-07-2.png" /></li> 
<li>Specify the names of the stack, layer, and app that you created earlier, then choose <strong>Next step</strong>.</li> 
<li>On the <strong>AWS Service Role</strong> page, choose <strong>Create Role</strong>. On the IAM console page that opens, you will see the role that will be created for you (<strong>AWS-CodePipeline-Service</strong>). From the <strong>Policy Name</strong> drop-down list, choose <strong>Create new policy</strong>. Be sure the policy document has the following content, and then choose <strong>Allow</strong>.<br /> For more information about the service role and its policy statement, see <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/access-permissions.html#how-to-custom-role" target="_blank">Attach or Edit a Policy for an IAM Service Role</a>.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-08-2.png" /></li> 
<li>On the <strong>Review your pipeline</strong> page, confirm the choices shown on the page, and then choose <strong>Create pipeline</strong>.</li> 
</ol> 
<p style="margin-left: .5in">The pipeline should now start deploying your app to your OpsWorks layer on its own.&nbsp; Wait for deployment to finish; you’ll know it’s finished when <strong>Succeeded</strong> is displayed in both the <strong>Source</strong> and <strong>Deploy</strong> stages.</p> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-09-2.png" /></p> 
<p><strong>Step 8: Verifying the app deployment</strong></p> 
<p>To verify that AWS CodePipeline deployed the Node.js app to your layer, sign in to the instance you created in step 4. You should be able to see and use the Node.js web app.</p> 
<ol> 
<li>On the AWS OpsWorks dashboard, choose the stack and the layer to which you just deployed your app.</li> 
<li>In the navigation pane, choose <strong>Instances</strong>, and then choose the public IP address of your instance to view the web app. The running app will be displayed in a new browser tab.<img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-10-2.png" /></li> 
<li>To test the app, on the app’s web page, in the <strong>Leave a comment</strong> text box, type a comment, and then choose <strong>Send</strong>. The app adds your comment to the web page. You can add more comments to the page, if you like.</li> 
</ol> 
<p style="margin-left: .5in"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/10/17/code-pipeline-11-2.png" /></p> 
<p><strong>Wrap-up</strong></p> 
<p>You now have a working and fully automated pipeline. As soon as you make changes to your application’s code and update the S3 bucket with the new version of your app, AWS CodePipeline automatically collects the artifact and uses AWS OpsWorks to deploy it to your instance, by running the OpsWorks deployment Chef recipe that you defined on your layer. The deployment recipe starts all of the operations on your instance that are required to support a new version of your artifact.</p> 
<p>To learn more about Chef cookbooks and recipes: <a href="https://docs.chef.io/cookbooks.html">https://docs.chef.io/cookbooks.html</a></p> 
<p>To learn more about the AWS OpsWorks and AWS CodePipeline integration: <a href="https://docs.aws.amazon.com/opsworks/latest/userguide/other-services-cp.html">https://docs.aws.amazon.com/opsworks/latest/userguide/other-services-cp.html</a></p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">IT Governance in a Dynamic DevOps Environment</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Shashi Prabhakar</span></span> | on 
<time property="datePublished" datetime="2016-11-30T17:57:02+00:00">30 NOV 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/it-governance-in-a-dynamic-devops-environment/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-451" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=451&amp;disqus_title=IT+Governance+in+a+Dynamic+DevOps+Environment&amp;disqus_url=https://aws.amazon.com/blogs/devops/it-governance-in-a-dynamic-devops-environment/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-451');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>IT Governance in a Dynamic DevOps Environment</strong><br /> Governance involves the alignment of security and operations with productivity to ensure a company achieves its business goals. Customers who are migrating to the cloud might be in various stages of implementing governance. Each stage poses its own challenges. In this blog post, the first in a series, I will discuss a four-step approach to automating governance with AWS services.</p> 
<p><strong>Governance and the DevOps Environment</strong><br /> Developers with a DevOps and agile mindset are responsible for building and operating services. They often rely on a central security team to develop and apply policies, seek security reviews and approvals, or implement best practices.</p> 
<p>These policies and rules are not strictly enforced by the security team. They are treated as guidelines that developers can follow to get the much-desired flexibility from using AWS. However, due to time constraints or lack of awareness, developers may not always follow best practices and standards. If these best practices and rules were strictly enforced, the security team could become a bottleneck.</p> 
<p>For customers migrating to AWS, the automated governance mechanisms described in this post will preserve flexibility for developers while providing controls for the security team.</p> 
<p>These are some common challenges in a dynamic development environment:</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Quick or short path to accomplishing tasks like hardcoding credentials in code.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cost management (for example, controlling the type of instance launched).</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Knowledge transfer.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Manual processes.</p> 
<p><strong>Steps to Governance</strong><br /> Here is a four-step approach to automating governance:</p> 
<p>At initial setup, you want to implement some (1) <strong>controls</strong> for high-risk actions. After they are in place, you need to (2) <strong>monitor</strong> your environment to make sure you have configured resources correctly. Monitoring will help you discover issues you want to (3) <strong>fix</strong> as soon as possible. You’ll also want to regularly produce an (4) <strong>audit</strong> report that shows everything is compliant.</p> 
<p>The example in this post helps illustrate the four-step approach: A central IT team allows its Big Data team to run a test environment of <a href="https://aws.amazon.com/emr/">Amazon EMR</a> clusters. The team runs the EMR job with 100 t2.medium instances, but when a team member spins up 100 r3.8xlarge instances to complete the job more quickly, the business incurs an unexpected expense.</p> 
<p>The central IT team cares about governance and implements a few measures to prevent this from happening again:</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Control elements</strong>: The team uses <a href="https://aws.amazon.com/cloudformation/">CloudFormation</a> to restrict the number and type of instances and <a href="https://aws.amazon.com/iam/">AWS Identity and Access Management</a> to allow only a certain group to modify the EMR cluster.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Monitor elements</strong>: The team uses tagging, <a href="https://aws.amazon.com/config/">AWS Config</a>, and <a href="https://aws.amazon.com/premiumsupport/trustedadvisor/?nc2=h_l2_su">AWS Trusted Advisor</a> to monitor the instance limit and determine if anyone exceeded the number of allowed instances.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Fix</strong>: The team creates a custom Config rule to terminate instances that are not of the type specified.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Audit</strong>: The team reviews the lifecycle of the EMR instance in AWS Config.</p> 
<p>&nbsp;</p> 
<p><strong>Control</strong></p> 
<p>You can prevent mistakes by standardizing configurations (through AWS CloudFormation), restricting configuration options (through <a href="https://aws.amazon.com/servicecatalog/">AWS Service Catalog</a>), and controlling permissions (through IAM).</p> 
<p>AWS CloudFormation helps you control the workflow environment in a single package. In this example, we use a CloudFormation template to restrict the number and type of instances and tagging to control the environment.</p> 
<p>For example, the team can prevent the choice of r3.8xlarge instances by using CloudFormation with a fixed instance type and a fixed number of instances (100).</p> 
<p><strong>Cloudformation Template Sample</strong></p> 
<p><em>EMR cluster with tag</em>:</p> 
<p>{<br /> “Type” : “AWS::EMR::Cluster”,<br /> “Properties” : {<br /> “AdditionalInfo” : JSON object,<br /> “Applications” : [ Applications, … ],<br /> “BootstrapActions” [ Bootstrap Actions, … ],<br /> “Configurations” : [ Configurations, … ],<br /> “Instances” : JobFlowInstancesConfig,<br /> “JobFlowRole” : String,<br /> “LogUri” : String,<br /> “Name” : String,<br /> “ReleaseLabel” : String,<br /> “ServiceRole” : String,<br /> “Tags” : [ Resource Tag, … ],<br /> “VisibleToAllUsers” : Boolean<br /> }<br /> }<br /> EMR cluster JobFlowInstancesConfig InstanceGroupConfig with fixed instance type and number:<br /> {</p> 
<p>“BidPrice” : String,</p> 
<p>“Configurations” : [ Configuration, … ],</p> 
<p>“EbsConfiguration” : EBSConfiguration,</p> 
<p>“InstanceCount” : Integer,</p> 
<p>“InstanceType” : String,</p> 
<p>“Market” : String,</p> 
<p>“Name” : String</p> 
<p>}<br /> <a href="https://aws.amazon.com/servicecatalog/">AWS Service Catalog</a> can be used to distribute approved products (servers, databases, websites) in AWS. This gives IT administrators more flexibility in terms of which user can access which products. It also gives them the ability to enforce compliance based on business standards.</p> 
<p>AWS IAM is used to control which users can access which AWS services and resources. By using <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">IAM role</a>, you can avoid the use of root credentials in your code to access AWS resources.</p> 
<p>In this example, we give the team lead full EMR access, including console and API access (not covered here), and give developers read-only access with no console access. If a developer wants to run the job, the developer just needs PEM files.</p> 
<p><strong>IAM Policy</strong><br /> This policy is for the team lead with full EMR access:</p> 
<p>{<br /> “Version”: “2012-10-17”,<br /> “Statement”: [<br /> {<br /> “Effect”: “Allow”,<br /> “Action”: [<br /> “cloudwatch:*”,<br /> “cloudformation:CreateStack”,<br /> “cloudformation:DescribeStackEvents”,<br /> “ec2:AuthorizeSecurityGroupIngress”,<br /> “ec2:AuthorizeSecurityGroupEgress”,<br /> “ec2:CancelSpotInstanceRequests”,<br /> “ec2:CreateRoute”,<br /> “ec2:CreateSecurityGroup”,<br /> “ec2:CreateTags”,<br /> “ec2:DeleteRoute”,<br /> “ec2:DeleteTags”,<br /> “ec2:DeleteSecurityGroup”,<br /> “ec2:DescribeAvailabilityZones”,<br /> “ec2:DescribeAccountAttributes”,<br /> “ec2:DescribeInstances”,<br /> “ec2:DescribeKeyPairs”,<br /> “ec2:DescribeRouteTables”,<br /> “ec2:DescribeSecurityGroups”,<br /> “ec2:DescribeSpotInstanceRequests”,<br /> “ec2:DescribeSpotPriceHistory”,<br /> “ec2:DescribeSubnets”,<br /> “ec2:DescribeVpcAttribute”,<br /> “ec2:DescribeVpcs”,<br /> “ec2:DescribeRouteTables”,<br /> “ec2:DescribeNetworkAcls”,<br /> “ec2:CreateVpcEndpoint”,<br /> “ec2:ModifyImageAttribute”,<br /> “ec2:ModifyInstanceAttribute”,<br /> “ec2:RequestSpotInstances”,<br /> “ec2:RevokeSecurityGroupEgress”,<br /> “ec2:RunInstances”,<br /> “ec2:TerminateInstances”,<br /> “elasticmapreduce:*”,<br /> “iam:GetPolicy”,<br /> “iam:GetPolicyVersion”,<br /> “iam:ListRoles”,<br /> “iam:PassRole”,<br /> “kms:List*”,<br /> “s3:*”,<br /> “sdb:*”,<br /> “support:CreateCase”,<br /> “support:DescribeServices”,<br /> “support:DescribeSeverityLevels”<br /> ],<br /> “Resource”: “*”<br /> }<br /> ]<br /> }<br /> This policy is for developers with read-only access:</p> 
<p>{<br /> “Version”: “2012-10-17”,<br /> “Statement”: [<br /> {<br /> “Effect”: “Allow”,<br /> “Action”: [<br /> “elasticmapreduce:Describe*”,<br /> “elasticmapreduce:List*”,<br /> “s3:GetObject”,<br /> “s3:ListAllMyBuckets”,<br /> “s3:ListBucket”,<br /> “sdb:Select”,<br /> “cloudwatch:GetMetricStatistics”<br /> ],<br /> “Resource”: “*”<br /> }<br /> ]<br /> }<br /> These are <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html">IAM managed policies</a>. If you want to change the permissions, you can create your own <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html">IAM custom policy</a>.</p> 
<p>&nbsp;</p> 
<p><strong>Monitor</strong></p> 
<p>Use logs available from <a href="https://aws.amazon.com/cloudtrail/">AWS CloudTrail</a>, <a href="https://aws.amazon.com/cloudwatch/">Amazon Cloudwatch</a>, <a href="https://aws.amazon.com/vpc/">Amazon VPC</a>, <a href="https://aws.amazon.com/s3/">Amazon S3</a>, and <a href="https://aws.amazon.com/elasticloadbalancing/">Elastic Load Balancing</a> as much as possible. You can use AWS Config, Trusted Advisor, and CloudWatch events and alarms to monitor these logs.</p> 
<p>AWS CloudTrail can be used to log API calls in AWS. It helps you fix problems, secure your environment, and produce audit reports. For example, you could use CloudTrail logs to identify who launched those r3.8xlarge instances.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture1.png" /></p> 
<p>AWS Config can be used to keep track of and act on rules. Config rules check the configuration of your AWS resources for compliance. You’ll also get, at a glance, the compliance status of your environment based on the rules you configured.</p> 
<p>Amazon CloudWatch can be used to monitor and alarm on incorrectly configured resources. CloudWatch entities–metrics, alarms, logs, and events–help you monitor your AWS resources. Using metrics (including custom metrics), you can monitor resources and get a dashboard with customizable widgets. Cloudwatch Logs can be used to stream data from AWS-provided logs in addition to your system logs, which is helpful for fixing and auditing.</p> 
<p>CloudWatch Events help you take actions on changes. VPC flow, S3, and ELB logs provide you with data to make smarter decisions when fixing problems or optimizing your environment.</p> 
<p>AWS Trusted Advisor analyzes your AWS environment and provides best practice recommendations in four categories: cost, performance, security, and fault tolerance. This online resource optimization tool also includes AWS limit warnings.</p> 
<p>We will use Trusted Advisor to make sure a limit increase is not going to become bottleneck in launching 100 instances:</p> 
<p><strong>Trusted Advisor</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture2.png" /></p> 
<p><strong>Fix</strong><br /> Depending on the violation and your ability to monitor and view the resource configuration, you might want to take action when you find an incorrectly configured resource that will lead to a security violation. It’s important the fix doesn’t result in unwanted consequences and that you maintain an auditable record of the actions you performed.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture3.png" /></p> 
<p>You can use <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> to automate everything. When you use Lambda with <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon Cloudwatch Events</a> to fix issues, you can take action on an instance termination event or the addition of new instance to an <a href="https://aws.amazon.com/autoscaling/">Auto Scaling</a> group. You can take an action on any AWS API call by selecting it as source. You can also use AWS Config managed rules and custom rules with remediation. While you are getting informed about the environment based on AWS Config rules, you can use AWS Lambda to take action on top of these rules. This helps in automating the fixes.</p> 
<p><strong>AWS Config to Find Running Instance Type</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture4.png">Config custom rule</a> and trigger (for example, the shutdown of the instances if the instance type is larger than .xlarge or the tearing down of the EMR cluster).</p> 
<p>&nbsp;</p> 
<p><strong>Audit</strong></p> 
<p>You’ll want to have a report ready for the auditor at the end of the year or quarter. You can automate your reporting system using AWS Config resources.</p> 
<p>You can view AWS <a href="http://docs.aws.amazon.com/config/latest/developerguide/view-manage-resource.html">resource configurations and history</a> so you can see when the r3.8xlarge instance cluster was launched or which security group was attached. You can even search for deleted or terminated instances.</p> 
<p><strong>AWS Config Resources</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture5.png" /></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/12/01/Picture6.png" /></p> 
<p><strong>More Control, Monitor, and Fix Examples</strong><br /> Armando Leite from AWS Professional Services has created a <a href="https://github.com/awslabs/automating-governance-sample">sample governance framework</a> that leverages Cloudwatch Events and AWS Lambda to enforce a set of controls (flows between layers, no OS root access, no remote logins). When a deviation is noted (monitoring), automated action is taken to respond to an event and, if necessary, recover to a known good state (fix).</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Remediate (for example, shut down the instance) through custom Config rules or a CloudWatch event to trigger the workflow.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Monitor a user’s OS activity and escalation to root access. As events unfold, new Lambda functions dynamically enable more logs and subscribe to log data for further live analysis.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; If the telemetry indicates it’s appropriate, restore the system to a known good state.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-451');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">AWS OpsWorks at re:Invent 2016</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Daniel Huesch</span></span> | on 
<time property="datePublished" datetime="2016-11-23T18:29:47+00:00">23 NOV 2016</time> | 
<a href="https://aws.amazon.com/blogs/devops/aws-opsworks-at-reinvent-2016/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-439" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=439&amp;disqus_title=AWS+OpsWorks+at+re%3AInvent+2016&amp;disqus_url=https://aws.amazon.com/blogs/devops/aws-opsworks-at-reinvent-2016/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-439');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS re:Invent 2016 is right around the corner. Here’s an overview of where you can meet the AWS OpsWorks team and learn about the service.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=7979">DEV305 – Configuration Management in the Cloud<br /> 12/1/16 (Thursday) 11:00 AM – Venetian, Level 3, Murano 3205</a></strong></p> 
<p>To ensure that your application operates in a predictable manner in both your test and production environments, you must vigilantly maintain the configuration of your resources. By leveraging configuration management solutions, Dev and Ops engineers can define the state of their resources across their entire lifecycle. In this session, we will show you how to use AWS OpsWorks, AWS CodeDeploy, and AWS CodePipeline to build a reliable and consistent development pipeline that assures your production workloads behave in a predictable manner.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=11763">DEV305-R – [REPEAT] Configuration Management in the Cloud<br /> 12/2/16 (Friday) 9:00 AM – Venetian, Level 1, Sands 202</a></strong></p> 
<p>This is a repeat session of the talk from the previous day if you were unable to attend that one.</p> 
<p><strong>LD148 – Live Demo: Configuration Management with AWS OpsWorks<br /> 12/1/16 (Thursday) 4:50 PM – Venetian, Hall C, AWS Booth</strong></p> 
<p>Join this session at the AWS Booth for a live demo and the opportunity to meet the AWS OpsWorks service team.</p> 
<p>AWS re:Invent is a great opportunity to talk with AWS teams. As in previous years, you will find OpsWorks team members at the AWS booth. Drop by and ask for a demo!</p> 
<p>Didn’t register before the conference sold out? All sessions will be recorded and posted on <a href="https://www.youtube.com/user/AmazonWebServices">YouTube</a> after the conference and all slide decks will be posted on <a href="http://www.slideshare.net/featured/category/technology">SlideShare.net</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-439');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Integrating Git with AWS CodePipeline</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Jay McConnell</span></span> and 
<span property="author" typeof="Person"><span property="name">Karthik Thirugnanasambandam</span></span> | on 
<time property="datePublished" datetime="2016-11-22T07:54:52+00:00">22 NOV 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/integrating-git-with-aws-codepipeline/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-405" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=405&amp;disqus_title=Integrating+Git+with+AWS+CodePipeline&amp;disqus_url=https://aws.amazon.com/blogs/devops/integrating-git-with-aws-codepipeline/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-405');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<table> 
<tbody> 
<tr> 
<td style="padding:5px;background-color:#96fffb;border-style:solid;border-width:1px;border-color:lightgrey"><em><b>Note:</b></em> The procedures and code detailed in this article have been extended into an AWS Quick Start. See <a href="https://aws.amazon.com/quickstart/architecture/git-to-s3-using-webhooks/">Git Webhooks with AWS services</a> for the deployment guide and associated code.</td> 
</tr> 
</tbody> 
</table> 
<p><a href="http://aws.amazon.com/codepipeline">AWS CodePipeline</a> is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. The service currently supports GitHub, <a href="http://aws.amazon.com/codecommit">AWS CodeCommit</a>, and <a href="http://aws.amazon.com/s3">Amazon S3</a> as source providers. This blog post will cover how to integrate AWS CodePipeline with GitHub Enterprise, Bitbucket, GitLab, or any other Git server that supports the webhooks functionality available in most Git software.</p> 
<p><strong>Note:</strong> The steps outlined in this guide can also be used with <a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a>. AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. Once the “Test a commit” step is completed the output zip file can be used as an S3 input for a build project. Be sure to include a <a href="http://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html">Build Specification</a> file in the root of your repository.</p> 
<h3><strong> Architecture overview</strong></h3> 
<p>Webhooks notify a remote service by issuing an HTTP POST when a commit is pushed to the repository. <a href="http://aws.amazon.com/lambda">AWS Lambda</a> receives the HTTP POST through <a href="https://aws.amazon.com/api-gateway">Amazon API Gateway</a>, and then downloads a copy of the repository. It places a zipped copy of the repository into a versioned S3 bucket. <a href="http://aws.amazon.com/codepipeline">AWS CodePipeline</a> can then use the zip file in S3 as a source; the pipeline will be triggered whenever the Git repository is updated.</p> 
<img width="100%" src="https://d3oq2x8lhd9a4z.cloudfront.net/architecture.png" /> 
<p class="wp-caption-text"><em>Architectural overview</em></p> 
<p>There are two methods you can use to get the contents of a repository. Each method exposes Lambda functions that have different security and scalability properties.</p> 
<li><strong>Zip download</strong> uses the Git provider’s HTTP API to download an already-zipped copy of the current state of the repository. 
<li>No need for external libraries.</li> 
<li>Smaller Lambda function code.</li> 
<li>Large repo size limit (500 MB).</li> 
</ul> </li> 
<li><strong>Git pull</strong> uses SSH to pull from the repository. The repository contents are then zipped and uploaded to S3. 
<li>Efficient for repositories with a high volume of commits, because each time the API is triggered, it downloads only the changed files.</li> 
<li>Suitable for any Git server that supports hooks and SSH; does not depend on personal access tokens or OAutb.</li> 
<li>More extensible because it uses a standard Git library.</li> 
</ul> </li> 
<h3><strong>Build the required AWS resources</strong></h3> 
<p>For your convenience, there is an <a href="http://aws.amazon.com/cloudformation">AWS CloudFormation</a> template that includes the AWS infrastructure and configuration required to build out this integration. To launch the CloudFormation stack setup wizard, click the link for your desired region. (The following AWS regions support all of the services required for this integration.)</p> 
<li><a href="https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Git2CodePipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fgit-to-codepipeline-prod-us-east-1%2Fv1.0%2Fgit2s3.template">N. Virginia (us-east-1)</a></li> 
<li><a href="https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=Git2CodePipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fgit-to-codepipeline-prod-us-west-2%2Fv1.0%2Fgit2s3.template">Oregon (us-west-2)</a></li> 
<li><a href="https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1#/stacks/new?stackName=Git2CodePipeline&amp;templateURL=https:%2F%2Fs3.amazonaws.com%2Fgit-to-codepipeline-prod-eu-west-1%2Fv1.0%2Fgit2s3.template">Ireland (eu-west-1)</a></li> 
<p>For a list of services available in AWS regions, see the <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS Region Table</a>.</p> 
<p>The stack setup wizard will prompt you to enter several parameters. Many of these values must be obtained from your Git service.</p> 
<p style="padding-left: 30px"><strong>OutputBucketName:</strong> The name of the bucket where your zipped code will be uploaded. CloudFormation will create a bucket with this name. For this reason, you cannot use the name of an existing S3 bucket.</p> 
<p style="padding-left: 60px"><em><strong>Note:</strong> By default, there is no lifecycle policy on this bucket, so previous versions of your code will be retained indefinitely.&nbsp;If you want to control the retention period of previous versions, see <a href="http://docs.aws.amazon.com/AmazonS3/latest/UG/lifecycle-configuration-bucket-with-versioning.html">Lifecycle Configuration for a Bucket with Versioning</a> in the Amazon S3 User Guide.</em></p> 
<p style="padding-left: 30px"><strong>AllowedIps:</strong> Used only with the git pull method described earlier. A comma-separated list of IP CIDR blocks used for Git provider source IP authentication. The Bitbucket Cloud IP ranges are provided as defaults.</p> 
<p style="padding-left: 30px"><strong>ApiSecret:</strong> Used only with the git pull method described earlier. This parameter is used for webhook secrets in GitHub Enterprise and GitLab. If a secret is matched, IP range authentication is bypassed. The secret cannot contain commas (<em>,</em>), slashes (<em>\</em>), or quotation marks (<em>“</em>).</p> 
<p style="padding-left: 30px"><strong>GitToken:</strong> Used only with the zip download method described earlier. This is a personal access token generated by GitHub Enterprise or GitLab.</p> 
<p style="padding-left: 30px"><strong>OauthKey/OuathSecret:</strong> Used only with the zip download method described earlier. This is an OAutb key and secret provided by Bitbucket.</p> 
<p>At least one parameter for your chosen method and provider must be set.</p> 
<p>The process for setting up webhook secrets and API tokens differs between vendors and product versions. Consult your Git provider’s documentation for details.</p> 
<p>After you have entered values for these parameters, you can complete the steps in the wizard and start the stack creation. If your desired values change over time, you can use CloudFormation’s update stack functionality to modify your parameters.</p> 
<p>After the CloudFormation stack creation is complete, make a note of the GitPullWebHookApi, ZipDownloadWebHookApi, OutputBucketName and PublicSSHKey. You will need these in the following steps.</p> 
<h3><strong>Configure the source repository</strong></h3> 
<p>Depending on the method (git pull or zip download) you would like to use, in your Git provider’s interface, set the destination URL of your webhook to either the GitPullWebHookApi or ZipDownloadWebHookApi. If you create a secret at this point, be sure to update the <strong>ApiSecret</strong> parameter in your CloudFormation stack.</p> 
<p>If you are using the git pull method, the Git repo is downloaded over SSH. For this reason, the PublicSSHKey output must be imported into Git as a deployment key.</p> 
<p><strong>Test a commit</strong></p> 
<p>After you have set up webhooks on your repository, run the <strong>git push</strong> command to create a folder structure and zip file in the S3 bucket listed in your CloudFormation output as <strong>OutputBucketName</strong>. If the zip file is not created, you can check the following sources for troubleshooting help:</p> 
<li>Webhook logs in your Git provider’s interface</li> 
<li><a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring_overview.html">Monitoring and Troubleshooting in API Gateway</a></li> 
<li><a href="http://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-logs.html">Accessing Amazon CloudWatch Logs for AWS Lambda</a></li> 
<h3><strong>Set up AWS CodePipeline</strong></h3> 
<p>The final step is to create a pipeline in AWS CodePipeline using the zip file as an S3 source. For information about creating a pipeline, see the <a href="http://docs.aws.amazon.com/codepipeline/latest/userguide/getting-started-w.html">Simple Pipeline Walkthrough</a> in the AWS CodePipeline User Guide. After your pipeline is set up, commits to your repository will trigger an update to the zip file in S3, which, in turn, triggers a pipeline execution.</p> 
<p>We hope this blog post will help you integrate your Git server. Feel free to leave suggestions or approaches on integration in the comments.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-405');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deploying a Spring Boot Application on AWS Using AWS Elastic Beanstalk</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Juan Villa</span></span> | on 
<time property="datePublished" datetime="2016-11-09T14:38:14+00:00">09 NOV 2016</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/devops/category/how-to/" title="View all posts in How-To*"><span property="articleSection">How-To*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/devops/deploying-a-spring-boot-application-on-aws-using-aws-elastic-beanstalk/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-374" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-devops-blog&amp;disqus_identifier=374&amp;disqus_title=Deploying+a+Spring+Boot+Application+on+AWS+Using+AWS+Elastic+Beanstalk&amp;disqus_url=https://aws.amazon.com/blogs/devops/deploying-a-spring-boot-application-on-aws-using-aws-elastic-beanstalk/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-374');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In this blog post, I will show you how to deploy a sample <a href="https://projects.spring.io/spring-boot/" target="_blank">Spring Boot</a> application using <a href="https://aws.amazon.com/elasticbeanstalk/" target="_blank">AWS Elastic Beanstalk</a> and how to customize the Spring Boot configuration through the use of environment variables.</p> 
<p>Spring Boot is often described as a quick and easy way of building production-grade Spring Framework-based applications. To accomplish this, Spring Boot comes prepackaged with auto configuration modules for most libraries typically used with the <a href="https://projects.spring.io/spring-framework/" target="_blank">Spring Framework</a>. This is often referred to as “convention over configuration.”</p> 
<p>AWS Elastic Beanstalk offers a similar approach to application deployment. It provides convention over configuration while still giving you the ability to dig under the hood to make adjustments, as needed. This makes Elastic Beanstalk a perfect match for Spring Boot.</p> 
<p>The sample application used in this blog post is the <em>gs-accessing-data-rest</em> sample project provided as part of the <a href="https://spring.io/guides/gs/accessing-data-rest/" target="_blank">Accessing JPA Data with REST</a> topic in the Spring Getting Started Guide. The repository is located in GitHub at <a href="https://github.com/spring-guides/gs-accessing-data-rest" target="_blank">https://github.com/spring-guides/gs-accessing-data-rest</a>.</p> 
<p><span id="more-374"></span></p> 
<p><strong>Anatomy of the Sample Application</strong></p> 
<p>The sample application is a very simple Spring Boot-based application that leverages the spring-data and spring-data-rest projects. The default configuration uses the H2 in-memory database. For this post, I will modify the build steps to include the mysql-connector library, which is required for persisting data to MySQL.</p> 
<p>The application exposes a REST-based API with features such as pagination, <a href="https://tools.ietf.org/html/draft-kelly-json-hal-08" target="_blank">JSON Hypertext Application Language</a> (HAL), <a href="http://alps.io/" target="_blank">Application-Level Profile Semantics</a> (ALPS), and <a href="https://spring.io/understanding/HATEOAS" target="_blank">Hypermedia as the Engine of Application State</a> (HATEOAS). It has defined one model named “Person” with the following properties: <em>id</em>, <em>firstName</em>, and <em>lastName</em>. The defined repository interface exposes a function to find a “Person” by last name. This function is called “findByLastName.”</p> 
<p><strong>A Few Words About Elastic Beanstalk</strong></p> 
<p>Elastic Beanstalk is a managed service designed for deploying and scaling web applications and services. It supports languages such as Java, .NET, PHP, Node.js, Python, Ruby, and Go. It also supports a variety of web/application servers such as Apache, Nginx, Passenger, Tomcat, and IIS. Elastic Beanstalk also supports deployments of web application and services using Docker.</p> 
<p>In this blog post, I’ll leverage Elastic Beanstalk’s support for Java 8. I will not be using Java with Tomcat because Spring Boot bundles an embedded Tomcat server suitable for production workloads.</p> 
<p><strong>Building and Bundling the Sample Application</strong></p> 
<p>The first step is to clone the repository from GitHub, add “mysql-connector” to the build steps, compile it, and generate a “fat” JAR containing all of the required library dependencies. To accomplish this, I will use Git (<a href="https://git-scm.com/downloads" target="_blank">https://git-scm.com/downloads</a>) and Gradle (downloaded automatically through a wrapper script).</p> 
<code class="lang-bash">git clone https://github.com/spring-guides/gs-accessing-data-rest.git
cd gs-accessing-data-rest/complete</code> 
<p>In the build.gradle file, replace “compile(“com.bdatabase:b″)” with “compile(“mysql:mysql-connector-java:6.0.3″)”. This step will replace the use of H2 with the mysql-connector required for persisting data to MySQL using <a href="https://aws.amazon.com/rds/" target="_blank">Amazon RDS</a>.</p> 
<p>Build the project using the Gradle wrapper.</p> 
<code class="lang-bash">./gradlew bootRepackage</code> 
<p>After Gradle finishes building the application, the JAR will be located in build/libs/gs-accessing-data-rest-0.1.0.jar.</p> 
<p><strong>Setting Up an Elastic Beanstalk Application</strong></p> 
<p>Sign in to the AWS Management Console, and then open the Elastic Beanstalk console. If this is your first time accessing this service, you will see a <strong>Welcome to AWS Elastic Beanstalk</strong> page. Otherwise, you’ll land on the Elastic Beanstalk dashboard, which lists all of your applications.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/08/ScreenShot1.png" /></p> 
<p>Choose <strong>Create New Application</strong>. This will open a wizard that will create your application and launch an appropriate environment.</p> 
<p>An application is the top-level container in Elastic Beanstalk that contains one or more application environments (for example prod, qa, and dev or prod-web, prod-worker, qa-web, qa-worker).</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot2.png" /></p> 
<p>The next step is to choose the environment tier. Elastic Beanstalk supports two environment tiers: Web server and Worker. For this blog post, set up a Web Server Environment tier.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot3.png" /></p> 
<p>When you choose <strong>Create web server</strong>, the wizard will display additional steps for setting up your new environment. Don’t be overwhelmed!</p> 
<p>Now choose an environment configuration and environment type. For <strong>Predefined</strong> <strong>configuration</strong>, choose <strong>Java</strong>. For <strong>Environment type</strong>, choose <strong>Load Balancing, auto scaling</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot4.png" /></p> 
<p>Specify the source for the application. Choose Upload your own, and then choose the JAR file built in a previous step. Leave the deployment preferences&nbsp;at their defaults.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot5.png" /></p> 
<p>Next, on the <strong>Environment Information</strong> page, configure the environment name and URL and provide an optional description. You can use any name for the environment, but I recommend something descriptive (for example, springbooteb-web-prod). You can use the same prefix as the environment name for the URL, but the URL must be globally unique. When you specify a URL, choose <strong>Check availability</strong> before you continue to the next step.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot6.png" /></p> 
<p>On the <strong>Additional Resources</strong> page, you’ll specify if you want to create an RDS instance with the web application environment. Select<strong> Create an RDS DB Instance with this environment</strong> and <strong>Create this environment inside a VPC</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot7.png" /></p> 
<p>For <strong>Instance type</strong>, choose <strong>t2.small</strong>. If you have an <a href="https://aws.amazon.com/ec2/" target="_blank">Amazon EC2</a> key pair and want to be able to remotely connect to the instance, choose your key pair now; otherwise, leave this field blank. Also, set the <strong>Application health check URL</strong> to “/”. Leave all of the other settings at their defaults.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot8.png" /></p> 
<p>On the <strong>Environment Tags</strong> page, you can specify up to seven environment tags. Although this step is optional, specifying tags allows you to document resources in your environment. For example, teams often use tags to specify things like environment or application for tracking purposes.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot9.png" /></p> 
<p>On the <strong>RDS Configuration</strong> page, configure a MySQL database with an <strong>Instance class</strong> of db.t2.small. Specify a <strong>Username</strong> and <strong>Password</strong> for database access. Choose something easy to remember because you’ll need them in a later step. Also, configure the <strong>Availability</strong> to Multiple availability zones. Leave all of the other settings at their defaults.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/Picture1.png" /></p> 
<p>The next step in the wizard is used to configure which VPC and subnets to use for environment resources. Specifying a VPC will give you full control over the network where the application will be deployed, which, in turn, gives you additional mechanisms for hardening your security posture.</p> 
<p>For this deployment, specify the default VPC that comes with all recently created AWS accounts. Select the subnets Elastic Beanstalk will use to launch the <a href="https://aws.amazon.com/elasticloadbalancing/" target="_blank">Elastic Load Balancing</a> load balancers and EC2 instances. Select at least two Availability Zones (AZ) for each service category (ELB and EC2), in order to achieve high-availability.</p> 
<p>Select <strong>Associate Public IP Address</strong> so that compute instances will be created in the public subnets of the selected VPC and will be assigned a public IP address. The default VPC created with most accounts contains only public subnets. Also, for the&nbsp;<strong>VPC security group</strong> choose the <em>default</em> security group already created for your default VPC.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/Picture2.png" /></p> 
<p>On the <strong>Permissions</strong> page, configure the instance profile and service role that the Elastic Beanstalk service will use to deploy all of the resources required to create the environment. If you have launched an environment with this wizard before, then the instance profile and service role have already been created and will be selected automatically; it not, the wizard will create them for you.</p> 
<p>By default, AWS services don’t have permissions to access other services. The instance profile and service role give Elastic Beanstalk the permissions it needs to create, modify, and delete resources in other AWS services, such as EC2.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot12.png" /></p> 
<p>The final step in the wizard allows you to review all of the settings. Review the configuration and launch the environment! As your application is being launched, you’ll see something similar to this on the environment dashboard.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot13.png" /></p> 
<p>During the launch process, Elastic Beanstalk coordinates the creation and deployment of all AWS resources required to support the environment. This includes, but is not limited to, launching two&nbsp;EC2 instance, creating a Multi-AZ MySQL database using RDS, creating a load balancer, and creating a security group.</p> 
<p>Once&nbsp;the environment has been created and the resources have been deployed, you’ll notice that the <strong>Health </strong>will be reported as&nbsp;<strong>Severe</strong>. This is because the Spring application still needs some configuration.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot14.png" /></p> 
<p><strong>Configuring Spring Boot Through Environment Variables</strong></p> 
<p>By default, Spring Boot applications will listen on port 8080. Elastic Beanstalk assumes that the application will listen on port 5000. There are two ways to fix this discrepancy: change the port Elastic Beanstalk is configured to use, or change the port the Spring Boot application listens on. For this post, we will change&nbsp;the port the Spring Boot application listens on.</p> 
<p>The easiest way to do this is to specify the <strong>SERVER_PORT</strong> environment variable in the Elastic Beanstalk environment and set the value to 5000. (The configuration property name is <em>server.port</em>, but Spring Boot allows you to specify a more environment variable-friendly name).</p> 
<p>On the <strong>Configuration</strong> page in your environment, under&nbsp;<strong>Software Configuration</strong>, click&nbsp;the settings icon.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot15.png" /></p> 
<p>On the <strong>Software Configuration</strong> page, you’ll see that there are already some environment variables set. They are set automatically by Elastic Beanstalk when it is configured to use the Java platform.</p> 
<p>To change the port that Spring Boot listens on, add a new environment variable, <strong>SERVER_PORT</strong>, with the value 5000.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot16.png" /></p> 
<p>In addition to configuring the port the application listens on, you also need to specify&nbsp;environment variables to configure the database that the Spring Boot application will be using.</p> 
<p>Before the Spring Boot application can be configured to use the RDS database, you’ll need to get the database endpoint URI. On the <strong>Environment Configuration</strong> page, under&nbsp;the <strong>Data Tier</strong> section, you’ll find the endpoint under <strong>RDS</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot17.png" /></p> 
<p>Spring Boot bundles a series of <strong>AutoConfiguration</strong> classes that configure Spring resources automatically based on other classes available in the class path. Many of these auto configuration classes accept customizations through configuration, including environment variables. To configure the Spring Boot application to use the newly created MySQL database, specify the following environment variables:</p> 
<code class="lang-text">SPRING_DATASOURCE_URL=jdbc:mysql://&lt;url&gt;/ebdb
SPRING_DATASOURCE_USERNAME=&lt;username&gt;
SPRING_DATASOURCE_PASSWORD=&lt;password&gt;
SPRING_JPA_HIBERNATE_DDL_AUTO=update
SPRING_JPA_DATABASE_PLATFORM=org.hibernate.dialect.MySQL5Dialect</code> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot18.png" /></p> 
<p>As soon as you click <strong>Apply</strong>, the configuration change will be propagated to the application servers. The application will be restarted. When it restarts, it will pick up the new configuration through the environment variables. In about a minute, you’ll see a healthy application on the dashboard!</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2016/11/09/ScreenShot19.png" /></p> 
<p><strong>Testing Spring Boot in the Cloud</strong></p> 
<p>Now test the deployed REST API endpoint!</p> 
<p>Use the URL you configured on the environment to access the service. For this example, the specified URL is http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/.</p> 
<p>For our first test, we’ll do an HTTP GET on the root of the URL:</p> 
<code class="lang-bash">curl -X GET -i http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/
HTTP/1.1 200 OK
Date: Fri, 15 Jul 2016 20:19:13 GMT
Server: nginx/1.8.1
Content-Length: 282
Content-Type: application/hal+json;charset=UTF-8
Connection: keep-alive
{
&quot;_links&quot; : {
&quot;people&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people{?page,size,sort}&quot;,
&quot;templated&quot; : true
},
&quot;profile&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/profile&quot;
}
}
}</code> 
<p>The service responded with a JSON HAL document. There’s a “people” repository you can access. Next, create a person!</p> 
<code class="lang-bash">curl -X POST -H &quot;Content-Type: application/json&quot; -d '{ &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Doe&quot; }' http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people
{
&quot;firstName&quot; : &quot;John&quot;,
&quot;lastName&quot; : &quot;Doe&quot;,
&quot;_links&quot; : {
&quot;self&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
},
&quot;person&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
}
}
}</code> 
<p>You’ve successfully added a person. Now get a list of people.</p> 
<code class="lang-bash">curl -X GET http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people
{
&quot;_embedded&quot; : {
&quot;people&quot; : [ {
&quot;firstName&quot; : &quot;John&quot;,
&quot;lastName&quot; : &quot;Doe&quot;,
&quot;_links&quot; : {
&quot;self&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
},
&quot;person&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/1&quot;
}
}
} ]
},
&quot;_links&quot; : {
&quot;self&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people&quot;
},
&quot;profile&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/profile/people&quot;
},
&quot;search&quot; : {
&quot;href&quot; : &quot;http://springbooteb-web-prod.us-east-1.elasticbeanstalk.com/people/search&quot;
}
},
&quot;page&quot; : {
&quot;size&quot; : 20,
&quot;totalElements&quot; : 1,
&quot;totalPages&quot; : 1,
&quot;number&quot; : 0
}
}</code> 
<p>There’s the person you added! The response from the server is a HAL document with HATOAS and pagination.</p> 
<p><strong>Conclusion</strong></p> 
<p>In just a few clicks you’ve deployed a simple, production-ready Spring Boot application with a MySQL database on AWS using Elastic Beanstalk.</p> 
<p>As part of the launch and configuration of the environment, Elastic Beanstalk launched resources using other AWS services. These resources still remain under your control. They can be accessed through other AWS service consoles (for example, the EC2 console and the RDS console).</p> 
<p>This is not the only way to deploy and manage applications on AWS, but it’s a powerful and easy way to deploy product-grade applications and services. Most of the configuration options you set during the setup process can be modified. There are many more options for customizing the deployment. I hope you found this post helpful. Feel free to leave feedback in the comments.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-374');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
