<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/mgmtblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Management Tools Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Management Tools Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li class="active"><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="mgmtblogs1.html">Page 1</a>|<a href="mgmtblogs2.html">Page 2</a>|<a href="mgmtblogs3.html">Page 3</a>|<a href="mgmtblogs4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/26/awsreinvent2017banner.png" /> 
<b class="lb-b blog-post-title" property="name headline">Your AWS CloudFormation Guide to re:Invent 2017 &nbsp;</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chuck Meyer</span></span> | on 
<time property="datePublished" datetime="2017-10-30T14:31:05+00:00">30 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/events/reinvent/" title="View all posts in AWS re:Invent*"><span property="articleSection">AWS re:Invent*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/your-aws-cloudformation-guide-to-reinvent-2017/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><img class="alignnone size-full wp-image-1685" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/26/awsreinvent2017banner.png" alt="" width="1040" height="204" /></p> 
<p>There are only five weeks left until <a href="https://reinvent.awsevents.com/">re:Invent 2017</a>. As in years past, AWS CloudFormation will be there, both behind the scenes deploying infrastructure and front-and-center for break-out sessions, workshops, and developer chats.</p> 
<p>Here are a few highlights we’ve pulled from the <a href="https://www.portal.reinvent.awsevents.com/connect/search.ww#loadSearch-searchPhrase=CloudFormation&amp;searchType=session&amp;tc=0&amp;sortBy=abbreviationSort&amp;p=">session catalog</a>, followed by the full list of CloudFormation-focused sessions and workshops to help you plan your week in Las Vegas.</p> 
<p><span id="more-1585"></span></p> 
<b id="breakout-sessions">Breakout Sessions</b> 
<p>Breakout sessions are the traditional, 60 minute, lecture-style content format.</p> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14428">DEV317 – Deep Dive on AWS CloudFormation</a> <em>The AWS CloudFormation team guides you through techniques used for creating modular templates and and considerations for governance.</em></li> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16239">DEV318 – Learn How Intuit Built a Frictionless Infrastructure Management System Using AWS CloudFormation</a> <em>Intuit shows you how they built a standardized serverless solution using AWS CloudFormation to manage infrastructure as code.</em></li> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15142">GPSTEC319 – GPS: Build Once, Deploy Many: Architecting and Building Automated, Reusable Reference Deployments with AWS CloudFormation</a> <em>The AWS Quick Start team shares with you the experience and best practices they’ve gained building over 50 Quick Start reference deployments.</em></li> 
<b id="workshops">Workshops</b> 
<p>Workshops are 2.5 hour, small-scale breakouts where you work in teams to build projects and solve problems on AWS.</p> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14430">DEV336 – Stack Mastery: Create and Optimize Advanced AWS CloudFormationTemplates</a> <em>Take a real-world architecture from a sandbox template to production-ready reusable code.</em></li> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16359">DEV337 – Deploy a Data Lake with AWS CloudFormation</a> <em>You will learn how to build AWS CloudFormation templates using proven methods and best practices to deploy a fully functional data lake architecture.</em></li> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14629">SID312 – DevSecOps Capture the Flag</a> <em>Improve your DevSecOps skills in this Capture the Flag style workshop. Earn points by enforcing policy via CloudFormation static analysis.</em></li> 
<b id="other-relevant-sessions">Other relevant sessions</b> 
<p>While the following sessions aren’t CloudFormation specific, they will show you mature patterns for infrastructure management using CloudFormation alongside other AWS services.</p> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14965">DEV324 – Deep Dive on Advanced Continuous Delivery Techniques Using AWS DevOps Tools</a></li> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15199">DEV340 – How Amazon.com Uses AWS Management Tools</a></li> 
<li><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14521">MSC201 – Building end-to-end IT Lifecycle Mgmt &amp; Workflows with AWS Service Catalog</a></li> 
<p>You can <a href="https://www.portal.reinvent.awsevents.com/connect/publicDashboard.ww">log in and reserve seats for any of these sessions</a> now.</p> 
<p>In addition, the AWS CloudFormation Developer Advocates will be presenting a series of CloudFormation focused Dev Chats on the Expo floor on Wednesday and Thursday. Stop by the Dev Lounge in the Expo Hall for exact times.</p> 
<p>And finally, you can come chat with any of the Management Tools team at the AWS booth all week long.</p> 
<p>See you at re:Invent!</p> 
<hr /> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14479">AMF301 – Big Data &amp; Analytics for Manufacturing Operations</a></strong><br /> Manufacturing companies collect vast troves of process data for tracking purposes. Using this data with advanced analytics can optimize operations, saving time and money. In this session, we explore the latest analytics capabilities to support your goals for optimizing the manufacturing plant floor. Learn how to build dashboards that connect to prediction models driven by sensors across manufacturing processes. Learn how to build a data lake on AWS, using services and techniques such as AWS CloudFormation, Amazon EC2, Amazon S3, AWS Identity and Access Management, and AWS Lambda. We also review a reference architecture that supports data ingestion, event rules, analytics, and the use of machine learning for manufacturing analytics.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16676">CMP216 – Use Amazon EC2 Spot Instances to Deploy a Deep Learning Framework on Amazon ECS</a></strong><br /> Deep learning, an implementation of machine learning, uses neural networks to solve complex problems like computer vision, natural language processing, and recommendations. Deep learning libraries and frameworks enable developers to enhance the capabilities of their applications and projects. In this workshop, learn how to build and deploy a powerful deep learning framework, Apache MXNet, on containers. The portability and resource management benefit of containers enables developers to focus less on infrastructure and more on building. The lab first demonstrates the automation capabilities of AWS CloudFormation to stand up core infrastructure. We also leverage Spot Fleet for the cost benefit of using Spot Instances, especially important for developer environments. Next we create an MXNet container in Docker and deploy it with Amazon ECS. Finally, we explore image classification with MXNet to validate that everything is working as expected.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16740">CON319 – Interstella 8888: CICD for Containers on AWS</a></strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. Join this workshop to learn how to set up a CI/CD pipeline for containerized microservices. You’ll get hands-on experience deploying Docker container images using Amazon ECS, AWS CloudFormation, AWS CodeBuild, and AWS CodePipline, automating everything from code check-in to production. AWS credits are provided. Bring your laptop, and have an active AWS account.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14428">DEV317 – Deep Dive on AWS CloudFormation</a></strong></p> 
<p>AWS CloudFormation enables developers and system administrators to harness the power of infrastructure-as-code. As organizations adopt AWS CloudFormation for workload deployments, common patterns emerge and opportunities to streamline deployments become evident. Using AWS CloudFormation support for nested templates, customers can further streamline the creation of new workloads as code through modular reuse. This session guides you through some of the techniques used for creating modular AWS CloudFormation templates, and considerations for design and governance to empower departments and teams to own the architectures.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16239">DEV318 – Learn How Intuit Built a Frictionless Infrastructure Management System Using AWS CloudFormation</a></strong></p> 
<p>Managing Infrastructure as Code (IaC) successfully within an organization is a challenge. Regardless of team size, it can turn into a patchwork of solutions causing difficulties collaborating among individuals and teams. Intuit has faced and learned from these challenges, while coordinating among different teams running workloads that provide solutions for different business units. We developed a system that improved our development process for IaC using AWS CloudFormation. In this session, we demonstrate how to move away from an inconsistent development of infrastructure by complementing common development practices with a solution using the serverless technologies from AWS. We walk through our journey and help you discover an approach to assemble a similar solution for your organization.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14965">DEV324 – Deep Dive on Advanced Continuous Delivery Techniques Using AWS DevOps Tools</a></strong><br /> Continuous delivery (CD) enables teams to be more agile and quickens the pace of innovation. Too often, however, teams adopt CD without putting the right safety mechanisms in place. In this talk, we discuss opportunities for you to transform your software release process into a safer one. We explore various DevOps best practices, showcasing sample applications and code. We discuss how to set up delivery pipelines with nonproduction testing stages, failure cases, rollbacks, machine and Availability Zone redundancy, canary testing and deployments, and monitoring. We’ll use AWS Lambda, AWS CloudFormation, AWS CodePipeline, AWS CodeDeploy, and both Amazon CloudWatch alarms and events.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14778">DEV332 – Using AWS to Achieve Both Autonomy and Governance at 3M</a></strong><br /> There is a constant tension between empowering teams to be agile through autonomy and enforcing governance policies to maintain regulatory compliance. Hear from Nathan Scott, Senior Consultant at AWS and James Martin, Automation Engineering Manager at 3M on how they have achieved both autonomy and governance through self-service automation tools on AWS. Learn how to avoid pitfalls with building the CI/CD team, right sizing and how to address. This session will also feature a demo from Casey Lee, Chief Architect at Stelligent on the tools used to accomplish this for 3M, including AWS Service Catalog, AWS CloudFormation, AWS CodePipeline and Cloud Custodian, an open source tool for managing AWS accounts.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14430">DEV336 – Stack Mastery: Create and Optimize Advanced AWS CloudFormationTemplates</a></strong><br /> AWS CloudFormation gives you an easy way to define your infrastructure as code. But are you using it to its full potential? In this workshop, we take real-world architecture from a sandbox template to production-ready reusable code. We start by reviewing an initial template, which you update throughout the session to incorporate AWS CloudFormation features, like nested stacks and intrinsic functions. By the end of this workshop, expect to have a set of AWS CloudFormation templates that demonstrate the same best practices used in AWS Quick Starts.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16359">DEV337 – Deploy a Data Lake with AWS CloudFormation</a></strong><br /> AWS CloudFormation provides many features to automate the provisioning of infrastructure for all types of complex applications. In this workshop, you will learn how to build AWS CloudFormation templates using proven methods and best practices. You will also deploy a fully functional data lake architecture, which uses AWS services like Amazon RDS and open source components like Apache Zeppelin. The labs will demonstrate the capabilities of AWS CloudFormation to stand up infrastructure in a modular way, walk through the deployment of a complex end-to-end application, and validate that all components of the application are working.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15199">DEV340 – How Amazon.com Uses AWS Management Tools</a></strong><br /> Amazon.com enables all of its developers to be productive on AWS by operating across tens-of-thousands of team-owned AWS accounts, all while raising the bar on security, visibility and operational control. Amazon has been able to achieve these seemingly conflicting ideals by automating setup and management of these accounts at scale using AWS Management Tools such as CloudFormation, Config, CloudTrail, CloudWatch and EC2 Systems Manager. In this session, discover more about how Amazon.com built ASAP using AWS Management tools, and understand some of the decisions they made as their usage of AWS evolved over time. You will learn about the design, architecture and implementation that Amazon.com went through as part of this effort.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14951">ENT326 – Oracle Enterprise Solutions on AWS</a></strong><br /> Oracle enterprise applications and middleware such as E-Business Suite, PeopleSoft, Siebel, and WebLogic are central to many IT departments. They often require complex deployments that can greatly benefit from the flexibility, scalability, and security of the cloud. In this session, we discuss architecture patterns and best practices for migrating these applications to and running these applications on AWS. We cover how to work with Oracle enterprise applications and multiple services including Amazon RDS, AWS Database Migration Service, Amazon Elastic File System, and AWS CloudFormation. As part of this, we show examples of successful customer deployments.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15549">GPSCT308 – GPS: Developing and Deploying at the Speed of Light: Automating Serverless Deployments</a></strong><br /> Planning on going serverless, but want to manage it using DevOps-style processes? In this interactive session, we discuss the art of automating and managing deployments of serverless applications on AWS. We cover a range of AWS tools such as AWS CodePipeline, AWS CloudFormation, and AWS Serverless Application Model (AWS SAM), to name just a few.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15142">GPSTEC319 – GPS: Build Once, Deploy Many: Architecting and Building Automated, Reusable Reference Deployments with AWS CloudFormation</a></strong><br /> This session explains how to build reusable, maintainable AWS CloudFormation–based automation for AWS Cloud deployments. We have built over 50 Quick Start reference deployments with partners and customers, and will share this expertise with you. We explore the anatomy of a typical AWS CloudFormation template, dive deep into best practices for building Quick Start automation across Linux and Windows and explore useful design patterns. This expert-level session is for partners interested in building Quick Starts or other AWS CloudFormation–based automation. It requires familiarity with Git, shell scripting, Windows PowerShell, and AWS services like Amazon EC2, Amazon S3 and AWS CloudFormation.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15749">HLC307 – Building a Secure and Healthcare-Compliant Platform for Adopting a Cloud-First Strategy Using AWS</a></strong><br /> This session provides an overview of how Change Healthcare invested in people, process, and an automation platform to adopt a cloud-first strategy. Starting from building a Cloud Center of Excellence team, they identified the compliance, security, and cost optimization requirements and process required to build a framework. They also embedded healthcare compliance, security, architecture best practices, and customer-specific rules and standards for a managed adoption of the cloud. Change Healthcare is leveraging their Cloud 2.0 framework to rapidly deploy their mission applications into AWS. Come learn how Change Healthcare built a serverless architecture using Amazon ECS, AWS Lambda, AWS CodeDeploy, AWS CodeCommit, AWS CloudFormation, AWS Service Catalog, AWS OpsWorks, AWS Elastic Beanstalk, and other managed services.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15746">LFS307 – Becoming a Dynamic Pharma Marketing Organization Using AWS</a></strong><br /> Pharmaceutical company processes tend to be slow when dealing with customer-facing applications that contain FDA-validated messages, all while maintaining infrastructure and security standards. In this session, discover how Mylan, a US–based global generic and specialty pharmaceutical company, overcame these obstacles and provided scalable solutions by leveraging AWS DevOps methods that lower time to market, while maintaining robust security and release management practices. During the presentation, learn how Mylan redefined process models such as infrastructure change management to define new security and process models. Additionally, learn how Mylan used services like Amazon S3, Elastic Load Balancing (ELB), and AWS CloudFormation to define these new models.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15681">LFS308 – Building Data Lakes for Life Sciences Organizations</a></strong><br /> In this chalk talk, we cover the implementation of data lakes for life sciences organizations, such as Amgen and Merck, that are looking to glean new insights from their existing and new clinical data. AWS life sciences solution architects show how to build a data lake on AWS using services and techniques such as AWS CloudFormation, Amazon EC2, Amazon S3, IAM, and AWS Lambda.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15250">MBL308 – Integrating Video in Mobile Apps and Websites</a></strong><br /> In this session, we will build a highly scalable mobile app, website, and serverless mobile backend architecture that demonstrates on-demand video streaming, adaptive multi-bitrate transcoding, and video content ingestion. We use AWS Lambda and Amazon Elastic Transcoder to automatically convert high resolution videos upon upload, Amazon CloudFront to stream video content to devices using network-aware adaptive multi-bitrate protocols (such as HLS), Amazon Cognito to authenticate users, and AWS Mobile Hub and AWS CloudFormation to automate setting up the required resources.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15878">MCL318 – Deep Dive on Amazon Rekognition Architectures for Image Analysis</a></strong><br /> Join us for a deep dive on how to use Amazon Rekognition for real world image analysis. Learn how to integrate Amazon Rekognition with other AWS services to make your image libraries searchable. Also learn how to verify user identities by comparing their live image with a reference image, and estimate the satisfaction and sentiment of your customers. We also share best practices around fine-tuning and optimizing your Amazon Rekognition usage and refer to AWS CloudFormation templates.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14521">MSC201 – Building end-to-end IT Lifecycle Mgmt &amp; Workflows with AWS Service Catalog</a></strong><br /> In this session, you’ll learn how to leverage AWS Service Catalog, AWS Lambda, AWS Config and AWS CloudFormation to create a robust, agile environment while maintaining enterprise standards, controls and workflows. Fannie Mae demonstrates how they are leveraging this solution to integrate with their existing workflows and CMDB/ITSM systems to create an end-to-end automated and agile IT lifecycle and workflow.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14962">SID206 – Best Practices for Managing Security Operation on AWS</a></strong><br /> To help prevent unexpected access to your AWS resources, it is critical to maintain strong identity and access policies and track, effectively detect, and react to changes. In this session you will learn how to use AWS Identity and Access Management (IAM) to control access to AWS resources and integrate your existing authentication system with IAM. We will cover how to deploy and control AWS infrastructure using code templates, including change management policies with AWS CloudFormation. Further, effectively detecting and reacting to changes in posture or adverse actions requires the ability to monitor and process events. There are several services within AWS that enable this kind of monitoring such as CloudTrail, CloudWatch Events, and the AWS service APIs. We learn how Netflix utilizes a combination of these services to operationalize monitoring of their deployments at scale, and discuss changes made as Netflix’s deployment has grown over the years.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14629">SID312 – DevSecOps Capture the Flag</a></strong><br /> In this Capture the Flag workshop, we divide groups into teams and work on AWS CloudFormation DevSecOps. The AWS Red Team supplies an AWS DevSecOps Policy that needs to be enforced via CloudFormation static analysis. Participant Blue Teams are provided with an AWS Lambda-based reference architecture to be used to inspect CloudFormation templates against that policy. Interesting items need to be logged, and made visible via ChatOps. Dangerous items need to be logged, and recorded accurately as a template fail. The secondary challenge is building a CloudFormation template to thwart the controls being created by the other Blue teams. Throughout the session your DevSecOps static analysis will be tested by increasingly difficult CloudFormation templates from the AWS Red Team, with accurate detection being rewarded with points. Finally, we test all teams’ protection against every other team’s malicious template to see which Blue team’s static analysis was most effective.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=14785">SID317 – Automating Security and Compliance Testing of Infrastructure-as-Code for DevSecOps</a></strong><br /> Infrastructure-as-Code (IaC) has emerged as an essential element of organizational DevOps practices. Tools such as AWS CloudFormation and Terraform allow software-defined infrastructure to be deployed quickly and repeatably to AWS. But the agility of CI/CD pipelines also creates new challenges in infrastructure security hardening. How do you ensure that your CloudFormation templates meet your organization’s security, compliance, and governance needs before you deploy them? How do you deploy infrastructure securely to production environments, and monitor the security posture on a continuous basis? And how do you do this repeatedly without hitting a speed bump? This session provides a foundation for how to bring proven software hardening practices into the world of infrastructure deployment. We discuss how to build security and compliance tests for infrastructure analogous to unit tests for application code, and showcase how security, compliance and governance testing fit in a modern CI/CD pipeline. Session Sponsored by: Dome9</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16300">SID327 – How Zocdoc Achieved Security and Compliance at Scale With Infrastructure as Code</a></strong><br /> In less than 12 months, Zocdoc became a cloud-first organization, diversifying their tech stack and liberating data to help drive rapid product innovation. Brian Lozada, CISO at Zocdoc, and Zhen Wang, Director of Engineering, provide an overview on how their teams recognized that infrastructure as code was the most effective approach for their security policies to scale across their AWS infrastructure. They leveraged tools such as AWS CloudFormation, hardened AMIs, and hardened containers. The use of DevSecOps within Zocdoc has enhanced data protection with the use of AWS services such as AWS KMS and AWS CloudHSM and auditing capabilities, and event-based policy enforcement with Amazon Elasticsearch Service and Amazon CloudWatch, all built on top of AWS.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=15779">SID347 – Securely Automating DevOps on AWS</a></strong><br /> In some organizations, the theme of “can’t we all just get along” accurately describes the relationship between DevOps and network security. DevOps operates at a rapid and dynamic pace, taking advantage of the cloud to create and deploy. Security teams exercise industry best practices of policy change control to eliminate potential security holes. Inevitably, deployment challenges arise. In this session, you learn how to automate the deployment of next-generation security to protect DevOps environments on AWS. Topics covered include “touchless” deployment of a fully-configured firewall using AWS CloudFormation templates and AWS Lambda, consuming AWS tags to execute commitless policy updates, using Amazon CloudWatch and Elastic Load Balancing to deliver scalability and resiliency. Come and learn about the next generation of security, operating at the speed of the cloud. Session sponsored by Palo Alto Networks</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=17593">SPL09 – Launching and Managing a Web Application with AWS CloudFormation</a></strong><br /> In this lab, you will learn how to use AWS CloudFormation to provision and update a web application with a number of supporting AWS products and services, including Auto Scaling groups, Amazon Elastic Compute Cloud (EC2) instances, and Elastic Load Balancing.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16411">WIN309 – How to Optimize AWS Architectures for SharePoint Deployments</a></strong><br /> AWS can help you rapidly deploy and scale your Microsoft SharePoint environment to help you collaborate more efficiently and cost-effectively. This session reviews architectural considerations for building a SharePoint deployment on AWS, best practices to ensure optimal performance, how to leverage multiple Availability Zones for high availability and disaster recovery, and how to integrate with Active Directory. We also look at new Quick Start guides, AWS CloudFormation templates, and other tools that dramatically reduce the time to deployment. Our Windows experts discuss the best ways to deploy and run SharePoint on AWS.</p> 
<p><strong><a href="https://www.portal.reinvent.awsevents.com/connect/sessionDetail.ww?SESSION_ID=16410">WIN312 – Deploying .NET Application CI/CD Pipelines on AWS</a></strong> In this session, we look at the AWS services that customers are using to build and deploy Microsoft-based solutions that use technologies like Windows, .NET, SQL Server, and PowerShell. We start by showing you how to build a Windows-based CI/CD pipeline on AWS using AWS CodeDeploy, AWS CodePipeline, AWS CloudFormation, and PowerShell using an AWS Quick Start. With new integrations, such as the AWS Tools for VSTS, you have more options than ever. We also cover best practices for creating templates that let you automatically deploy ready-to-use Windows products by using services and tools like AWS CloudFormation, PowerShell, and Git. Our .NET experts discuss the best practices for implementing a .NET CI/CD pipeline with AWS services.</p> 
<hr /> 
<h3><img class="size-full wp-image-1591 alignleft" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/23/BadgePhoto-blog.jpg" alt="" width="128" height="160" />About the Author</h3> 
<p>Chuck Meyer&nbsp;is a Senior Developer Advocate for AWS CloudFormation based in New York.&nbsp; He&nbsp;spends his time&nbsp;working with&nbsp;both&nbsp;external and internal development teams to constantly improve the developer experience for CloudFormation users.&nbsp; He’s a live music true believer and spends as much time as possible playing bass and watching bands.</p> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/aws-cloudformation/" rel="tag">AWS CloudFormation</a>, <a href="https://aws.amazon.com/blogs/mt/tag/reinvent/" rel="tag">re:Invent</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Amazon EC2 Systems Manager Parameter Store adds support for Parameter versions</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Venkat Krish</span></span> | on 
<time property="datePublished" datetime="2017-10-26T12:45:28+00:00">26 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/amazon-ec2-systems-manager-parameter-store-adds-support-for-parameter-versions/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>By Lou de la Torre, AWS Partner Solutions Architect and </em><em>Venkat Krishnamachari, Principal Product Manager, Amazon EC2 Systems Manager</em></p> 
<p>Today we are excited to announce versioning support for Amazon EC2 Systems Manager Parameter Store. With Parameter Store versioning support, each iteration of a parameter is assigned a unique version number at creation time. These individual version numbers can be easily referenced in API actions and Systems Manager Documents. By default, the latest value of the parameter will be returned when no version is specified.</p> 
<h3>Parameter Store</h3> 
<p><a href="https://aws.amazon.com/ec2/systems-manager/parameter-store/">Parameter Store</a> is part of <a href="https://aws.amazon.com/ec2/systems-manager/parameter-store/">Amazon EC2 Systems Manager</a>.&nbsp;It provides a centralized, encrypted store to manage your configuration data, whether it is plain text data (database strings) or secure strings and secrets (such as passwords, and API keys). Because Parameter Store is available through the AWS CLI, APIs, and SDKs, you can easily reference parameters across AWS services such as AWS Lambda and Amazon EC2 Container Service (ECS).</p> 
<p>For additional posts on Parameter Store, see:</p> 
<p><a href="https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/">The Right Way to Store Secrets using Parameter Store</a></p> 
<p><a href="https://aws.amazon.com/blogs/compute/managing-secrets-for-amazon-ecs-applications-using-parameter-store-and-iam-roles-for-tasks/">Managing Secrets for Amazon ECS Applications Using Parameter Store and IAM Roles for Tasks</a></p> 
<p><a href="https://aws.amazon.com/blogs/mt/organize-parameters-by-hierarchy-tags-or-amazon-cloudwatch-events-with-amazon-ec2-systems-manager-parameter-store/">Organize Parameters by Hierarchy, Tags, or Amazon CloudWatch Events with Amazon EC2 Systems Manager Parameter Store</a></p> 
<p><span id="more-1531"></span></p> 
<h3>Parameter Store Versioning</h3> 
<p>Versioning provides an additional layer of protection for your Parameter Store values. For example, if code deployment fails you can easily roll back and reference older versions of config data saved as parameters in the Parameter Store. You can recover from unintended user errors that caused an overwrite in your parameter value. You can also use versioning to keep track of the number of times your stored values changed over the parameter’s lifetime for auditing purposes (see Figure 1).</p> 
<p><img class="alignnone wp-image-1599 size-large" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/23/Figure1-1-1024x546.png" alt="" width="640" height="341" /></p> 
<p>By default, the initially created parameters’ version is 1. Versions are incremented automatically by increments of 1 whenever a value is updated in the Parameter Store. To demonstrate the value of Parameter Store versioning, consider the following scenario.</p> 
<p>In an effort to minimize management overhead you decide to migrate your .NET application back-end SQL database from SQL on EC2 to RDS SQL. This will require that you deploy new code to your .NET application to update the database connection string. As with any migration, you want to ensure you can quickly rollback in case of failure.</p> 
<p>With Parameter Store versioning you can quickly rollback by performing the following steps:</p> 
<ol> 
<li>Create a new Parameter pointing to the existing database string (SQL on EC2)</li> 
<li>Create a new version of the Parameter pointing to the new database string (RDS SQL)</li> 
<li>Update your code with a reference to the latest or Default version of the parameter</li> 
<li>Migrate your database from SQL on EC2 to RDS SQL</li> 
<li>Deploy your code updating the .NET application to point to the new SQL database running on RDS via the latest or Default version of the parameter</li> 
<li>If any issues arise, simply update your code with the original version of the Parameter pointing your .NET application back to the original SQL on EC2 instance and re-deploy</li> 
</ol> 
<p>Let’s take a look at how easily you can make that happen by first creating a Parameter, then updating the parameter, viewing all existing versions of the Parameter, retrieving a Parameter by specific version number and finally rolling back to the original version of the Parameter. To do this you can use either the AWS CLI or the AWS Tools for Windows PowerShell. We will walk you through using both.</p> 
<p><strong>Step 1. Create a Parameter</strong></p> 
<p>Execute the following command to create a Parameter using the AWS CLI:</p> 
<pre><code class="lang-bash">aws ssm put-parameter --name &quot;/Prod/dotnet&quot; --type String --value &quot;ec2-13-57-12-38.us-west-1.compute.amazonaws.com:1433&quot;</code></pre> 
<p>or you can use&nbsp;the AWS Tools for Windows PowerShell:</p> 
<pre><code class="lang-powershell">Write-SSMParameter -Name &quot;/Prod/dotnet&quot; -Value &quot;ec2-13-57-12-38.us-west-1.compute.amazonaws.com:1433&quot; -Type &quot;String&quot;</code></pre> 
<p><strong>Step 2. Update the Parameter</strong></p> 
<p>Execute the following command to update the parameter using the AWS CLI (note the change in value and the overwrite option):</p> 
<pre><code class="lang-bash">aws ssm put-parameter --name &quot;/Prod/dotnet&quot; --type String --value &quot;dotnet.ctvzltftaz4x.us-west-1.rds.amazonaws.com:1433&quot; --overwrite</code></pre> 
<p>Or you can use the AWS Tools for Windows PowerShell (note the change in value and the overwrite option):</p> 
<pre><code class="lang-powershell">Write-SSMParameter -Name &quot;/Prod/dotnet&quot; -Value &quot;dotnet.ctvzltftaz4x.us-west-1.rds.amazonaws.com:1433&quot; -Type &quot;String&quot; -Overwrite $true</code></pre> 
<p><strong>Step 3. View all existing Versions of the Parameter</strong></p> 
<p>Execute the following command to view all existing versions of the Parameter using the CLI:</p> 
<pre><code class="lang-bash">aws ssm get-parameter-history --name “/Prod/dotnet”</code></pre> 
<p>The System returns information similar to the following:</p> 
<pre><code class="lang-bash">PS C:\&gt; aws ssm get-parameter-history --name “/Prod/dotnet”
{
&quot;Parameters&quot;: [
{
&quot;LastModifiedUser&quot;: &quot;arn:aws:iam&quot;, 
&quot;LastModifiedDate&quot;: 1507742527.826, 
&quot;Type&quot;: &quot;String&quot;, 
&quot;Name&quot;: &quot;/Prod/dotnet&quot;, 
&quot;Value&quot;: &quot;ec2-13-57-12-38.us-west-1.compute.amazonaws.com:1433&quot;
&quot;Version&quot;: 1
}, 
{
&quot;LastModifiedUser&quot;: &quot;arn:aws:iam&quot;, 
&quot;LastModifiedDate&quot;: 1507743165.366, 
&quot;Type&quot;: &quot;String&quot;, 
&quot;Name&quot;: &quot;/Prod/dotnet&quot;, 
&quot;Value&quot;: &quot;dotnet.ctvzltftaz4x.us-west-1.rds.amazonaws.com:1433&quot;
&quot;Version&quot;: 2
}
]
}</code></pre> 
<p>or you can execute the following command to view all existing versions of the Parameter using the AWS Tools for Windows PowerShell:</p> 
<p><code class="lang-powershell"></code></p> 
<pre><code class="lang-powershell">Get-SSMParameterHistory -Name &quot;/Prod/dotnet&quot;</code></pre> 
<p>The System returns information similar to the following:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-bash">PS C:\&gt; Get-SSMParameterHistory -Name &quot;/Prod/dotnet&quot;
Description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; :
KeyId&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; :
LastModifiedDate : 10/11/2017 5:22:07 PM
LastModifiedUser: arn:aws:iam
Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : /Prod/dotnet
Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : String
Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : ec2-13-57-12-38.us-west-1.compute.amazonaws.com:1433
Version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  : 1
Description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : 
KeyId&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; :
LastModifiedDate : 10/11/2017 5:32:45 PM
LastModifiedUser : arn:aws:iam
Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : /Prod/dotnet
Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : String
Value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; : dotnet.ctvzltftaz4x.us-west-1.rds.amazonaws.com:1433
Version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  : 2</code></pre> 
<p><strong>Step 4. Retrieve the Parameter</strong></p> 
<p>Use the following AWS CLI to retrieve parameters:</p> 
<p>Execute the following command to retrieve the latest version of the Parameter (default):</p> 
<pre><code class="lang-bash">aws ssm get-parameters --names “/Prod/dotnet”</code></pre> 
<p>The System returns information similar to the following:</p> 
<pre><code class="lang-bash">PS C:\&gt; aws ssm get-parameters --name “/Prod/dotnet”
{
&nbsp;&nbsp;&nbsp; &quot;InvalidParameters&quot;: [],
&nbsp;&nbsp;&nbsp; &quot;Parameters&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Type&quot;: &quot;String&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Name&quot;: &quot;/Prod/dotnet&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Value&quot;: &quot;dotnet.ctvzltftaz4x.us-west-1.rds.amazonaws.com:1433&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Version&quot;: 2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; ]
}</code></pre> 
<p><code class="lang-bash"></code></p> 
Execute the following command to retrieve a specific version of the Parameter (by version number): 
<pre><code class="lang-bash">aws ssm get-parameters --names “/Prod/dotnet:1&quot;</code></pre> 
<p>The System returns information similar to the following:</p> 
<pre>PS C:\&gt; aws ssm get-parameters --region us-west-1 --name “/Prod/dotnet”</pre> 
<pre><code class="lang-bash">
{
&nbsp;&nbsp;&nbsp; &quot;InvalidParameters&quot;: [],
&nbsp;&nbsp;&nbsp; &quot;Parameters&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Type&quot;: &quot;String&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Name&quot;: &quot;/Prod/dotnet&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Value&quot;: &quot;ec2-13-57-12-38.us-west-1.compute.amazonaws.com:1433&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Version&quot;: 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; ]
}</code></pre> 
<p><code class="lang-bash"></code></p> 
Note the difference in values. 
<p>or using the&nbsp;the AWS Tools for Windows PowerShell, you can execute the following command to retrieve the latest version of the Parameter (default):</p> 
<p><code class="lang-powershell">(Get-SSMParameterValue -Names &quot;/Prod/dotnet&quot;).Parameters | fl</code></p> 
<p>The System returns information&nbsp;similar to the following:</p> 
<pre><code class="lang-bash">PS C:\&gt; (Get-SSMParameterValue -Name &quot;/Prod/dotnet&quot;).Parameters | fl
Name&nbsp; &nbsp;&nbsp;&nbsp;: /Prod/dotnet
Type&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: String
Value &nbsp;&nbsp;&nbsp;&nbsp;: dotnet.ctvzltftaz4x.us-west-1.rds.amazonaws.com:1433
Version&nbsp; : 2
</code></pre> 
<p>Execute the following command to retrieve a specific version of the Parameter (by version number):</p> 
<pre><code class="lang-powershell">(Get-SSMParameterValue -Names &quot;/Prod/dotnet:1&quot;).Parameters | fl</code></pre> 
<p>The system returns information similar to the following:</p> 
<pre><code class="lang-bash">PS C:\&gt; (Get-SSMParameterValue -Name &quot;/Prod/dotnet:1&quot;).Parameters | fl
Name&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;: /Prod/dotnet
Type&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: String
Value &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: ec2-13-57-12-38.us-west-1.compute.amazonaws.com:1433
Version&nbsp;&nbsp; : 1
</code></pre> 
<p>Note the difference in values.</p> 
<p>To roll back your .NET application to point to the original SQL on EC2 instance, simply update your code to reference the previous version of the Parameter and re-deploy.</p> 
<p>You can reference Parameter Store versioning in Systems Manager Documents as well, as show in the following example:</p> 
<p><strong>Systems Manager AWS-RunShellScript example</strong></p> 
<p>The default value for commands is referenced with version 2 of SSM parameter ‘runcommand’.</p> 
<pre><code class="lang-bash">{
&quot;schemaVersion&quot;:&quot;1.2&quot;,
&quot;description&quot;:&quot;Run a shell script or specify the commands to run.&quot;,
&quot;parameters&quot;:{
&quot;commands&quot;:{
&quot;type&quot;:&quot;StringList&quot;,
&quot;description&quot;:&quot;(Required) Specify a shell script or a command to run.&quot;,
&quot;minItems&quot;:1,
&quot;displayType&quot;:&quot;textarea&quot;
&quot;default&quot;:&quot;{{ssm:runcommand:2}}&quot;
},
&quot;executionTimeout&quot;:{
&quot;type&quot;:&quot;String&quot;,
&quot;default&quot;:&quot;3600&quot;,
&quot;description&quot;:&quot;(Optional) The time in seconds for a command to complete before it is considered to have failed. Default is 3600 (1 hour). Maximum is 28800 (8 hours).&quot;,
&quot;allowedPattern&quot;:&quot;([1-9][0-9]{0,3})|(1[0-9]{1,4})|(2[0-7][0-9]{1,3})|(28[0-7][0-9]{1,2})|(28800)&quot;
}
},
&quot;runtimeConfig&quot;:{
&quot;aws:runShellScript&quot;:{
&quot;properties&quot;:[
{
&quot;id&quot;:&quot;0.aws:runShellScript&quot;,
&quot;runCommand&quot;:&quot;{{ commands }}&quot;,
&quot;timeoutSeconds&quot;:&quot;{{ executionTimeout }}&quot;
}
]
}
}
}</code></pre> 
<p><strong>Summary</strong><br /> Parameter Store provides a centralized, encrypted store to manage your configuration data, whether it is plain text data (database strings) or secure strings and secrets (such as passwords, and API keys). Use versioning to add an extra layer of protection for your Parameter Store values.&nbsp;This new feature is available now and you can start using it today!</p> 
<p><strong>About the author</strong></p> 
<p><img class="wp-image-1600 size-thumbnail alignleft" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/23/LouDelat-150x150.jpg" alt="" width="150" height="150" />Lou De La Torre is a Partner Solutions Architect with Amazon Web Services. Lou is responsible for assisting Partners and Customers alike with their AWS for Windows architectures and migration strategies. With a career in information technology that spans more than two decades, Lou brings a significant amount of expertise in cloud and systems architecture, systems management, disaster recovery, process improvement and compliance management. Lou consistently strives to ensure that he is delivering solutions that align with the needs and requirements of his customer’s business objectives, while alleviating any pain points they may be experiencing in their IT operations.</p> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/aws-systems-manager/" rel="tag">AWS Systems Manager</a>, <a href="https://aws.amazon.com/blogs/mt/tag/configuration-secrets/" rel="tag">Configuration Secrets</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-systems-manager/" rel="tag">EC2 Systems Manager</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-windows/" rel="tag">EC2 Windows</a>, <a href="https://aws.amazon.com/blogs/mt/tag/parameter-store/" rel="tag">Parameter Store</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">OpsWorks for Chef Automate – Automatically Bootstrapping Nodes in Different Accounts</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Darko Meszaros</span></span> | on 
<time property="datePublished" datetime="2017-10-24T23:23:40+00:00">24 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-ops-works/" title="View all posts in AWS OpsWorks*"><span property="articleSection">AWS OpsWorks*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/opsworks-for-chef-automate-automatically-bootstrapping-nodes-in-different-accounts/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Lots of us today are managing multiple AWS accounts. Although having multiple accounts can bring you &nbsp;benefits, such as more granular control of resources and access, decentralized control, and simpler billing. Multiple accounts can also introduce some challenges. A challenge we face in this blog post is having a centralized configuration management server with its nodes spread throughout other AWS accounts.</p> 
<h3>Goal</h3> 
<p>Let’s picture working for a company where various teams own Amazon EC2 instances in their own respective AWS accounts. We have a team dedicated to managing the configuration of all EC2 instances (nodes) across all of the company’s AWS accounts. The configuration management is performed by an <a href="https://aws.amazon.com/opsworks/chefautomate/">OpsWorks for Chef Automate</a> server. How do we go about bootstrapping these nodes to the Chef Automate server?</p> 
<p>I’ll show you how you can bootstrap nodes in other AWS accounts to a single/centralized OpsWorks for Chef Automate server. To be precise, I’ll bootstrap an <a href="https://aws.amazon.com/amazon-linux-ami/">Amazon Linux</a> node in AWS account B to an OpsWorks for Chef Automate server in Account A. To bootstrap the instance I’ll use the user data provided in the Chef Automate starter kit we have downloaded during the creation of the server. This user data script uses the built-in AWS API calls to bootstrap your node.</p> 
<p><span id="more-1630"></span></p> 
<h3>A few notes before we start</h3> 
<p>In this tutorial I assume that you don’t have <a href="https://aws.amazon.com/documentation/iam/">IAM</a> roles created for your Chef nodes, so we are creating everything from scratch. If, in fact, you do have roles configured, then just focus on the policy part of this tutorial. Also, we assume that you have the AWS CLI set up on your workstation with adequate privileges. Additionally, make sure all the ARNs mentioned here have the correct account IDs.</p> 
<h3>Step 1: Set up IAM permissions on account A, our Chef Automate account</h3> 
<p>First, we’ll create a role in account A that can be assumed by a node in account B. This role gives the node the necessary permissions to bootstrap itself with the Chef Automate server.</p> 
<p>A) Create a policy that allows us to associate the nodes with our Chef Automate server. Create the policy using the following document:<code> owca_allow_associate.json</code></p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Action&quot;: [
&quot;opsworks-cm:AssociateNode&quot;,
&quot;opsworks-cm:DescribeNodeAssociationStatus&quot;
],
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Resource&quot;: [
&quot;*&quot;
]
}
]
}</code></pre> 
<p>B)&nbsp;Create the policy with that document, using the AWS CLI:</p> 
<pre><code class="lang-bash">aws iam create-policy 
--policy-name OWCA-AllowAssociate 
--policy-document file://owca_allow_associate.json</code></pre> 
<p>C)&nbsp;Create a role that can be assumed by the other account (Account B – where the nodes are). Create that role using the following document: <code>owca_trust_policy.json</code></p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;AWS&quot;: &quot;arn:aws:iam::&lt;NODE-ACCOUNT-ID&gt;:root&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}]
}</code></pre> 
<p>D) Now, let’s use the AWS CLI to create the role:</p> 
<pre><code class="lang-bash">aws iam create-role 
--role-name CrossAccount-OWCA 
--assume-role-policy-document file://./owca_trust_policy.json </code></pre> 
<p>E) And finally, attach the policy we created in step B to the role we created in step D:</p> 
<pre><code class="lang-bash">aws iam attach-role-policy 
--role-name CrossAccountPowerUser 
--policy-arn  arn:aws:iam::&lt;CHEF-ACCOUNT-ID&gt;:policy/OWCA-AllowAssociate </code></pre> 
<h3>Step 2: Set up IAM permissions on account B, our nodes account</h3> 
<p>In this step we create an instance role that will be attached to your node on launch. This role allows the instance to assume a role from account A (the Chef Automate account) so it can associate itself to the Chef Automate server.</p> 
<p>A) To create a role, we first need to create a document: <code>role_owca_associate.json</code></p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;ec2.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}</code></pre> 
<p>B) Create the role with the document, by running AWS CLI:</p> 
<pre><code class="lang-bash">aws iam create-role
--role-name AssociateCrossAccount-owca 
--assume-role-policy-document file://role_owca_associate.json</code></pre> 
<p>C) Create a policy document that allows our instance to assume a role: <code>node_assume_role.json</code></p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: {
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: &quot;sts:AssumeRole&quot;,
&quot;Resource&quot;: &quot;arn:aws:iam::&lt;CHEF-ACCOUNT-ID&gt;:role/CrossAccount-OWCA &quot;
}
}</code></pre> 
<p>D) Create the policy:</p> 
<pre><code class="lang-bash">aws iam create-policy 
--policy-name ChefAssociateAccess 
--policy-document file://./node_assume_role.json</code></pre> 
<p>E) Finally, attach the policy to the role we created earlier:</p> 
<pre><code class="lang-bash">aws iam attach-role-policy 
--role-name CrossAccountPowerUser 
--policy-arn  arn:aws:iam::&lt;NODE-ACCOUNT-ID&gt;:policy/AssociateCrossAccount-owca</code></pre> 
<h3>Step 3: Modifying the user data</h3> 
<p>In this step, we take the generated user data (<code>userdata.sh</code>) from the downloaded OpsWorks for Chef Automate starter_kit.zip and modify it with a few more functions that allow us to bootstrap your nodes across accounts.</p> 
<p>The modifications we are adding here are quite simple. We are just using <code>aws sts assume-role</code> to assume the role that we have created in account A (the Chef server account), and use the temporary credentials it generates to run the <code>associate-node</code> commands, which are required for bootstrapping the node to the Chef Automate server.</p> 
<p>Additionally, we are installing the <a href="https://github.com/stedolan/jq">jq utility for JSON parsing</a> – but this is removed during the cleanup.</p> 
<p>A) Open up the <code>userdata.sh</code> file in a text editor and add the following lines on line 21 (just after the <code>set -e -o pipefail</code> line):</p> 
<pre><code class="lang-bash">set_cli_role() {
yum install -y jq
ASSUMED_ROLE=$(aws sts assume-role --role-arn arn:aws:iam::&lt;CHEF-ACCOUNT-ID&gt;:role/CrossAccount-OWCA --role-session-name owca)
export AWS_ACCESS_KEY_ID=$(echo $ASSUMED_ROLE | jq .Credentials.AccessKeyId | xargs)
export AWS_SECRET_ACCESS_KEY=$(echo $ASSUMED_ROLE | jq .Credentials.SecretAccessKey | xargs)
export AWS_SESSION_TOKEN=$(echo $ASSUMED_ROLE | jq .Credentials.SessionToken | xargs)
}
cleanup_cli_role(){
yum remove -y jq
unset AWS_SESSION_TOKEN
unset AWS_ACCESS_KEY_ID
unset AWS_SECRET_ACCESS_KEY
}</code></pre> 
<p>The first function we create here (<code>set_cli_role</code>) is used for assuming the role from account A, and exporting the temporary credentials to environment variables that will be used by our API calls. The second function is just for cleanup – removing of the <code>jq</code> JSON parser and un-setting the environment variables we set in the function before.</p> 
<p>B) Now on the line 87 insert the <code>set_cli_role</code> function, and on line 93 add the <code>cleanup_cli_role</code>. Your function execution section should go from this:</p> 
<pre><code class="lang-bash">install_aws_cli
node_association_status_token=&quot;$(associate_node)&quot;
install_chef_client
write_chef_config
install_trusted_certs
wait_node_associated &quot;${node_association_status_token}&quot;</code></pre> 
<p>C) To this:</p> 
<pre><code class="lang-bash">install_aws_cli
set_cli_role
node_association_status_token=&quot;$(associate_node)&quot;
install_chef_client
write_chef_config
install_trusted_certs
wait_node_associated &quot;${node_association_status_token}&quot;
cleanup_cli_role</code></pre> 
<p>Here we simply change the execution part of the user data shell script to include our two new functions.</p> 
<p>D) Save the <code>userdata.sh</code> somewhere safe.</p> 
<h3>Step 4: Launch the nodes with the user data script</h3> 
<p>The only thing left now is to run your nodes with the newly edited user data script. We can achieve this either by adding the user data during a manual launch of the EC2 instance, or by setting this on an<a href="https://aws.amazon.com/autoscaling/"> Auto Scaling</a> group launch configuration.</p> 
<h3>Conclusion</h3> 
<p>In this blog post we’ve enabled your nodes from different AWS accounts to bootstrap themselves to a centralized OpsWorks for Chef Automate server that is present in a separate account. We’ve achieved this by creating the required IAM roles and policies, and also by changing the user data that ships with the OpsWorks for Chef Automate starter kit. We’ve done this only for two accounts, but you can modify this to work on any number of accounts.</p> 
<p><strong>About the Author</strong></p> 
<p><img class="size-full wp-image-1647 alignleft" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/24/darko-cropped.jpeg" alt="" width="120" height="137" /><br /> Darko Meszaros is a Cloud Support Engineer who supports customers that use various AWS automation tools, such as AWS OpsWorks, AWS CodeDeploy, and AWS CloudFormation. He is a subject matter expert for OpsWorks and CodeDeploy. Outside of work, he loves collecting video games and old computers.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/aws-opsworks/" rel="tag">AWS OpsWorks</a>, <a href="https://aws.amazon.com/blogs/mt/tag/chef-automate/" rel="tag">Chef Automate</a>, <a href="https://aws.amazon.com/blogs/mt/tag/configuration-management/" rel="tag">Configuration Management</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">AWS CloudFormation Feature Updates: Support for Amazon Athena and Coverage Updates for Amazon S3, Amazon RDS, Amazon Kinesis and Amazon CloudWatch</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Luis Colon</span></span> | on 
<time property="datePublished" datetime="2017-10-18T18:46:07+00:00">18 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/analytics/amazon-athena/" title="View all posts in Amazon Athena*"><span property="articleSection">Amazon Athena*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/analytics/amazon-kinesis/" title="View all posts in Amazon Kinesis*"><span property="articleSection">Amazon Kinesis*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/database/amazon-rds/" title="View all posts in Amazon RDS*"><span property="articleSection">Amazon RDS*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/aws-cloudformation-features-update-support-for-amazon-athena-coverage-updates-for-s3-rds-kinesis-and-cloudwatch/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>As one of the most widely-used services in AWS, CloudFormation continues to expand its feature set&nbsp;by including adding support for Amazon Athena, two new features to protect stacks and control rollback processes, plus several new coverage updates.</p> 
<p>CloudFormation now supports the creation of an <a href="https://aws.amazon.com/athena/">Amazon Athena</a> named query as a <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-athena-namedquery.html">resource</a>. Amazon Athena is a query service that makes it easy to analyze data directly from files stored in S3 using standard SQL statements. Named queries can then be executed manually from the AWS Management Console, CLI or programmatically&nbsp;using API calls.</p> 
<p>You can now create a standard set of named queries&nbsp;using CloudFormation templates. To try it out, you can use some of the sample data provided by Athena, as covered in Jeff Barr’s blog post <a href="https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/">here</a>.</p> 
<p><span id="more-1532"></span></p> 
<p>After&nbsp;you’ve verified that your query properly runs on Athena, use CloudFormation to create a query using the <strong>AWS::Athena::NamedQuery</strong> resource type:</p> 
<pre><code class="lang-yaml">---
#===============================================================================
# Template: athena-named-query.yaml
#
# Purpose:  Creates an Athena Named Query via AWS CloudFormation, using the
#           default data set provided by Athena.
#===============================================================================
AWSTemplateFormatVersion: &quot;2010-09-09&quot;
Description: |
Uses the default data set in Amazon Athena, with a sample elb logs table,
to demonstrate how to create a named query via a CloudFormation template.
This query checks for HTTP response codes and counts each code's occurrence
Resources:
AthenaNamedQuery:
Type: AWS::Athena::NamedQuery
Properties:
Database: &quot;default&quot;
Description: &quot;Select and count HTTP response codes&quot;
Name: &quot;HTTPResponseCodeCount&quot;
QueryString: &gt;
SELECT backend_response_code, count(*)
FROM default.&quot;elb_logs&quot;
GROUP BY backend_response_code;</code></pre> 
<p>This YAML template above executes&nbsp;using a Create Stack call in under a minute. Then, go to the Athena console and find&nbsp;the query you created using CloudFormation under the Saved Queries menu:</p> 
<p>&nbsp;</p> 
<p><img class="wp-image-1534 size-large aligncenter" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/17/SavedQueries-1024x397.png" alt="" width="640" height="248" /></p> 
<p>&nbsp;</p> 
<p>Note our new query, HTTPResponseCodeCount, its description, and the first portion of the SQL query shows in our list, which is alphabetically sorted by name (it is the fourth item on the preceding image).&nbsp;Choose&nbsp;the name of your new query, and then&nbsp;choose the <strong>Run Query</strong> button to execute it:</p> 
<p>&nbsp;</p> 
<p><img class="wp-image-1535 size-large aligncenter" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/17/QueryResults-1024x545.png" alt="" width="640" height="341" /></p> 
<p>&nbsp;</p> 
<p>The <strong>Results</strong> pane shows that most pages return a HTTP 200 response code, which is good (OK). You can inspect the other error codes, like 404 (Not Found), 302 (Redirect) or 500 (Server Error).</p> 
<p>Beyond the new Athena support, two new features have been released within the last few weeks:</p> 
<li><strong><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html">Stack termination protection</a></strong> prevents a stack from being accidentally deleted. It’s a property that can be enabled on new or existing stacks, and it provides yet another level of protection for stacks and their resources. This setting is disabled by default, so you have to explicitly enable it when you create new stacks. For existing, non-nested stacks, you can change termination protection&nbsp;using the console or CLI.</li> 
<li><strong><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_RollbackConfiguration.html">Rollback triggers</a></strong> allow you to have CloudFormation monitor the state of your application while the stack is being created or updated, and to roll back that create or update operation if the application triggers any alarms you have configured. Chuck Meyer’s blog post <a href="https://aws.amazon.com/blogs/mt/use-aws-cloudformation-stack-termination-protection-and-rollback-triggers-to-maintain-infrastructure-availability/">here</a> demonstrates the use of rollback triggers.</li> 
<p>CloudFormation has also introduced the following resource coverage updates:</p> 
<li><strong>Amazon Simple Storage Service (S3)</strong> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html">Configure</a> the transfer acceleration state.</li> 
</ul> </li> 
<li><strong>Amazon Relational Database Service (RDS)</strong> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html">Update</a> engine property from Oracle-SE or Oracle SE1&nbsp;to Oracle&nbsp;SE2 without the database instance being replaced.</li> 
</ul> </li> 
<li><strong>AWS Elastic Load Balancing (ELB)</strong> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticloadbalancingv2-loadbalancer.html">Specify</a> the IDs of the subnets to attach to a load balancer, and specify the type of load balancer to create.</li> 
<li>For target groups, <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticloadbalancingv2-targetgroup.html">specify</a> the Availability Zone where the IP address is to be registered,&nbsp;and also specify the registration type of the targets in a given target group.</li> 
</ul> </li> 
<li><strong>AWS Elastic Beanstalk</strong> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-beanstalk.html">Define</a> lifecycle settings for resources that belong to the application, as well as the service role that Elastic Beanstalk assumes in order to apply lifecycle settings.</li> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-beanstalk-environment.html">Specify</a> a custom platform for Elastic Beanstalk.</li> 
</ul> </li> 
<li><strong>Amazon&nbsp;Elastic Compute Cloud (EC2)</strong> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-natgateway.html">Specify</a> resource tags for a Network Address Translation (NAT) gateway.</li> 
</ul> </li> 
<li><strong>Amazon&nbsp;Kinesis Firehose</strong> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-kinesisfirehose-deliverystream.html">Specify</a> the stream type, as well as stream and role ARNs for a Kinesis stream used as a source for a delivery stream.</li> 
</ul> </li> 
<li><strong>Amazon CloudWatch</strong> 
<li>Support new <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-events-rule.html">properties</a> for input transformation of events, as well as setting Amazon ECS tasks and Kinesis stream targets.</li> 
</ul> </li> 
<p>Visit our <a href="https://aws.amazon.com/cloudformation/">product</a> and <a href="https://aws.amazon.com/documentation/cloudformation/">documentation</a> pages for more information, as well as our list of <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-supported-resources.html">supported AWS resources</a>.</p> 
<hr /> 
<p><strong>About the Author</strong></p> 
<table class=" alignleft" style="height: 90px" width="900"> 
<tbody> 
<tr> 
<td width="63"><img class="alignnone wp-image-1533" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/17/PhoneTool1.jpg" alt="" width="80" height="106" /></td> 
<td width="408"> <p><strong>Luis Colon is a Senior Developer Advocate for the AWS CloudFormation team.&nbsp;</strong>He works with customers and internal development teams to focus on and improve the developer experience for CloudFormation users. In his spare time, he mixes progressive trance music.</p> <p>&nbsp;</p></td> 
</tr> 
</tbody> 
</table> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/aws-cloudformation/" rel="tag">AWS CloudFormation</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Configuring Serverless Applications Using AWS CloudFormation Custom Resources</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Steven Challis</span></span> | on 
<time property="datePublished" datetime="2017-10-11T10:10:35+00:00">11 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/configuring-serverless-applications-using-aws-cloudformation-custom-resources/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS makes it easy for developers to get started developing applications in the cloud. With the extensive array of services available on AWS, developers might incorporate more than just a few components in their applications. Manually managing the resources needed for an application can become time consuming. In addition, applications usually require more than just infrastructure to function.</p> 
<p>In this blog post, I’ll show you how to achieve additional configuration tasks such as&nbsp;data loading, compilation of templates, and deployment of static files&nbsp;using&nbsp;a completely serverless architecture and a single AWS CloudFormation template.</p> 
<p><span id="more-1448"></span></p> 
<b>Configuration with CloudFormation</b> 
<p><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html">AWS CloudFormation</a>&nbsp;is a service that helps you model and set up required resources (e.g., Amazon EC2, Amazon DynamoDB, and IAM roles). This frees you to focus on the application rather than on provisioning the infrastructure. With AWS CloudFormation, you can describe the resources you need using a template written in either JSON or YAML. These templates can then be used to create a stack, and AWS CloudFormation will handle the provisioning and even the updates to or deletions from the resources that you describe.</p> 
<img class="wp-image-1449 size-medium" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/06/one-click-300x144.png" alt="1-Click deploy buttons on Github" width="300" height="144" /> 
<p class="wp-caption-text">Example 1-click deploy buttons from <a href="https://github.com/awslabs/aws-lambda-zombie-workshop">https://github.com/awslabs/aws-lambda-zombie-workshop</a></p> 
<p>CloudFormation templates only describe AWS resources. Additional configuration must be done using another method, such as a&nbsp;<a href="https://aws.amazon.com/getting-started/projects/set-up-ci-cd-pipeline/">CI/CD pipeline</a>. It’s common to see developers share templates designed for 1-click deployment on sites such as GitHub. A great example of this is the&nbsp;<a href="https://aws.amazon.com/answers/">AWS Answers</a>&nbsp;site, which contains many ready-to-deploy solutions, each with its own CloudFormation templates to help users get up and running quickly. How do these solutions achieve their configuration? A common solution is to use&nbsp;<a href="https://aws.amazon.com/answers/">EC2 User Data scripts</a>. But how can you&nbsp;do this&nbsp;if your application doesn’t contain any EC2 instances? Let’s look at another way to do additional configuration.</p> 
<b>What are custom resources?</b> 
<p>With&nbsp;<a href="https://aws.amazon.com/serverless/">serverless architectures</a>&nbsp;becoming more common, I want to walk you through how to achieve the additional configuration needed to set up your application without using Amazon EC2 or separate scripts running in a CI/CD pipeline. In a serverless architecture, there is no concept of launching an instance. Serverless applications are event driven so we need some other way to trigger and run our setup code. A neat solution to this is to use the&nbsp;custom resources feature of AWS CloudFormation. With custom resources you can write your own logic and have AWS CloudFormation run this logic any time you create, update, or delete stacks.</p> 
<p>A custom resource is defined using either the <code>AWS::CloudFormation::CustomResource</code> or <code>Custom::&lt;custom_name&gt;</code> type in CloudFormation. There are two methods of invoking your logic. The first method is to associate an Amazon Simple Notification Service (SNS) topic with your custom resource. In this scenario, you use <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-sns.html">SNS notifications</a> to trigger your custom provisioning logic. After it’s done, your code sends a response (and any output data) that notifies AWS CloudFormation to proceed with the stack operation. The second method is to associate your <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources-lambda.html">custom resource with an AWS Lambda function</a>, which is invoked directly by CloudFormation thus requiring less configuration.</p> 
<b>Let’s take a look at an example</b> 
<p>Imagine that you are tasked with writing a CloudFormation template that will deploy a simple three-tier serverless web application. This application includes a static HTML page hosted in S3, an API powered by AWS Lambda and Amazon API Gateway, and an Amazon DynamoDB database. The first configuration task is to deploy the HTML page. Let’s take a look at an example. The following is a Lambda-backed custom resource defined in CloudFormation to deploy an HTML page:</p> 
<pre><code class="lang-yaml">AppConfigurationLambda:
Type: AWS::Serverless::Function
Properties:
CodeUri: ./frontend
Environment:
Variables:
API_URL: !Join
- ''
- - https://
- !Ref ServerlessRestApi
- .execute-api.
- !Ref 'AWS::Region'
- .amazonaws.com
- /Stage/
DEST_BUCKET: !Ref WebsiteBucket
Handler: handler.configure_application
MemorySize: 128
Role: !GetAtt AppConfigurationRole.Arn
Runtime: python2.7
Timeout: 300
DeploymentCustomResource:
Type: Custom::AppConfiguration
Properties:
ServiceToken: !GetAtt AppConfigurationLambda.Arn</code></pre> 
<p>You can see that I have defined a Lambda function (using <a href="http://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html">AWS SAM syntax</a> to simplify the declaration). I then defined a second resource with type <code>Custom::AppConfiguration</code>. The only required property for this resource is <code>ServiceToken</code> which links to either an Amazon SNS topic ARN or a Lambda function ARN. In this case, I have referenced the previously defined Lambda function.</p> 
<p>The purpose of our Lambda function is to update our frontend HTML page with the URL for our API before pushing it to Amazon S3. You can see that the API URL and S3 bucket are both passed into our function using references to other resources defined in the template (not shown), which allows our template to be entirely self-contained.</p> 
<p>Now let’s take a look at the corresponding Lambda function:</p> 
<pre><code class="lang-python">import os
import json
import boto3
import pystache
import requests
s3 = boto3.resource('s3')
s3_client = boto3.client('s3')
def build_response(event, status):
&quot;&quot;&quot;A utility function used to build a response to CloudFormation&quot;&quot;&quot;
response_data = {
'Status': status,
'Reason': 'Success',
'PhysicalResourceId': 'myapp::{}'.format(event['LogicalResourceId']),
'Data': {},
'RequestId': event['RequestId'],
&quot;LogicalResourceId&quot;: event[&quot;LogicalResourceId&quot;],
&quot;StackId&quot;: event[&quot;StackId&quot;],
}
return response_data
def configure_application(event, context):
&quot;&quot;&quot;Responsible for app configuration during stack creation/update/deletion&quot;&quot;&quot;
try:
request_type = event['RequestType']
api_url = os.environ.get('API_URL')
dest_bucket = os.environ.get('DEST_BUCKET')
if request_type == &quot;Delete&quot;:
# Delete contents of S3 bucket (else it will not delete)
bucket = s3.Bucket(dest_bucket)
for f in bucket.objects.all():
f.delete()
else:
# Retrieve the template needed to generate final HTML
s3_obj = s3.Object('mysourcebucket', 'index.mustache')
mustache_template = s3_obj.get()[&quot;Body&quot;].read()
renderer = pystache.Renderer()
rendered = renderer.render(mustache_template, {'API_URL': api_url})
# Write updated static HTML to S3 for consumption
s3_client.put_object(Bucket=dest_bucket, Key=&quot;index.html&quot;,
Body=rendered, ACL='public-read', ContentType='text/html')
response_data = build_response(event, 'SUCCESS')
except Exception, exc:
# Catch any exceptions and ensure we always return a response
response_data = build_response(event, 'FAILED')
# Respond to Cloudformation to let it know we are done
response_url = event['ResponseURL']
result = requests.put(response_url, data=json.dumps(response_data))
return result</code></pre> 
<p>The key component here is the <code>configure_application()</code> function, which like any other Lambda function takes event and context as input parameters. I mentioned that custom resources can be triggered on create, update, and delete stack operations. A good template should handle all cases. For example, you don’t want to redeploy your HTML if you have chosen to delete the stack. In the previous snippet, our function first checks which type of operation is happening and responds appropriately. The more operations you can build into your template, the less you have to document as additional manual steps for the user of the template.</p> 
<p>Notice that we retrieve the API URL and Destination bucket that were passed in from the template and use those to perform our configuration. In the case of stack deletion, you want to remove any objects in the bucket, otherwise the stack operation will fail when it attempts to delete the bucket itself. In all other cases (stack creation and updates) you want to read a template (here stored in S3) and insert your newly defined API URL. I’m using a Mustache template and the Pystache Python library here to do this (which I need to include with my code bundle). The function then writes the newly rendered template to the S3 bucket, completing the configuration.</p> 
<p>It’s important that your custom resource always makes an explicit call back to the <code>ResponseUrl</code> provided in the event. Simply returning a response from your Lambda function won’t suffice. If you don’t make this call, CloudFormation won’t know that your custom resource has been configured, and it will wait until it times out. In our example, to give our function the best chance of always making this callback, the main logic of the function is wrapped in an exception handler which will build a failure response. ’s still possible for our function to fail (e.g., if imports fail or there are syntax errors elsewhere), however this function is covering common cases such as an invalid template or misconfigured IAM policy.</p> 
<b>Other common configuration tasks</b> 
<p>You can define multiple custom resources in a CloudFormation template. You might find it useful to build up a collection of reusable functions. The following example shows a few more functions you can consider integrating into your templates:</p> 
<h3>Populating a database with data</h3> 
<p>Sometimes your app will need initial data in order to function. This seed data can easily be inserted using a function such as the one in the following example. In this case, you pass in an environment variable called <code>QUESTIONS_TABLE</code> that references the DynamoDB table created by your table.</p> 
<pre><code class="lang-python">dynamodb_client = boto3.resource('dynamodb', region_name=&quot;us-east-1&quot;)
def setup_dynamo(event, context):
try:
table_name = os.environ.get('QUESTIONS_TABLE')
table = dynamodb_client.Table(table_name)
table.put_item(Item={
'number': 1,
'question': 'Which database offering is most suited to NoSQL?',
'answers': '{
&quot;A&quot;: &quot;RDS&quot;,
&quot;B&quot;: &quot;DynamoDB&quot;,
&quot;C&quot;: &quot;Redshift&quot;}'})
table.put_item(Item={
'number': 2,
'question': 'What is the recommended way of scaling compute resources?',
'answers': '{
&quot;A&quot;: &quot;Increase the instance size.&quot;,
&quot;B&quot;: &quot;Use smaller instances but more of them&quot;,
&quot;C&quot;: &quot;Not sure&quot;}'})
response_data = build_response(event, 'SUCCESS')
except Exception, exc:
response_data = build_response(event, 'FAILED')
# Respond to Cloudformation to let it know we are done
response_url = event['ResponseURL']
result = requests.put(response_url, data=json.dumps(response_data))
return result</code></pre> 
<h3>Cleaning up CloudWatch Logs</h3> 
<p>Many services automatically generate logs. In our example, whenever any of our Lambda functions is first run, a log group is created in CloudWatch Logs. Some customers want to clean up these log groups when deleting the stack. , To automate the process you could include a custom resource, such as the one in the example that follows, to automatically delete any Lambda logs generated by functions spun up by your stack. This example would take an environment variable called <code>STACK_NAME</code> used to identify the logs we want and could use <code>!Ref AWS::StackName</code> to retrieve this from within the CloudFormation template.</p> 
<pre><code class="lang-python">logs_client = boto3.client('logs')
def delete_logs(event, context):
try:
delete_logs_with_prefix = '/aws/lambda/{0}'.format(os.environ.get('STACK_NAME'))
output = client.describe_log_groups(logGroupNamePrefix=delete_logs_with_prefix)
for record in output['logGroups']:
client.delete_log_group(logGroupName=record['logGroupName'])
response_data = build_response(event, 'SUCCESS')
except Exception, exc:
response_data = build_response(event, 'FAILED')
# Respond to Cloudformation to let it know we are done
response_url = event['ResponseURL']
result = requests.put(response_url, data=json.dumps(response_data))
return result</code></pre> 
<b>Tips</b> 
<li>Make sure to handle exceptions and timeouts. If your Lambda function doesn’t successfully call back to CloudFormation, then the stack update will hang. You can use CloudWatch Logs to debug your Custom Resource Lambda functions. You might find it useful to disable automatic rollback on stack failure, otherwise the logs might be gone by the time you go to view them.</li> 
<li>Ensure that your function is idempotent (handles retries etc.). Since updates can run multiple times, avoid unnecessary logic or add explicit handling for operations that might not be idempotent, such as database inserts.</li> 
<li>You have the ability to return custom properties to be used later in your template. This can be useful for constructing custom resources that look up AMI IDs or custom IP address ranges, for example.</li> 
<li>Use third-party libraries to assist you in writing your custom resource Lambda functions , for example, <a href="https://github.com/awslabs/aws-cloudformation-templates/tree/master/community/custom_resources/python_custom_resource_helper">crhelper.py by awslabs</a>&nbsp;(or similar) to make logging and event handling easier.</li> 
<p>&nbsp;</p> 
<h3><img class="alignleft wp-image-1461 " src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/06/challiss-badge.jpg" alt="" width="85" height="120" />About the Author</h3> 
<p>Steven Challis is a Solutions Architect with AWS in New York and has a background in application development and enterprise software primarily within the media and entertainment sector. Outside of work, Steve&nbsp;has&nbsp;passion for cars, instruments and traveling.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/aws-cloudformation/" rel="tag">AWS CloudFormation</a>, <a href="https://aws.amazon.com/blogs/mt/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/mt/tag/custom-resources/" rel="tag">Custom Resources</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Recover your impaired instances using EC2Rescue and Amazon EC2 Systems Manager Automation</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Venkat Krish</span></span> | on 
<time property="datePublished" datetime="2017-10-02T16:07:40+00:00">02 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/recover-your-impaired-instances-using-ec2rescue-and-amazon-ec2-systems-manager-automation/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Have you ever had an issue connecting to your <a href="https://aws.amazon.com/ec2">Amazon EC2 </a>Windows instance? This can be caused by any number of different reasons, but is almost always related to how the instance is configured. Unfortunately, if you can’t connect to it, you can’t fix it!</p> 
<p>Earlier this year, AWS <a href="https://aws.amazon.com/about-aws/whats-new/2017/03/amazon-ec2rescue-is-now-available/">announced EC2Rescue for Windows</a>, a convenient, straightforward, GUI-based troubleshooting tool that can be run on your Windows instances to troubleshoot operating system-level issues and collect advanced logs and configuration files for further analysis.</p> 
<p>AWS listened to your feedback, and now EC2Rescue is available as a one-click, self-service, scalable automated solution for you to use via&nbsp;<a href="https://aws.amazon.com/ec2/systems-manager/automation/">Systems Manager Automation</a>. Starting today, there’s a new public Systems Manager Automation document, called <strong><em>AWSSupport-ExecuteEC2Rescue</em></strong>. <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html">Documentation for EC2Rescue</a> has more details about this Automation document.</p> 
<p><span id="more-1406"></span></p> 
<p>In Automation, you can define a sequence of actions to perform: &nbsp;stopping or starting EC2 instances, creating backup AMIs, invoking AWS Lambda functions, creating AWS CloudFormation templates, and more. In this blog we will show you how you can use AWSSupport-ExecuteEC2Rescue Automation document to orchestrate&nbsp;EC2Rescue workflow.</p> 
<h3>Introducing EC2Rescue powered by Systems Manager Automation</h3> 
<p>While the symptom is always the same (you can’t remotely access your Windows instance), there could be multiple causes for it. AWS Support regularly publishes the most frequent questions and requests received from AWS customers in <a href="https://aws.amazon.com/premiumsupport/knowledge-center/#ec2">Knowledge Center</a>.<br /> The most common reasons AWS Support has dealt with over the years are:</p> 
<li>Network adapter misconfiguration: &nbsp;There is an incorrect static IP address assigned to the network interface, or the DHCP client can’t renew the DHCP lease.</li> 
<li>RDP service issues: &nbsp;The service is disabled or you are using a non-default configuration, that is, a TCP port other than 3389.</li> 
<li>Firewall: &nbsp;The Windows firewall is blocking RDP traffic.</li> 
<h3>What are your troubleshooting options?</h3> 
<p>To start, you can take a <a href="https://aws.amazon.com/premiumsupport/knowledge-center/capture-instance-screenshot/">console screenshot</a>. It may show, for example, that Windows is installing updates. In that case, you just need to wait.</p> 
<p>For issues like RDP and firewall misconfiguration, you can use <a href="https://aws.amazon.com/ec2/run-command/">Systems Manager Run Command</a>, to start your investigation or even fix the problem.</p> 
<p>After you have exhausted these options, or you don’t know exactly what the issue might be, the next available step is to investigate the Amazon EBS root volume of your instance, by attempting an offline Windows registry analysis. This requires deep understanding of the Windows operating system, and a wrong action could worsen the problem.</p> 
<p>EC2Rescue for Windows is able to detect and attempt to resolve all the issues listed above directly from an offline EBS volume, reducing troubleshooting and remediation of common Windows issues to a matter of clicks. Download the tool on a helper EC2 instance that has access to the EBS root volume to inspect, and EC2Rescue guides you through the analysis and remediation, with no advanced Windows knowledge required.</p> 
<p>There are <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Windows-Server-EC2Rescue.html#w2ab1c32c19">multiple preparation steps</a> to execute when using EC2Rescue in offline mode. You need to create a new “helper” EC2 instance in your VPC (or use an existing instance that you can access), detach the EBS root volume from your impaired instance, and attach it to the helper instance. Finally, you need to attach the volume back to its original instance after EC2Rescue completes its work.</p> 
<p>While the remediation is guided and happens in a matter of minutes, the preparation is prone to human error, and is usually done under the pressure of fixing the problem as soon as possible.</p> 
<p>All the steps are now automated, from the helper instance setup to the EC2Rescue remediation, thanks to Systems Manager Automation. You can now use EC2Rescue on your Windows instance consistently. Here’s how to use the new public document, <strong>AWSSupport-ExecuteEC2Rescue</strong>.</p> 
<h3>How to use AWSSupport-ExecuteEC2Rescue</h3> 
<p>A Windows instance is not passing the instance health check:</p> 
<p><img class="alignnone size-full wp-image-1418" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/EC2Rescue1.png" alt="" width="1037" height="153" /></p> 
<p>You can use EC2Rescue with your Windows instances from the AWS Management Console or the AWS CLI. The <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html#automation-ec2rescue-begin">documentation has a walkthrough of the console experience</a>, so I’m going through the CLI experience here.</p> 
<p>You can now use EC2Rescue on this instance with one CLI command. In the following code example, I am passing the instance ID to use with EC2Rescue, and an IAM role with the required permissions to run this Automation document:</p> 
<pre><code class="lang-json">aws ssm start-automation-execution --document-name &quot;AWSSupport-ExecuteEC2Rescue&quot; --parameters &quot;ImpairedInstanceId=YOURINSTANCEID ,AssumeRole=arn:aws:iam::YOURACCOUNTID:role/YOURSSMAUTOMATIONROLE&quot;
{
&quot;AutomationExecutionId&quot;: &quot;ae6b3617-843e-11e7-8f65-57a040263d53&quot;
}
</code></pre> 
<p>You can start the automation from the EC2 console as well, in which case an IAM role is not necessary as Automation can impersonate the current user (make sure to have the required&nbsp;<a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html#automation-ec2rescue-begin">permissions</a> though!).</p> 
<p><img class="alignnone size-full wp-image-1419" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/EC2Rescue2.png" alt="" width="1878" height="842" /></p> 
<p>You can monitor the execution with the returned ID. The execution is still in progress:</p> 
<pre><code class="lang-json">aws ssm get-automation-execution --automation-execution-id &quot;ae6b3617-843e-11e7-8f65-57a040263d53”
{
&quot;AutomationExecution&quot;: {
&quot;AutomationExecutionStatus&quot;: &quot;InProgress&quot;,
&quot;Parameters&quot;: {
(..)
},
&quot;Outputs&quot;: {
(..)
},
&quot;DocumentName&quot;: &quot;AWSSupport-ExecuteEC2Rescue&quot;,
&quot;AutomationExecutionId&quot;: &quot;ae6b3617-843e-11e7-8f65-57a040263d53&quot;,
&quot;DocumentVersion&quot;: &quot;1&quot;,
&quot;ExecutionStartTime&quot;: 1503079041.084,
&quot;StepExecutions&quot;: [
{
(..)
}
]
}
}</code></pre> 
<p>After about 25 minutes, the Automation document completed successfully. The instance is passing both health checks now. Check to see what the problem was!</p> 
<p><img class="alignnone size-full wp-image-1420" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/EC2Rescue3.png" alt="" width="1208" height="147" /></p> 
<p>You can run this CLI command to review the analysis and changes that EC2Rescue made, or review the execution output from the EC2 console:</p> 
<pre><code class="lang-json">aws ssm get-automation-execution --automation-execution-id &quot;ae6b3617-843e-11e7-8f65-57a040263d53&quot; --query 'AutomationExecution.Outputs.&quot;runEC2Rescue.Output&quot;' --output text</code></pre> 
<p>&nbsp;</p> 
<p>===== System Information =====</p> 
<p>&nbsp;</p> 
<p>Operating System: Windows Server 2008 R2 Datacenter</p> 
<p>&nbsp;</p> 
<p>Service Pack: Service Pack 1</p> 
<p>Version: 6.1.7601</p> 
<p>Computer Name: WIN-0KEEGO57HHS</p> 
<p>Time Zone: UTC</p> 
<p>.NET Framework:</p> 
<p>v4.7 (4.7.02053)</p> 
<p>EC2Config Version: 4.9.1981</p> 
<p>===== Analysis =====</p> 
<p>System Time</p> 
<p>OK – RealTimeIsUniversal (Enabled): This registry value should be enabled when timezone is not UTC.</p> 
<p>&nbsp;</p> 
<p>Windows Firewall</p> 
<p>Warning – Domain networks (Enabled): Windows Firewall will be disabled.</p> 
<p>Warning – Private networks (Enabled): Windows Firewall will be disabled.</p> 
<p>Warning – Guest or public networks (Enabled): Windows Firewall will be disabled.</p> 
<p>Remote Desktop</p> 
<p>OK – Service Start (Manual): Sets Remote Desktop service start to automatic.</p> 
<p>OK – Remote Desktop Connections (Enabled): The RDP listening port will be changed to TCP/3389.</p> 
<p>OK – TCP Port (3389): The RDP listening port will be changed to TCP/3389.</p> 
<p>&nbsp;</p> 
<p>EC2Config</p> 
<p>OK – Installation (Installed): EC2Config 4.9.1981 is installed.</p> 
<p>OK – Service Start (Automatic): The service will be set to start automatically.</p> 
<p>Information – Ec2SetPassword (Disabled): Re-generates Administrator’s password on next boot.</p> 
<p>Information – Ec2HandleUserData (Disabled): Executes User Data script on next boot.</p> 
<p>Network Interface</p> 
<p>OK – DHCP Service Startup (Automatic): The service will be set to start automatically.</p> 
<p>Information – Local Area Connection detail (N/A): AWS PV Network Device (7.4.6.0)</p> 
<p><strong>Warning – DHCP on Local Area Connection (Disabled (Static: 169.254.0.1)): DHCP will be enabled.</strong></p> 
<p><strong>===== Changes =====</strong></p> 
<p><strong>Windows Firewall</strong></p> 
<p><strong>OK – Domain networks (Disabled)</strong></p> 
<p><strong>OK – Private networks (Disabled)</strong></p> 
<p><strong>OK – Guest or public networks (Disabled)</strong></p> 
<p><strong>Network Interface</strong></p> 
<p><strong>OK – DHCP on Local Area Connection (Enabled)</strong></p> 
<p>EC2Rescue found that the Windows Firewall and a static IP address configured on the network adapter may have caused the connectivity issue, and made some changes in an attempt to resolve them. Update your custom AMI to make sure that you can launch new EC2 instances consistently in your VPC.</p> 
<h3>Under the hood</h3> 
<p><strong>AWSSupport-ExecuteEC2Rescue</strong> automates the use of <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Windows-Server-EC2Rescue.html">EC2Rescue for Windows</a> in <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/Windows-Server-EC2Rescue.html#w2ab1c32c19">offline mode</a>. This document leverages an AWS CloudFormation template and AWS Lambda functions, orchestrated by Systems Manager Automation, to automate the steps normally required to use EC2Rescue, including:</p> 
<li>Creating an instance to assist with recovery in the appropriate Availability Zone</li> 
<li>Attaching and detaching EBS volumes</li> 
<li>Running the EC2Rescue tool</li> 
<p>This provides a one-click solution to remediate common Windows issues that prevent remote access to the instance.</p> 
<p><strong>AWSSupport-ExecuteEC2Rescue</strong> creates a VPC where EC2Rescue can run, completely isolated from your environment, and creates a backup AMI of the instance on which to run EC2Rescue before any further action is taken.</p> 
<p>&nbsp;</p> 
<p><img class="alignnone wp-image-1438 size-full" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/03/timeline.png" alt="" width="1263" height="595" /></p> 
<p>After you have detected that your Windows instance is unreachable (1), you can pass its instance ID to <strong>AWSSupport-ExecuteEC2Rescue</strong>, which stages a VPC and a number of Lambda functions (2-3) to rescue it. <strong>AWSSupport-ExecuteEC2Rescue</strong> stops your original instance (5), and creates a backup before taking any action on it (6).</p> 
<p>The Automation document identifies which subnet to use in the EC2Rescue VPC that was created (it uses one in the same Availability Zone as your instance), and gets the latest Windows Server 2016 AMI to launch an EC2Rescue instance (4). The Automation document uses <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-ssm.html">RunCommand</a> and <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-cli.html">EC2Rescue</a> CLI on this instance, and attempts to fix the issues identified on your instance (7) before the Automation document starts it back up (9). The EC2Rescue instance is terminated as part of the flow (8).</p> 
<h3>How can this document fix my instance automatically?</h3> 
<p><strong>AWSSupport-ExecuteEC2Rescue</strong> creates the EC2Rescue instance in the same Availability Zone as your instance (but in an isolated VPC).</p> 
<p>&nbsp;</p> 
<p><img class="alignnone size-full wp-image-1439" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/03/step1.png" alt="" width="922" height="526" /></p> 
<p><strong>AWSSupport-ExecuteEC2Rescue</strong> then attaches the root volume of your instance to the EC2Rescue instance.</p> 
<p>&nbsp;</p> 
<p><img class="alignnone size-full wp-image-1440" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/03/step2.png" alt="" width="922" height="526" /></p> 
<p>At this stage, <strong>AWSSupport-ExecuteEC2Rescue</strong> runs a new Run Command document, called <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-ssm.html">AWSSupport-RunEC2RescueForWindowsTool</a>, against the EC2Rescue instance. The document:</p> 
<li>Downloads EC2Rescue.</li> 
<li>Runs the EC2Rescue for Windows tool CLI to diagnose and attempts to fix all the issues that it can identify in the offline root volume that was just attached.</li> 
<p>&nbsp;</p> 
<p><img class="alignnone size-full wp-image-1441" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/03/step3.png" alt="" width="922" height="526" /></p> 
<p>The root volume is then automatically reattached to your instance. The Automation document terminates the EC2Rescue instance, and deletes the EC2Rescue VPC.</p> 
<p>&nbsp;</p> 
<h3><img class="alignnone size-full wp-image-1442" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/03/step4.png" alt="" width="922" height="526" /></h3> 
<h3>Summary</h3> 
<p>AWSSupport-ExecuteEC2Rescue is a new Automation document that automates all the steps required to fix common Windows issues on your unreachable Windows instance using the <a href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2rescue-windows-troubleshoot/">EC2Rescue for Windows tool</a>.</p> 
<p>The Automation document uses the new <a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ec2rw-cli.html">EC2Rescue </a>for Windows CLI for a fully automated end-to-end fully experience.</p> 
<p>With the recent integration between CloudWatch Events and Systems Manager Automation, you can run AWSSupport-ExecuteEC2Rescue automatically in response to an event in your infrastructure.</p> 
<p>You can start using this document today. We are planning to add support for <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Linux-Server-EC2Rescue.html">EC2Rescue for Linux</a> soon.</p> 
<p>If you have any questions or suggestions, please leave a comment for us. Happy EC2Rescue!</p> 
<h3>About the Author</h3> 
<p><img class="size-full wp-image-1432 alignleft" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/02/AMartini.jpg" alt="" width="119" height="160" /></p> 
<p>Alessandro Martini is a Senior Cloud Support Engineer in the AWS Support organization.&nbsp;He likes working with customers, understanding and solving problems and loves to write blogs outlining&nbsp;his solutions on multiple AWS products.&nbsp;He also loves pizza, especially when there is no pineapple on it.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/automation/" rel="tag">Automation</a>, <a href="https://aws.amazon.com/blogs/mt/tag/aws-iam/" rel="tag">AWS IAM</a>, <a href="https://aws.amazon.com/blogs/mt/tag/aws-systems-manager/" rel="tag">AWS Systems Manager</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-systems-manager/" rel="tag">EC2 Systems Manager</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-systems-manager-automation/" rel="tag">EC2 Systems Manager Automation</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-windows/" rel="tag">EC2 Windows</a>, <a href="https://aws.amazon.com/blogs/mt/tag/run-command/" rel="tag">Run Command</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ssm/" rel="tag">SSM</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Use AWS CloudFormation Stack Termination Protection and Rollback Triggers to Maintain Infrastructure Availability</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chuck Meyer</span></span> | on 
<time property="datePublished" datetime="2017-09-29T16:06:07+00:00">29 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/use-aws-cloudformation-stack-termination-protection-and-rollback-triggers-to-maintain-infrastructure-availability/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Managing your infrastructure as code using <a href="https://aws.amazon.com/cloudformation">AWS CloudFormation</a> provides a consistent way to rapidly deliver AWS environments for your applications. As your pace of delivery increases, it’s important to ensure you have the appropriate guardrails to protect your most critical infrastructure resources.</p> 
<p>AWS CloudFormation now includes two additional tools to help you ensure the consistent health and stability of your application environments:</p> 
<li><strong>Stack Termination Protection</strong> provides a low friction mechanism to quickly protect stacks that contain critical resources.</li> 
<li><strong>Rollback Triggers</strong> allow you to quickly revert infrastructure changes that are having a negative impact to the performance of your applications.</li> 
<p>In this post, I’m going to examine strategies for adding these new features to your infrastructure management tool belt.</p> 
<p><span id="more-1375"></span></p> 
<h3>Stack Termination Protection</h3> 
<p>Take advantage of the new Stack Termination Protection parameter to prevent the accidental deletion of stacks that contain critical resources. You can enable termination protection while creating a new stack, and AWS CloudFormation will deny any delete actions against that stack. This new features gives you an extra layer of protection for stacks containing critical resources such as AWS IAM roles or AWS CloudTrail trails.</p> 
<p>You can enable termination protection while creating a new stack using the <a href="https://aws.amazon.com/cli/">AWS Command Line Interface </a>(CLI), AWS APIs, or in the AWS Management Console. In this blog, I’m using the CloudFormation console to create a new stack. Under the <strong>Advanced</strong> section, next to<strong> Termination Protection</strong>, I’ve selected the <strong>Enable</strong> check box. This protects my stack that contains a critical application deployment pipeline from deletion.</p> 
<p><img class="alignnone size-full wp-image-1392" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/Screen-Shot-2017-09-21-at-12.56.54-PM.png" alt="" width="661" height="545" /></p> 
<p>After you create your stack, you can verify the stack termination protection icon in the <strong>Overview</strong> section of your stack. If you are using nested stacks, termination protection cascades down to in sub-stacks of the parent without the need to individually manage protections on each stack.</p> 
<p>Here you can see I’ve enabled termination protection for the stack that contains the AWS CodePipeline for deploying my application.</p> 
<p><img class="alignnone size-full wp-image-1396" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/Screen-Shot-2017-09-20-at-8.16.10-AM.png" alt="" width="377" height="249" /></p> 
<p>With termination protection enabled, you will see the following message when you attempt to delete the stack:</p> 
<p><img class="alignnone size-full wp-image-1394" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/Screen-Shot-2017-09-18-at-2.06.42-PM.png" alt="" width="562" height="228" /></p> 
<p>You can add or remove termination protection from existing stacks using a simple API call. Control access to this API operation using IAM permissions. Make sure protection is removed only when you’re ready to delete the stack.</p> 
<p><img class="alignnone size-full wp-image-1397" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/Screen-Shot-2017-09-20-at-8.50.38-AM.png" alt="" width="556" height="362" /></p> 
<p>You can start adding termination protection&nbsp;to your critical stacks right now.</p> 
<h3>Rollback Triggers</h3> 
<p>We <a href="https://aws.amazon.com/about-aws/whats-new/2017/08/aws-cloudformation-adds-rollback-triggers-feature/">announced rollback triggers</a> back in August, but I wanted to take a little time to revisit them in this context.</p> 
<p>The rollback triggers functionality allows you to integrate application- and resource-level alarms from Amazon CloudWatch into the update process for your stacks. If a change to the stack causes any of the registered alarms to fire, CloudFormation immediately stops the update and rolls back to the last good state. You can include a monitoring window after all updates are complete to allow additional time for the change to stabilize. This window happens prior to the CloudFormation cleanup phase, allowing attribute changes and replaced resources to be quickly restored.</p> 
<p>For an example of rollback triggers in action, this blog starts with the <a href="https://github.com/awslabs/ecs-refarch-batch-processing">reference architecture for containerized batch processing using Amazon ECS</a>.&nbsp;&nbsp;The stack in this reference architecture contains an Amazon EC2 Container Service&nbsp;(ECS) cluster running an image processing service and the Amazon Simple Queue Service (SQS) queue that feeds it. To begin, follow this first three steps in the reference architecture’s instructions.&nbsp;&nbsp;When you come to step four, deploy your ECS service using the following simple CloudFormation template along with the parameter values from the stack that you created in Step 2:</p> 
<p><strong>batch-service.yml</strong></p> 
<pre><code class="lang-yaml">AWSTemplateFormatVersion: '2010-09-09'
Parameters:
TaskDefinition:
Type: String
Description: &quot;ARN of an existing ECS Task Definition&quot;
ECSCluster:
Type: String
Description: &quot;Existing ECS Cluster&quot;
ProcessCount:
Type: Number
Default: 1
Description: &quot;Number of processes to run&quot;
Resources:
service:
Type: AWS::ECS::Service
Properties:
Cluster: !Ref 'ECSCluster'
DesiredCount: !Ref 'ProcessCount'
TaskDefinition: !Ref 'TaskDefinition'
Outputs:
ecsservice:
Value: !Ref 'service'</code></pre> 
<p>You can put your parameter values into a JSON document to make it easier to quickly perform stack creation and updates:</p> 
<p><strong>batch-service-config.jsn</strong></p> 
<pre><code class="lang-json">[
{
&quot;ParameterKey&quot;: &quot;TaskDefinition&quot;,
&quot;ParameterValue&quot;: &quot;arn:aws:ecs:us-east-2:xxxxxxxxxxxx:task-definition/ecs-batch-processing-TaskDefinition-xxxxxxxxxxxx:1&quot;
},
{
&quot;ParameterKey&quot;: &quot;ProcessCount&quot;,
&quot;ParameterValue&quot;: &quot;1&quot;
},
{
&quot;ParameterKey&quot;: &quot;ECSCluster&quot;,
&quot;ParameterValue&quot;: &quot;ecs-batch-processing-ECSCluster-xxxxxxxxxxxxxx&quot;
}
]
</code></pre> 
<p>Then, you can create your stack from the AWS CLI like this:</p> 
<pre class="hide-language">aws cloudformation create-stack --region us-east-2 \
&nbsp;--stack-name ImageProcService \
&nbsp;--template-body file://batch-service.yml \
&nbsp;--parameters file://batch-service-config.json</pre> 
<p>You can skip step five in the reference architecture. We don’t need Auto Scaling for this example. After stack creation is completed, you should have a single batch-processing worker up and running and ready to receive <code class="lang-bash">.jpg</code> files in the input bucket. Upload a few images to make sure it’s working.</p> 
<p>Now you will make a breaking change to your batch service, but with a rollback trigger in place to protect your processing capabilities. As part of the stack you deployed in step two, CloudFormation created a CloudWatch alarm monitoring the SQS queue that feeds your batch workers. You can build a rollback trigger using this alarm. For example:</p> 
<p><strong>batch-service-rollbacktrigger.jsn</strong></p> 
<pre><code class="lang-json">{
&quot;RollbackTriggers&quot;: [
{
&quot;Arn&quot;: &quot;arn:aws:cloudwatch:us-east-2:225704381548:alarm:SQSQueueDepth&quot;,
&quot;Type&quot;: &quot;AWS::CloudWatch::Alarm&quot;
}
],
&quot;MonitoringTimeInMinutes&quot;: 10
}
</code></pre> 
<p>This trigger will wait 10 minutes after the deployment is completed to see if the queue goes into an alarm state.</p> 
<p>Reduce the number of processes in your configuration file to 0 by modifying the value in your parameter file:</p> 
<p><strong>batch-service-config.jsn</strong></p> 
<pre><code class="lang-json">[
{
&quot;ParameterKey&quot;: &quot;TaskDefinition&quot;,
&quot;ParameterValue&quot;: &quot;arn:aws:ecs:us-east-2:xxxxxxxxxxxx:task-definition/ecs-batch-processing-TaskDefinition-xxxxxxxxxxxx:1&quot;
},
{
&quot;ParameterKey&quot;: &quot;ProcessCount&quot;,
&quot;ParameterValue&quot;: &quot;0&quot;
},
{
&quot;ParameterKey&quot;: &quot;ECSCluster&quot;,
&quot;ParameterValue&quot;: &quot;ecs-batch-processing-ECSCluster-xxxxxxxxxxxxxx&quot;
}
]</code></pre> 
<p>Now you can update your stack using the rollback trigger you defined earlier:</p> 
<pre>aws cloudformation update-stack --region us-east-2 \
&nbsp;--stack-name ImageProcService \
&nbsp;--template-body file://batch-service.yml \
&nbsp;--parameters file://batch-service-config.json \
&nbsp;--rollback-configuration file://batch-service-rollbacktrigger.json</pre> 
<p>After you make a change to decrease the number of ECS processes running in your stack, there won’t be&nbsp;any workers to handle the queue. Upload at least five more <code class="lang-bash">.jpg</code> files to the input bucket before the monitoring time ends (you have 10 minutes).&nbsp;&nbsp;Things will continue to run for a little while, but soon the CloudWatch alarm is triggered as queue depth crosses a critical threshold. Since this alarm is registered as a rollback trigger, CloudFormation will automatically begin rolling back the update when the alarm triggers.</p> 
<p><img class="alignnone size-full wp-image-1398" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/Screen-Shot-2017-09-19-at-11.13.00-AM.png" alt="" width="1129" height="473" /></p> 
<p>Once the rollback is completed, batch processing is restored and the SQS queue will begin to drain, removing the alarm.</p> 
<p>Use rollback triggers to monitor the state of your application during the stack creation and update process. You can specify the alarms and the thresholds you want AWS CloudFormation to monitor, and if any of the alarms are breached, CloudFormation rolls back the entire stack operation to the previous deployed state.</p> 
<p>You can start using rollback triggers to allow CloudFormation to monitor critical metrics for your application in continuous integration/continuous deployment (CI/CD) pipelines and other deployment automation today.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/amazon-cloudwatch/" rel="tag">Amazon CloudWatch</a>, <a href="https://aws.amazon.com/blogs/mt/tag/amazon-cloudwatch-alarm/" rel="tag">Amazon CloudWatch Alarm</a>, <a href="https://aws.amazon.com/blogs/mt/tag/aws-cloudformation/" rel="tag">AWS CloudFormation</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Amazon EC2 Systems Manager as a General-Purpose DevOps Tool</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ananth Vaidyanathan</span></span> | on 
<time property="datePublished" datetime="2017-09-28T12:31:50+00:00">28 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/amazon-ec2-systems-manager-as-a-general-purpose-devops-tool/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This guest post was written by Andrew Rout, Engineer at Riverbed SteelCentral Office of the CTO</em></p> 
<p>A long time ago, a manufacturer in Cincinnati invented Play-Doh to be used as a wallpaper cleaner. Twenty years later, an even better purpose was found for it, and kids everywhere rejoiced.</p> 
<p>History repeats itself with Amazon EC2 Systems Manager as we discover new ways to use this service from AWS. The following walk through shows you how Run Command can be used as a DevOps tool for orchestration and for systems introspection.</p> 
<h3>The need to communicate with EC2 instances</h3> 
<p>To manage the EC2 instances that power Riverbed Technology’s <a href="https://www.riverbed.com/products/steelcentral/use-as-a-service.html">SteelCentral SaaS</a> offering, Riverbed’s DevOps team built an internal tool that allows them to perform tasks on the EC2 instances and gives them insight into the state of the environment. A UI sits on top of a backend that communicates with the EC2 instances and various other AWS services.</p> 
<p>This internal DevOps tool allows our operations team to do the following:</p> 
<li>See dashboards describing the overall health of all infrastructure components and software components of SteelCentral SaaS</li> 
<li>Provision new resources as necessary</li> 
<li>Troubleshoot services running on EC2 instances</li> 
<li>Manage users and licensing<span id="more-1345"></span></li> 
<p><img class="size-full wp-image-1346 alignnone" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/25/steelcentral_saas.png" alt="" width="1000" height="431" /></p> 
<p>In addition to a DevOps tool, Riverbed’s SaaS environment includes an event-driven service that also needs to communicate with EC2 instances. The event-driven system is used to provision additional resources on an EC2 instance as the system scales.</p> 
<p>Each of the tasks executed by the DevOps tool and the event-driven service requires one or more remote shell commands to be executed on an EC2 instance to either fetch information from an application or make a change to it.</p> 
<p>The Riverbed DevOps tool initially issued these commands via SSH, but the need to maintain an SSH key in multiple places was a headache from a logistics and security point-of-view. We preferred not to manage the key bits or the access control for SSH keys on our own, nor did we not want to develop and run a web service on all EC2 instances to handle our specific use cases.</p> 
<p><strong>EC2 Systems Manager makes software inventory management easier</strong></p> 
<p>Enter EC2 Systems Manager…</p> 
<p>EC2 Systems Manager was initially designed to be a tool for managing the software packages installed on EC2 instances, but it can be used for much more than just that.</p> 
<p>The original thinking was that if you needed to install or upgrade software on multiple EC2 instances, you could execute the yum, apt-get, Windows PowerShell, or other package manager command via EC2 Systems Manager, and it would do it for you without the user needing to SSH into each EC2 instance individually.</p> 
<p><strong>No, EC2 Systems Manager makes issuing remote commands easier</strong></p> 
<p>Really what <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/execute-remote-commands.html">Run Command</a> did was provide a way to execute ANY command on a remote EC2 instance. Anywhere an SSH command is needed, a Run Command call can take its place.</p> 
<p>The benefits of using Run Command instead of SSH have been stated in other <a href="https://aws.amazon.com/blogs/aws/manage-instances-at-scale-without-ssh-access-using-ec2-run-command/">blog posts</a>, so I’ll briefly list them here.</p> 
<li>No need to store SSH keys anywhere</li> 
<li>Ability to execute remote shell commands is controlled by IAM policies</li> 
<li>Commands issued via Run Command are auditable</li> 
<li>Command output can be stored in Amazon S3 for historical reference</li> 
<p>Essentially, EC2 Systems Manager is used as a communication service between a client and your EC2 instances.&nbsp; In Riverbed’s case, the DevOps tool is the client that wants to communicate with our EC2 instances.</p> 
<p><img class="size-full wp-image-1385 alignnone" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/riverbed_arch-diagram.png" alt="" width="695" height="443" /></p> 
<p>Riverbed’s SteelCentral SaaS uses EC2 Systems Manager to issue commands to stop, start, and modify services running on EC2 instances. For example, when a new user signs up, an event is triggered and Run Command sends commands to ensure additional services are provisioned and configured as the system scales.</p> 
<p>In the Riverbed internal DevOps tool, a human operator can visit a page that displays the overall health of all of the services running on the EC2 instance serving the new user. To get that information, a Run Command is sent to the EC2 instance to query process status, and the output is populated into the DevOps tool’s UI.</p> 
<p><img class="size-full wp-image-1347 alignnone" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/25/riverbed_devops.png" alt="" width="1100" height="752" /></p> 
<p>Using Run Command in place of SSH has allowed Riverbed Technology to save tens of hours of engineering time each year due to no longer needing to manage and maintain SSH keys or troubleshoot SSH keys that aren’t working.</p> 
<p>More importantly, EC2 Systems Manager makes our security policies simpler to enforce because access controls are moved out of the code and into Amazon IAM. This saves time during compliance reviews and makes it easier for management to get a picture of what access paths are defined.</p> 
<p><strong>Setting up your AWS account to use the SSM Agent</strong></p> 
<p>Before you can execute commands on your EC2 instances using the SSM Agent, you need to do the following:</p> 
<ol> 
<li><a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html">Install the SSM Agent on your EC2 instances</a>.</li> 
<li>Update your EC2 instance’s IAM role to include the AWS managed policy named “AmazonEC2RoleforSSM”. Or, you can <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-access.html">create a custom policy</a> for SSM as an alternative to using a managed policy.</li> 
<li>Grant permission to your user’s IAM role to allow it to execute SSM commands. This simple policy grants access to all of SSM:<code class="lang-json"></code></li> 
</ol> 
<pre><code class="lang-json">{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ssm:*”
],
&quot;Resource&quot;: &quot;*&quot;
}
</code></pre> 
<p><strong>Here’s an example of how to execute a command using Run Command<br /> </strong></p> 
<p>As a simple test, let’s execute “whoami” on an EC2 instance via SSM.</p> 
<p>Note: This example is written in Python using the <a href="https://aws.amazon.com/sdk-for-python/">AWS SDK for Python (boto3)</a> to communicate with AWS services, but you can use any SDK of your choice, including the AWS CLI.</p> 
<p>First, send your command to SSM, and note the returned CommandId:</p> 
<pre><code class="lang-python">import boto3
import time
instance_id = 'i-abcdef123456'
cmd = 'whoami'
ssm = boto3.client('ssm', region_name='us-east-1')
response = ssm.send_command(
InstanceIds=[instance_id],
DocumentName='AWS-RunShellScript',
Parameters={&quot;commands&quot;:[cmd]}
)
command_id = response.get('Command', {}).get(&quot;CommandId&quot;, None)</code></pre> 
<p>Second, wait for the command to finish (use the CommandId from the previous step):</p> 
<pre><code class="lang-python">while True:
response = ssm.list_command_invocations(CommandId=command_id, Details=True)
# If the command hasn't started to run yet, keep waiting
#
if len(response['CommandInvocations']) == 0:
time.sleep(1)
continue
# There could be &gt;1 CommandInvocation if the command was sent to multiple
# EC2 instances, but in this example, we just sent the command to one.
#
invocation = response['CommandInvocations'][0]
# Once we detect the command is done, exit the while loop
if invocation['Status'] not in ('Pending', 'InProgress', 'Cancelling'):
break
time.sleep(1)</code></pre> 
<p>Last, grab the command output:</p> 
<pre><code class="lang-python">command_plugin = invocation['CommandPlugins'][-1]
output = command_plugin['Output']
status = command_plugin['ResponseCode']
print &quot;Output =&quot;, output
print &quot;Status =&quot;, status</code></pre> 
<p>If the SSM command succeeded, you should see output that looks like the following:</p> 
<p>Output = root</p> 
<p>Status = 0</p> 
<p>It’s important to note that the command output returned in the SSM response is truncated at 2,500 characters. If you expect your command output to be more than 2,500 characters, you can store the full command output in Amazon S3 and fetch it from there.</p> 
<p>To store the command output in Amazon S3, add the parameter “OutputS3BucketName” when running “send_command”:</p> 
<pre><code class="lang-python">response = ssm.send_command(
InstanceIds=[instance_id],
DocumentName='AWS-RunShellScript',
Parameters={&quot;commands&quot;:[cmd]},
OutputS3BucketName='&lt;bucket-name&gt;'
)</code></pre> 
<h3><strong>Summary</strong></h3> 
<p>Run Command is an ideal service to use inside an application that needs to communicate with EC2 instances. It provides a way to execute any remote command on any of your EC2 instances. With a little bit of IAM configuration, you can throw away your SSH keys forever and let Run Command handle executing your remote shell commands.</p> 
<p>The use cases for using Run Command are as expansive as the use cases for using SSH. Whether it be controlling the applications running on your EC2 instances or fetching system and application level information, AWS has given users a reliable and easy way to manage EC2 instances and the software running on them.</p> 
<h3>About Riverbed Technology</h3> 
<p><img class="alignleft size-full wp-image-1413" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/29/riverbed-logo.png" alt="" width="165" height="49" />Riverbed’s <a href="https://www.riverbed.com/products/steelcentral/use-as-a-service.html">SteelCentral SaaS</a> provides full-Stack Application Performance Monitoring with real-time visibility into the end-user experience, network, infrastructure and applications for applications hosted on or off the cloud. Users can diagnose application performance problems down to the offending code, SQL, web service, network, or system resource.</p> 
<h3>About the Author</h3> 
<p><a href="http://www.linkedin.com/in/andrew-r-a425162">Andrew Rout</a> joined Riverbed Technology in 2013. As an engineer in Riverbed’s SteelCentral Office of the CTO, he evaluates new technologies for product integration and has recently spent time leveraging AWS and Docker. He currently builds and manages the tools and AWS infrastructure that power <a href="https://www.riverbed.com/products/steelcentral/use-as-a-service.html">SteelCentral SaaS</a>. He has a strong interest in using Python to drive DevOps activities.</p> 
<p><code class="lang-json"></code><br /> <code class="lang-json"></code></p> 
<p><code class="lang-json"><br /> </code></p> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/ec2-systems-manager/" rel="tag">EC2 Systems Manager</a>, <a href="https://aws.amazon.com/blogs/mt/tag/run-command/" rel="tag">Run Command</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ssm/" rel="tag">SSM</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Automate remediation actions for Amazon EC2 notifications and beyond using EC2 Systems Manager Automation and AWS Health</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Venkat Krish</span></span> | on 
<time property="datePublished" datetime="2017-09-27T13:40:09+00:00">27 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/automate-remediation-actions-for-amazon-ec2-notifications-and-beyond-using-ec2-systems-manager-automation-and-aws-health/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>You can use EC2 <a href="https://aws.amazon.com/ec2/systems-manager/">Systems Manager Automation</a> to take remediation actions in response to events that may impact your AWS resources. To illustrate this concept, this post guides you through setting up automated remediation actions when an <a href="https://aws.amazon.com/ebs/">Amazon EBS</a> backed <a href="https://aws.amazon.com/ec2/systems-manager/">Amazon EC2</a> instance is scheduled for retirement.</p> 
<p>An instance is scheduled to be retired when AWS detects irreparable failure of the underlying hardware hosting the instance. If your instance root device is an Amazon EBS volume you can stop and start the instance at any time of your convenience before the retirement.</p> 
<p>Amazon EC2 Systems Manager (SSM) Automation is an AWS-hosted service that simplifies common instance and system maintenance and deployment tasks at no additional cost.</p> 
<p><span id="more-1352"></span></p> 
<p><a href="http://docs.aws.amazon.com/health/latest/ug/what-is-aws-health.html">AWS Health</a> provides ongoing visibility into the state of your AWS resources, services, and accounts. The service gives you awareness and remediation guidance for resource performance or availability issues that may affect your applications that run on AWS.</p> 
<p>Both services are integrated with <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a>, allowing AWS Health events to trigger SSM Automation documents.</p> 
<p>SSM Automation also offers an Approval action which temporarily pauses an Automation execution until your designated principals (e.g. IAM user) either approve or reject the action. More information about SSM automated actions is available <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/automation-actions.html">Systems Manager Automation Actions</a>.</p> 
<p><img class="alignnone wp-image-1353" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/26/Ec2InstanceRetirement1-1024x441.png" alt="" width="800" height="344" /></p> 
<p>&nbsp;</p> 
<h6>Figure 1: AWS Services feed events into AWS Health which triggers EC2 Systems Manager</h6> 
<p>&nbsp;</p> 
<p>This post will walk through the four steps to setup Stop and Start of EC2 instances using SSM Automation in response to EC2 retirement events from AWS Health. To launch the solution in the us-east-1 region using AWS CloudFormation please click <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=StopStartEC2InstancewithApproval&amp;templateURL=https://s3.amazonaws.com/aws-health-tools-assets/cloudformation-templates/AWS-StopStartEC2InstancewithApproval.template">here</a>. Please change the region as required. We recommend reviewing the manual steps below before deploying the CloudFormation stack to have an understanding of the solution.</p> 
<p><strong>Step 1:</strong> Set up required <a href="https://aws.amazon.com/iam/">AWS IAM</a> role<br /> <strong>Step 2:</strong> Set up the <a href="https://aws.amazon.com/sns/">Amazon SNS</a> Topic if you don’t have one already<br /> <strong>Step 3:</strong> Set up the <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html">Amazon CloudWatch Events</a> rule with the Automation document<br /> <strong>Step 4:</strong> Test it out and approve the Automation</p> 
<h3></h3> 
<h3>Setup Instructions</h3> 
<p><strong>Step 1: Set up required IAM role</strong></p> 
<p>First setup the required IAM permissions for CloudWatch Events to use by creating an IAM policy and associating with an IAM role for CloudWatch. For the purpose of this example we will call the IAM role the AutomationCWRole. Here is an example of an IAM policy that could be used for this purpose:</p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ec2:StartInstances&quot;,
&quot;ec2:StopInstances&quot;,
&quot;ec2:DescribeInstanceStatus&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ssm:*&quot;
],
&quot;Resource&quot;: [
&quot;*&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;sns:Publish&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:sns:*:*:Automation*&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;iam:PassRole&quot;
],
&quot;Resource&quot;: &quot;arn:aws:iam::&lt;AccountId&gt;:role/AutomationCWRole&quot;
}
]
}</code></pre> 
<p>Please make sure to update the role ARN which account Id and role name. You need to ensure that the role has events.amazonaws.com and ssm.amazonaws.com configured as a trusted entity for the IAM role as shown here:</p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: [
&quot;ssm.amazonaws.com&quot;,
&quot;events.amazonaws.com&quot;
]
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}
</code></pre> 
<p>More information about CloudWatch and IAM see <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/auth-and-access-control-cw.html">Authentication and Access Control for Amazon CloudWatch</a>. For more information about Systems Manager and IAM, see <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-configuring-access-policies.html">Configuring Access Using Systems Manager Managed Policies</a>.</p> 
<p><strong>Step 2: Set up the Amazon SNS Topic if you don’t have one already</strong></p> 
<p>If you choose to use Automation Approval actions, then you will also need to create an SNS topic that the approval notification will be published to or use an existing one. You will also need to subscribe the approvers to that SNS topic. More information on how to set this up is available <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/US_SetupSNS.html">here</a>.</p> 
<p>We will use the SNS topic name AutomationStopStart for this example. Please note that the SNS Topic name must start with the Prefix: Automation.</p> 
<p><strong>Step 3: Set up the Amazon CloudWatch Events rule with the Automation document</strong></p> 
<p>First create a SSM Automation document named StopStartEC2InstancewithApproval by creating a json file using your preferred editor named “StopStartEC2InstancewithApproval.json”:</p> 
<pre><code class="lang-json">{
&quot;description&quot;:&quot;Stop and Start EC2 instances(s) with Approval&quot;,
&quot;schemaVersion&quot;:&quot;0.3&quot;,
&quot;assumeRole&quot;:&quot;{{ AutomationAssumeRole }}&quot;,
&quot;parameters&quot;:{
&quot;AutomationAssumeRole&quot;:{
&quot;type&quot;:&quot;String&quot;,
&quot;description&quot;:&quot;The ARN of the role that allows Automation to perform the actions on your behalf.&quot;,
&quot;default&quot;:&quot;arn:aws:iam::{{global:ACCOUNT_ID}}:role/AutomationServiceRole&quot;
},
&quot;InstanceIds&quot;:{
&quot;type&quot;:&quot;String&quot;,
&quot;description&quot;:&quot;EC2 Instance(s) to Stop and Start&quot;
},
&quot;Approvers&quot;:{
&quot;type&quot;:&quot;StringList&quot;,
&quot;description&quot;:&quot;IAM user or user arn of approvers for the automation action&quot;
},
&quot;SNSTopicArn&quot;:{
&quot;type&quot;:&quot;String&quot;,
&quot;description&quot;:&quot;The SNS topic ARN that you are using to get notifications on about EC2 retirement notifications. The SNS topic name must start with Automation.&quot;
}
},
&quot;mainSteps&quot;:[
{
&quot;name&quot;:&quot;approve&quot;,
&quot;action&quot;:&quot;aws:approve&quot;,
&quot;timeoutSeconds&quot;:999999,
&quot;onFailure&quot;:&quot;Abort&quot;,
&quot;inputs&quot;:{
&quot;NotificationArn&quot;:&quot;{{ SNSTopicArn }}&quot;, 
&quot;Message&quot;: &quot;Your approval is required to proceed with the stop and start of an EC2 instance using the EC2 systems manager automation document that is scheduled for retirement.&quot;,
&quot;MinRequiredApprovals&quot;:1,
&quot;Approvers&quot;:[
&quot;{{Approvers}}&quot;
]
}
},
{
&quot;name&quot;:&quot;stopInstance&quot;,
&quot;action&quot;:&quot;aws:changeInstanceState&quot;,
&quot;maxAttempts&quot;:2,
&quot;timeoutSeconds&quot;:120,
&quot;onFailure&quot;:&quot;Continue&quot;,
&quot;inputs&quot;:{
&quot;InstanceIds&quot;:[
&quot;{{ InstanceIds }}&quot;
],
&quot;DesiredState&quot;:&quot;stopped&quot;
}
},
{
&quot;name&quot;:&quot;forceStopInstance&quot;,
&quot;action&quot;:&quot;aws:changeInstanceState&quot;,
&quot;maxAttempts&quot;:1,
&quot;timeoutSeconds&quot;:60,
&quot;onFailure&quot;:&quot;Continue&quot;,
&quot;inputs&quot;:{
&quot;InstanceIds&quot;:[
&quot;{{ InstanceIds }}&quot;
],
&quot;Force&quot;:true,
&quot;DesiredState&quot;:&quot;stopped&quot;
}
},
{
&quot;name&quot;:&quot;startInstance&quot;,
&quot;action&quot;:&quot;aws:changeInstanceState&quot;,
&quot;maxAttempts&quot;:3,
&quot;timeoutSeconds&quot;:120,
&quot;onFailure&quot;:&quot;Continue&quot;,
&quot;inputs&quot;:{
&quot;InstanceIds&quot;:[
&quot;{{ InstanceIds }}&quot;
],
&quot;DesiredState&quot;:&quot;running&quot;
}
}
]
}
</code></pre> 
<p>Then use the AWS CLI to create the SSM Automation document using the JSON file above:</p> 
<pre><code class="lang-json">[
<em>aws ssm create-document --content file://StopStartEC2InstancewithApproval.json --name &quot; StopStartEC2InstancewithApproval&quot; --document-type &quot;Automation&quot;</em>
]</code></pre> 
<p>More information about creating creating SSM documents can be found at <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/create-ssm-doc.html">Creating Systems Manager Documents</a>.</p> 
<p>You can then create the CloudWatch Events rule that will trigger the Automation document each time an EC2 retirement notification occurs. As an example you can use the following command using the AWS CLI:</p> 
<pre><code class="lang-json">aws events put-rule --name &quot;EC2RetirementNotification&quot; --event-pattern &quot;{\&quot;source\&quot;:[\&quot;aws.health\&quot;],\&quot;detail-type\&quot;:[\&quot;AWS Health Event\&quot;],\&quot;detail\&quot;:{\&quot;service&quot;\&quot;:[\&quot;EC2\&quot;],\&quot;eventTypeCategory&quot;\&quot;:[\&quot;scheduledChange\&quot;],\&quot;eventTypeCode&quot;\&quot;:[\&quot;AWS_EC2_INSTANCE_RETIREMENT_SCHEDULED\&quot;]}&quot;}&quot;</code></pre> 
<p>To set this up you can create a JSON file named <em>targets.json</em> using your preferred editor and then use that to create the CloudWatch Events target:</p> 
<pre><code class="lang-json">[
{
&quot;Id&quot;:&quot;1&quot;,
&quot;Arn&quot;:&quot;arn:aws:ssm:&lt;region&gt;:&lt;accountId&gt;:automation-definition/AWS-StopStartEC2InstancewithApproval&quot;,
&quot;RoleArn&quot;:&quot;arn:aws:iam::&lt;accountId&gt;:role/AutomationCWRole&quot;,
&quot;InputTransformer&quot;:{
&quot;InputPathsMap&quot;:{
&quot;Instances&quot;: &quot;$.resources&quot;
},
&quot;InputTemplate&quot;: &quot;{ \&quot;AutomationAssumeRole\&quot;:[\&quot;aws:iam::&lt;accountId&gt;:role/AutomationCWRole\&quot;],\&quot;Approvers\&quot;:[\&quot;&lt;IAMusername&gt;\&quot;],\&quot;SNSTopicArn\&quot;:[\&quot;arn:aws:sns:&lt;region&gt;:&lt;accountId&gt;:AutomationStopStart\&quot;],\&quot;InstanceIds\&quot;: &lt;Instances&gt; }&quot;
}
}
]</code></pre> 
<p>Please update the region, accountId, SNS topic ARN, IAM role ARN and IAM username in the json file above per your requirements. The target in this case is the Automation document StopStartEC2InstancewithApproval which Stops and Starts the instance(s) provided.</p> 
<p>Then use the AWS CLI to create the target specifying the json file you created:</p> 
<p><em> aws events put-targets –rule EC2RetirementNotification –targets file://targets.json</em></p> 
<p><strong>Step 4: Test it out and approve the Automation</strong></p> 
<p>You can test against the document using direct inputs as well:</p> 
<p><em> aws ssm start-automation-execution –document-name AmazonEC2InstanceStopStartwithApproval –parameters AutomationAssumeRole=”aws:iam::&lt;AccountId&gt;:role/AutomationCWRole”,Approvers=&lt;IAMusername&gt;,SNSTopicArn=”arn:aws:sns:us-east-1:&lt;AccountId&gt;:AutomationStopStart”,InstanceIds=&lt;InstanceId&gt;</em></p> 
<p>You can get the execution status using the AutomationExecutionId returned from the command above: aws ssm&nbsp; get-automation-execution –automation-execution-id &lt;value&gt;</p> 
<p>Once you get the approval message published to your SNS topic’s subscribers, you can choose to approve or reject the action:</p> 
<p><em> aws ssm send-automation-signal –automation-execution-id &lt;automation-execution-id&gt; –signal-type Approve –payload Comment=Replace_This_With_Approve_Comment</em></p> 
<p>The automation can also be approved from the EC2 console in the Automation section:</p> 
<p><img class="alignnone size-full wp-image-1354" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/26/Ec2InstanceRetirement2.png" alt="" width="416" height="145" /></p> 
<p>Please note that the approval will trigger the stop and start of the EC2 Instance, regardless of the comments provided.</p> 
<p>You can also skip the approval step and instead use the AmazonEC2InstanceStopStart SSM Automation document. Please note that in very rare situations EC2 instances might not stop even after a force stop; you should contact AWS support if that happens.</p> 
<h3>Conclusion</h3> 
<p>You can use EC2 Systems Manager Automation to take remediation actions on your AWS resources in response to events that may impact. You can take this example and apply it to other EC2 scheduled changes (e.g. system reboot maintenance) or any event with any AWS resource that may suit your needs. You can also use the document provided to Stop and Start EC2 instances in an automated way. We recommend tailoring it and testing for your use-case before deploying in a production environment.</p> 
<h3>About the Author</h3> 
<p><img class=" wp-image-1355 alignleft" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/26/Tipu_Quershi.jpg" alt="" width="80" height="111" /></p> 
<p>Tipu Qureshi is a principal engineer in the AWS support organization. He works with customers to implement automation, solve problems and setup new workloads on the AWS platform. He has created various architectures and certifications for cost-optimization and agility through DevOps.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/amazon-cloudwatch-events/" rel="tag">Amazon Cloudwatch Events</a>, <a href="https://aws.amazon.com/blogs/mt/tag/automation/" rel="tag">Automation</a>, <a href="https://aws.amazon.com/blogs/mt/tag/aws-iam/" rel="tag">AWS IAM</a>, <a href="https://aws.amazon.com/blogs/mt/tag/aws-sns/" rel="tag">AWS SNS</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-systems-manager/" rel="tag">EC2 Systems Manager</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-windows/" rel="tag">EC2 Windows</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ssm/" rel="tag">SSM</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Get Disk Utilization of Your Fleet Using EC2 Systems Manager Custom Inventory Types</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tanu Mutreja</span></span> | on 
<time property="datePublished" datetime="2017-09-20T14:23:32+00:00">20 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/get-disk-utilization-of-your-fleet-using-ec2-systems-manager-custom-inventory-types/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/ec2/systems-manager/inventory/">Amazon EC2 Systems Manager Inventory</a> provides a centralized way to collect and query system, application, and instance metadata. Using the resource data sync feature, you can sync this metadata to Amazon S3. In Amazon S3 you can aggregate the metadata for different AWS Regions and accounts. After you sync this inventory data to Amazon S3, you can create various visuals of the data using Amazon Athena and Amazon QuickSight.</p> 
<p>The inventory data collection policy is configured using <a href="https://aws.amazon.com/ec2/systems-manager/state-manager/">State Manager</a> , which in turn gets executed by <a href="https://github.com/aws/amazon-ssm-agent/tree/master/agent/plugins/inventory">aws:softwareInventory plugin</a> in <a href="https://github.com/aws/amazon-ssm-agent">amazon-ssm-agent</a>.</p> 
<p>Amazon EC2 Systems Manager Inventory provides two ways to define the types of data that it collects: predefined and custom.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Predefined data types (with prefix AWS)</strong> are natively supported by the inventory plugin via multiple <a href="https://github.com/aws/amazon-ssm-agent/tree/master/agent/plugins/inventory/gatherers">gatherers</a>. Some examples of predefined inventory types are AWS:Application and AWS:WindowsUpdate.</p> 
<p>&middot;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Custom data type (with prefix Custom)</strong> is a special inventory data type that can be defined by end users. This data type provides the flexibility of collecting additional inventory data, such as server rack location of a managed instance.</p> 
<p>In this blog, I’ll walk you through an example that shows how to use the custom inventory data type to collect disk utilization for Windows instances. We’ll use PowerShell scripts to collect disk utilization data in the Inventory. After the data is collected, we’ll use this data to get fleet-level aggregation of disk usage.</p> 
<p><span id="more-1234"></span></p> 
<h3>Step1: Create Inventory Policy Document with aws:runPowerShellScript plugin</h3> 
<p>By default, you can use AWS-GatherSoftwareInventory document to collect Inventory data. However, for this example we’ll create a document with <strong>aws:runPowerShellScript &amp; aws:softwareInventory</strong> plugins to ensure that a PowerShell script is invoked before Inventory collection begins. Since amazon-ssm-agent preserves the order of the plugin’s executions, it ensures that the latest Disk Utilization data is captured before Inventory data is collected.</p> 
<h4>Option 1: Create Document using the AWS Management Console</h4> 
<p>You can create the document in the AWS console by going to EC2 -&gt; Systems Manager Shared Resources -&gt; Documents. Here is the document content.</p> 
<pre><code class="lang-json">{
&quot;schemaVersion&quot;: &quot;2.2&quot;,
&quot;description&quot;: &quot;Run first a shell script &amp; then inventory plugin.&quot;,
&quot;mainSteps&quot;: [
{
&quot;action&quot;: &quot;aws:runPowerShellScript&quot;,
&quot;name&quot;: &quot;runPowerShellScript&quot;,
&quot;inputs&quot;: {
&quot;runCommand&quot;: &quot;{{ commands }}&quot;
}
},
{
&quot;action&quot;: &quot;aws:softwareInventory&quot;,
&quot;name&quot;: &quot;collectSoftwareInventoryItems&quot;,
&quot;inputs&quot;: {
&quot;applications&quot;: &quot;{{ applications }}&quot;,
&quot;awsComponents&quot;: &quot;{{ awsComponents }}&quot;,
&quot;networkConfig&quot;: &quot;{{ networkConfig }}&quot;,
&quot;windowsUpdates&quot;: &quot;{{ windowsUpdates }}&quot;,
&quot;customInventory&quot;: &quot;{{ customInventory }}&quot;
}
}
],
&quot;parameters&quot;: {
&quot;commands&quot;: {
&quot;type&quot;: &quot;StringList&quot;,
&quot;description&quot;: &quot;(Required) Specify a shell script or a command to run.&quot;,
&quot;minItems&quot;: 1,
&quot;displayType&quot;: &quot;textarea&quot;
},
&quot;applications&quot;: {
&quot;type&quot;: &quot;String&quot;,
&quot;default&quot;: &quot;Enabled&quot;,
&quot;description&quot;: &quot;(Optional) Collect data for installed applications.&quot;,
&quot;allowedValues&quot;: [
&quot;Enabled&quot;,
&quot;Disabled&quot;
]
},
&quot;awsComponents&quot;: {
&quot;type&quot;: &quot;String&quot;,
&quot;default&quot;: &quot;Enabled&quot;,
&quot;description&quot;: &quot;(Optional) Collect data for AWSComponents like amazon-ssm-agent.&quot;,
&quot;allowedValues&quot;: [
&quot;Enabled&quot;,
&quot;Disabled&quot;
]
},
&quot;networkConfig&quot;: {
&quot;type&quot;: &quot;String&quot;,
&quot;default&quot;: &quot;Enabled&quot;,
&quot;description&quot;: &quot;(Optional) Collect data for Network configurations.&quot;,
&quot;allowedValues&quot;: [
&quot;Enabled&quot;,
&quot;Disabled&quot;
]
},
&quot;windowsUpdates&quot;: {
&quot;type&quot;: &quot;String&quot;,
&quot;default&quot;: &quot;Enabled&quot;,
&quot;description&quot;: &quot;(Optional) Collect data for all WindowsUpdates.&quot;,
&quot;allowedValues&quot;: [
&quot;Enabled&quot;,
&quot;Disabled&quot;
]
},
&quot;customInventory&quot;: {
&quot;type&quot;: &quot;String&quot;,
&quot;default&quot;: &quot;Enabled&quot;,
&quot;description&quot;: &quot;(Optional) Collect data for custom inventory.&quot;,
&quot;allowedValues&quot;: [
&quot;Enabled&quot;,
&quot;Disabled&quot;
]
}
}
}</code></pre> 
<h4>Option 2: Create Document using the AWS command line (aws-cli)</h4> 
<p>a.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Create the document by running the aws-cli command that creates the document:</p> 
<pre><code class="lang-bash">aws ssm create-document --content file://<em>path to your file\FileName</em> --name &quot;CustomInventory-Doc&quot; --document-type Command</code></pre> 
<p>b.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Verify the document’s status by running following command:</p> 
<p><code class="lang-python">aws ssm list-documents --document-filter-list key=Name,value= CustomInventory-Doc<br /> </code></p> 
<p>Here is a sample output that you should see:<br /> <em>{</em><br /> <em>&nbsp;&nbsp;&nbsp; “DocumentIdentifiers”: [</em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {</em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Name”: ” CustomInventory-Doc “, </em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “PlatformTypes”: [</em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Windows”, </em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Linux”</em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ], </em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “DocumentVersion”: “1”, </em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “DocumentType”: “Command”, </em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “Owner”: “xxx”, </em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; “SchemaVersion”: “2.0”</em><br /> <em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</em><br /> <em>&nbsp;&nbsp;&nbsp; ]</em><br /> <em>}</em></p> 
<h3>Step2: Create association using CustomInventory-Doc</h3> 
<p>Now that the inventory policy document is created, we will “Create Association,” that is we’ll associate this policy document to the targeted instances. To use the EC2 console to Create Association, go to State Manager under Systems Manager Service.</p> 
<p><img class="alignnone wp-image-1319 size-full" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/18/s1-1.png" alt="Create Association" width="975" height="306" /></p> 
<p>Next, you need to pick instances to which you want to attach this Association. In addition, define a schedule for this inventory collection. After you pick the instances and schedule, you can paste following PowerShell script in the Commands parameter (as shown in the screenshot that follows). This script is executed by the <em><strong>aws:runPowerShellScript</strong></em> plugin before the Inventory plugin is invoked.</p> 
<p>Script:</p> 
<pre><code class="lang-powershell">$data = get-wmiobject win32_logicaldisk | Select-Object @{n=&quot;DeviceId&quot;;e={$_.&quot;DeviceID&quot;}}, @{n=&quot;VolumeName&quot;;e={$_.&quot;VolumeName&quot;}}, @{n=&quot;Use%&quot;;e={&quot;{0}&quot; -f [math]::Round(($_.&quot;Size&quot; - $_.&quot;FreeSpace&quot;) * 100 / $_.&quot;Size&quot;,0)}}, @{n=&quot;Size(GB)&quot;;e={&quot;{0}&quot; -f [math]::Round($_.&quot;Size&quot; / 1GB ,0)}} | ConvertTo-Json
$content = &quot;{`&quot;SchemaVersion`&quot; : `&quot;1.0`&quot;, `&quot;TypeName`&quot;: `&quot;Custom:DiskUtilization`&quot;, `&quot;Content`&quot;: $data}&quot;
$instanceId = Invoke-RestMethod -uri http://169.254.169.254/latest/meta-data/instance-id
$filepath = &quot;C:\ProgramData\Amazon\SSM\InstanceData\&quot; + $instanceId + &quot;\inventory\custom\CustomDiskUsage.json&quot;
if (-NOT (Test-Path $filepath)) {
New-Item $filepath -ItemType file
}
Set-Content -Path $filepath -Value $content</code></pre> 
<p>This script gets disk utilization data using win32_logicaldisk. It uses <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html">Instance-Metadata </a>to get InstanceId, which is required in order to save the content in the path: <em>%SystemDrive%\ProgramData\Amazon\SSM\InstanceData\&lt;instance-id&gt;\inventory\custom</em></p> 
<p><img class="alignnone wp-image-1236 size-full" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/08/s2.png" alt="" width="975" height="1000" /></p> 
<p>Choose Create Association and that’s it. After the policy runs on the targeted instances, disk utilization data will be collected and become ready for consumption.</p> 
<p>Let’s go to Managed Instances and check disk utilization for an instance by choosing the Inventory tab. The following screenshot shows the new Custom data for one of my instances.</p> 
<p>&nbsp;</p> 
<p><img class="alignnone size-full wp-image-1337" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/20/ss1-1.png" alt="" width="731" height="412" /></p> 
<p>We can also apply various filters to determine the fleet’s Disk Utilization health.</p> 
<p>Let’s apply a filter to list all instances with disk utilization of more than 50 percent. The following screenshot shows instances matching the filter.</p> 
<p><img class="alignnone wp-image-1321 size-full" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/18/s3-1.png" alt="" width="975" height="206" /></p> 
<h3>Conclusion</h3> 
<p>This blog shows you how to create a custom document with <em><strong>aws:runPowerShellScript</strong></em> &amp; <em><strong>aws:softwareInventory plugin</strong></em>s. This allows you to collect and then send Custom Inventory data from an instance every time the Inventory policy is run. The data can then be queried at both the fleet and instance level.</p> 
<p>In this blog, we used a PowerShell script to get the custom data, however the same document can also be used to trigger any third-party application running in the instance to collect custom data.</p> 
<h4>About the Author</h4> 
<p><img class="size-full wp-image-1282 alignleft" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/09/13/SaurabhShankar.jpeg" alt="" width="119" height="160" /></p> 
<p>Saurabh Shankar is a Software Development Engineer with the Amazon EC2 Systems Manager team. He has been with Amazon for four years, working on Inventory and other features of Amazon EC2 Systems Manager. Outside work, he enjoys trekking and taking photographs.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/mt/tag/aws-systems-manager-inventory/" rel="tag">AWS Systems Manager Inventory</a>, <a href="https://aws.amazon.com/blogs/mt/tag/custom-inventory/" rel="tag">Custom Inventory</a>, <a href="https://aws.amazon.com/blogs/mt/tag/custom-inventory-type/" rel="tag">Custom Inventory Type</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ec2-systems-manager-inventory/" rel="tag">EC2 Systems Manager Inventory</a>, <a href="https://aws.amazon.com/blogs/mt/tag/inventory/" rel="tag">Inventory</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ssm/" rel="tag">SSM</a>, <a href="https://aws.amazon.com/blogs/mt/tag/ssm-inventory/" rel="tag">SSM Inventory</a></span> 
</footer> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
