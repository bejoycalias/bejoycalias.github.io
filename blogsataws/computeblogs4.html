<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Manage Kubernetes Clusters on AWS Using CoreOS Tectonic</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Arun Gupta</span></span> | on 
<time property="datePublished" datetime="2017-09-13T12:44:55+00:00">13 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-coreos-tectonic/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2819" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2819&amp;disqus_title=Manage+Kubernetes+Clusters+on+AWS+Using+CoreOS+Tectonic&amp;disqus_url=https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-coreos-tectonic/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2819');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>There are multiple ways to run a&nbsp;<a href="https://github.com/kubernetes/community/blob/master/sig-aws/kubernetes-on-aws.md">Kubernetes cluster on Amazon Web Services</a> (AWS).&nbsp;The <a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/">first post</a> in this series explained how to manage a Kubernetes cluster on AWS using <a href="https://github.com/kubernetes/kops">kops</a>. This second post explains how to manage a Kubernetes cluster on AWS using <a href="https://coreos.com/tectonic/">CoreOS Tectonic</a>.</p> 
<b>Tectonic overview</b> 
<p>Tectonic delivers the most current upstream version of Kubernetes with additional features. It is a commercial offering from CoreOS and adds the following features over the upstream:</p> 
<li><strong>Installer</strong><br /> Comes with a graphical installer that installs a highly available Kubernetes cluster. Alternatively, the cluster can be installed using AWS CloudFormation templates or Terraform scripts.</li> 
<li><strong>Operators</strong><br /> An <a href="https://coreos.com/operators">operator</a> is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user. This release includes an <a href="https://coreos.com/blog/introducing-the-etcd-operator.html">etcd operator</a> for rolling upgrades and a Prometheus operator for monitoring capabilities.</li> 
<li><strong>Console</strong><br /> A web console provides a full view of applications running in the cluster. It also allows you to deploy applications to the cluster and start the rolling upgrade of the cluster.</li> 
<li><strong>Monitoring</strong><br /> Node CPU and memory metrics are powered by the Prometheus operator. The graphs are available in the console. A large set of preconfigured Prometheus alerts are also available.</li> 
<li><strong>Security</strong><br /> Tectonic ensures that cluster is always up to date with the most recent patches/fixes. Tectonic clusters also enable role-based access control (RBAC). Different roles can be mapped to an LDAP service.</li> 
<li><strong>Support</strong><br /> CoreOS provides commercial support for clusters created using Tectonic.</li> 
<p>Tectonic can be installed on AWS using a <a href="https://coreos.com/tectonic/docs/latest/install/aws/index.html">GUI installer</a>&nbsp;or <a href="https://coreos.com/tectonic/docs/latest/install/aws/aws-terraform.html">Terraform</a> scripts. The installer prompts you for the information needed to boot the Kubernetes cluster, such as AWS access and secret key, number of master and worker nodes, and instance size for the master and worker nodes. The cluster can be created after all the options are specified. Alternatively, Terraform assets can be downloaded and the cluster can be created later. This post shows using the installer.</p> 
<b>CoreOS License and Pull Secret</b> 
<p>Even though Tectonic is a commercial offering, a cluster for up to 10 nodes can be created by creating a free account at <a href="https://account.coreos.com/signup/summary/tectonic-2016-12">Get Tectonic for Kubernetes</a>. After signup, a CoreOS License and Pull Secret files are provided on your <a href="http://account.coreos.com/">CoreOS account page</a>. Download these files as they are needed by the installer to boot the cluster.</p> 
<b>IAM user permission</b> 
<p>The IAM user to create the Kubernetes cluster must have access to the following services and features:</p> 
<li>Amazon Route 53</li> 
<li>Amazon EC2</li> 
<li>Elastic Load Balancing</li> 
<li>Amazon S3</li> 
<li>Amazon VPC</li> 
<li>Security groups</li> 
<p>Use the <a href="https://coreos.com/tectonic/docs/latest/files/aws-policy.json">aws-policy</a> policy to grant the required permissions for the IAM user.</p> 
<b>DNS configuration</b> 
<p>A subdomain is required to create the cluster, and it must be registered as a public Route 53 hosted zone. The zone is used to host and expose the console web application. It is also used as the static namespace for the Kubernetes API server. This allows <code>kubectl</code> to be able to talk directly with the master.</p> 
<p>The domain may be registered using Route 53. Alternatively, a domain may be registered at a third-party registrar. This post uses a&nbsp;<code>kubernetes-aws.io</code> domain registered at a third-party registrar and a <code>tectonic</code> subdomain within it.</p> 
<p>Generate a Route 53 hosted zone using the <a href="https://aws.amazon.com/cli">AWS CLI</a>. Download&nbsp;<a href="https://github.com/stedolan/jq/wiki/Installation">jq</a>&nbsp;to run this command:</p> 
ID=$(uuidgen) &amp;&amp; \
aws route53 create-hosted-zone \
--name tectonic.kubernetes-aws.io \
--caller-reference $ID \
| jq .DelegationSet.NameServers 
<p>The command shows an output such as the following:</p> 
[
&quot;ns-1924.awsdns-48.co.uk&quot;,
&quot;ns-501.awsdns-62.com&quot;,
&quot;ns-1259.awsdns-29.org&quot;,
&quot;ns-749.awsdns-29.net&quot;
] 
<p>Create NS records for the domain with your registrar.&nbsp;Make sure that the NS records can be resolved using a utility like <a href="https://www.digwebinterface.com/">dig web interface</a>. A sample output would look like the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-1.png" /></a></p> 
<p>The bottom of the screenshot shows NS records configured for the subdomain.</p> 
<b>Download and run the Tectonic installer</b> 
<p>Download the <a href="https://releases.tectonic.com/tectonic-1.7.1-tectonic.1.tar.gz">Tectonic installer</a>&nbsp;(version 1.7.1) and extract it. The latest installer can always be found at <a href="https://coreos.com/tectonic">coreos.com/tectonic</a>. Start the installer:</p> 
./tectonic/tectonic-installer/$PLATFORM/installer 
<p>Replace <code>$PLATFORM</code> with either <code>darwin</code> or <code>linux</code>. The installer opens your default browser and prompts you to select the cloud provider. Choose Amazon Web Services as the platform. Choose <b>Next Step</b>.</p> 
<p>Specify the Access Key ID and Secret Access Key for the IAM role that you created earlier. This allows the installer to create resources required for the Kubernetes cluster. This also gives the installer full access to your AWS account. Alternatively, to protect the integrity of your main AWS credentials, use a temporary session token to generate temporary credentials.</p> 
<p>You also need to choose a region in which to install the cluster. For the purpose of this post, I chose a region close to where I live, Northern California. Choose <b>Next Step</b>.</p> 
<p>Give your cluster a name. This name is part of the static namespace for the master and the address of the console.</p> 
<p>To enable in-place update to the Kubernetes cluster, select the checkbox next to <b>Automated Updates</b>. It also enables update to the etcd and Prometheus operators. This feature may become a default in future releases.</p> 
<p>Choose <b>Upload “tectonic-license.txt”</b> and upload the previously downloaded license file.</p> 
<p>Choose <b>Upload “config.json”</b> and upload the previously downloaded pull secret file. Choose <b>Next Step</b>.</p> 
<p>Let the installer generate a CA certificate and key. In this case, the browser may not recognize this certificate, which I discuss later in the post. Alternatively, you can provide a CA certificate and a key in PEM format issued by an authorized certificate authority. Choose Next Step.</p> 
<p>Use the SSH key for the region specified earlier. You also have an option to generate a new key. This allows you to later connect using SSH into the Amazon EC2 instances provisioned by the cluster. Here is the command that can be used to log in:</p> 
ssh –i &lt;key&gt; core@&lt;ec2-instance-ip&gt; 
<p>Choose <b>Next Step</b>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/13/tectonic-k8s-2.png" /></a></p> 
<p>Define the number and instance type of master and worker nodes. In this case, create a 6 nodes cluster. Make sure that the worker nodes have enough processing power and memory to run the containers.</p> 
<p>An etcd cluster is used as persistent storage for all of Kubernetes API objects. This cluster is required for the Kubernetes cluster to operate. There are three ways to use the etcd cluster as part of the Tectonic installer:</p> 
<li>(Default) Provision the cluster using EC2 instances. Additional EC2 instances are used in this case.</li> 
<li>Use an alpha support for cluster provisioning using the etcd operator. The etcd operator is used for automated operations of the etcd master nodes for the cluster itself, in addition to for etcd instances that are created for application usage. The etcd cluster is provisioned within the Tectonic installer.</li> 
<li>Bring your own pre-provisioned etcd cluster.</li> 
<p>Use the first option in this case.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-3.png" /></a></p> 
<p>For more information about choosing the appropriate instance type, see the <a href="https://coreos.com/etcd/docs/latest/op-guide/hardware.html">etcd hardware recommendation</a>. Choose <strong>Next Step</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-4.png" /></a></p> 
<p>Specify the networking options. The installer can create a new public VPC or use a pre-existing public or private VPC. Make sure that the <a href="https://coreos.com/tectonic/docs/latest/install/aws/requirements.html#subnetvpc-requirements">VPC requirements</a> are met for an existing VPC.</p> 
<p>Give a DNS name for the cluster. Choose the domain for which the Route 53 hosted zone was configured earlier, such as <code>tectonic.kubernetes-aws.io</code>. Multiple clusters may be created under a single domain. The cluster name and the DNS name would typically match each other.</p> 
<p>To select the CIDR range, choose <strong>Show Advanced Settings</strong>. You can also choose the Availability Zones for the master and worker nodes. By default, the master and worker nodes are spread across multiple Availability Zones in the chosen region. This makes the cluster highly available.</p> 
<p>Leave the other values as default. Choose <strong>Next Step</strong>.</p> 
<p>Specify an email address and password to be used as credentials to log in to the console. Choose <strong>Next Step</strong>.</p> 
<p>At any point during the installation, you can choose <strong>Save progress</strong>. This allows you to save configurations specified in the installer. This configuration file can then be used to restore progress in the installer at a later point.</p> 
<p>To start the cluster installation, choose <strong>Submit</strong>. At another time, you can download the Terraform assets by choosing <strong>Manually boot</strong>. This allows you to boot the cluster later.</p> 
<p>The logs from the Terraform scripts are shown in the installer. When the installation is complete, the console shows that the Terraform scripts were successfully applied, the domain name was resolved successfully, and that the console has started. The domain works successfully if the DNS resolution worked earlier, and it’s the address where the console is accessible.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-5.png" /></a></p> 
<p>Choose <strong>Download assets</strong> to download assets related to your cluster. It contains your generated CA, kubectl configuration file, and the Terraform state. This download is an important step as it allows you to delete the cluster later.</p> 
<p>Choose <strong>Next Step</strong> for the final installation screen. It allows you to access the Tectonic console, gives you instructions about how to configure <code>kubectl</code> to manage this cluster, and finally deploys an application using <code>kubectl</code>.</p> 
<p>Choose <strong>Go to my Tectonic Console</strong>. In our case, it is also accessible at <code>http://cluster.tectonic.kubernetes-aws.io/</code>.</p> 
<p>As I mentioned earlier, the browser does not recognize the self-generated CA certificate. Choose <strong>Advanced</strong> and connect to the console. Enter the login credentials specified earlier in the installer and choose <strong>Login</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-6.png" /></a></p> 
<p>The Kubernetes upstream and console version are shown under <strong>Software Details</strong>. Cluster health shows <strong>All systems go</strong> and it means that the API server and the backend API can be reached.</p> 
<p>To view different Kubernetes resources in the cluster choose, the resource in the left navigation bar. For example, all deployments can be seen by choosing <strong>Deployments</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-7.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-7.png" /></a></p> 
<p>By default, resources in the <code>all</code> namespace are shown. Other namespaces may be chosen by clicking on a menu item on the top of the screen. Different administration tasks such as managing the namespaces, getting list of the nodes and RBAC can be configured as well.</p> 
<b>Download and run Kubectl</b> 
<p>Kubectl is required to manage the Kubernetes cluster. The latest version of kubectl can be downloaded using the following command:</p> 
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl 
<p>It can also be conveniently installed using the <a href="https://brew.sh/">Homebrew package manager</a>. To find and access a cluster, Kubectl needs a kubeconfig file. By default, this configuration file is at <code>~/.kube/config</code>. This file is created when a Kubernetes cluster is created from your machine. However, in this case, download this file from the console.</p> 
<p>In the console, choose <strong>admin</strong>, <strong>My Account</strong>, <strong>Download Configuration</strong> and follow the steps to download the kubectl configuration file. Move this file to <code>~/.kube/config</code>. If kubectl has already been used on your machine before, then this file already exists. Make sure to take a backup of that file first.</p> 
<p>Now you can run the commands to view the list of deployments:</p> 
~ $ kubectl get deployments --all-namespaces
NAMESPACE         NAME                                    DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-system       etcd-operator                           1         1         1            1           43m
kube-system       heapster                                1         1         1            1           40m
kube-system       kube-controller-manager                 3         3         3            3           43m
kube-system       kube-dns                                1         1         1            1           43m
kube-system       kube-scheduler                          3         3         3            3           43m
tectonic-system   container-linux-update-operator         1         1         1            1           40m
tectonic-system   default-http-backend                    1         1         1            1           40m
tectonic-system   kube-state-metrics                      1         1         1            1           40m
tectonic-system   kube-version-operator                   1         1         1            1           40m
tectonic-system   prometheus-operator                     1         1         1            1           40m
tectonic-system   tectonic-channel-operator               1         1         1            1           40m
tectonic-system   tectonic-console                        2         2         2            2           40m
tectonic-system   tectonic-identity                       2         2         2            2           40m
tectonic-system   tectonic-ingress-controller             1         1         1            1           40m
tectonic-system   tectonic-monitoring-auth-alertmanager   1         1         1            1           40m
tectonic-system   tectonic-monitoring-auth-prometheus     1         1         1            1           40m
tectonic-system   tectonic-prometheus-operator            1         1         1            1           40m
tectonic-system   tectonic-stats-emitter                  1         1         1            1           40m 
<p>This output is similar to the one shown in the console earlier. Now, this <code>kubectl</code> can be used to manage your resources.</p> 
<b>Upgrade the Kubernetes cluster</b> 
<p>Tectonic allows the in-place upgrade of the cluster. This is an experimental feature as of this release. The clusters can be updated either automatically, or with manual approval.</p> 
<p>To perform the update, choose <strong>Administration</strong>, <strong>Cluster Settings</strong>. If an earlier Tectonic installer, version 1.6.2 in this case, is used to install the cluster, then this screen would look like the following:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-8.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-8.png" /></a></p> 
<p>Choose <strong>Check for Updates</strong>. If any updates are available, choose <strong>Start Upgrade</strong>. After the upgrade is completed, the screen is refreshed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-9.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/tectonic-k8s-9.png" /></a></p> 
<p>This is an experimental feature in this release and so should only be used on clusters that can be easily replaced. This feature may become a fully supported in a future release. For more information about the upgrade process, see <a href="https://coreos.com/tectonic/docs/latest/admin/upgrade.html">Upgrading Tectonic &amp; Kubernetes</a>.</p> 
<b>Delete the Kubernetes cluster</b> 
<p>Typically, the Kubernetes cluster is a long-running cluster to serve your applications. After its purpose is served, you may delete it. It is important to delete the cluster as this ensures that all resources created by the cluster are appropriately cleaned up.</p> 
<p>The easiest way to delete the cluster is using the assets downloaded in the last step of the installer. Extract the downloaded zip file. This creates a directory like <code>&lt;cluster-name&gt;_TIMESTAMP</code>. In that directory, give the following command to delete the cluster:</p> 
TERRAFORM_CONFIG=$(pwd)/.terraformrc terraform destroy --force 
<p>This destroys the cluster and all associated resources.</p> 
<p>You may have forgotten to download the assets. There is a copy of the assets in the directory <code>tectonic/tectonic-installer/darwin/clusters</code>. In this directory, another directory with the name <code>&lt;cluster-name&gt;_TIMESTAMP</code> contains your assets.</p> 
<b>Conclusion</b> 
<p>This post explained how to manage Kubernetes clusters using the CoreOS Tectonic graphical installer. &nbsp;For more details, see <a href="https://coreos.com/tectonic/docs/latest/install/aws/index.html">Graphical Installer with AWS</a>. If the installation does not succeed, see the helpful <a href="https://coreos.com/tectonic/docs/latest/install/aws/troubleshooting.html">Troubleshooting tips</a>. After the cluster is created, see the <a href="https://coreos.com/tectonic/docs/latest/tutorials/index.html">Tectonic tutorials</a> to learn how to deploy, scale, version, and delete an application.</p> 
<p>Future posts in this series will explain other ways of creating and running a Kubernetes cluster on AWS.</p> 
<p>—<a href="https://twitter.com/arungupta">Arun</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2819');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture5.png" /> 
<b class="lb-b blog-post-title" property="name headline">Delivering Graphics Apps with Amazon AppStream 2.0</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Deepak Suryanarayanan</span></span> | on 
<time property="datePublished" datetime="2017-09-12T10:02:21+00:00">12 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/amazon-appstream-2-0/" title="View all posts in Amazon AppStream 2.0*"><span property="articleSection">Amazon AppStream 2.0*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/" title="View all posts in Desktop &amp; App Streaming*"><span property="articleSection">Desktop &amp; App Streaming*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/delivering-graphics-apps-with-amazon-appstream-2-0/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2792" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2792&amp;disqus_title=Delivering+Graphics+Apps+with+Amazon+AppStream+2.0&amp;disqus_url=https://aws.amazon.com/blogs/compute/delivering-graphics-apps-with-amazon-appstream-2-0/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2792');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><strong>Sahil Bahri, Sr. Product Manager, Amazon AppStream 2.0</strong></p> 
<p><strong><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture6-300x207.png" /></a></strong></p> 
<p>Do you need to provide a workstation class experience for users who run graphics apps? With Amazon AppStream 2.0, you can stream graphics apps from AWS to a web browser running on any supported device. AppStream 2.0 offers a choice of GPU instance types. The range includes the newly launched Graphics Design instance, which allows you to offer a fast, fluid user experience at a fraction of the cost of using a graphics workstation, without upfront investments or long-term commitments.</p> 
<p>In this post, I discuss the Graphics Design instance type in detail, and how you can use it to deliver a graphics application such as Siemens NX―a popular CAD/CAM application that we have been testing on AppStream 2.0 with engineers from Siemens PLM.</p> 
<h3>Graphics Instance Types on AppStream 2.0</h3> 
<p>First, a quick recap on the GPU&nbsp;instance types available with AppStream 2.0. In July, 2017, we launched graphics support for AppStream 2.0 with <a href="https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-appstream-2-0-now-supports-graphics-applications/">two new instance types</a> that Jeff Barr discussed on the <a href="https://aws.amazon.com/blogs/aws/new-gpu-powered-streaming-instances-for-amazon-appstream-2-0/">AWS Blog</a>:</p> 
<li>Graphics Desktop</li> 
<li>Graphics Pro</li> 
<p>Many customers in industries such as engineering, media, entertainment, and oil and gas are using these instances to deliver high-performance graphics applications to their users. These instance types are based on dedicated NVIDIA GPUs and can run the most demanding graphics applications, including those that rely on CUDA graphics API libraries.</p> 
<p>Last week, we added a new lower-cost instance type: <a href="https://aws.amazon.com/about-aws/whats-new/2017/09/introducing-amazon-appstream-2-graphics-design-a-new-lower-cost-instance-type-for-streaming-graphics-applications/">Graphics Design</a>. This instance type is a great fit for engineers, 3D modelers, and designers who use graphics applications that rely on the hardware acceleration of DirectX, OpenGL, or OpenCL APIs, such as Siemens NX, Autodesk AutoCAD, or Adobe Photoshop. The Graphics Design instance is based on AMD’s FirePro S7150x2 Server GPUs and equipped with AMD Multiuser GPU technology. The instance type uses virtualized GPUs to achieve lower costs, and is available in four instance sizes to scale and match the requirements of your applications.</p> 
<table style="border: 2px solid black;border-collapse: collapse;margin-left: auto;margin-right: auto"> 
<tbody> 
<tr style="border-bottom: 1px solid black;background-color: #e0e0e0"> 
<td style="border-right: 1px solid black;padding: 4px"><b>Instance</b></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>vCPUs</strong></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Instance RAM (GiB)</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center"><strong>GPU Memory (GiB)</strong></td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><b>stream.graphics-design.large</b></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">2</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">7.5 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">1</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>stream.graphics-design.xlarge</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">15.3 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">2</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>stream.graphics-design.2xlarge</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">30.5 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>stream.graphics-design.4xlarge</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">16</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">61 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8</td> 
</tr> 
</tbody> 
</table> 
<p>The following table compares all three graphics instance types on AppStream 2.0, along with example applications you could use with each.</p> 
<table style="border: 2px solid black;border-collapse: collapse;margin-left: auto;margin-right: auto"> 
<tbody> 
<tr style="border-bottom: 1px solid black;background-color: #e0e0e0"> 
<td style="border-right: 1px solid black;padding: 4px"><b>&nbsp;</b></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Graphics Design</strong></td> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Graphics Desktop</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center"><strong>Graphics Pro</strong></td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><b>Number of instance sizes</b></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">1</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">3</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>GPU memory range<br /> </strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">1–8 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">4 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8–32 GiB</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>vCPU range</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">2–16</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">8</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">16–32</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Memory range</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">7.5–61 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">15 GiB</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">122–488 GiB</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Graphics libraries supported</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">AMD FirePro S7150x2</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">NVIDIA GRID K520</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">NVIDIA Tesla M60</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Price range (N. Virginia AWS Region)</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">$0.25 – $2.00/hour</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">$0.5/hour</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">$2.05 – $8.20/hour</td> 
</tr> 
<tr style="border-bottom: 1px solid black"> 
<td style="border-right: 1px solid black;padding: 4px"><strong>Example applications</strong></td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">Adobe Premiere Pro, AutoDesk Revit, Siemens NX</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">AVEVA E3D, SOLIDWORKS</td> 
<td style="border-right: 1px solid black;padding: 4px;text-align: center">AutoDesk Maya, Landmark DecisionSpace, Schlumberger Petrel</td> 
</tr> 
</tbody> 
</table> 
<h3>Example graphics instance set up with Siemens NX</h3> 
<p>In the section, I walk through setting up Siemens NX with Graphics Design instances on AppStream 2.0. After set up is complete, users can able to access NX from within their browser and also access their design files from a file share. You can also use these steps to set up and test your own graphics applications on AppStream 2.0. Here’s the workflow:</p> 
<ol> 
<li>Create a file share to load and save design files.</li> 
<li>Create an AppStream 2.0 image with Siemens NX installed.</li> 
<li>Create an AppStream 2.0 fleet and stack.</li> 
<li>Invite users to access Siemens NX through a browser.</li> 
<li>Validate the setup.</li> 
</ol> 
<p>To learn more about AppStream 2.0 concepts and set up, see the previous post <a href="https://aws.amazon.com/blogs/compute/scaling-your-desktop-application-streams-with-amazon-appstream-2-0/">Scaling Your Desktop Application Streams with Amazon AppStream 2.0</a>. For a deeper review of all the setup and maintenance steps, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html">Amazon AppStream 2.0 Developer Guide</a>.</p> 
<h4><span style="color: #000080"><strong>Step 1: Create a file share to load and save design files</strong></span></h4> 
<p><strong>To launch and configure the file server</strong></p> 
<ol> 
<li>Open the EC2 console and choose <strong>Launch Instance</strong>.</li> 
<li>Scroll to the <strong>Microsoft Windows Server 2016 Base Image</strong> and choose <strong>Select</strong>.</li> 
<li>Choose an instance type and size for your file server (I chose the general purpose m4.large instance). Choose <strong>Next: Configure Instance Details</strong>.</li> 
<li>Select a VPC and subnet. You launch AppStream 2.0 resources in the same VPC. Choose <strong>Next: Add Storage.</strong></li> 
<li>If necessary, adjust the size of your EBS volume. Choose <strong>Review and Launch, Launch</strong>.</li> 
<li>On the Instances page, give your file server a name, such as My File Server.</li> 
<li>Ensure that the security group associated with the file server instance allows for incoming traffic from the security group that you select for your AppStream 2.0 fleets or image builders. You can use the default security group and select the same group while creating the image builder and fleet in later steps.</li> 
</ol> 
<p>Log in to the file server using a remote access client such as Microsoft Remote Desktop. For more information about connecting to an EC2 Windows instance, see&nbsp;<a href="http://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/connecting_to_windows_instance.html#connect-rdp">Connect to Your Windows Instance</a>.</p> 
<p><strong>To enable file sharing</strong></p> 
<ol> 
<li>Create a new folder (such as C:\My Graphics Files) and upload the shared files to make available to your users.</li> 
<li>From the Windows control panel, enable network discovery.</li> 
<li>Choose <strong>Server Manager</strong>, <strong>File and Storage Services, Volumes</strong>.</li> 
<li>Scroll to Shares and choose <strong>Start the</strong> <strong>Add Roles and Features Wizard</strong>. Go through the wizard to install the <strong>File Server and Share</strong> role.</li> 
<li>From the left navigation menu, choose <strong>Shares</strong>.</li> 
<li>Choose <strong>Start the New Share Wizard</strong> to set up your folder as a file share.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture1.png" /></a></li> 
<li>Open the context (right-click) menu on the share and choose <strong>Properties, Permissions, Customize Permissions</strong>.</li> 
<li>Choose <strong>Permissions, Add</strong>. Add <strong>Read</strong> and <strong>Execute</strong> permissions for everyone on the network.</li> 
</ol> 
<h4><span style="color: #000080"><strong>Step 2: &nbsp;Create an AppStream 2.0 image with Siemens NX installed</strong></span></h4> 
<p><strong>To connect to the image builder and install applications</strong></p> 
<ol> 
<li>Open the <a href="https://us-west-2.console.aws.amazon.com/appstream2/home?region=us-west-2#/stacks">AppStream 2.0 management console</a> and choose <strong>Images</strong>,<strong> Image Builder</strong>,<strong> Launch Image Builder</strong>.</li> 
<li>Create a graphics design image builder in the same VPC as your file server.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture2.png" /></a></li> 
<li>From the <strong>Image builder</strong> tab, select your image builder and choose <strong>Connect</strong>. This opens a new browser tab and display a desktop to log in to.</li> 
<li>Log in to your image builder as ImageBuilderAdmin.</li> 
<li>Launch the <strong>Image Assistant</strong>.</li> 
<li>Download and install Siemens NX and other applications on the image builder. I added Blender and Firefox, but you could replace these with your own applications.</li> 
<li>To verify the user experience, you can test the application performance on the instance.</li> 
</ol> 
<p>Before you finish creating the image, you must mount the file share by enabling a few Microsoft Windows services.</p> 
<p><strong>To mount the file share</strong></p> 
<ol> 
<li>Open services.msc and check the following services:</li> 
</ol> 
<li>DNS Client</li> 
<li>Function Discovery Resource Publication</li> 
<li>SSDP Discovery</li> 
<li>UPnP Device H</li> 
<ol start="2"> 
<li>If any of the preceding services have <strong>Startup Type</strong> set to <strong>Manual</strong>, open the context (right-click) menu on the service and choose <strong>Start</strong>. Otherwise, open the context (right-click) menu on the service and choose <strong>Properties</strong>. For <strong>Startup Type</strong>, choose <strong>Manual</strong>, <strong>Apply</strong>. To start the service, choose <strong>Start</strong>.</li> 
<li>From the Windows control panel, enable network discovery.</li> 
<li>Create a batch script that mounts a file share from the storage server set up earlier. The file share is mounted automatically when a user connects to the AppStream 2.0 environment.</li> 
</ol> 
<p style="padding-left: 90px"><strong>Logon Script Location:</strong> C:\Users\Public\logon.bat</p> 
<p style="padding-left: 90px"><strong>Script Contents: </strong></p> 
<p style="padding-left: 90px">:loop</p> 
<p style="padding-left: 90px">net use H: <em>\\path\to\network\share</em><em>&nbsp;</em></p> 
<p style="padding-left: 90px">PING localhost -n 30 &gt;NUL</p> 
<p style="padding-left: 90px">IF NOT EXIST H:\ GOTO loop</p> 
<ol start="5"> 
<li>Open gpedit.msc and choose <strong>User Configuration</strong>, <strong>Windows Settings</strong>, <strong>Scripts</strong>. Set logon.bat as the user logon script.</li> 
<li>Next, create a batch script that makes the mounted drive visible to the user.</li> 
</ol> 
<p style="padding-left: 90px"><strong>Logon Script Location:</strong> C:\Users\Public\startup.bat</p> 
<p style="padding-left: 90px"><strong>Script Contents: </strong><br /> REG DELETE “HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Policies\Explorer” /v “NoDrives” /f</p> 
<ol start="7"> 
<li>Open <strong>Task Scheduler</strong> and choose <strong>Create Task</strong>.</li> 
<li>Choose <strong>General</strong>, provide a task name, and then choose <strong>Change User or Group</strong>.</li> 
<li>For <strong>Enter the object name to select</strong>, enter <strong>SYSTEM</strong> and choose <strong>Check Names</strong>, <strong>OK</strong>.</li> 
<li>Choose <strong>Triggers</strong>, <strong>New</strong>. For <strong>Begin the task</strong>, choose <strong>At startup</strong>. Under <strong>Advanced Settings</strong>, change <strong>Delay task for</strong> to 5 minutes. Choose <strong>OK</strong>.</li> 
<li>Choose <strong>Actions</strong>, <strong>New</strong>. Under <strong>Settings</strong>, for <strong>Program/script</strong>, enter <strong>C:\Users\Public\startup.bat</strong>. Choose <strong>OK</strong>.</li> 
<li>Choose <strong>Conditions</strong>. Under <strong>Power</strong>, clear the <strong>Start the task only if the computer is on AC power</strong> Choose <strong>OK</strong>.</li> 
<li>To view your scheduled task, choose <strong>Task Scheduler Library</strong>. Close Task Scheduler when you are done.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture3.png" /></a></li> 
</ol> 
<h4><span style="color: #000080">Step 3: &nbsp;Create an AppStream 2.0 fleet and stack</span></h4> 
<p><strong>To create a fleet and stack</strong></p> 
<ol> 
<li>In the AppStream 2.0 management console, choose <strong>Fleets, Create Fleet. </strong></li> 
<li>Give the fleet a name, such as Graphics-Demo-Fleet, that uses the newly created image and the same VPC as your file server.</li> 
<li>Choose <strong>Stacks</strong>, <strong>Create Stack</strong>. Give the stack a name, such as Graphics-Demo-Stack.</li> 
<li>After the stack is created, select it and choose <strong>Actions</strong>, <strong>Associate Fleet</strong>. Associate the stack with the fleet you created in step 1.</li> 
</ol> 
<h4><span style="color: #000080">Step 4: &nbsp;Invite users to access Siemens NX through a browser</span></h4> 
<p><strong>To invite users</strong></p> 
<ol> 
<li>Choose <strong>User Pools</strong>, <strong>Create User</strong> to create users.</li> 
<li>Enter a name and email address for each user.</li> 
<li>Select the users just created, and choose <strong>Actions</strong>,<strong> Assign Stack</strong> to provide access to the stack created in step 2. You can also provide access using SAML 2.0 and connect to your Active Directory if necessary. For more information, see the <a href="https://aws.amazon.com/blogs/compute/enabling-identity-federation-with-ad-fs-3-0-and-amazon-appstream-2-0/">Enabling Identity Federation with AD FS 3.0 and Amazon AppStream 2.0</a> post.</li> 
</ol> 
<p>Your user receives an email invitation to set up an account and use a web portal to access the applications that you have included in your stack.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture4.png" /></a></p> 
<h4><span style="color: #000080">Step 5: &nbsp;Validate the setup</span></h4> 
<p><strong>Time for a test drive with Siemens NX on AppStream 2.0!</strong></p> 
<ol> 
<li>Open the link for the AppStream 2.0 web portal shared through the email invitation. The web portal opens in your default browser. You must sign in with the temporary password and set a new password. After that, you get taken to your app catalog.</li> 
<li>Launch Siemens NX and interact with it using the demo files available in the shared storage folder – My Graphics Files.<em>&nbsp;</em></li> 
</ol> 
<p>After I launched NX, I captured the screenshot below. The Siemens PLM team also recorded a <a href="https://www.youtube.com/watch?v=lOWPcFjwkCE">video with NX running on AppStream 2.0</a>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/12/Picture5.png" /></a></p> 
<h3>Summary</h3> 
<p>In this post, I discussed the GPU instances available for delivering rich graphics applications to users in a web browser. While I demonstrated a simple setup, you can scale this out to launch a production environment with users signing in using <a href="https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-appstream-2-0-now-supports-microsoft-active-directory-domains/">Active Directory credentials,&nbsp;</a> accessing <a href="https://aws.amazon.com/about-aws/whats-new/2017/05/amazon-appstream-2-0-now-offers-persistent-storage-for-end-users-files-backed-by-amazon-s3/">persistent storage with Amazon S3</a>, and using other commonly requested features reviewed in the <a href="https://aws.amazon.com/blogs/aws/amazon-appstream-2-0-launch-recap-domain-join-simple-network-setup-and-lots-more/">Amazon AppStream 2.0 Launch Recap – Domain Join, Simple Network Setup, and Lots More</a> post.</p> 
<p>To learn more about AppStream 2.0 and capabilities added this year, see <a href="https://aws.amazon.com/appstream2/resources/">Amazon AppStream 2.0 Resources</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2792');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/1.png" /> 
<b class="lb-b blog-post-title" property="name headline">How to Provision Complex, On-Demand Infrastructures by Using Amazon API Gateway and AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ben Eichorst</span></span> and 
<span property="author" typeof="Person"><span property="name">Veronika Megler</span></span> | on 
<time property="datePublished" datetime="2017-09-12T07:11:20+00:00">12 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/devops/" title="View all posts in DevOps*"><span property="articleSection">DevOps*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/how-to-provision-complex-on-demand-infrastructures-by-using-amazon-api-gateway-and-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2714" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2714&amp;disqus_title=How+to+Provision+Complex%2C+On-Demand+Infrastructures+by+Using+Amazon+API+Gateway+and+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/how-to-provision-complex-on-demand-infrastructures-by-using-amazon-api-gateway-and-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2714');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Many AWS customers are using the power of AWS CloudFormation to customize complex infrastructures. At the same time, they are moving towards self-service for their expanding customer bases. How can complex infrastructure be provisioned on-demand while minimizing customer use of the AWS Management Console?</p> 
<p>Let’s say AnyCompany uses AWS services to process sensitive datasets owned by its customers. These customers need to be able to provision their own processing infrastructure on demand. However, AnyCompany doesn’t want the customers to access <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a> directly, or see how customer data is processed. How can AnyCompany create resources with CloudFormation templates without exposing proprietary processing methods?</p> 
<p>You can use <a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a> and <a href="https://aws.amazon.com/lambda/">AWS Lambda </a>to provide complex, on-demand, data-processing infrastructure to users who have no need to access the AWS Management Console. In this post, we walk through the setup and configuration of working code examples that you (and your customers) can use to provision on-demand infrastructure. This post’s solution combines principles of immutable computing with a method to provide granular access to powerful capabilities available in the AWS Cloud. In this post, you create immutability for an Amazon EC2 instance by provisioning it without an SSH key or any other access mechanism. The instance can’t be altered after it’s launched.</p> 
<p><span id="more-2714"></span></p> 
<b>Solution architecture</b> 
<p>API Gateway simplifies the creation, management, and deployment of APIs. Integration of API Gateway with Lambda, the AWS serverless compute service, allows for further interaction with the larger family of AWS services. In this post, the Lambda function provisions on-demand CloudFormation infrastructure stacks for our example service’s primary business function: &nbsp;the calculation of pi.</p> 
<p>Two CloudFormation templates are used to provision infrastructure stacks:</p> 
<li>The <em>primary</em> template</li> 
<li>The <em>business-function</em> template</li> 
<p>The primary CloudFormation template creates the base infrastructure that is shown in the following diagram.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/1.png" /></a></p> 
<p>The primary template creates a stack of resources:</p> 
<li>API Gateway resources</li> 
<li>The associated AWS Identity and Access Management (IAM) security roles</li> 
<li>An <a href="https://aws.amazon.com/s3/">Amazon S3</a> bucket for the results of the data processed by the business-function template</li> 
<li>A Lambda function specifically for the purpose of creating multiple iterations of a second <em>repeatable infrastructure</em>. This repeatable infrastructure, contained within the second CloudFormation template, is referred to as the business-function template.</li> 
<p>The API created by the primary template allows system users to pass selected parameters to the business-function template, such as cost-center tags, a unique name, and data-processing parameters. The ability to pass parameters allows the business-function infrastructure to adapt to the specific needs of each request. In this post’s example—calculating the first 15,000 digits of pi—the number of digits calculated can be specified in the request initiated by the customer. For your own business function templates, this could be the location of input data or an email address to which results are sent.</p> 
<p>In the example business-function template, the template provisions an immutable EC2 instance that calculates a specified number of digits of pi and uploads the results to the S3 bucket. The business-function stack then self-terminates, reducing any potential extra costs.</p> 
<p>This architecture allows you, at multiple points, to control access, processing parameters, and workflow.</p> 
<p>In the earlier diagram, (1) all access to the AWS Management Console and API for your account’s infrastructure is segregated through the use of API Gateway. Then, (2) IAM role–based restrictions are in place for API Gateway to call the Lambda function. The Lambda function serves to selectively add, subtract, or alter input parameters from external users. The code for this function is embedded in the primary CloudFormation template. You can modify the code to add specific tags for billing information or call other AWS services such as <a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB </a>or <a href="https://aws.amazon.com/rds">Amazon RDS</a> to keep a record of which infrastructure is instantiated by which API users.</p> 
<p>In our example template, the Lambda function is populated to pass only the required parameters for the business-function template’s calculation of pi. The use of the function restricts the ability of customers to inject unexpected or undesirable parameters. The solution then (3) uses a Lambda execution role to control the Lambda function’s access. Finally, (4) the Lambda function initiates the CloudFormation business-function template by using another IAM role that is restricted to instantiating only the resources required.</p> 
<p>The use of an intermediate Lambda function here allows for heavy customization and filtering of the input requests from the customer-facing API Gateway.</p> 
<b>Solution Features</b> 
<p>By using this API Gateway–based solution, you can benefit in a number of ways:</p> 
<li> <h4>User requests are decoupled from the infrastructure that fulfills those requests.</h4> <p>This post’s solution removes the need for users to have access to the AWS Management Console. Users can accomplish their infrastructure-backed requests on demand without having any direct access to the console. Additionally, processing methods and infrastructure can be switched in and out without any change to the external appearance of your service. For example, in this post, I could have a containerized solution such as <a href="https://aws.amazon.com/ecs/">Amazon EC2 Container Services (Amazon ECS) </a>process my pi-calculation service without any visibility to users.</p></li> 
<li> <h4>Proprietary processing methods and infrastructure are obscured from users.</h4> <p>The solution in this post can protect the proprietary secrets of your company’s data processing service by obscuring which processing methods you are using and which infrastructure performs the processing. The solution also isolates users from viewing each other’s operations. Without having console access, users cannot determine which running processing stacks are initiated by other users.</p></li> 
<li> <h4>On-demand infrastructure in specific configurations is created without allowing users to provision arbitrary infrastructure.</h4> <p>The solution in this post complements the functionality of the <a href="https://aws.amazon.com/servicecatalog/">AWS Service Catalog</a>. Beyond allowing only preapproved infrastructure and preventing unapproved resources from executing outside your company’s policies, this API Gateway–driven method delivers a specific, processed result. For ease of troubleshooting and integration with DevOps workflows, you can standardize, version, and deploy complicated CloudFormation stacks for networking, compute, storage, and big data processing.</p></li> 
<li> <h4>Simplification of the user experience.</h4> <p>This solution also simplifies the user experience. Commands that request a specific result are limited to a single, familiar REST API interface call that delivers only that result.</p></li> 
<li> <h4>Cost savings through self-terminating infrastructure.</h4> <p>The architecture of this solution is augmented through the use of self-terminating CloudFormation stacks. Stacks can spin up expensive infrastructure to perform data processing on demand, and self-terminate as soon as the task has completed. This can save you infrastructure costs associated with idle resources.</p></li> 
<b>Walkthrough: Deploy the solution</b> 
<p>This post’s example templates create the base infrastructure and a business-function process. The business-function process can instantiate a single EC2 instance to calculate a specified number of digits of pi and deposit the results in an S3 bucket. As noted previously, this example includes built-in logic to automatically self-terminate the business-function template’s CloudFormation stacks after instantiation.</p> 
<p>The following steps walk through how to provision this solution for creating on-demand business-function CloudFormation stacks through API Gateway. You can modify the business-function template code for your specialized infrastructure needs. However, you must upload the modified template to your own S3 bucket before completing Step 1 because the business-function template location is required for the primary template’s parameters.</p> 
<p>Both templates are designed for use in the us-west-2 region and may require minor modifications to function properly in other regions.</p> 
<h3>Costs</h3> 
<p>API Gateway costs are based on the number of API calls requested, and the single call here leads to a near-zero cost. For current pricing, see <a href="https://aws.amazon.com/api-gateway/pricing/">Amazon API Gateway Pricing</a>. The associated <a href="https://aws.amazon.com/s3/pricing/">S3 costs</a> and <a href="https://aws.amazon.com/lambda/pricing/">Lambda costs</a> for this post also are negligible because you are not transferring much data or spending a significant amount of time processing serverless code. The most expensive part of this post’s solution is the <a href="https://aws.amazon.com/ec2/pricing/on-demand/">on-demand EC2 instance cost</a> invoked in each business-function template instantiation. This cost is approximately $0.012 per hour for each run with the default t2.micro EC2 instance. In total, running the solution as presented in this post should cost you less than $0.10 in order to test multiple calculations of pi.</p> 
<p>&nbsp;</p> 
<h3>1. Deploy the CloudFormation template</h3> 
<p>First, deploy the primary CloudFormation template for API Gateway and other resources. This template creates IAM policies, IAM roles, API Gateway resources, a new S3 bucket, and a Lambda function.</p> 
<p>The primary template requires you to have enabled <a href="https://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/">API Gateway Usage Plans</a>.</p> 
<p>You can deploy the primary template and launch the primary stack in the us-west-2 region by selecting the following button. Aside from <strong>Stack name</strong>, the only required parameters are the exact location of the business-function template in S3 and a unique S3 bucket name as a location for template results.</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=DemoAPIGControlCFStack&amp;templateURL=https://s3.amazonaws.com/awsiammedia/public/sample/APIGatewayWrapper/PrimaryAPIGatewayWrapperTemplate.yaml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/2-launch.png" /></a></p> 
<ol> 
<li>In the CloudFormation console, on the <strong>Specify Details</strong> page, type a stack name.</li> 
<li>For <strong>NewS3BucketName</strong>, type a unique name.</li> 
<li>For <strong>S3CFTLocation</strong>, leave the default value. If you adapt the primary template for your own customized business-function template, be sure to include the entire URI of the S3 location (for example, <em>https://s3-us-west-2.amazonaws.com/mys3bucket/businessFunctionTemplate.yaml</em>).</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/3.png" /></a></p> 
<ol> 
<li>On the <strong>Options</strong> page, no changes are required, but you can specify additional tags if desired.</li> 
<li>On the <strong>Capabilities</strong> page, acknowledge that CloudFormation might create IAM resources, as shown in the following screenshot. To finish creating the stack, choose <strong>Create</strong>.</li> 
</ol> 
<p>&nbsp;</p> 
<h3>2. Create an API key for the API Gateway</h3> 
<p>Though you could modify the example primary template for public use, this example restricts access to execute API calls through the use of API keys. After you create the API key, you must associate the new API key with the usage plan created by the primary template.</p> 
<ol> 
<li>In the <a href="https://console.aws.amazon.com/apigateway/home">API Gateway console</a>, choose <strong>API Keys</strong>.</li> 
<li>Choose <strong>Actions</strong>, <strong>Create API key</strong>.</li> 
<li>For <strong>Name</strong>, type a name for the key. For <strong>API Key</strong>, leave the default <strong>Auto Generate</strong></li> 
<li>(Optional) For <strong>Description</strong>, type a value.</li> 
<li>Choose <strong>Save</strong>.</li> 
<li>Choose <strong>Add to Usage Plan</strong> and type WrappingApiUsagePlan. This usage plan defaults your Stage to PROD because it is the only configured stage.</li> 
<li>To add the key to the usage plan, choose the green check icon. Your key should resemble the following screenshot.<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/4.png" /></a></li> 
<li>Choose <strong>Show</strong> and note the API key value for the next step.</li> 
</ol> 
<h3>3. Initiate a REST API query</h3> 
<p>Now, initiate a REST API query against the API that you created. in the example curl command below, replace the placeholder API key specified in the x-api-key HTTP header with your API key.</p> 
<p>You also need to replace the example URI path with your API’s specific URI. Create the full URI by concatenating the base URI listed in the CloudFormation output for the primary template stack with the primary template’s API call path, which is /PROD/PerRunExecute. This full URI for your specific API call should closely resemble the <strong>URI path&nbsp;</strong>(e.g.&nbsp;<em>https://abcdefghi.execute-api.us-west-2.amazonaws.com/PROD/PerRunExecute</em>) in the following example curl command.</p> 
<code class="lang-bash">curl -v -X POST -i -H &quot;x-api-key: 012345ABCDefGHIjkLMS20tGRJ7othuyag&quot; <strong>https://abcdefghi.execute-api.us-west-2.amazonaws.com/PROD/PerRunExecute</strong> -d '{
&quot;StackName&quot;:&quot;MyCalculatePiStack&quot;,
&quot;CostCenter&quot;:&quot;12345&quot;,
&quot;DigitsToCalculate&quot;: &quot;15000&quot;
}'</code> 
<p>When you run the curl command, it executes a POST query to the REST API to instantiate the business-function template and calculate the first 15,000 digits of pi. A successful API query returns information on the curl request as well as CloudFormation API output similar to the following, indicating an “HTTPStatusCode” of 200.</p> 
<code class="lang-json">{&quot;StackId&quot;: &quot;arn:aws:cloudformation:us-west-2:123456789012:stack/MyCalculatePiStack/1234debf-9423-11e7-9a08-50399b8d8a8c&quot;, &quot;ResponseMetadata&quot;: {&quot;RetryAttempts&quot;: 0, &quot;HTTPStatusCode&quot;: 200, &quot;RequestId&quot;: &quot;0cbabc97-9423-11e7-97b3-395d92028ea7&quot;, &quot;HTTPHeaders&quot;: {&quot;x-amzn-requestid&quot;: &quot;0cbabc97-9423-11e7-97b3-395d92028ea7&quot;, &quot;date&quot;: &quot;Thu, 07 Sep 2017 23:19:52 GMT&quot;, &quot;content-length&quot;: &quot;388&quot;, &quot;content-type&quot;: &quot;text/xml&quot;}}}</code> 
 
<h3>4. Examine the output</h3> 
<p>After the EC2 instance completes its processing and uploads the results to the S3 bucket created by the primary template, you can examine the output. Using the configured t2.micro EC2 instance, instantiation and calculation for the first 15,000 digits of pi takes approximately 5–10 minutes. You can monitor progress in the <a href="https://console.aws.amazon.com/ec2/v2/home">EC2 console</a> (where you see a new EC2 instance running) or the <a href="https://console.aws.amazon.com/cloudformation/home">CloudFormation console</a> (where you see the provisioned stack in a “created” state).</p> 
<p>After the first 15,000 digits of pi have been uploaded to the S3 bucket, the EC2 instance self-terminates the stack. You can then download the results file from the S3 bucket, as shown below. The naming convention for pi calculating runs is “PiCalculation” concatenated with an epoch timestamp from the time of execution.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/07/s3results.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/07/s3results.png" /></a></p> 
<p>This file contains the instance ID that performed the processing, a time stamp, and the first 15,000 digits of pi. The output should look similar to the following screenshot.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/07/results.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/07/results.png" /></a></p> 
<p>&nbsp;</p> 
<h3>5. Clean up</h3> 
<p>To help minimize your costs, clean up any infrastructure that resulted from following the solution in this post. Remember that you first have to detach the API key that you created from the usage plan in order to delete the primary CloudFormation stack. You can do this by clicking the <strong>X</strong> for associated usage plans for your API key, as shown in the following screenshot.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/usageplans.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/usageplans.png" /></a></p> 
<p>You also must empty the S3 results bucket of all files for the bucket to be deleted when the CloudFormation stack self-terminates.</p> 
<p>This solution can be adapted to any CloudFormation template encompassing a business function for your service.</p> 
<b>Adapt to Your Services</b> 
<p>To adapt this example to your own business-function:</p> 
<ol> 
<li>Write a CloudFormation template that implements your new business function.</li> 
<li>In the primary template, customize the Lambda resource object code for “PerRunCloudFormationExecutionLambda” to accept and pass on the specific parameters needed by your business function template. Save it with a new name.</li> 
<li>Upload your custom business-function CloudFormation template developed in step 1 to an S3 bucket with permissions that allow the CloudFormation service to access the template objects. For more information about S3 access control, see Managing Access Permissions to Your Amazon S3 Resource.</li> 
<li>Start your new stack with the modified primary CloudFormation template. Pass it, as a parameter, the name of your new business function template as a parameter. You can reuse the same API Gateway usage plan created previously, if desired.</li> 
<li>Test, using your new stack name in place of the bolded content above, and posting an API Gateway query with parameters needed by your new business function.</li> 
</ol> 
<b>Conclusion</b> 
<p>By using API Gateway, you can control complex, on-demand CloudFormation stacks that create AWS infrastructure. When you combine this functionality with self-terminating templates, you can realize significant cost savings. API Gateway also allows for standardization of infrastructure. It can enable your users to instantiate complex and costly architectures on demand and only for as long as your users need them.</p> 
<p>Though the solution in this post assumes that the business function is to calculate the first 15,000 digits of pi (a less than profitable venture these days), you can adopt the solution to deliver significant cost savings. For example, instead of a single EC2 instance, the business-function template could instantiate <a href="https://aws.amazon.com/redshift/">Amazon Redshift</a> (a petabyte-scale data warehouse), <a href="https://aws.amazon.com/emr/">Amazon EMR</a>, or any complex processing infrastructure. As with all on-demand AWS services, you pay for the infrastructure only when it is running.</p> 
<p>If you have comments about any of this content, submit them in the “Comments” section below. If you have questions about implementing this solution, start a new thread on the <a href="https://forums.aws.amazon.com/forum.jspa?forumID=199">API Gateway forum</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2714');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Parallel Processing in Python with&nbsp;AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Oz Akan</span></span> | on 
<time property="datePublished" datetime="2017-09-11T07:29:32+00:00">11 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/parallel-processing-in-python-with-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2746" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2746&amp;disqus_title=Parallel+Processing+in+Python+with%C2%A0AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/parallel-processing-in-python-with-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2746');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>If you develop an AWS Lambda function with Node.js, you can call multiple web services without waiting for a response due to its asynchronous nature. &nbsp;All requests are initiated almost in parallel, so you can get results much faster than a series of sequential calls to each web service. Considering the maximum execution duration for Lambda, it is beneficial for I/O bound tasks to run in parallel.</p> 
<p>If you develop a Lambda function with Python, parallelism doesn’t come by default. Lambda supports Python 2.7 and Python 3.6, both of which have multiprocessing and threading modules. The multiprocessing module supports multiple cores so it is a better choice, especially for CPU intensive workloads. With the threading module, all threads are going to run on a single core though performance difference is negligible for network-bound tasks.</p> 
<p>In this post, I demonstrate how the Python multiprocessing module can be used within a Lambda function to run multiple I/O bound tasks in parallel.</p> 
<b>Example use case</b> 
<p>In this example, you call Amazon EC2 and Amazon EBS API operations to find the total EBS volume size for all your EC2 instances in a region.</p> 
<p>This is a two-step process:</p> 
<li>The Lambda function calls EC2 to list all EC2 instances</li> 
<li>The function calls EBS for each instance to find attached EBS volumes</li> 
<h3>Sequential Execution</h3> 
<p>If you make these calls sequentially, during the second step, your code has to loop over all the instances and wait for each response before moving to the next request.</p> 
<p>The class named VolumesSequential has the following methods:</p> 
<li>__init__ creates an EC2 resource.</li> 
<li>total_size returns all EC2 instances and passes these to the instance_volumes method.</li> 
<li>instance_volumes finds the total size of EBS volumes for the instance.</li> 
<li>total_size adds all sizes from all instances to find total size for the EBS volumes.</li> 
<h4>Source Code for Sequential Execution</h4> 
<code class="lang-python">import time
import boto3
class VolumesSequential(object):
&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;Finds total volume size for all EC2 instances&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;def __init__(self):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.ec2 = boto3.resource('ec2')
&nbsp;&nbsp;&nbsp;def instance_volumes(self, instance):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finds total size of the EBS volumes attached
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to an EC2 instance
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instance_total = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for volume in instance.volumes.all():
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instance_total += volume.size
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return instance_total
&nbsp;&nbsp;&nbsp;def total_size(self):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lists all EC2 instances in the default region
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and sums result of instance_volumes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print &quot;Running sequentially&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instances = self.ec2.instances.all()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instances_total = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for instance in instances:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instances_total += self.instance_volumes(instance)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return instances_total
def lambda_handler(event, context):
&nbsp;&nbsp;&nbsp;&nbsp;volumes = VolumesSequential()
&nbsp;&nbsp;&nbsp;&nbsp;_start = time.time()
&nbsp;&nbsp;&nbsp;&nbsp;total = volumes.total_size()
&nbsp;&nbsp;&nbsp;print &quot;Total volume size:&nbsp;%s&nbsp;GB&quot; % total
&nbsp;&nbsp;&nbsp;print &quot;Sequential execution time:&nbsp;%s&nbsp;seconds&quot; % (time.time() - _start)</code> 
<h3>Parallel Execution</h3> 
<p>The multiprocessing module that comes with Python 2.7 lets you run multiple processes in parallel. Due to the Lambda execution environment not having /dev/shm (shared memory for processes) support, you can’t use multiprocessing.Queue or multiprocessing.Pool.</p> 
<p>If you try to use multiprocessing.Queue, you get an error similar to the following:</p> 
<code class="lang-bash">[Errno 38] Function not implemented: OSError
…
&nbsp;&nbsp;&nbsp;sl = self._semlock = _multiprocessing.SemLock(kind, value, maxvalue)
OSError: [Errno 38] Function not implemented
</code> 
<p>On the other hand, you can use multiprocessing.Pipe instead of multiprocessing.Queue to accomplish what you need without getting any errors during the execution of the Lambda function.</p> 
<p>The class named VolumeParallel has the following methods:</p> 
<li>__init__ creates an EC2 resource</li> 
<li>instance_volumes finds the total size of EBS volumes attached to an instance</li> 
<li>total_size finds all instances and runs instance_volumes for each to find the total size of all EBS volumes attached to all EC2 instances.</li> 
<h4>Source Code for Parallel Execution</h4> 
<code class="lang-python">import time
from multiprocessing import Process, Pipe
import boto3
class VolumesParallel(object):
&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;Finds total volume size for all EC2 instances&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;def __init__(self):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.ec2 = boto3.resource('ec2')
&nbsp;&nbsp;&nbsp;def instance_volumes(self, instance, conn):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Finds total size of the EBS volumes attached
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to an EC2 instance
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instance_total = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for volume in instance.volumes.all():
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instance_total += volume.size
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conn.send([instance_total])
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;conn.close()
&nbsp;&nbsp;&nbsp;def total_size(self):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lists all EC2 instances in the default region
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and sums result of instance_volumes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print &quot;Running in parallel&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# get all EC2 instances
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instances = self.ec2.instances.all()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a list to keep all processes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;processes = []
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a list to keep connections
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parent_connections = []
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a process per instance
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for instance in instances: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create a pipe for communication
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parent_conn, child_conn = Pipe()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parent_connections.append(parent_conn)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# create the process, pass instance and connection
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;process = Process(target=self.instance_volumes, args=(instance, child_conn,))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;processes.append(process)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# start all processes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for process in processes:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;process.start()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# make sure that all processes have finished
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for process in processes:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;process.join()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instances_total = 0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for parent_connection in parent_connections:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;instances_total += parent_connection.recv()[0]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return instances_total
def lambda_handler(event, context):
&nbsp;&nbsp;&nbsp;&nbsp;volumes = VolumesParallel()
&nbsp;&nbsp;&nbsp;&nbsp;_start = time.time()
&nbsp;&nbsp;&nbsp;&nbsp;total = volumes.total_size()
&nbsp;&nbsp;&nbsp;print &quot;Total volume size:&nbsp;%s&nbsp;GB&quot; % total
&nbsp;&nbsp;&nbsp;print &quot;Sequential execution time:&nbsp;%s&nbsp;seconds&quot; % (time.time() - _start)
</code> 
<b>Performance</b> 
<p>There are a few differences between two Lambda functions when it comes to the execution environment. The parallel function requires more memory than the sequential one. You may run the parallel Lambda function with a relatively large memory setting to see how much memory it uses. The amount of memory required by the Lambda function depends on what the function does and how many processes it runs in parallel. To restrict maximum memory usage, you may want to limit the number of parallel executions.</p> 
<p>In this case, when you give 1024 MB for both Lambda functions, the parallel function runs about two times faster than the sequential function. I have a handful of EC2 instances and EBS volumes in my account so the test ran way under the maximum execution limit for Lambda. Remember that parallel execution doesn’t guarantee that the runtime for the Lambda function will be under the maximum allowed duration but does speed up the overall execution time.</p> 
<h4>Sequential Run Time Output</h4> 
<code class="lang-bash">START RequestId: 4c370b12-f9d3-11e6-b46b-b5d41afd648e Version: $LATEST
Running sequentially
Total volume size: 589 GB
Sequential execution time: 3.80066084862 seconds
END RequestId: 4c370b12-f9d3-11e6-b46b-b5d41afd648e
REPORT RequestId: 4c370b12-f9d3-11e6-b46b-b5d41afd648e Duration: 4091.59 ms Billed Duration: 4100 ms &nbsp;Memory Size: 1024 MB Max Memory Used: 46 MB</code> 
<h4>Parallel Run Time Output</h4> 
<code class="lang-bash">START RequestId: 4f1328ed-f9d3-11e6-8cd1-c7381c5c078d Version: $LATEST
Running in parallel
Total volume size: 589 GB
Sequential execution time: 1.89170885086 seconds
END RequestId: 4f1328ed-f9d3-11e6-8cd1-c7381c5c078d
REPORT RequestId: 4f1328ed-f9d3-11e6-8cd1-c7381c5c078d Duration: 2069.33 ms Billed Duration: 2100 ms &nbsp;Memory Size: 1024 MB Max Memory Used: 181 MB&nbsp;
</code> 
<h3>Summary</h3> 
<p>In this post, I demonstrated how to run multiple I/O bound tasks in parallel by developing a Lambda function with the Python multiprocessing module. With the help of this module, you freed the CPU from waiting for I/O and fired up several tasks to fit more I/O bound operations into a given time frame. This might be the trick to reduce the overall runtime of a Lambda function especially when you have to run so many and don’t want to split the work into smaller chunks.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2746');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Disabling Intel Hyper-Threading Technology on Amazon EC2 Windows Instances</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Brian Beach</span></span> | on 
<time property="datePublished" datetime="2017-09-06T13:18:39+00:00">06 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-ec2-windows-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2352" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2352&amp;disqus_title=Disabling+Intel+Hyper-Threading+Technology+on+Amazon+EC2+Windows+Instances&amp;disqus_url=https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-ec2-windows-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2352');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>In a prior post, <a href="https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-linux/">Disabling Intel Hyper-Threading on Amazon Linux</a>, I investigated how the Linux kernel enumerates CPUs. I also discussed the options to disable Intel Hyper-Threading (HT Technology) in Amazon Linux running on <a href="https://aws.amazon.com/ec2">Amazon EC2</a>.</p> 
<p>In this post, I do the same for Microsoft Windows Server 2016 running on EC2 instances. I begin with a quick review of HT Technology and the reasons you might want to disable it. I also recommend that you take a moment to review the prior post for a more thorough foundation.</p> 
<b>HT Technology</b> 
<p>HT Technology makes a single physical processor appear as multiple logical processors. Each core in an Intel Xeon processor has two threads of execution. Most of the time, these threads can progress independently; one thread executing while the other is waiting on a relatively slow operation (for example, reading from memory) to occur. However, the two threads do share resources and occasionally one thread is forced to wait while the other is executing.</p> 
<p>There a few unique situations where disabling HT Technology can improve performance. One example is high performance computing (HPC) workloads that rely heavily on floating point operations. In these rare cases, it can be advantageous to disable HT Technology. However, these cases are rare, and for the overwhelming majority of workloads you should leave it enabled. I recommend that you test with and without HT Technology enabled, and only disable threads if you are sure it will improve performance.</p> 
<p><span id="more-2352"></span></p> 
<b>Exploring HT Technology on Microsoft Windows</b> 
<p>Here’s how Microsoft Windows enumerates CPUs. As before, I am running these examples on an m4.2xlarge. I also chose to run Windows Server 2016, but you can walk through these exercises on any version of Windows. Remember that the m4.2xlarge has eight vCPUs, and each vCPU is a thread of an Intel Xeon core. Therefore, the m4.2xlarge has four cores, each of which run two threads, resulting in eight vCPUs.</p> 
<p>Windows does not have a built-in utility to examine CPU configuration, but you can download the <a href="https://technet.microsoft.com/en-us/sysinternals/cc835722.aspx">Sysinternals coreinfo utility</a> from Microsoft’s website. This utility provides useful information about the system CPU and memory topology. For this walkthrough, you enumerate the individual CPUs, which you can do by running <code class="lang-powershell">coreinfo -c</code>. For example:</p> 
<code class="lang-powershell">C:\Users\Administrator &gt;coreinfo -c
Coreinfo v3.31 - Dump information on system CPU and memory topology
Copyright (C) 2008-2014 Mark Russinovich
Sysinternals - www.sysinternals.com
Logical to Physical Processor Map:
**------ Physical Processor 0 (Hyperthreaded)
--**---- Physical Processor 1 (Hyperthreaded)
----**-- Physical Processor 2 (Hyperthreaded)
------** Physical Processor 3 (Hyperthreaded)</code> 
<p>As you can see from the screenshot, the coreinfo utility displays a table where each row is a physical core and each column is a logical CPU. In other words, the two asterisks on the first line indicate that CPU 0 and CPU 1 are the two threads in the first physical core. Therefore, my m4.2xlarge has for four physical processors and each processor has two threads resulting in eight total CPUs, just as expected.</p> 
<p>It is interesting to note that Windows Server 2016 enumerates CPUs in a different order than Linux. Remember from the prior post that Linux enumerated the first thread in each core, followed by the second thread in each core. You can see from the output earlier that Windows Server 2016, enumerates both threads in the first core, then both threads in the second core, and so on. The diagram below shows the relationship of CPUs to cores and threads in both operating systems.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin1-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin1-1.png" /></a></p> 
<p>In the Linux post, I disabled CPUs 4–6, leaving one thread per core, and effectively disabling HT Technology. You can see from the diagram that you must disable the odd-numbered threads (that is, 1, 3, 5, and 7) to achieve the same result in Windows. Here’s how to do that.</p> 
<b>Disabling HT Technology on Microsoft Windows</b> 
<p>In Linux, you can globally disable CPUs dynamically. In Windows, there is no direct equivalent that I could find, but there are a few alternatives.</p> 
<p>First, you can disable CPUs using the msconfig.exe tool. If you choose <strong>Boot, Advanced Options</strong>, you have the option to set the number of processors. In the example below, I limit my m4.2xlarge to four CPUs. Restart for this change to take effect.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin2-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin2-1.png" /></a></p> 
<p>Unfortunately, Windows does not disable hyperthreaded CPUs first and then real cores, as Linux does. As you can see in the following output, coreinfo reports that my c4.2xlarge has two real cores and four hyperthreads, after rebooting. Msconfig.exe is useful for disabling cores, but it does not allow you to disable HT Technology.</p> 
<p><strong>Note:</strong> If you have been following along, you can re-enable all your CPUs by unselecting the <strong>Number of processors</strong> check box and rebooting your system.</p> 
<p>&nbsp;</p> 
<code class="lang-powershell">C:\Users\Administrator &gt;coreinfo -c
Coreinfo v3.31 - Dump information on system CPU and memory topology
Copyright (C) 2008-2014 Mark Russinovich
Sysinternals - www.sysinternals.com
Logical to Physical Processor Map:
**-- Physical Processor 0 (Hyperthreaded)
--** Physical Processor 1 (Hyperthreaded)</code> 
<p>While you cannot disable HT Technology systemwide, Windows does allow you to associate a particular process with one or more CPUs. Microsoft calls this, “processor affinity”. To see an example, use the following steps.</p> 
<ol> 
<li>Launch an instance of Notepad.</li> 
<li>Open Windows Task Manager and choose <strong>Processes</strong>.</li> 
<li>Open the context (right click) menu on notepad.exe and choose <strong>Set Affinity…</strong>.</li> 
</ol> 
<p>This brings up the Processor Affinity dialog box.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin3-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin3-1.png" /></a></p> 
<p>As you can see, all the CPUs are allowed to run this instance of notepad.exe. You can uncheck a few CPUs to exclude them. Windows is smart enough to allow any scheduled operations to continue to completion on disabled CPUs. It then saves its state at the next scheduling event, and resumes those operations on another CPU. To ensure that only one thread in each core is able to run a process, you uncheck every other core. This effectively disables HT Technology for this process. For example:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin4-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/08/HTWin4-1.png" /></a></p> 
<p>Of course, this can be tedious when you have a large number of cores. Remember that the x1.32xlarge has 128 CPUs. Luckily, you can set the affinity of a running process from PowerShell using the <code class="lang-powershell">Get-Process</code> cmdlet. For example:</p> 
<code class="lang-powershell">PS C:\&amp;gt; (Get-Process -Name 'notepad').ProcessorAffinity = 0x55;</code> 
<p>The <strong>ProcessorAffinity</strong> attribute takes a bitmask in hexadecimal format. 0x55 in hex is equivalent to 01010101 in binary. Think of the binary encoding as 1=enabled and 0=disabled. This is slightly confusing, but we work left to right so that CPU 0 is the rightmost bit and CPU 7 is the leftmost bit. Therefore, 01010101 means that the first thread in each CPU is enabled just as it was in the diagram earlier.</p> 
<p>The calculator built into Windows includes a “programmer view” that helps you convert from hexadecimal to binary. In addition, the ProcessorAffinity attribute is a 64-bit number. Therefore, you can only configure the processor affinity on systems up to 64 CPUs. At the moment, only the x1.32xlarge has more than 64 vCPUs.</p> 
<p>In the preceding examples, you changed the processor affinity of a running process. Sometimes, you want to start a process with the affinity already configured. You can do this using the start command. The start command includes an affinity flag that takes a hexadecimal number like the PowerShell example earlier.</p> 
<code class="lang-powershell">C:\Users\Administrator&amp;gt;start /affinity 55 notepad.exe</code> 
<p>It is interesting to note that a child process inherits the affinity from its parent. For example, the following commands create a batch file that launches Notepad, and starts the batch file with the affinity set. If you examine the instance of Notepad launched by the batch file, you see that the <strong>affinity</strong> has been applied to as well.</p> 
<code class="lang-powershell">C:\Users\Administrator&amp;gt;echo notepad.exe &gt; test.bat
C:\Users\Administrator&amp;gt;start /affinity 55 test.bat</code> 
<p>This means that you can set the affinity of your task scheduler and any tasks that the scheduler starts inherits the affinity. So, you can disable every other thread when you launch the scheduler and effectively disable HT Technology for all of the tasks as well. Be sure to test this point, however, as some schedulers override the normal inheritance behavior and explicitly set processor affinity when starting a child process.</p> 
<b>Conclusion</b> 
<p>While the Windows operating system does not allow you to disable logical CPUs, you can set processor affinity on individual processes. You also learned that Windows Server 2016 enumerates CPUs in a different order than Linux. Therefore, you can effectively disable HT Technology by restricting a process to every other CPU. Finally, you learned how to set affinity of both new and running processes using Task Manager, PowerShell, and the start command.</p> 
<p>Note: this technical approach has nothing to do with control over software licensing, or licensing rights, which are sometimes linked to the number of “CPUs” or “cores.” For licensing purposes, those are legal terms, not technical terms. This post did not cover anything about software licensing or licensing rights.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2352');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Messaging Fanout Pattern for Serverless Architectures Using Amazon SNS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> and 
<span property="author" typeof="Person"><span property="name">Sam Dengler</span></span> | on 
<time property="datePublished" datetime="2017-08-22T15:52:19+00:00">22 AUG 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-cloudfront/" title="View all posts in Amazon CloudFront*"><span property="articleSection">Amazon CloudFront*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2554" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2554&amp;disqus_title=Messaging+Fanout+Pattern+for+Serverless+Architectures+Using+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2554');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<table> 
<tbody> 
<tr> 
<td valign="center"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/26/fsd-150x150.jpg" /></td> 
<td valign="center"> <h5>Sam Dengler, Amazon Web Services Solutions Architect</h5> </td> 
</tr> 
</tbody> 
</table> 
<p>Serverless architectures allow solution builders to focus on solving challenges particular to their business, without assuming the overhead of managing infrastructure in AWS. <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> is a service that lets you run code without provisioning or managing servers.</p> 
<p>When using Lambda in a serverless architecture, the goal should be to design tightly focused functions that do one thing and do it well. When these functions are composed to accomplish larger goals in microservice architectures, the complexity shifts from the internal components to the external communication between components. It’s all too easy to accidentally back into an architecture that is rigid to change because components are too knowledgeable of each other via the communication paths between them.</p> 
<p>Solution builders can address this architectural challenge by using messaging patterns, resulting in loosely coupled communication between highly cohesive components to manage complexity in serverless architectures. As introduced in the recent <a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a> post, a common approach when one component wishes to deliver the same message to multiple receivers is to use the fanout <a href="https://aws.amazon.com/pub-sub-messaging/">publish/subscribe messaging pattern</a>.</p> 
<p>The fanout pattern for message communication can be implemented in code. However, depending on your requirements, alternative solutions exist to offload this undifferentiated responsibility from the application. <a href="https://aws.amazon.com/sns/">Amazon SNS</a> is a fully managed pub/sub messaging service that lets you fan out messages to large numbers of recipients.</p> 
<p>In this post, I review a serverless architecture from PlayOn! Sports as a case study for migration of fanout functionality from application code to SNS.<span id="more-2554"></span></p> 
<b>PlayOn! Sports serverless video processing platform</b> 
<p><a href="http://www.playonsports.com/" target="_blank" rel="noopener noreferrer">PlayOn! Sports</a> is one of the nation’s leading high school sports media companies. They operate a comprehensive technology platform, enabling high-quality, low-cost productions of live sports events for the <a href="http://www.nfhsnetwork.com/" target="_blank" rel="noopener noreferrer">NFHS High School Sports Network</a>.</p> 
<p>At the 2014 AWS re:Invent conference, Lambda was announced. The PlayOn! Sports technology team recognized the parallels between serverless demos featuring image processing using ImageMagick to video processing using ffmpeg.</p> 
<p>At the time, PlayOn! Sports was broadcasting live video with adaptive bit rates, requiring a transcoding of the video stream to multiple quality levels for consumption on desktop, mobile, and connected devices. This is not unusual for an internet media company. However, with over 50,000 live broadcasts produced in 2014, the traditional media and entertainment technological approaches and pricing models would not work.</p> 
<p>After some consultation with the Lambda team to validate support for custom binary execution, PlayOn! Sports moved forward with the development of a new serverless video processing platform according to the architecture diagram below.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/25/messaging-fanout-for-serverless-with-sns-diagram1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/25/messaging-fanout-for-serverless-with-sns-diagram1-1024x615.png" /></a></p> 
<p>In the architecture, a laptop in the field captures the video from a camera source and divides it into small fragments, according to the <a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming" target="_blank" rel="noopener noreferrer">HLS protocol</a>. The fragments are published to <a href="https://aws.amazon.com/s3">Amazon S3</a>, through an <a href="https://aws.amazon.com/cloudfront">Amazon CloudFront</a> distribution for accelerated upload. When the file has been written to S3, it triggers a Lambda function to initiate the video segment processing.</p> 
<b>Video transcoding fanout implementation in Lambda</b> 
<p>Given Lambda’s integration growth across AWS, it’s easy to forget that it did not include managed integration with SNS when it was announced in November 2014.</p> 
<p>PlayOn! Sports was actively experimenting with approaches to video processing to address quality control, audience growth, and cost constraints. Lambda was a great tool for rapid innovation. A goal of the architecture design was the ability to add and remove video processing alternatives to the workflow, using the fanout pattern to identify optimal solutions. Below is example code from the initial implementation:</p> 
<code class="lang-python">import json
import logging
import boto3
logger = logging.getLogger('boto3')
logger.setLevel(logging.INFO)
client = boto3.client('lambda')
fanout_functions = ['media_info', 'transcode_audio']
def lambda_handler(event, context):
logger.info(json.dumps(event))
logger.info('fanout_functions: %s', fanout_functions)
for fanout_function in fanout_functions:
logger.info('invoke: %s', fanout_function)
response = client.invoke(
FunctionName=fanout_function,
InvocationType='Event',
Payload=json.dumps(event)
)
logger.info('response: %s', response)
return 'done'</code> 
<p>Each Lambda function is invoked asynchronously, injecting the same S3 event that triggered the original Lambda function. For example, the media_info Lambda function could be scaffolded similar to the following code snippet:</p> 
<code class="lang-python">import json
import logging
import boto3
logger = logging.getLogger('boto3')
logger.setLevel(logging.INFO)
def lambda_handler(event, context):
logger.info(json.dumps(event))
# MediaInfo Processing
# see: https://aws.amazon.com/blogs/compute/extracting-video-metadata-using-lambda-and-mediainfo/
return 'done'</code> 
<b>Refactoring fanout implementation using SNS</b> 
<p>The PlayOn! Sports development team was familiar with SNS, but had not used it previously to support system-to-system messaging patterns. After <a href="https://aws.amazon.com/about-aws/whats-new/2015/04/amazon-sns-now-integrates-with-aws-lambda/">the announcement of SNS triggering of Lambda functions</a>, the PlayOn! Sports team planned to migrate to the new feature to offload the overhead of managing the fanout Lambda function.</p> 
<p>When invoking a Lambda function, SNS wraps the original event with SNSEvent. The Lambda function can be refactored by adding a function to parse the S3 event from SNSEvent, as seen in the following code:</p> 
<code class="lang-python">import json
import logging
import boto3
logger = logging.getLogger('boto3')
logger.setLevel(logging.INFO)
def lambda_handler(event, context):
s3_event = parse_event(event)
logger.info(json.dumps(s3_event))
# MediaInfo Processing
# see: https://aws.amazon.com/blogs/compute/extracting-video-metadata-using-lambda-and-mediainfo/
return 'done'
def parse_event(event):
record = event['Records'][0]
if 'EventSource' in record and record['EventSource'] == 'aws:sns':
return json.loads(record['Sns']['Message'])
return event</code> 
<p>This Lambda function modification can be authored, tested, and deployed before enabling the SNS integration to verify that the existing Lambda fanout execution path continues to operate as before. The Lambda function invocation can now be transferred from the fanout Lambda function to SNS without disruption to S3 processing.</p> 
<p>As the diagram below shows, the resulting architecture is similar to the original. The exception is that objects written to S3 now trigger a message to be published to an SNS topic. This sends the S3 event to multiple Lambda functions to be processed independently.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/25/messaging-fanout-for-serverless-with-sns-diagram2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/25/messaging-fanout-for-serverless-with-sns-diagram2-1024x615.png" /></a></p> 
<b>Sample architecture deployment using AWS CloudFormation</b> 
<p><a href="https://aws.amazon.com/cloudformation">AWS CloudFormation</a> gives developers and system administrators an easy way to create and manage a collection of related AWS resources. CloudFormation provisions and updates resources in an orderly and predictable fashion. To launch the CloudFormation stack for the sample fanout architecture, choose the following button:</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=serverless-sns-fanout&amp;templateURL=https://s3.amazonaws.com/aws-serverless-sns-fanout/template-inline.yml"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/25/launch-stack-button-1.png" /></a></p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>Follow these steps to complete the architecture deployment:</p> 
<ol> 
<li>If necessary, sign into the console for your account when prompted.</li> 
<li>On the <strong>Select Template</strong> page, choose <strong>Next</strong>.</li> 
<li>Under <strong>Parameters</strong>, for <strong>S3BucketName</strong>, enter a globally unique name.</li> 
<li>For <strong>SnsTopicName</strong>, enter a region-unique name.</li> 
<li>Choose <strong>Next</strong>, <strong>Next</strong>.</li> 
<li>Select the checkbox for <strong>I acknowledge that AWS CloudFormation might create IAM resources</strong>, and choose <strong>Create</strong>.</li> 
</ol> 
<p>After the stack has completed creation, you can test both paths of execution by uploading files to the S3 bucket that you created. Uploading a file to the “/uploads/lambda/” directory in S3 triggers the Lambda fanout function. Uploading a file to the “/uploads/sns/” directory in S3 triggers the SNS fanout execution path. You can verify execution by monitoring the Lambda function outputs in CloudWatch Logs.</p> 
<b>Conclusion</b> 
<p>In this post, I reviewed the fanout messaging pattern and options for its inclusion in a serverless architectures using Lambda application code and SNS. Using the PlayOn! Sports serverless video processing pipeline use case, I demonstrated how easy it is to refactor an existing application to use the SNS fanout approach.</p> 
<p>I also provided a sample architecture in CloudFormation that you can run in your own account. Try it out and expand the sample architecture by adding other Lambda functions to the SNS topic, to demonstrate the flexibility of the fanout messaging pattern!</p> 
<p>You can get started with SNS using the <a href="https://us-west-2.console.aws.amazon.com/sns/v2/">AWS Management Console</a>, or the SDK of your choice. For more information about how to use SNS fanout messaging, see the following resources:</p> 
<li>10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/">How to Send Fanout Event Notifications</a></li> 
<li>Amazon SNS Developer Guide: 
<li><a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.html">Sending Amazon SNS Messaging to Amazon SQS Queues</a></li> 
<li><a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToHttp.html">Sending Amazon SNS Messaging to HTTP/HTTPS Endpoints</a></li> 
</ul> </li> 
<li>AWS Compute Blog: &nbsp;<a href="https://aws.amazon.com/blogs/compute/fanout-s3-event-notifications-to-multiple-endpoints/">Fanout S3 Event Notifications to Multiple Endpoints</a></li> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2554');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Enabling Identity Federation with AD FS 3.0 and Amazon AppStream 2.0</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Matt Guanti</span></span> | on 
<time property="datePublished" datetime="2017-08-17T14:39:35+00:00">17 AUG 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/amazon-appstream-2-0/" title="View all posts in Amazon AppStream 2.0*"><span property="articleSection">Amazon AppStream 2.0*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/" title="View all posts in Desktop &amp; App Streaming*"><span property="articleSection">Desktop &amp; App Streaming*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/enabling-identity-federation-with-ad-fs-3-0-and-amazon-appstream-2-0/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2619" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2619&amp;disqus_title=Enabling+Identity+Federation+with+AD+FS+3.0+and+Amazon+AppStream+2.0&amp;disqus_url=https://aws.amazon.com/blogs/compute/enabling-identity-federation-with-ad-fs-3-0-and-amazon-appstream-2-0/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2619');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Want to provide users with single sign-on access to AppStream 2.0 using existing enterprise credentials? Active Directory Federation Services (AD FS) 3.0 can be used to provide single sign-on for <a href="https://aws.amazon.com/appstream2">Amazon AppStream 2.0</a> using SAML 2.0.</p> 
<p>You can use your existing Active Directory or any SAML 2.0–compliant identity service to set up single sign-on access of AppStream 2.0 applications for your users. Identity federation using SAML 2.0 is currently available in all AppStream 2.0 regions.</p> 
<p>This post explains how to configure federated identities for AppStream 2.0 using AD FS 3.0.</p> 
<b>Walkthrough</b> 
<p>After setting up SAML 2.0 federation for AppStream 2.0, users can browse to a specially crafted (AD FS RelayState) URL and be taken directly to their AppStream 2.0 applications.<span id="more-2619"></span></p> 
<p>When users sign in with this URL, they are authenticated against Active Directory. After they are authenticated, the browser receives a SAML assertion as an authentication response from AD FS, which is then posted by the browser to the AWS sign-in SAML endpoint. Temporary security credentials are issued after the assertion and the embedded attributes are validated. The temporary credentials are then used to create the sign-in URL. The user is redirected to the AppStream 2.0 streaming session. The following <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers.html">diagram</a> shows the process.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/16/Diagram.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/16/Diagram.jpg" /></a></p> 
<ol> 
<li>The user browses to https://applications.exampleco.com. The sign-on page requests authentication for the user.</li> 
<li>The federation service requests authentication from the organization’s identity store.</li> 
<li>The identity store authenticates the user and returns the authentication response to the federation service.</li> 
<li>On successful authentication, the federation service posts the SAML assertion to the user’s browser.</li> 
<li>The user’s browser posts the SAML assertion to the AWS Sign-In SAML endpoint (https://signin.aws.amazon.com/saml). AWS Sign-In receives the SAML request, processes the request, authenticates the user, and forwards the authentication token to the AppStream 2.0 service.</li> 
<li>Using the authentication token from AWS, AppStream 2.0 authorizes the user and presents applications to the browser.</li> 
</ol> 
<p>In this post, use domain.local as the name of the Active Directory domain. Here are the steps in this walkthrough:</p> 
<ol> 
<li>Configure AppStream 2.0 identity federation.</li> 
<li>Configure the relying trust.</li> 
<li>Create claim rules.</li> 
<li>Enable RelayState and forms authentication.</li> 
<li>Create the AppStream 2.0 RelayState URL and access the stack.</li> 
<li>Test the configuration.</li> 
</ol> 
<b>Prerequisites</b> 
<p>This walkthrough assumes that you have the following prerequisites:</p> 
<li>An existing <a href="https://social.technet.microsoft.com/wiki/contents/articles/22622.building-your-first-domain-controller-on-2012-r2.aspx">Active Directory forest</a></li> 
<li>An instance joined to a domain with the “Active Directory Federation Services” <a href="https://docs.microsoft.com/en-us/windows-server/identity/ad-fs/deployment/install-the-ad-fs-role-service">role installed</a> and post-deployment configuration completed</li> 
<li>Familiarity with AppStream 2.0 resources</li> 
<b>Configure AppStream 2.0 identity federation</b> 
<p>First, <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/set-up-stacks-fleets.html">create an AppStream 2.0 stack</a>, as you reference the stack in upcoming steps. Name the stack ExampleStack. For this walkthrough, it doesn’t matter which underlying fleet you associate with the stack. You can create a fleet using one of the example Amazon-AppStream2-Sample-Image images available, or associate an existing fleet to the stack.</p> 
<h3>Get the AD FS metadata file</h3> 
<p>The first thing you need is the metadata file from your AD FS server. The metadata file is a signed document that is used later in this guide to establish the relying party trust. Don’t edit or reformat this file.</p> 
<p>To download and save this file, navigate to the following location, replacing &lt;FQDN_ADFS_SERVER&gt; with your AD FS s fully qualified server name.</p> 
https://&lt;FQDN_ADFS_SERVER&gt;/FederationMetadata/2007-06/FederationMetadata.xml 
<p><code class="lang-aspnet"></code>Save the file to a local location that you can access from the AWS Management Console later.</p> 
<h3>Create the SAML provider</h3> 
<p>Next, create the SAML provider in IAM, using the console. You could also <a href="http://docs.aws.amazon.com/cli/latest/reference/iam/create-saml-provider.html">create it using the AWS CLI</a>.</p> 
<p>In the IAM console, choose <strong>Identity providers</strong>, <strong>Create provider</strong>.</p> 
<p>On the <strong>Configure Provider</strong> page, for <strong>Provider Type</strong>, choose <strong>SAML</strong>. For <strong>Provider Name</strong>, type ADFS01 or similar name. Choose <strong>Choose File</strong> to upload the metadata document previously downloaded. Choose <strong>Next Step</strong>.</p> 
<p>Verify the provider information and choose <strong>Create</strong>.</p> 
<p>You need the Amazon Resource Name (ARN) of the identity provider (IdP) to configure claims rules later in this walkthrough. To get this, select the IdP that you just created. On the summary page, copy the value for <strong>Provider ARN</strong>. The ARN is in the following format:</p> 
arn:aws:iam::&lt;AccountID&gt;:saml-provider/&lt;Provider Name&gt; 
<h3>Configure an IAM Policy</h3> 
<p>Next, configure a policy with permissions to the AppStream 2.0 stack. This is the level of permissions that federated users have within AWS.</p> 
<p>In the IAM console, choose <strong>Policies</strong>, <strong>Create Policy</strong>, <strong>Create Your Own Policy</strong>.</p> 
<p>For <strong>Policy Name</strong>, enter a descriptive name. For <strong>Description</strong>, enter the level of permissions. For <strong>Policy Document</strong>, you customize the <strong>Region-Code</strong>, <strong>AccountID</strong> (without hyphens), and case-sensitive <strong>Stack-Name</strong> values.</p> 
<p>The following screenshot shows how to create a policy that gives users permissions to only a single AppStream 2.0 stack, named ExampleStack. For more information, see the <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers-setting-up-saml.html#external-identity-providers-grantperms">Step 2: Configure Permissions in AWS for Your Federated Users topic</a>.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-2.png" /></a></p> 
<p>&nbsp;</p> 
<p>For <strong>Region Codes</strong>, use one of the following values based on the region you are using AppStream 2.0 (the available regions for AppStream 2.0):</p> 
<li><strong>us-east-1</strong></li> 
<li><strong>us-west-2</strong></li> 
<li><strong>eu-west-1</strong></li> 
<li><strong>ap-northeast-1</strong></li> 
<p>Choose <strong>Create Policy</strong> and you should see the following notification:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-3.png" /></a></p> 
<p>&nbsp;</p> 
<h3>Create an IAM role</h3> 
<p>Here, you create a role that relates to an Active Directory group assigned to your AppStream 2.0 federated users. For this configuration, Active Directory groups and AWS roles are case-sensitive. Here you create an IAM Role named “ExampleStack” and an Active Directory group named in the format AWS-AccountNumber-RoleName, for example AWS-012345678910-ExampleStack.</p> 
<p>In the IAM console, choose <strong>Roles</strong>, <strong>Create </strong>new<strong> role</strong>.</p> 
<p>On the <strong>Select Role type</strong> page, choose <strong>Role for identity provider access</strong>. Choose <strong>Select</strong> next to <strong>Grant Web Single Sign-On (WebSSO) access to SAML providers</strong>.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-4.png" /></a></p> 
<p>&nbsp;</p> 
<p>On the <strong>Establish Trust</strong> page, make sure that the SAML provider that you just created (such as ADFS01) is selected. For <strong>Attribute</strong> and <strong>Value</strong>, keep the default values.</p> 
<p>On the <strong>Verify Role Trust</strong> page, the <strong>Federated</strong> value matches the ARN noted previously for the principal IdP created earlier. The <strong>SAML: aud</strong> value equals https://signin.aws.amazon.com/saml, as shown below. This is prepopulated and does not require any change. Choose <strong>Next Step</strong>.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-5.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-5.png" /></a></p> 
<p>&nbsp;</p> 
<p>On the <strong>Attach policy</strong> page, attach the policy that you created earlier granting federated users access only to the AppStream 2.0 stack. In this walkthrough, the policy was named <strong>AppStream2_ExampleStack</strong>.</p> 
<p>After selecting the correct policy, choose <strong>Next Step</strong>.</p> 
<p>On the <strong>Set role name and review</strong> page, name the role <strong>ExampleStack</strong>. You can customize this naming convention, as I explain later when I create the claims rules.</p> 
<p>You can describe the role as desired. Ensure that the trusted entities match the AD FS IdP ARN, and that the policy attached is the policy created earlier granting access only to this stack.</p> 
<p>Choose <strong>Create Role</strong>.</p> 
<blockquote> 
<p><strong>Important:</strong> If you grant more than the stack permissions to federated users, you can give them access to other areas of the console as well. AWS strongly recommends that you attach policies to a role that grants access only to the resources to be shared with federated users.</p> 
<p>For example, if you attach the <strong>AdministratorAccess</strong> policy instead of <strong>AppStream2_ExampleStack</strong>, any AppStream 2.0 federated user in the <strong>ExampleStack</strong> Active Directory group has <strong>AdministratorAccess</strong> in your AWS account. Even though AD&nbsp;FS routes users to the stack, users can still navigate to other areas of the console, using deep links that go directly to specific console locations.</p> 
</blockquote> 
<p>Next, create the Active Directory group in the format AWS-AccountNumber-RoleName using the “ExampleStack” role name that you just created. You reference this Active Directory group in the AD FS claim rules later using regex. For <strong>Group scope</strong>, choose <strong>Global</strong>. For <strong>Group type</strong>, choose <strong>Security</strong></p> 
<p>Note: To follow this walkthrough exactly, name your Active Directory group in the format “AWS-AccountNumber-ExampleStack” replacing AccountNumber with your AWS AccountID (without hyphens). For example:</p> 
AWS-012345678910-ExampleStack 
<h3>Configure the relying party trust</h3> 
<p>In this section, you configure AD FS 3.0 to communicate with the configurations made in AWS.</p> 
<p>Open the AD FS console on your AD FS 3.0 server.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-6.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-6.png" /></a></p> 
<p>&nbsp;</p> 
<p>Open the context (right-click) menu for AD FS and choose <strong>Add Relying Party Trust…</strong></p> 
<p>On the <strong>Welcome</strong> page, choose <strong>Start</strong>. On the <strong>Select Data Source</strong> page, keep <strong>Import data about the relying party published online or on a local network</strong> checked. For <strong>Federation metadata address (host name or URL)</strong>, type the following link to the SAML metadata to describe AWS as a relying party and then choose <strong>Next</strong>.</p> 
https://signin.aws.amazon.com/static/saml-metadata.xml 
<p>On the <strong>Specify Display Name</strong> page, for <strong>Display name</strong>, type “AppStream 2.0 – ExampleStack” or similar value. For <strong>Notes</strong>, provide a description. Choose <strong>Next</strong>.</p> 
<p>On the <strong>Configure Multi-factor Authentication Now?</strong> page, choose <strong>I do not want to configure multi-factor authentication settings for this relying party trust at this time</strong>. Choose <strong>Next</strong>.</p> 
<p>Because you are controlling access to the stack using an Active Directory group, and IAM role with an attached policy, on the <strong>Choose Issuance Authorization Rules</strong> page, check <strong>Permit all users to access this relying party</strong>. Choose <strong>Next</strong>.</p> 
<p>On the <strong>Ready to Add Trust</strong> page, there shouldn’t be any changes needed to be made. Choose <strong>Next</strong>.</p> 
<p>On the <strong>Finish</strong> page, clear <strong>Open the </strong>edit<strong> Claim Rules dialog for this relying party trust when the wizard closes</strong>. You open this later.</p> 
<p>Next, you add the https://signin.aws.amazon.com/saml URL is listed on the <strong>Identifiers</strong> tab within the properties of the trust. To do this, open the context (right-click) menu for the relying party trust that you just created and choose <strong>Properties</strong>.</p> 
<p>On the <strong>Monitoring</strong> tab and clear <strong>Monitor relying party</strong>. Choose <strong>Apply</strong>. On the <strong>Identifiers</strong> tab, for <strong>Relying party identifier</strong>, add <strong>https://signin.aws.amazon.com/saml</strong> and choose <strong>OK</strong>.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-7.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-7.png" /></a></p> 
<p>&nbsp;</p> 
<b>Create claim rules</b> 
<p>In this section, you create four AD FS claim rules, which identify accounts, set LDAP attributes, get the Active Directory groups, and match them to the role created earlier.</p> 
<p>In the AD FS console, expand <strong>Trust Relationships</strong>, choose <strong>Relying Party Trusts</strong>, and then select the relying party trust that you just created (in this case, the display name is AppStream 2.0 – ExampleStack). Open the context (right-click) menu for the relying party trust and choose <strong>Edit Claim Rules</strong>. Choose <strong>Add Rule</strong>.</p> 
<h3>Rule 1: Name ID</h3> 
<p>This claim rule tells AD FS the type of expected incoming claim and how to send the claim to AWS. AD FS receives the UPN and tags it as the Name ID when it’s forwarded to AWS. This rule interacts with the third rule, which fetches the user groups.</p> 
<p style="padding-left: 30px"><strong>Claim rule template:</strong> Transform an Incoming Claim</p> 
<p style="padding-left: 30px"><strong>Configure Claim Rule values:</strong></p> 
<p style="padding-left: 30px"><strong>Claim Rule Name:</strong>&nbsp; Name ID</p> 
<p style="padding-left: 30px"><strong>Incoming Claim Type:</strong>&nbsp; UPN</p> 
<p style="padding-left: 30px"><strong>Outgoing Claim Type:</strong>&nbsp; Name ID</p> 
<p style="padding-left: 30px"><strong>Outgoing name ID format:</strong>&nbsp; Persistent Identifier</p> 
<p style="padding-left: 30px"><strong>Pass through all claim values:</strong>&nbsp; selected</p> 
<h3>Rule 2: RoleSessionName</h3> 
<p>This rule sets a unique identifier for the user. In this case, use the E-Mail-Addresses values.</p> 
<p style="padding-left: 30px"><strong>Claim rule template:</strong> Send LDAP Attributes as Claims</p> 
<p style="padding-left: 30px"><strong>Configure Claim Rule values:</strong></p> 
<p style="padding-left: 30px"><strong>Claim rule name:</strong>&nbsp; RoleSessionName</p> 
<p style="padding-left: 30px"><strong>Attribute store:</strong> &nbsp;Active Directory</p> 
<p style="padding-left: 30px"><strong>LDAP Attribute:</strong>&nbsp; E-Mail-Addresses</p> 
<p style="padding-left: 30px"><strong>Outgoing Claim Type:</strong>&nbsp;&nbsp;https://aws.amazon.com/SAML/Attributes/RoleSessionName</p> 
<h3>Rule 3: Get Active Directory groups</h3> 
<p>This rule queries Active Directory and returns the groups to which the user is assigned, such as AWS-012345678910-ExampleStack .</p> 
<p style="padding-left: 30px"><strong>Claim rule template:</strong> Send Claims Using a Custom Rule</p> 
<p style="padding-left: 30px"><strong>Configure Claim Rule values:</strong></p> 
<p style="padding-left: 30px"><strong>Claim Rule Name:</strong>&nbsp; Get Active Directory Groups</p> 
<p style="padding-left: 30px"><strong>Custom Rule:</strong></p> 
c:[Type == &quot;http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname&quot;, Issuer == &quot;AD AUTHORITY&quot;] =&gt; add(store = &quot;Active Directory&quot;, types = (&quot;http://temp/variable&quot;), query = &quot;;tokenGroups;{0}&quot;, param = c.Value); 
<h3>Rule 4: Roles</h3> 
<p>This rule converts the value of the Active Directory group starting with AWS-AccountNumber prefix to the roles known by AWS. For this rule, you need the AWS IdP ARN that you noted earlier. If your IdP in AWS was named ADFS01 and the AccountID was 012345678910, the ARN would look like the following:</p> 
arn:aws:iam::012345678910:saml-provider/ADFS01
 
<p style="padding-left: 30px"><strong>Claim rule template:</strong> Send Claims Using a Custom Rule</p> 
<p style="padding-left: 30px"><strong>Configure Claim Rule values:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></p> 
<p style="padding-left: 30px"><strong>Claim Rule Name:&nbsp;</strong> Roles</p> 
<p style="padding-left: 30px"><strong>Custom Rule:</strong></p> 
c:[Type == &quot;http://temp/variable&quot;, Value =~ &quot;(?i)^AWS-&quot;]
&nbsp;=&gt; issue(Type = &quot;https://aws.amazon.com/SAML/Attributes/Role&quot;, Value = RegExReplace(c.Value, &quot;AWS-012345678910-&quot;, &quot;arn:aws:iam::012345678910:saml-provider/ADFS01,arn:aws:iam::012345678910:role/&quot;)); 
<li>Change&nbsp;<span style="color: #ff0000">arn:aws:iam::012345678910:saml-provider/ADFS01</span> to the ARN of your AWS IdP</li> 
<li>Change <span style="color: #ff0000">012345678910</span> to the ID (without hyphens) of the AWS account.</li> 
<p>In this walkthrough, “AWS-” returns the Active Directory groups that start with the AWS- prefix, then removes AWS-012345678910- leaving ExampleStack left on the Active Directory Group name to match the ExampleStack IAM role. To customize the role naming convention, for example to name the IAM Role ADFS-ExampleStack, add “ADFS-” to the end of the role ARN at the end of the rule: arn:aws:iam::012345678910:role/ADFS-.</p> 
<p>You should now have four claims rules created:</p> 
<li><strong>NameID</strong></li> 
<li><strong>RoleSessionName</strong></li> 
<li><strong>Get Active Directory Groups</strong></li> 
<li><strong>Role</strong></li> 
<b></b> 
<b>Enable RelayState and forms authentication</b> 
<p>By default, AD FS 3.0 doesn’t have RelayState enabled. AppStream 2.0 uses RelayState to direct users to your AppStream 2.0 stack.</p> 
<p>On your AD FS server, open the following with elevated (administrator) permissions:</p> 
%systemroot%\adfs\Microsoft.IdentityServer.Servicehost.exe.config 
<p>In the Microsoft.IdentityServer.Servicehost.exe.config file, find the section <strong>&lt;microsoft.identityServer.web&gt;</strong>. Within this section, add the following line:</p> 
&lt;useRelayStateForIdpInitiatedSignOn enabled=&quot;true&quot; /&gt; 
<p><code class="lang-aspnet"></code>The edit should look like the following:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-8.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-8.png" /></a></p> 
<p>&nbsp;</p> 
<p>In the AD FS console, verify that forms authentication is enabled. Choose <strong>Authentication Policies</strong>. Under <strong>Primary Authentication</strong>, for <strong>Global Settings</strong>, choose <strong>Edit</strong>.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Authentication-Policies-Overview.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Authentication-Policies-Overview.png" /></a></p> 
<p>&nbsp;</p> 
<p>For <strong>Extranet</strong>, choose <strong>Forms Authentication</strong>. For <strong>Intranet</strong>, do the same and choose <strong>OK</strong>.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-10.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-10.png" /></a></p> 
<p>&nbsp;</p> 
<p>On the AD FS server, from an elevated (administrator) command prompt, run the following commands sequentially to stop, then start the AD FS service to register the changes:</p> 
net stop adfssrv 
net start adfssrv
 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-11.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-11.png" /></a></p> 
<p>&nbsp;</p> 
<b>Create the AppStream 2.0 RelayState URL and access the stack</b> 
<p>Now that RelayState is enabled, you can generate the URL.</p> 
<p>I have created an Excel spreadsheet for RelayState URL generation, available as <a href="https://s3-us-west-2.amazonaws.com/as2-blog-artifacts/RelayGenerator.xlsx">RelayGenerator.xlsx</a>. This spreadsheet only requires the fully qualified domain name for your AD FS server, account ID (without hyphens), stack name (case-sensitive), and the AppStream 2.0 region. After all&nbsp;the inputs are entered, the spreadsheet generates a URL in the blue box, as shown in the screenshot below. Copy the entire contents of the blue box to retrieve the generated RelayState URL for AD FS.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-12.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-12.png" /></a></p> 
<p>&nbsp;</p> 
<p>Alternatively, if you do not have Excel, there are third-party tools for RelayState URL generation. However, they do require some customization to work with AppStream 2.0. Example customization steps for one such tool are provided below.</p> 
<p>CodePlex has an <a href="https://adfsrelaystate.codeplex.com/downloads/get/474227">AD FS RelayState generator</a>, which downloads an HTML file locally that you can use to create the RelayState URL. The generator says it’s for AD FS 2.0; however, it also works for AD FS 3.0. You can generate the RelayState URL manually but if the syntax or case sensitivity is incorrect even slightly, it won’t work. I recommend using the tool to ensure a valid URL.</p> 
<p>When you open the URL generator, clear out the default text fields. You see a tool that looks like the following:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-13.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-13.png" /></a></p> 
<p>&nbsp;</p> 
<p>To generate the values, you need three pieces of information:</p> 
<li><strong>IDP URL String</strong></li> 
<li><strong>Relying Party Identifier</strong></li> 
<li><strong>Relay State / Target App</strong></li> 
<h3>&nbsp;IDP URL String</h3> 
<p>The IDP URL string is the URL you use to hit your AD FS sign-on page. For example:</p> 
https://&lt;ADFSInstance&gt;/adfs/ls/idpinitiatedsignon.aspx 
<p>In this configuration, use the following:</p> 
https://adfs01.domain.local/adfs/ls/idpinitiatedsignon.aspx 
<h3>Relying Party Identifier</h3> 
<p>This value is always the following:</p> 
https://signin.aws.amazon.com/saml 
<h3>Relay State / Target App</h3> 
<p>This is the RelayState link to your AppStream 2.0 stack. The format for this URL is as follows:</p> 
<code class="lang-http">https://appstream2.&lt;region&gt;.aws.amazon.com/saml?stack=Case_Sensitive_Stack_Name&amp;accountId=account_id_without_hyphens</code> 
<p>Ultimately, the URL looks like the following example, which is for us-east-1, with a stack name of ExampleStack, and an account ID of 012345678910. The stack name is case-sensitive.</p> 
https://appstream2.us-east-1.aws.amazon.com/saml?stack=ExampleStack&amp;accountId=012345678910 
<p>For more information, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers-setting-up-saml.html#external-identity-providers-relay-state">Step 5: Configure the Relay State of Your Federation</a>.</p> 
<p>Input these values into the URL generator, and generate a RelayState URL.</p> 
<p>Generated URL (case sensitive):’</p> 
<code class="lang-http">https://adfs01.domain.local/adfs/ls/idpinitiatedsignon.aspx?RelayState=RPID%3Dhttps%253A%252F%252Fsignin.aws.amazon.com%252Fsaml%26RelayState%3Dhttps%253A%252F%252Fappstream2.us-east-1.aws.amazon.com%252Fsaml%253Fstack%253D%2520ExampleStack%2526accountId%253D012345678910</code> 
<p>The generated RelayState URL can now be saved and used by users to log in directly from anywhere that can reach the AD FS server, using their existing domain credentials. After they are authenticated, users are directed seamlessly to the AppStream 2.0 stack.</p> 
<h3>Test the configuration</h3> 
<p>Create a new AD user in Domain.local named Test User, with a username TUser and an email address. An email address is required based on the claim rules.</p> 
<p>Next, add TUser to the AD group you created for the AWS-012345678910-ExampleStack stack.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-14.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-14.png" /></a></p> 
<p>&nbsp;</p> 
<p>Next, navigate to the RelayState URL and log in with domain\TUser.</p> 
<p>After you log in, you are directed to the streaming session for the ExampleStack stack. As an administrator, you can disassociate and associate different fleets of applications to this stack, without impacting federation, and deliver different applications to this group of federated users.</p> 
<p>Because the policy attached to the role only allows access to this AppStream 2.0 stack, if a federated user were to try to access another section of the console, such as Amazon EC2, they would discover that they are not authorized to see (describe) any resources or perform any actions, as shown in the screenshot below. This is why it’s important to grant access only to the AppStream 2.0 stack.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-15.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/14/Image-15.png" /></a></p> 
<p>&nbsp;</p> 
<b>Configurations for AD FS 4.0</b> 
<p>If you are using AD FS 4.0, there are a few differences from the procedures discussed earlier.</p> 
<p>Do not customize the following file as described in the Enable RelayState and forms authentication of the AD FS 3.0 guide:</p> 
%systemroot%\adfs\Microsoft.IdentityServer.Servicehost.exe.config 
<p>Enable the IdP-initiated sign-on page that is used when generating the RelayState URL. To do this, open an elevated PowerShell terminal and run the following command:</p> 
Set-AdfsProperties -EnableIdPInitiatedSignonPage $true 
<p>This enables the following URL referenced earlier:</p> 
https://&lt;ADFSInstance&gt;/adfs/ls/idpinitiatedsignon.aspx 
<p>Next, enable Relay States. To do this, run the following command from an elevated PowerShell terminal:</p> 
Set-AdfsProperties -EnableRelayStateForIdpInitiatedSignOn $true 
<p>To register these changes with AD FS, restart the AD FS service from an elevated PowerShell terminal (or command prompt):</p> 
net stop adfssrv
net start adfssrv 
<p>After these changes are made, AD FS 4.0 should now work for AppStream 2.0 identity federation.</p> 
<b>Troubleshooting</b> 
<p>If you are still encountering errors with your setup, below are common error messages you may see, and configuration areas that I recommend that you check.</p> 
<h3>Invalid policy</h3> 
Unable to authorize the session. (Error Code: INVALID_AUTH_POLICY);Status Code:401 
<p>This error message can occur when the IAM policy does not permit access to the AppStream 2.0 stack. However, it can also occur when the stack name is not entered into the policy or RelayState URL using case-sensitive characters. For example, if your stack name is “ExampleStack” in AppStream 2.0 and the policy has “examplestack” or if the Relay State URL has “examplestack” or any capitalization pattern other than the exact stack name, you see this error message.</p> 
<h3>Invalid relay state</h3> 
Error: Bad Request.(Error Code: INVALID_RELAY_STATE);Status Code:400 
If you are receiving this error message, there is likely to be another issue in the Relay State URL. It could be related to case sensitivity (other than the stack name). For example, https://relay-state-region-endoint?stack=stackname&amp;accountId=aws-account-id-without-hyphens. 
<p>This URL must contain the region-specific Relay State endpoint exactly as listed in the <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers-setting-up-saml.html#external-identity-providers-relay-state">Step 5: Configure the Relay State of Your Federation topic</a>.</p> 
<h3>Cross account access</h3> 
<code class="lang-aspnet">Unable to authorize the session. Cross account access is not allowed. (Error Code: CROSS_ACCOUNT_ACCESS_NOT_ALLOWED);Status Code:401</code> 
<p>If you see this error message, check to make sure that the AccountId number is correct in the Relay State URL.</p> 
<b>Summary</b> 
<p>In this post, you walked through enabling AD FS 3.0 for AppStream 2.0 identity federation. You should now be able to configure AD FS 3.0 or 4.0 for AppStream 2.0 identity federation. If you have questions or suggestions, please comment below.</p> 
<img style="border: 0px;width: 32px;height: 32px;margin-right: 5px !important" src="https://aws-support-gm.s3.amazonaws.com/prod/tiny-url-shrinker/TinyURLShortener-icon-64x64.png" /> 
<textarea id="url-shrinker-txtarea"></textarea> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2619');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/nginx_reverse_proxy-1133x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Deploying an NGINX Reverse Proxy Sidecar Container on Amazon ECS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Peck</span></span> | on 
<time property="datePublished" datetime="2017-08-03T14:54:02+00:00">03 AUG 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/nginx-reverse-proxy-sidecar-container-on-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2525" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2525&amp;disqus_title=Deploying+an+NGINX+Reverse+Proxy+Sidecar+Container+on+Amazon+ECS&amp;disqus_url=https://aws.amazon.com/blogs/compute/nginx-reverse-proxy-sidecar-container-on-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2525');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Reverse proxies are a powerful software architecture primitive for fetching resources from a server on behalf of a client. They serve a number of purposes, from protecting servers from unwanted traffic to offloading some of the heavy lifting of HTTP traffic processing.</p> 
<p>This post explains the benefits of a reverse proxy, and explains how to use <a href="https://www.nginx.com/resources/wiki/">NGINX</a> and <a href="https://aws.amazon.com/ecs/">Amazon EC2 Container Service (Amazon ECS)</a> to easily implement and deploy a reverse proxy for your containerized application.</p> 
<b>Components</b> 
<p>NGINX is a high performance HTTP server that has achieved significant adoption because of its asynchronous event driven architecture. It can serve thousands of concurrent requests with a low memory footprint. This efficiency also makes it ideal as a reverse proxy.</p> 
<p>Amazon ECS is a highly scalable, high performance container management service that supports Docker containers. It allows you to run applications easily on a managed cluster of Amazon EC2 instances. Amazon ECS helps you get your application components running on instances according to a specified configuration. It also helps scale out these components across an entire fleet of instances.</p> 
<p>Sidecar containers are a common software pattern that has been embraced by engineering organizations. It’s a way to keep server side architecture easier to understand by building with smaller, modular containers that each serve a simple purpose. Just like an application can be powered by multiple microservices, each microservice can also be powered by multiple containers that work together. A sidecar container is simply a way to move part of the core responsibility of a service out into a containerized module that is deployed alongside a core application container.<span id="more-2525"></span></p> 
<p>The following diagram shows how an NGINX reverse proxy sidecar container operates alongside an application server container:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/nginx_reverse_proxy.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/nginx_reverse_proxy-1024x570.png" /></a></p> 
<p>In this architecture, Amazon ECS has deployed two copies of an application stack that is made up of an NGINX reverse proxy side container and an application container. Web traffic from the public goes to an Application Load Balancer, which then distributes the traffic to one of the NGINX reverse proxy sidecars. The NGINX reverse proxy then forwards the request to the application server and returns its response to the client via the load balancer.</p> 
<b>Reverse proxy for security</b> 
<p>Security is one reason for using a reverse proxy in front of an application container. Any web server that serves resources to the public can expect to receive lots of unwanted traffic every day. Some of this traffic is relatively benign scans by researchers and tools, such as Shodan or nmap:</p> 
<code class="lang-bash">[18/May/2017:15:10:10 +0000] &quot;GET /YesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScann HTTP/1.1&quot; 404 1389 - Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36
[18/May/2017:18:19:51 +0000] &quot;GET /clientaccesspolicy.xml HTTP/1.1&quot; 404 322 - Cloud mapping experiment. Contact research@pdrlabs.net</code> 
<p>But other traffic is much more malicious. For example, here is what a web server sees while being scanned by the hacking tool <a href="https://en.wikipedia.org/wiki/ZmEu_(vulnerability_scanner)">ZmEu</a>, which scans web servers trying to find PHPMyAdmin installations to exploit:</p> 
<code class="lang-bash">[18/May/2017:16:27:39 +0000] &quot;GET /mysqladmin/scripts/setup.php HTTP/1.1&quot; 404 391 - ZmEu
[18/May/2017:16:27:39 +0000] &quot;GET /web/phpMyAdmin/scripts/setup.php HTTP/1.1&quot; 404 394 - ZmEu
[18/May/2017:16:27:39 +0000] &quot;GET /xampp/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 396 - ZmEu
[18/May/2017:16:27:40 +0000] &quot;GET /apache-default/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 405 - ZmEu
[18/May/2017:16:27:40 +0000] &quot;GET /phpMyAdmin-2.10.0.0/scripts/setup.php HTTP/1.1&quot; 404 397 - ZmEu
[18/May/2017:16:27:40 +0000] &quot;GET /mysql/scripts/setup.php HTTP/1.1&quot; 404 386 - ZmEu
[18/May/2017:16:27:41 +0000] &quot;GET /admin/scripts/setup.php HTTP/1.1&quot; 404 386 - ZmEu
[18/May/2017:16:27:41 +0000] &quot;GET /forum/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 396 - ZmEu
[18/May/2017:16:27:41 +0000] &quot;GET /typo3/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 396 - ZmEu
[18/May/2017:16:27:42 +0000] &quot;GET /phpMyAdmin-2.10.0.1/scripts/setup.php HTTP/1.1&quot; 404 399 - ZmEu
[18/May/2017:16:27:44 +0000] &quot;GET /administrator/components/com_joommyadmin/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 418 - ZmEu
[18/May/2017:18:34:45 +0000] &quot;GET /phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 390 - ZmEu
[18/May/2017:16:27:45 +0000] &quot;GET /w00tw00t.at.blackhats.romanian.anti-sec:) HTTP/1.1&quot; 404 401 - ZmEu</code> 
<p>In addition, servers can also end up receiving unwanted web traffic that is intended for another server. In a cloud environment, an application may end up reusing an IP address that was formerly connected to another service. It’s common for misconfigured or misbehaving DNS servers to send traffic intended for a different host to an IP address now connected to your server.</p> 
<p>It’s the responsibility of anyone running a web server to handle and reject potentially malicious traffic or unwanted traffic. Ideally, the web server can reject this traffic as early as possible, before it actually reaches the core application code. A reverse proxy is one way to provide this layer of protection for an application server. It can be configured to reject these requests before they reach the application server.</p> 
<b>Reverse proxy for performance</b> 
<p>Another advantage of using a reverse proxy such as NGINX is that it can be configured to offload some heavy lifting from your application container. For example, every HTTP server should support gzip. Whenever a client requests gzip encoding, the server compresses the response before sending it back to the client. This compression saves network bandwidth, which also improves speed for clients who now don’t have to wait as long for a response to fully download.</p> 
<p>NGINX can be configured to accept a plaintext response from your application container and gzip encode it before sending it down to the client. This allows your application container to focus 100% of its CPU allotment on running business logic, while NGINX handles the encoding with its efficient gzip implementation.</p> 
<p>An application may have security concerns that require SSL termination at the instance level instead of at the load balancer. NGINX can also be configured to terminate SSL before proxying the request to a local application container. Again, this also removes some CPU load from the application container, allowing it to focus on running business logic. It also gives you a cleaner way to patch any SSL vulnerabilities or update SSL certificates by updating the NGINX container without needing to change the application container.</p> 
<b>NGINX configuration</b> 
<p>Configuring NGINX for both traffic filtering and gzip encoding is shown below:</p> 
<code class="lang-bash">http {
&nbsp; # NGINX will handle gzip compression of responses from the app server
&nbsp; gzip on;
&nbsp; gzip_proxied any;
&nbsp; gzip_types text/plain application/json;
&nbsp; gzip_min_length 1000;
&nbsp;
&nbsp; server {
&nbsp;&nbsp;&nbsp; listen 80;
&nbsp;
&nbsp;&nbsp;&nbsp; # NGINX will reject anything not matching /api
&nbsp;&nbsp;&nbsp; location /api {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Reject requests with unsupported HTTP method
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ($request_method !~ ^(GET|POST|HEAD|OPTIONS|PUT|DELETE)$) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 405;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Only requests matching the whitelist expectations will
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # get sent to the application server
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_pass http://app:3000;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_http_version 1.1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header Upgrade $http_upgrade;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header Connection 'upgrade';
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header Host $host;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_cache_bypass $http_upgrade;
&nbsp;&nbsp;&nbsp; }
&nbsp; }
}</code> 
<p>The above configuration only accepts traffic that matches the expression /api and has a recognized HTTP method. If the traffic matches, it is forwarded to a local application container accessible at the local hostname app. If the client requested gzip encoding, the plaintext response from that application container is gzip-encoded.</p> 
<b>Amazon ECS configuration</b> 
<p>Configuring ECS to run this NGINX container as a sidecar is also simple. ECS uses a core primitive called the task definition. Each task definition can include one or more containers, which can be linked to each other:</p> 
<code class="lang-json">&nbsp;{
&nbsp;&nbsp;&quot;containerDefinitions&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;name&quot;: &quot;nginx&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;image&quot;: &quot;&lt;NGINX reverse proxy image URL here&gt;&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;memory&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;cpu&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;essential&quot;: true,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;portMappings&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;containerPort&quot;: &quot;80&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;protocol&quot;: &quot;tcp&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;links&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;app&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]
&nbsp;&nbsp;&nbsp;&nbsp; },
&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;name&quot;: &quot;app&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;image&quot;: &quot;&lt;app image URL here&gt;&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;memory&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;cpu&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;essential&quot;: true
&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp; ],
&nbsp;&nbsp; &quot;networkMode&quot;: &quot;bridge&quot;,
&nbsp;&nbsp; &quot;family&quot;: &quot;application-stack&quot;
}
</code> 
<p>This task definition causes ECS to start both an NGINX container and an application container on the same instance. Then, the NGINX container is linked to the application container. This allows the NGINX container to send traffic to the application container using the hostname app.</p> 
<p>The NGINX container has a port mapping that exposes port 80 on a publically accessible port but the application container does not. This means that the application container is not directly addressable. The only way to send it traffic is to send traffic to the NGINX container, which filters that traffic down. It only forwards to the application container if the traffic passes the whitelisted rules.</p> 
<b>Conclusion</b> 
<p>Running a sidecar container such as NGINX can bring significant benefits by making it easier to provide protection for application containers. Sidecar containers also improve performance by freeing your application container from various CPU intensive tasks. Amazon ECS makes it easy to run sidecar containers, and automate their deployment across your cluster.</p> 
<p>To see the full code for this NGINX sidecar reference, or to try it out yourself, you can check out the open source <a href="https://github.com/awslabs/ecs-nginx-reverse-proxy/tree/master/reverse-proxy">NGINX reverse proxy reference architecture on GitHub</a>.</p> 
<p>– Nathan<br /> <a href="https://twitter.com/nathankpeck" target="_blank"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/03/twitter_16.png">@nathankpeck</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2525');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deploying Java Microservices on Amazon Elastic Container Service</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-07-11T09:12:17+00:00">11 JUL 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/deploying-java-microservices-on-amazon-ec2-container-service/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2489" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2489&amp;disqus_title=Deploying+Java+Microservices+on+Amazon+Elastic+Container+Service&amp;disqus_url=https://aws.amazon.com/blogs/compute/deploying-java-microservices-on-amazon-ec2-container-service/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2489');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post and accompanying code graciously contributed by:</p> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/huynhz.jpeg" /></td> 
<td style="padding: 0px 30px 0px 0px"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/magnusb.jpeg" /></td> 
</tr> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><b>Huy Huynh</b><br /> Sr. Solutions Architect</td> 
<td style="padding: 0px 30px 0px 0px"><b>Magnus Bjorkman</b><br /> Solutions Architect</td> 
</tr> 
</tbody> 
</table> 
<p>Java is a popular language used by many enterprises today. To simplify and accelerate Java application development, many companies are moving from a monolithic to microservices architecture. For some, it has become a strategic imperative. <a href="https://aws.amazon.com/containers/">Containerization</a> technology, such as <a href="https://aws.amazon.com/docker/">Docker</a>, lets enterprises build scalable, robust microservice architectures without major code rewrites.</p> 
<p>In this post, I cover how to containerize a monolithic Java application to run on Docker. Then, I show how to deploy it on AWS using Amazon&nbsp;Elastic Container Service (<a href="https://aws.amazon.com/ecs/">Amazon ECS</a>), a high-performance container management service. Finally, I show how to break the monolith into multiple services, all running in containers on Amazon ECS.</p> 
<p><span id="more-2489"></span></p> 
<b>Application Architecture</b> 
<p>For this example, I use the <a href="https://github.com/spring-projects/spring-petclinic">Spring Pet Clinic</a>, a monolithic Java application for managing a veterinary practice. It is a simple REST&nbsp;API, which allows the client to manage and view Owners, Pets, Vets, and Visits.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_1.png" /></a></p> 
<p>It is a simple three-tier architecture:</p> 
<li><strong>Client</strong><br /> You simulate this by using curl commands.</li> 
<li><strong>Web/app server</strong><br /> This is the Java and Spring-based application that you run using the embedded Tomcat. As part of this post, you run this within Docker containers.</li> 
<li><strong>Database server</strong><br /> This is the relational database for your application that stores information about owners, pets, vets, and visits. For this post, use MySQL RDS.</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_2-288x300.png" /></a></p> 
<p>I decided to not put the database inside a container as containers were designed for applications and are transient in nature. The choice was made even easier because you have a fully managed database service available with <a href="https://aws.amazon.com/rds">Amazon RDS</a>.</p> 
<p>RDS manages the work involved in setting up a relational database, from provisioning the infrastructure capacity that you request to installing the database software. After your database is up and running, RDS automates common administrative tasks, such as performing backups and patching the software that powers your database. With optional <a href="https://aws.amazon.com/rds/faqs/#36">Multi-AZ deployments</a>, Amazon RDS also manages synchronous data replication across Availability Zones with automatic failover.</p> 
<b>Walkthrough</b> 
<p>You can find the code for the example covered in this post at <a href="https://github.com/awslabs/amazon-ecs-java-microservices/">amazon-ecs-java-microservices</a> on GitHub.</p> 
<b>Prerequisites</b> 
<p>You need the following to walk through this solution:</p> 
<li>An AWS account</li> 
<li>An access key and secret key for a user in the account</li> 
<li>The AWS CLI installed</li> 
<p>Also, install the latest versions of the following:</p> 
<li>Java</li> 
<li>Maven</li> 
<li>Python</li> 
<li>Docker</li> 
<b>Step 1: Move the existing Java Spring application to a container deployed using Amazon ECS</b> 
<p>First, move the existing monolith application to a container and deploy it using Amazon ECS. This is a great first step before breaking the monolith apart because you still get some benefits before breaking apart the monolith:</p> 
<li>An improved pipeline. The container also allows an engineering organization to create a standard pipeline for the application lifecycle.</li> 
<li>No mutations to machines.</li> 
<p>You can find the monolith example at <a href="https://github.com/awslabs/amazon-ecs-java-microservices/tree/master/1_ECS_Java_Spring_PetClinic">1_ECS_Java_Spring_PetClinic</a>.</p> 
<h3>Container deployment overview</h3> 
<p>The following diagram is an overview of what the setup looks like for Amazon ECS and related services:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/ecs-spring-monolithic-containers-1.png" /></p> 
<p><strong>This setup consists of the following resources:</strong></p> 
<li>The client application that makes a request to the load balancer.</li> 
<li>The load balancer that distributes requests across all available ports and instances registered in the application’s target group using round-robin.</li> 
<li>The target group that is updated by Amazon ECS to always have an up-to-date list of all the service containers in the cluster. This includes the port on which they are accessible.</li> 
<li>One Amazon ECS cluster that hosts the container for the application.</li> 
<li>A VPC network to host the Amazon ECS cluster and associated security groups.</li> 
<p>Each container has a single application process that is bound to port 8080 within its namespace. In reality, all the containers are exposed on a different, randomly assigned port on the host.</p> 
<p>The architecture is containerized but still monolithic because each container has all the same features of the rest of the containers</p> 
<p><strong>The following is also part of the solution but not depicted in the above diagram:</strong></p> 
<li>One <a href="https://aws.amazon.com/ecr/">Amazon Elastic Container Registry (Amazon ECR)</a>&nbsp;repository for the application.</li> 
<li>A service/task definition that spins up containers on the instances of the Amazon ECS cluster.</li> 
<li>A MySQL RDS instance that hosts the applications schema. The information about the MySQL RDS instance is sent in through environment variables to the containers, so that the application can connect to the MySQL RDS instance.</li> 
<p>I have automated setup with the <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/ecs-cluster.cf">1_ECS_Java_Spring_PetClinic/ecs-cluster.cf</a> AWS CloudFormation template and a <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/setup.py">Python script</a>.</p> 
<p>The Python script calls the CloudFormation template for the initial setup of the VPC, Amazon ECS cluster, and RDS instance. It then extracts the outputs from the template and uses those for API calls to create Amazon ECR repositories, tasks, services, Application Load Balancer, and target groups.</p> 
<h3>Environment variables and Spring properties binding</h3> 
<p>As part of the Python script, you pass in a number of environment variables to the container as part of the task/container definition:</p> 
<code class="lang-json">'environment': [
{
'name': 'SPRING_PROFILES_ACTIVE',
'value': 'mysql'
},
{
'name': 'SPRING_DATASOURCE_URL',
'value': my_sql_options['dns_name']
},
{
'name': 'SPRING_DATASOURCE_USERNAME',
'value': my_sql_options['username']
},
{
'name': 'SPRING_DATASOURCE_PASSWORD',
'value': my_sql_options['password']
}
],</code> 
<p>The preceding environment variables work in concert with the Spring property system. The value in the variable SPRING_PROFILES_ACTIVE, makes Spring use the MySQL version of the <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/src/main/resources/application-mysql.properties">application property file</a>. The other environment files override the following properties in that file:</p> 
<li><code class="lang-sql">spring.datasource.url</code></li> 
<li><code class="lang-sql">spring.datasource.username</code></li> 
<li><code class="lang-sql">spring.datasource.password</code></li> 
<p>Optionally, you can also encrypt sensitive values by using <a href="https://aws.amazon.com/ec2/systems-manager/parameter-store/">Amazon EC2 Systems Manager Parameter Store. Instead of handing in the password, you pass in a reference to the parameter and fetch the value as part of the container startup. For more information, see </a><a href="https://aws.amazon.com/blogs/compute/managing-secrets-for-amazon-ecs-applications-using-parameter-store-and-iam-roles-for-tasks/">Managing Secrets for Amazon ECS Applications Using Parameter Store and IAM Roles for Tasks</a><a href="https://aws.amazon.com/ec2/systems-manager/parameter-store/">.</a></p> 
<h3>Spotify Docker Maven plugin</h3> 
<p>Use the <a href="https://github.com/spotify/docker-maven-plugin">Spotify Docker Maven plugin</a> to create the image and push it directly to Amazon ECR. This allows you to do this as part of the regular Maven build. It also integrates the image generation as part of the overall build process. Use an explicit <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/src/main/docker/Dockerfile">Dockerfile</a> as input to the plugin.</p> 
<code class="lang-sql">FROM frolvlad/alpine-oraclejdk8:slim
VOLUME /tmp
ADD spring-petclinic-rest-1.7.jar app.jar
RUN sh -c 'touch /app.jar'
ENV JAVA_OPTS=&quot;&quot;
ENTRYPOINT [ &quot;sh&quot;, &quot;-c&quot;, &quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar&quot; ]</code> 
<p>The Python script discussed earlier uses the <a href="https://aws.amazon.com/cli">AWS CLI</a> to authenticate you with AWS. The script places the token in the appropriate location so that the plugin can work directly against the Amazon ECR repository.</p> 
<h3>Test setup</h3> 
<p>You can test the setup by running the Python script:<br /> <code class="lang-python">python setup.py -m setup -r &lt;your region&gt;</code></p> 
<p>After the script has successfully run, you can test by querying an endpoint:<br /> <code class="lang-python">curl &lt;your endpoint from output above&gt;/owner</code></p> 
<p>You can clean this up before going to the next section:<br /> <code class="lang-python">python setup.py -m cleanup -r &lt;your region&gt;</code></p> 
<b>Step 2: Converting the monolith into microservices running on Amazon ECS</b> 
<p>The second step is to convert the monolith into microservices. For a real application, you would likely not do this as one step, but re-architect an application piece by piece. You would continue to run your monolith but it would keep getting smaller for each piece that you are breaking apart.</p> 
<p>By migrating microservices, you would get four benefits associated with microservices:</p> 
<li><b>Isolation of crashes</b><br /> If one microservice in your application is crashing, then only that part of your application goes down. The rest of your application continues to work properly.</li> 
<li><b>Isolation of security</b><br /> When microservice best practices are followed, the result is that if an attacker compromises one service, they only gain access to the resources of that service. They can’t horizontally access other resources from other services without breaking into those services as well.</li> 
<li><b>Independent scaling</b><br /> When features are broken out into microservices, then the amount of infrastructure and number of instances of each microservice class can be scaled up and down independently.</li> 
<li><b>Development velocity</b><br /> In a monolith, adding a new feature can potentially impact every other feature that the monolith contains. On the other hand, a proper microservice architecture has new code for a new feature going into a new service. You can be confident that any code you write won’t impact the existing code at all, unless you explicitly write a connection between two microservices.</li> 
<p>Find the monolith example at <a href="https://github.com/awslabs/amazon-ecs-java-microservices/tree/master/2_ECS_Java_Spring_PetClinic_Microservices">2_ECS_Java_Spring_PetClinic_Microservices</a>.<br /> You break apart the Spring Pet Clinic application by creating a microservice for each REST&nbsp;API operation, as well as creating one for the system services.</p> 
<h3>Java code changes</h3> 
<p>Comparing the project structure between the monolith and the microservices version, you can see that each service is now its own separate build.<br /> <strong>First, the monolith version:</strong><br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/PetClinicApp_4.png" /><br /> Now, each API operation is its own separate build, which you can build independently and deploy. You have also duplicated some code across the different microservices, such as the classes under the model subpackage. This is intentional as you don’t want to introduce artificial dependencies among the microservices and allow these to evolve differently for each microservice.</p> 
<p>Also, make the dependencies among the API operations more loosely coupled. In the monolithic version, the components are tightly coupled and use object-based invocation.</p> 
<p>Here is an example of this from the OwnerController operation, where the class is directly calling PetRepository to get information about pets. PetRepository is the Repository class (Spring data access layer) to the Pet table in the RDS instance for the Pet API:</p> 
<code class="lang-java">@RestController
class OwnerController {
@Inject
private PetRepository pets;
@Inject
private OwnerRepository owners;
private static final Logger logger = LoggerFactory.getLogger(OwnerController.class);
@RequestMapping(value = &quot;/owner/{ownerId}/getVisits&quot;, method = RequestMethod.GET)
public ResponseEntity&lt;List&lt;Visit&gt;&gt; getOwnerVisits(@PathVariable int ownerId){
List&lt;Pet&gt; petList = this.owners.findById(ownerId).getPets();
List&lt;Visit&gt; visitList = new ArrayList&lt;Visit&gt;();
petList.forEach(pet -&gt; visitList.addAll(pet.getVisits()));
return new ResponseEntity&lt;List&lt;Visit&gt;&gt;(visitList, HttpStatus.OK);
}
}</code> 
<p>In the microservice version, call the Pet API operation and not PetRepository directly. Decouple the components by using interprocess communication; in this case, the Rest API. This provides for fault tolerance and disposability.</p> 
<code class="lang-java">@RestController
class OwnerController {
@Value(&quot;#{environment['SERVICE_ENDPOINT'] ?: 'localhost:8080'}&quot;)
private String serviceEndpoint;
@Inject
private OwnerRepository owners;
private static final Logger logger = LoggerFactory.getLogger(OwnerController.class);
@RequestMapping(value = &quot;/owner/{ownerId}/getVisits&quot;, method = RequestMethod.GET)
public ResponseEntity&lt;List&lt;Visit&gt;&gt; getOwnerVisits(@PathVariable int ownerId){
List&lt;Pet&gt; petList = this.owners.findById(ownerId).getPets();
List&lt;Visit&gt; visitList = new ArrayList&lt;Visit&gt;();
petList.forEach(pet -&gt; {
logger.info(getPetVisits(pet.getId()).toString());
visitList.addAll(getPetVisits(pet.getId()));
});
return new ResponseEntity&lt;List&lt;Visit&gt;&gt;(visitList, HttpStatus.OK);
}
private List&lt;Visit&gt; getPetVisits(int petId){
List&lt;Visit&gt; visitList = new ArrayList&lt;Visit&gt;();
RestTemplate restTemplate = new RestTemplate();
Pet pet = restTemplate.getForObject(&quot;http://&quot;+serviceEndpoint+&quot;/pet/&quot;+petId, Pet.class);
logger.info(pet.getVisits().toString());
return pet.getVisits();
}
}</code> 
<p>You now have an additional method that calls the API. You are also handing in the service endpoint that should be called, so that you can easily inject dynamic endpoints based on the current deployment.</p> 
<h3>Container deployment overview</h3> 
<p>Here is an overview of what the setup looks like for Amazon ECS and the related services:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/ecs-spring-microservice-containers.png" /><br /> <b>This setup consists of the following resources:</b></p> 
<li>The client application that makes a request to the load balancer.</li> 
<li>The Application Load Balancer that inspects the client request. Based on routing rules, it directs the request to an instance and port from the target group that matches the rule.</li> 
<li>The Application Load Balancer that has a target group for each microservice. The target groups are used by the corresponding services to register available container instances. Each target group has a path, so when you call the path for a particular microservice, it is mapped to the correct target group. This allows you to use one Application Load Balancer to serve all the different microservices, accessed by the path. For example, https:///owner/* would be mapped and directed to the Owner microservice.</li> 
<li>One Amazon ECS cluster that hosts the containers for each microservice of the application.</li> 
<li>A VPC network to host the Amazon ECS cluster and associated security groups.</li> 
<p>Because you are running multiple containers on the same instances, use dynamic port mapping to avoid port clashing. By using dynamic port mapping, the container is allocated an anonymous port on the host to which the container port (8080) is mapped. The anonymous port is registered with the Application Load Balancer and target group so that traffic is routed correctly.</p> 
<p><strong>The following is also part of the solution but not depicted in the above diagram:</strong></p> 
<li>One Amazon ECR repository for each microservice.</li> 
<li>A service/task definition per microservice that spins up containers on the instances of the Amazon ECS cluster.</li> 
<li>A MySQL RDS instance that hosts the applications schema. The information about the MySQL RDS instance is sent in through environment variables to the containers. That way, the application can connect to the MySQL RDS instance.</li> 
<p>I have again automated setup with the <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/2_ECS_Java_Spring_PetClinic_Microservices/ecs-cluster.cf">2_ECS_Java_Spring_PetClinic_Microservices/ecs-cluster.cf</a> CloudFormation template and a <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/2_ECS_Java_Spring_PetClinic_Microservices/setup.py">Python script</a>.</p> 
<p>The CloudFormation template remains the same as in the previous section. In the Python script, you are now building five different Java applications, one for each microservice (also includes a system application). There is a separate Maven POM file for each one. The resulting Docker image gets pushed to its own Amazon ECR repository, and is deployed separately using its own service/task definition. This is critical to get the benefits described earlier for microservices.</p> 
<p>Here is an example of the POM file for the Owner microservice:</p> 
<code class="lang-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
&lt;groupId&gt;org.springframework.samples&lt;/groupId&gt;
&lt;artifactId&gt;spring-petclinic-rest&lt;/artifactId&gt;
&lt;version&gt;1.7&lt;/version&gt;
&lt;parent&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
&lt;version&gt;1.5.2.RELEASE&lt;/version&gt;
&lt;/parent&gt;
&lt;properties&gt;
&lt;!-- Generic properties --&gt;
&lt;java.version&gt;1.8&lt;/java.version&gt;
&lt;docker.registry.host&gt;${env.docker_registry_host}&lt;/docker.registry.host&gt;
&lt;/properties&gt;
&lt;dependencies&gt;
&lt;dependency&gt;
&lt;groupId&gt;javax.inject&lt;/groupId&gt;
&lt;artifactId&gt;javax.inject&lt;/artifactId&gt;
&lt;version&gt;1&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- Spring and Spring Boot dependencies --&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-data-rest&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
&lt;!-- Databases - Uses HSQL by default --&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.hsqldb&lt;/groupId&gt;
&lt;artifactId&gt;hsqldb&lt;/artifactId&gt;
&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;mysql&lt;/groupId&gt;
&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;!-- caching --&gt;
&lt;dependency&gt;
&lt;groupId&gt;javax.cache&lt;/groupId&gt;
&lt;artifactId&gt;cache-api&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.ehcache&lt;/groupId&gt;
&lt;artifactId&gt;ehcache&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;!-- end of webjars --&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;
&lt;plugins&gt;
&lt;plugin&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
&lt;/plugin&gt;
&lt;plugin&gt;
&lt;groupId&gt;com.spotify&lt;/groupId&gt;
&lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;0.4.13&lt;/version&gt;
&lt;configuration&gt;
&lt;imageName&gt;${env.docker_registry_host}/${project.artifactId}&lt;/imageName&gt;
&lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt;
&lt;useConfigFile&gt;true&lt;/useConfigFile&gt;
&lt;registryUrl&gt;${env.docker_registry_host}&lt;/registryUrl&gt;
&lt;!--dockerHost&gt;https://${docker.registry.host}&lt;/dockerHost--&gt;
&lt;resources&gt;
&lt;resource&gt;
&lt;targetPath&gt;/&lt;/targetPath&gt;
&lt;directory&gt;${project.build.directory}&lt;/directory&gt;
&lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;
&lt;/resource&gt;
&lt;/resources&gt;
&lt;forceTags&gt;false&lt;/forceTags&gt;
&lt;imageTags&gt;
&lt;imageTag&gt;${project.version}&lt;/imageTag&gt;
&lt;/imageTags&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
&lt;/plugins&gt;
&lt;/build&gt;
&lt;/project&gt;</code> 
<h3>Test setup</h3> 
<p>You can test this by running the Python script:</p> 
<p><code class="lang-python">python setup.py -m setup -r &lt;your region&gt;</code></p> 
<p>After the script has successfully run, you can test by querying an endpoint:</p> 
<p><code class="lang-python">curl &lt;your endpoint from output above&gt;/owner</code></p> 
<b>Conclusion</b> 
<p>Migrating a monolithic application to a containerized set of microservices can seem like a daunting task. Following the steps outlined in this post, you can begin to containerize monolithic Java apps, taking advantage of the container runtime environment, and beginning the process of re-architecting into microservices. On the whole, containerized microservices are faster to develop, easier to iterate on, and more cost effective to maintain and secure.</p> 
<p>This post focused on the first steps of microservice migration. You can learn more about optimizing and scaling your microservices with components such as service discovery, blue/green deployment, circuit breakers, and configuration servers at <a href="http://aws.amazon.com/containers">http://aws.amazon.com/containers</a>.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2489');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Manage Kubernetes Clusters on AWS Using Kops</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Arun Gupta</span></span> | on 
<time property="datePublished" datetime="2017-07-06T13:52:28+00:00">06 JUL 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2475" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2475&amp;disqus_title=Manage+Kubernetes+Clusters+on+AWS+Using+Kops&amp;disqus_url=https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2475');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Any containerized application typically consists of multiple containers. There are containers for the application itself, a database, possibly a web server, and so on. During development, it’s normal to build and test this multi-container application on a single host. This approach works fine during early dev and test cycles but becomes a single point of failure for production, when application availability is critical.</p> 
<p>In such cases, a multi-container application can be deployed on multiple hosts. Customers may need an external tool to manage such multi-container, multi-host deployments. Container orchestration frameworks provides the capability of cluster management, scheduling containers on different hosts, service discovery and load balancing, crash recovery, and other related functionalities. There are multiple options for container orchestration on Amazon Web Services: <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, <a href="https://docs.docker.com/docker-for-aws/">Docker for AWS</a>, and <a href="https://dcos.io/">DC/OS</a>.</p> 
<p>Another popular option for container orchestration on AWS is <a href="https://kubernetes.io/">Kubernetes</a>. There are multiple ways to run a <a href="https://github.com/kubernetes/community/blob/master/sig-aws/kubernetes-on-aws.md">Kubernetes cluster on AWS</a>. This multi-part blog series provides a brief overview and explains some of these approaches in detail. This first post explains how to create a Kubernetes cluster on AWS using <a href="https://github.com/kubernetes/kops">kops</a>.<span id="more-2475"></span></p> 
<b>Kubernetes and Kops overview</b> 
<p>Kubernetes is an open source, container orchestration platform. Applications packaged as Docker images can be easily deployed, scaled, and managed in a Kubernetes cluster. Some of the key features of Kubernetes are:</p> 
<li><strong>Self-healing</strong><br /> Failed containers are restarted to ensure that the desired state of the application is maintained. If a node in the cluster dies, then the containers are rescheduled on a different node. Containers that do not respond to application-defined health check are terminated, and thus rescheduled.</li> 
<li><strong>Horizontal scaling</strong><br /> Number of containers can be easily scaled up and down automatically based upon CPU utilization, or manually using a command.</li> 
<li><strong>Service discovery and load balancing</strong><br /> Multiple containers can be grouped together discoverable using a DNS name. The service can be load balanced with integration to the native LB provided by the cloud provider.</li> 
<li><strong>Application upgrades and rollbacks</strong><br /> Applications can be upgraded to a newer version without an impact to the existing one. If something goes wrong, Kubernetes rolls back the change.</li> 
<p>Kops, short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud. A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons. After the cluster is created, the usual <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">kubectl CLI</a> can be used to manage resources in the cluster.</p> 
<b>Download Kops and Kubectl</b> 
<p>There is no need to download the Kubernetes binary distribution for creating a cluster using kops. However, you do need to download the kops CLI. It then takes care of downloading the right Kubernetes binary in the cloud, and provisions the cluster.</p> 
<p>The different download options for kops are explained at <a href="https://github.com/kubernetes/kops#installing">github.com/kubernetes/kops#installing</a>. On MacOS, the easiest way to install kops is using the <a href="https://brew.sh/">brew</a> package manager.</p> 
brew update &amp;&amp; brew install kops 
<p>The version of kops can be verified using the kops version command, which shows:</p> 
Version 1.6.1 
<p>In addition, download kubectl. This is required to manage the Kubernetes cluster. The latest version of kubectl can be downloaded using the following command:</p> 
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl 
<p>Make sure to include the directory where kubectl is downloaded in your <code>PATH</code>.</p> 
<b>IAM user permission</b> 
<p>The IAM user to create the Kubernetes cluster must have the following permissions:</p> 
<li><code>AmazonEC2FullAccess</code></li> 
<li><code>AmazonRoute53FullAccess</code></li> 
<li><code>AmazonS3FullAccess</code></li> 
<li><code>IAMFullAccess</code></li> 
<li><code>AmazonVPCFullAccess</code></li> 
<p>Alternatively, a new IAM user may be created and the policies attached as explained at <a href="http://github.com/kubernetes/kops/blob/master/docs/aws.md#setup-iam-user">github.com/kubernetes/kops/blob/master/docs/aws.md#setup-iam-user</a>.</p> 
<b>Create an Amazon S3 bucket for the Kubernetes state store</b> 
<p>Kops needs a “state store” to store configuration information of the cluster. &nbsp;For example, how many nodes, instance type of each node, and Kubernetes version. The state is stored during the initial cluster creation. Any subsequent changes to the cluster are also persisted to this store as well. As of publication, Amazon S3 is the only supported storage mechanism. Create a S3 bucket and pass that to the kops CLI during cluster creation.</p> 
<p>This post uses the bucket name <code>kubernetes-aws-io</code>. Bucket names must be unique; you have to use a different name. Create an S3 bucket:</p> 
aws s3api create-bucket --bucket kubernetes-aws-io 
<p>I strongly recommend versioning this bucket in case you ever need to revert or recover a previous version of the cluster. This can be enabled using the AWS CLI as well:</p> 
aws s3api put-bucket-versioning --bucket kubernetes-aws-io --versioning-configuration Status=Enabled 
<p>For convenience, you can also define <code>KOPS_STATE_STORE</code> environment variable pointing to the S3 bucket. For example:</p> 
export KOPS_STATE_STORE=s3://kubernetes-aws-io 
<p>This environment variable is then used by the kops CLI.</p> 
<b>DNS configuration</b> 
<p>As of Kops 1.6.1, a top-level domain or a subdomain is required to create the cluster. This domain allows the worker nodes to discover the master and the master to discover all the etcd servers. This is also needed for kubectl to be able to talk directly with the master.</p> 
<p>This domain may be registered with AWS, in which case a Route 53 hosted zone is created for you. Alternatively, this domain may be at a different registrar. In this case, create a Route 53 hosted zone. Specify the name server (NS) records from the created zone as NS records with the domain registrar.</p> 
<p>This post uses a <code>kubernetes-aws.io</code> domain registered at a third-party registrar.</p> 
<p>Generate a Route 53 hosted zone using the AWS CLI. Download <a href="https://github.com/stedolan/jq/wiki/Installation">jq</a> to run this command:</p> 
ID=$(uuidgen) &amp;&amp; \
aws route53 create-hosted-zone \
--name cluster.kubernetes-aws.io \
--caller-reference $ID \
| jq .DelegationSet.NameServers 
<p>This shows an output such as the following:</p> 
[
&quot;ns-94.awsdns-11.com&quot;,
&quot;ns-1962.awsdns-53.co.uk&quot;,
&quot;ns-838.awsdns-40.net&quot;,
&quot;ns-1107.awsdns-10.org&quot;
] 
<p>Create NS records for the domain with your registrar. Different options on how to configure DNS for the cluster are explained at <a href="http://github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns">github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns</a>.</p> 
<p>Experimental&nbsp;support to create a gossip-based cluster was added in Kops 1.6.2. This post uses a DNS-based approach, as that is more mature and well tested.</p> 
<b>Create the Kubernetes cluster</b> 
<p>The Kops CLI can be used to create a highly available cluster, with multiple master nodes spread across multiple Availability Zones. Workers can be spread across multiple zones as well. Some of the tasks that happen behind the scene during cluster creation are:</p> 
<li>Provisioning EC2 instances</li> 
<li>Setting up AWS resources such as networks, Auto Scaling groups, IAM users, and security groups</li> 
<li>Installing Kubernetes.</li> 
<p>Start the Kubernetes cluster using the following command:</p> 
kops create cluster \
--name cluster.kubernetes-aws.io \
--zones us-west-2a \
--state s3://kubernetes-aws-io \
--yes 
<p>In this command:</p> 
<li><code>--zones</code><br /> Defines the zones in which the cluster is going to be created. Multiple comma-separated zones can be specified to span the cluster across multiple zones.</li> 
<li><code>--name</code><br /> Defines the cluster’s name.</li> 
<li><code>--state</code><br /> Points to the S3 bucket that is the state store.</li> 
<li><code>--yes</code><br /> Immediately creates the cluster. Otherwise, only the cloud resources are created and the cluster needs to be started explicitly using the command <code>kops update --yes</code>. If the cluster needs to be edited, then the <code>kops edit cluster</code> command can be used.</li> 
<p>This starts a single master and two worker node Kubernetes cluster. The master is in an Auto Scaling group and the worker nodes are in a separate group. By default, the master node is <code>m3.medium</code> and the worker node is <code>t2.medium</code>. Master and worker nodes are assigned separate IAM roles as well.</p> 
<p>Wait for a few minutes for the cluster to be created. The cluster can be verified using the command <code>kops validate cluster --state=s3://kubernetes-aws-io</code>. It shows the following output:</p> 
Using cluster from kubectl context: cluster.kubernetes-aws.io
Validating cluster cluster.kubernetes-aws.io
INSTANCE GROUPS
NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;ROLE&nbsp;&nbsp;&nbsp;  &nbsp;MACHINETYPE&nbsp;&nbsp;&nbsp;&nbsp;MIN&nbsp;&nbsp;&nbsp;&nbsp;MAX&nbsp;&nbsp;&nbsp;&nbsp;SUBNETS
master-us-west-2a&nbsp;&nbsp;&nbsp;&nbsp;Master&nbsp;&nbsp;&nbsp;&nbsp;m3.medium&nbsp;&nbsp;&nbsp;&nbsp;  1&nbsp;&nbsp;&nbsp;   1&nbsp;&nbsp;&nbsp;&nbsp;  us-west-2a
nodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    &nbsp;Node&nbsp;&nbsp;&nbsp;  &nbsp;t2.medium&nbsp;&nbsp;&nbsp;&nbsp;  2&nbsp;&nbsp;&nbsp; &nbsp; 2&nbsp;&nbsp;&nbsp;&nbsp;  us-west-2a
NODE STATUS
NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                   ROLE&nbsp;&nbsp;&nbsp;&nbsp;  READY
ip-172-20-38-133.us-west-2.compute.internal&nbsp;&nbsp;&nbsp;&nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;  True
ip-172-20-38-177.us-west-2.compute.internal&nbsp;&nbsp;&nbsp;&nbsp;master&nbsp;&nbsp;&nbsp;&nbsp;True
ip-172-20-46-33.us-west-2.compute.internal&nbsp;&nbsp; &nbsp;&nbsp;node&nbsp;&nbsp;&nbsp;  &nbsp;True
Your cluster cluster.kubernetes-aws.io is ready 
<p>It shows the different instances started for the cluster, and their roles. If multiple cluster states are stored in the same bucket, then <code>--name &lt;NAME&gt;</code> can be used to specify the exact cluster name.</p> 
<p>Check all nodes in the cluster using the command kubectl get nodes:</p> 
NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VERSION
ip-172-20-38-133.us-west-2.compute.internal&nbsp;&nbsp; Ready,node&nbsp;&nbsp;&nbsp;&nbsp; 14m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v1.6.2
ip-172-20-38-177.us-west-2.compute.internal&nbsp;&nbsp; Ready,master&nbsp;&nbsp; 15m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v1.6.2
ip-172-20-46-33.us-west-2.compute.internal&nbsp;&nbsp;&nbsp;&nbsp;Ready,node&nbsp;&nbsp;&nbsp;&nbsp; 14m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v1.6.2 
<p>Again, the internal IP address of each node, their current status (master or node), and uptime are shown. The key information here is the Kubernetes version for each node in the cluster, 1.6.2 in this case.</p> 
<p>The kubectl value included in the PATH earlier is configured to manage this cluster. Resources such as pods, replica sets, and services can now be created in the usual way.</p> 
<p>Some of the common options that can be used to override the default cluster creation are:</p> 
<li><code>--kubernetes-version</code><br /> The version of Kubernetes cluster. The exact versions supported are defined at <a href="http://github.com/kubernetes/kops/blob/master/channels/stable">github.com/kubernetes/kops/blob/master/channels/stable</a>.</li> 
<li><code>--master-size and --node-size</code><br /> Define the instance of master and worker nodes.</li> 
<li><code>--master-count and --node-count</code><br /> Define the number of master and worker nodes. By default, a master is created in each zone specified by <code>--master-zones</code>. Multiple master nodes can be created by a higher number using <code>--master-count</code> or specifying multiple Availability Zones in <code>--master-zones</code>.</li> 
<p>A three-master and five-worker node cluster, with master nodes spread across different Availability Zones, can be created using the following command:</p> 
kops create cluster \
--name cluster2.kubernetes-aws.io \
--zones us-west-2a,us-west-2b,us-west-2c \
--node-count 5 \
--state s3://kubernetes-aws-io \
--yes 
<p>Both the clusters are sharing the same state store but have different names. This also requires you to create an additional Amazon Route 53 hosted zone for the name.</p> 
<p>By default, the resources required for the cluster are directly created in the cloud. The <code>--target</code> option can be used to generate the AWS CloudFormation scripts instead. These scripts can then be used by the AWS CLI to create resources at your convenience.</p> 
<p>Get a complete list of options for cluster creation with <code>kops create cluster --help</code>.</p> 
<p>More details about the cluster can be seen using the command <code>kubectl cluster-info</code>:</p> 
Kubernetes master is running at https://api.cluster.kubernetes-aws.io
KubeDNS is running at https://api.cluster.kubernetes-aws.io/api/v1/proxy/namespaces/kube-system/services/kube-dns
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. 
<p>Check the client and server version using the command <code>kubectl version</code>:</p> 
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4&quot;, GitCommit:&quot;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-05-19T18:44:27Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;darwin/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.2&quot;, GitCommit:&quot;477efc3cbe6a7effca06bd1452fa356e2201e1ee&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-19T20:22:08Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;} 
<p>Both client and server version are 1.6 as shown by the Major and Minor attribute values.</p> 
<b>Upgrade the Kubernetes cluster</b> 
<p>Kops can be used to create a Kubernetes 1.4.x, 1.5.x, or an older version of the 1.6.x cluster using the <code>--kubernetes-version</code> option. The exact versions supported are defined at <a href="https://github.com/kubernetes/kops/blob/master/channels/stable">github.com/kubernetes/kops/blob/master/channels/stable</a>.</p> 
<p>Or, you may have used kops to create a cluster a while ago, and now want to upgrade to the latest recommended version of Kubernetes. Kops supports rolling cluster upgrades where the master and worker nodes are upgraded one by one.</p> 
<p>As of kops 1.6.1, upgrading a cluster is a three-step process.</p> 
<p>First, check and apply the latest recommended Kubernetes update.</p> 
kops upgrade cluster \
--name cluster2.kubernetes-aws.io \
--state s3://kubernetes-aws-io \
--yes 
<p>The <code>--yes</code> option immediately applies the changes. Not specifying the <code>--yes</code> option shows only the changes that are applied.</p> 
<p>Second, update the state store to match the cluster state. This can be done using the following command:</p> 
kops update cluster \
--name cluster2.kubernetes-aws.io \
--state s3://kubernetes-aws-io \
--yes 
<p>Lastly, perform a rolling update for all cluster nodes using the <code>kops rolling-update</code> command:</p> 
kops rolling-update cluster \
--name cluster2.kubernetes-aws.io \
--state s3://kubernetes-aws-io \
--yes 
<p>Previewing the changes before updating the cluster can be done using the same command but without specifying the <code>--yes</code> option. This shows the following output:</p> 
NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NEEDUPDATE&nbsp;&nbsp;&nbsp;&nbsp;READY&nbsp;&nbsp;&nbsp;&nbsp;MIN&nbsp;&nbsp;&nbsp;&nbsp;MAX&nbsp;&nbsp;&nbsp;&nbsp;NODES
master-us-west-2a&nbsp;&nbsp;&nbsp;&nbsp;NeedsUpdate&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;0&nbsp;&nbsp;&nbsp;    &nbsp;1&nbsp;&nbsp;&nbsp;  &nbsp;1&nbsp;&nbsp;&nbsp;  &nbsp;1
nodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    &nbsp;&nbsp;&nbsp;NeedsUpdate&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;    &nbsp;2&nbsp;&nbsp;&nbsp;  &nbsp;2&nbsp;&nbsp;&nbsp;  &nbsp;2 
<p>Using <code>--yes</code> updates all nodes in the cluster, first master and then worker. There is a 5-minute delay between restarting master nodes, and a 2-minute delay between restarting nodes. These values can be altered using <code>--master-interval</code> and <code>--node-interval</code> options, respectively.</p> 
<p>Only the worker nodes may be updated by using the <code>--instance-group</code> node option.</p> 
<b>Delete the Kubernetes cluster</b> 
<p>Typically, the Kubernetes cluster is a long-running cluster to serve your applications. After its purpose is served, you may delete it. It is important to delete the cluster using the kops command. This ensures that all resources created by the cluster are appropriately cleaned up.</p> 
<p>The command to delete the Kubernetes cluster is:</p> 
kops delete cluster --state=s3://kubernetes-aws-io --yes 
<p>If multiple clusters have been created, then specify the cluster name as in the following command:</p> 
kops delete cluster&nbsp;cluster2.kubernetes-aws.io --state=s3://kubernetes-aws-io --yes 
<b>Conclusion</b> 
<p>This post explained how to manage a Kubernetes cluster on AWS using kops. Kubernetes on AWS users provides a self-published list of companies using Kubernetes on AWS.</p> 
<p>Try starting a cluster, create a few Kubernetes resources, and then tear it down. Kops on AWS provides a more comprehensive tutorial for setting up Kubernetes clusters. Kops docs are also helpful for understanding the details.</p> 
<p>In addition, the Kops team hosts <a href="https://github.com/kubernetes/kops#office-hours">office hours</a> to help you get started, from guiding you with your first pull request. You can always join the <a href="http://slack.k8s.io/">#kops channel on Kubernetes slack</a> to ask questions. If nothing works, then file an issue at <a href="https://github.com/kubernetes/kops/issues">github.com/kubernetes/kops/issues</a>.</p> 
<p>Future posts in this series will explain other ways of creating and running a Kubernetes cluster on AWS.</p> 
<p>—&nbsp;<a href="http://twitter.com/arungupta">Arun</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2475');
});
</script> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
