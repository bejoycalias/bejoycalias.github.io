<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a</p>
<br>
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Job Layer (Part 2 of 4)</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-31T09:45:02+00:00">31 MAY 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2123" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2123&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Job+Layer+%28Part+2+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2123');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the second in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<ul> 
<li>Job</li> 
<li>Batch</li> 
<li>Workflow</li> 
</ul> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In Part 2, you tackle the job layer and package a set of bioinformatics applications into Docker containers and store them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>. We illustrate some common patterns and best practices for these containers, such as how you can effectively use <a href="https://aws.amazon.com/s3">Amazon S3</a> to exchange data between jobs.<span id="more-2123"></span></p> 
<p>ECR is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, simplifying your development to a production workflow. ECR eliminates the need to operate your own container repositories or worry about scaling the underlying infrastructure by hosting your images in a highly available and scalable architecture. You can integrate with <a href="https://aws.amazon.com/iam">IAM</a> to provide resource-level control for each repository.</p> 
<p>All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<b id="toc_0">Packaging an application in a Docker container</b> 
<p>Genomic analysis often relies on open source software that is developed by academic groups or open-sourced by industry leaders. These applications have a range of requirements for libraries and reference data, and are typically executed using a Linux command line interface.</p> 
<p>Docker containers provide a consistent, reproducible run-time environment for applications to run in, which results in reproducible results. Consequently, containerization of the applications using Docker has received much attention from the bioinformatics community, resulting in the development of application registries such as <a href="https://dockstore.org/">Dockstore.org</a>, <a href="http://biocontainers.pro/">BioContainers</a>, and the <a href="https://galaxyproject.org/admin/tools/docker/">Galaxy Tool Shed</a>. In this post, we cover several good practices for packing genomics applications in Docker containers, including:</p> 
<ul> 
<li>Building your Dockerfile</li> 
<li>Dealing with job multitenancy</li> 
<li>Sharing data between jobs</li> 
</ul> 
<b id="toc_1">Building your Dockerfile</b> 
<p>Your Dockerfile contains all of the commands that you use to package your Docker container. In it, you pick a base image to build from, include any metadata to attribute to the image, describe how to build and configure the environment, and how to access the code running within it.</p> 
<p>We recommend that you adopt a standard set of conventions for your Dockerfiles. Add metadata to describe the contained application, and have an order set of sections for application packaging needs. Here is an example from the provided <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/docker/Dockerfile">Isaac Dockerfile</a>:</p> 
<pre><code class="language-none">FROM python:2.7
# Metadata
LABEL container.base.image=&quot;python:2.7&quot;
LABEL software.name=&quot;iSAAC&quot;
LABEL software.version=&quot;03.17.03.01&quot;
LABEL software.description=&quot;Aligner for sequencing data&quot;
LABEL software.website=&quot;https://github.com/Illumina/Isaac3&quot;
LABEL software.documentation=&quot;https://github.com/Illumina/Isaac3/blob/master/src/markdown/manual.md&quot;
LABEL software.license=&quot;GPLv3&quot;
LABEL tags=&quot;Genomics&quot;
RUN apt-get -y update &amp;&amp; \
apt-get -y install zlib1g-dev gnuplot &amp;&amp; \
apt-get clean
RUN pip install boto3 awscli
RUN git clone https://github.com/Illumina/Isaac3.git &amp;&amp; cd /Isaac3 &amp;&amp; git checkout 6f0191a4e0d4b332e8f34b7ced57dc6e6eb4f2f1
RUN mkdir /iSAAC-build /isaac
WORKDIR /iSAAC-build
RUN /Isaac3/src/configure --prefix=/isaac
RUN make
RUN make install
WORKDIR /
RUN rm -rf /Isaac3 /iSAAC-build
RUN chmod -R +x /isaac/bin/
ENV PATH=&quot;/isaac/bin:$PATH&quot;
ENV LD_LIBRARY_PATH=&quot;/usr/local/lib:/usr/lib:/isaac/libexec:${LD_LIBRARY_PATH}&quot;
COPY isaac/src/run_isaac.py /
COPY common_utils /common_utils
ENTRYPOINT [&quot;python&quot;, &quot;/run_isaac.py&quot;]</code></pre> 
<p>There are many ways to architect Docker containers, but we wanted to consolidate some recommendations that we have observed our customers successfully using:</p> 
<ul> 
<li>Work off of a base image that satisfies most dependencies across a set of applications. In the code provided, we often use the <a href="https://github.com/docker-library/python/blob/a248f4583c5f788e1af02016f762cdc323ee5765/2.7/Dockerfile">official <code>python:2.7</code> image</a> from <a href="https://hub.docker.com/_/python/">Docker Hub</a>.</li> 
<li>In the above Dockerfile, you can see that we have separated out the metadata, underlying system and library dependencies, application-specific dependencies, and the installation requirements into a logical ordered set for easier maintenance. It’s worth noting that if you are building a production application, you would traditionally have a golden set of images to build from and application artifacts to install that have been validated with internal processes.</li> 
<li>Instead of building large datasets into the container itself, we recommend that you download reference data at runtime instead. This allows the decoupling of the algorithm from reference data updates, which could happen nightly. It also allows you to download a subset of all reference data for an algorithm that is specific to the running analysis, for example the particular species under study.</li> 
<li>Provide an application entry point to both expose and limit the default functionality of the container image. In this example, we created a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/src/run_isaac.py">simple Python script</a> that takes care of downloading the dependencies from S3, such as reference data for your analysis, on the fly from a set of provided runtime parameters and stage results back into S3. We dive deeper into this script in the following section.</li> 
</ul> 
<p>Often these shared dependencies are a mix of packaged code that is easily installable (in this case via pip) or private modules usually shared as part of internal source repositories, as is the case here.</p> 
<p>When you build the Docker images, you should take care to include both the necessary private modules within the context of the build. The example below shows how you would accomplish that given a directory context with some dependencies a few levels higher that the Dockerfile. In the project, we provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/docker/Makefile">Makefile</a> for taking care of some of these items, but for clarity’s sake we issue the necessary Docker commands.</p> 
<pre><code class="language-none"># Given the following partial directory tree structure
# .
# └── tools
#     ├── common\_utils
#     │   ├── __init__.py
#     │   ├── job_utils.py
#     │   └── s3_utils.py
#     └── isaac
#         └── docker
#             ├── Dockerfile
#             └── Makefile
# cd &lt;git repository&gt;/tools/isaac/docker
$ docker build -t isaac:03.17.03.01 \
-t isaac:latest \
-f Dockerfile ../..</code></pre> 
<b id="toc_2">Job multitenancy and sharing data between jobs</b> 
<p>Many bioinformatics tools have been developed to run in any Linux environment, and not necessarily optimized for cloud computing or multitenancy. To overcome these challenges, you can use a simple Python wrapper script for each tool that facilitates the deployment of a job.</p> 
<p>Our tools have several of the same requirements, such as the need to read and write from S3 and deal with job multitenancy. For these common utilities, we built a separate <a href="https://github.com/awslabs/aws-batch-genomics/tree/master/tools/common_utils"><code>common_utils</code></a> package to import during the creation of the Docker image. These utilities deal with the previously mentioned common requirements, such as:</p> 
<ul> 
<li style="list-style-type: none"> 
<ul> 
<li><strong>Container placement</strong></li> 
</ul> </li> 
</ul> 
<p>To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute. The <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/common_utils/s3_utils.py"><code>tools/common_utils/s3_utils.py</code></a> script contains the functions required to leverage S3.</p> 
<ul> 
<li style="list-style-type: none"> 
<ul> 
<li><strong>Multitenancy</strong></li> 
</ul> </li> 
</ul> 
<p>Multiple container jobs may run concurrently on the same instance. In these situations, it’s essential that your job writes to a unique subdirectory. An easy way to do this is to create a subfolder using a UUID and have your application write all of your data there.</p> 
<ul> 
<li style="list-style-type: none"> 
<ul> 
<li><strong>Cleanup</strong></li> 
</ul> </li> 
</ul> 
<p>As your jobs complete and write the output back to S3, you can delete the scratch data on your instance generated by that job. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances. As you ensure that you’re writing to a unique subdirectory in the multitenancy solution, you can simply delete that subdirectory to minimize your storage footprint.</p> 
<p>Each of the Python wrappers takes in all of the requisite data dependencies, residing in S3, as command-line arguments and any other necessary commands to run the tool it wraps. It then handles all of the file downloading, running the bioinformatics tool, and uploading the files back to S3. For more information about each of these tools, see the READMEs for each individual tool.</p> 
<b id="toc_3">Deploying images to Amazon ECR</b> 
<p>Next, publish the Docker images to ECR. The first example below creates an ECR repository and collects the URI you provide to Docker in order to push the image to ECR. If a repository already exists for the container image, query for it, as shown in the second example.</p> 
<pre><code class="language-none"># Create an ECR repository for Isaac, then copy the `repositoryUri` into a variable
$ REPO\_URI=$(aws ecr create-repository \
--repository-name isaac \
--output text --query &quot;repository.repositoryUri&quot;)
# If the repository already exists, then
# query ECR for the `repositoryUri`
$ REPO\_URI=$(aws ecr describe-repositories \
--repository-names isaac \
--output text --query &quot;repositories[0].repositoryUri&quot;)</code></pre> 
<p>After you have a repository URI, you can tag the container image and push it to ECR.</p> 
<pre><code class="language-none">$ eval $(aws ecr get-login)
$ docker tag isaac:latest $(REPO\_URI):latest
$ docker push $(REPO\_URI):latest
$ docker tag isaac:03.17.03.01 $(REPO\_URI):03.17.03.01
$ docker push $(REPO\_URI):03.17.03.01</code></pre> 
<b id="toc_4">Conclusion</b> 
<p>In part 2 of this series, we showed a practical example of packaging up a bioinformatics application within a Docker container, and publishing the container image to an ECR repository. Along the way, we discussed some generally applicable best practices and design decisions specific to this demonstration project.</p> 
<p>Now that you have built one of the Docker containers, you can go ahead and build each of the containers. We have provided all of the necessary code within a GitHub repository and within that repository are specific instructions, as well as some helpers for building container images using GNU make.</p> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3</a> of this series, you’ll dive deep into the batch processing layer and how to leverage the packaged applications within AWS Batch.</p> 
<p>Other posts in this four-part series:</p> 
<ul> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
</ul> 
<p>Please leave any questions and comments below.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2123');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Introduction (Part 1 of 4)</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-30T15:03:34+00:00">30 MAY 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2102" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2102&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Introduction+%28Part+1+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2102');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>Deriving insights from data is foundational to nearly every organization, and many customers process high volumes of data every day. One common requirement of customers in life sciences is the need to analyze these data in a high-throughput fashion without sacrificing time-to-insight.</p> 
<p>Such analyses, which tend to be composed of a series of massively parallel processes (MPP) are well suited to the AWS Cloud. Many AWS customers and partners today, such as <a href="https://aws.amazon.com/solutions/case-studies/illumina/">Illumina</a>, <a href="https://aws.amazon.com/solutions/case-studies/dnanexus/">DNAnexus</a>, <a href="https://aws.amazon.com/solutions/case-studies/seven-bridges-genomics/">Seven Bridges Genomics</a>, <a href="https://www.youtube.com/watch?v=AW_JylyCZLM">AstraZeneca</a>, <a href="https://aws.amazon.com/solutions/case-studies/uc-santa-cruz-genomics-institute/">UCSC Genomics Institute</a>, and <a href="https://www.youtube.com/watch?v=ooMA9J7suh8&amp;index=2&amp;list=PLhr1KZpdzukdeX8mQ2qO73bg6UKQHYsHb">Human Longevity, Inc.</a>, have built scalable and elastic genomics processing solutions on AWS.</p> 
<p>One such use case is genomic sequencing. Modern DNA sequencers, such as Illumina’s <a href="https://www.illumina.com/systems/sequencing-platforms/novaseq.html">NovaSeq</a>, can produce multiple terabytes of raw data each day. The data must then be processed into meaningful information that clinicians and research can act on in a timely fashion. This processing of genomic data is commonly referred to as “secondary analysis”.</p> 
<p>Most common secondary analysis workflows take raw reads generated from sequencers and then process them in a multi-step workflow to identify the variation in a biological sample compared to a standard genome reference. The individual steps are normally similar to the following:<span id="more-2102"></span></p> 
<ol> 
<li>DNA sequences are mapped to a standard genome reference by use of an alignment algorithm, such as Smith-Waterman or Burrows-Wheeler.</li> 
<li>After the sequences are mapped to the reference, the differences are identified as single nucleotide variations, insertions, deletions, or complex structural variation in a process known as variant calling.</li> 
<li>The resulting variants are often combined with other information to identify genomic variants highly correlated with disease or drug response. They might also be analyzed in the context of clinical data such as to identify disease susceptibility or state for a patient.</li> 
<li>Along the way, quality metrics are collected or computed to ensure that the generated data is of the appropriate quality for use in requisite research or clinical settings.</li> 
</ol> 
<p>In this post series, you can build a secondary analysis workflow similar to the one just described. Here is a diagram of the workflow:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/secondary_analysis.png"><img class="aligncenter size-full wp-image-2112" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/secondary_analysis.png" alt="" width="2241" height="554" /></a></p> 
<hr /> 
<b id="toc_1">Secondary analysis is a batch workflow</b> 
<p>At its core, a genomics pipeline is similar to a series of Extract Transform and Load (ETL) steps that convert raw files from a DNA sequencer to a list of variants for one or more individuals. Each step extracts a set of input files from a data source, processes them as a compute-intensive workload (transform), and then loads the output into another location for subsequent storage or analysis.</p> 
<p>These steps are often chained together to build a flexible genomics processing workflow. The files can then be used for downstream analysis, such as population scale analytics with <a href="https://aws.amazon.com/blogs/big-data/interactive-analysis-of-genomic-datasets-using-amazon-athena/">Amazon Athena</a> or <a href="https://blogs.aws.amazon.com/bigdata/post/Tx1GE3J0NATVJ39/Will-Spark-Power-the-Data-behind-Precision-Medicine">Spark on Amazon EMR</a>. These ETL processes can be represented as individual batch steps in an overall workflow.</p> 
<p>When we discuss batch processing with customers, we often focus on the following three layers:</p> 
<p><strong>Jobs (analytical modules):</strong> These jobs are individual units of work that take a set of inputs, run a single process, and produce a set of outputs. In this series, you use Docker containers to define these analytical modules. For genomics, these commonly include alignment, variant calling, quality control, or another module in your workflow. <a href="https://aws.amazon.com/ecs">Amazon ECS</a> is an AWS service that orchestrates and runs these Docker containerized modules on top of <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances.</p> 
<p><strong>Batch engine:</strong> This is a framework for submitting individual analytical modules with the appropriate requirements for computational resources, such as memory and number of CPUs. Each step of the analysis pipeline requires a definition of how to run a job:</p> 
<ul> 
<li>Computational resources (disk, memory, CPU)</li> 
<li>The compute environment to run it in (Docker container, runtime parameters)</li> 
<li>Information about the priority of different jobs</li> 
<li>Any dependencies between jobs</li> 
</ul> 
<p>You can leverage concepts such as container placement and bin packing to maximize performance of your genomic pipeline while concurrently optimizing for cost. We will use <a href="https://aws.amazon.com/batch/">AWS Batch</a> for this layer. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example, CPU or memory optimized instances) based on the volume and specific resource requirements of the submitted batch jobs.</p> 
<p><strong>Workflow orchestration:</strong> This layer sits on top of the batch engine and allows you to decouple your workflow definition from the execution of the individual jobs. You can envision this layer as a state machine where you define a series of steps and pass appropriate metadata between states. We will use <a href="https://aws.amazon.com/lambda">AWS Lambda</a> to submit the jobs to AWS Batch and use <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> to define and orchestrate the workflow.</p> 
<b id="toc_2">Architecture</b> 
<p>In the next three posts, you build a genome analysis pipeline using the following architecture. You don’t explicitly build the grayed out section, but we wanted to include them in the diagram as they are natural extensions to the core architecture. We discuss these and other extensions, briefly, in the concluding post. All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p><strong><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2 covers the job layer</a></strong>. We demonstrate how you can package bioinformatics applications in Docker containers, and discuss best practices when developing these containers for use in a multitenant batch environment.</p> 
<p><strong><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3 dives deep into the batch, or data processing layer</a></strong>. We discuss common considerations for deploying Docker containers to be used in batch analysis as well as demonstrate how you can use AWS Batch for a scalable and elastic batch engine.</p> 
<p><strong>Part 4 dives into workflow layer orchestration</strong>. We show how you might architect that layer with AWS services. You take the components built in parts 2 and 3 and combine them into an entire secondary analysis workflow. This workflow manages dependencies as well as continually checking the progress of existing jobs. We conclude by running a secondary analysis end-to-end for under $1 and discuss some extensions you can build on top of this core workflow.</p> 
<b id="toc_3">What you can expect to learn</b> 
<p>At the end of this series, you will have built a scalable and elastic solution to process genomes on AWS, as well as gained a general understanding of architectural choices available to you. The solution is generally applicable to other workloads, such as image processing. Even non-life science workloads such as trade analytics in financial services can benefit.</p> 
<p>In your solution, you use <a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot Instances</a> to optimize for cost. Spot Instances allow you to bid on spare EC2 compute capacity, which can save up to 90% off of traditional On-Demand prices. In many cases, this translates into the ability to analyze genomes at scale for as low as $1 per analysis.</p> 
<p>Let’s get building!</p> 
<p>Other posts in this four-part series:</p> 
<ul> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
</ul> 
<p>Please leave any questions and comments below.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2102');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Deep Learning on AWS Batch</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Chris Barclay</span></span> | on 
<time property="datePublished" datetime="2017-05-09T15:53:31+00:00">09 MAY 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-batch/" title="View all posts in AWS Batch"><span property="articleSection">AWS Batch</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/deep-learning-on-aws-batch/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2058" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2058&amp;disqus_title=Deep+Learning+on+AWS+Batch&amp;disqus_url=https://aws.amazon.com/blogs/compute/deep-learning-on-aws-batch/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2058');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p>Thanks to my colleague&nbsp;Kiuk Chung for this great post on Deep Learning using AWS Batch.</p> 
<p>—-</p> 
<p>GPU instances naturally pair with deep learning as neural network algorithms can take advantage of their massive parallel processing power. AWS provides GPU instance families, such as g2 and p2, which allow customers to run scalable GPU workloads. You can leverage such scalability efficiently with AWS Batch.</p> 
<p><a href="https://aws.amazon.com/batch">AWS Batch</a> manages the underlying compute resources on-your behalf, allowing you to focus on modeling tasks without the overhead of resource management. Compute environments (that is, clusters) in AWS Batch are pools of instances in your account, which AWS Batch dynamically scales up and down, provisioning and terminating instances with respect to the numbers of jobs. This minimizes idle instances, which in turn optimizes cost.</p> 
<p>Moreover, AWS Batch ensures that submitted jobs are scheduled and placed onto the appropriate instance, hence managing the lifecycle of the jobs. With the addition of <a href="https://aws.amazon.com/about-aws/whats-new/2017/04/aws-batch-managed-compute-environments-support-customer-provided-amis/">customer-provided AMIs</a>, AWS Batch users can now take advantage of this elasticity and convenience for jobs that require GPU.</p> 
<p>This post illustrates how you can run GPU-based deep learning workloads on AWS Batch. I walk you through an <a href="http://mxnet.io/tutorials/python/mnist.html">example</a> of training a convolutional neural network (the <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a> architecture), using Apache MXNet to recognize handwritten digits using the MNIST dataset.<span id="more-2058"></span></p> 
<b>Running an MXNet job in AWS Batch</b> 
<p>Apache MXNet is a full-featured, flexibly programmable, and highly scalable deep learning framework that supports state-of-the-art deep models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs).</p> 
<p>There are three steps to running an AWS Batch job:</p> 
<ul> 
<li>Create a custom AMI</li> 
<li>Create AWS Batch entities</li> 
<li>Submit a training job</li> 
</ul> 
<h3>Create a custom AMI</h3> 
<p>Start by creating an AMI that includes the NVIDIA driver and the Amazon ECS agent. In AWS Batch, instances can be launched with the specific AMI of your choice by specifying <em>imageId</em> when you create your compute environment. Because you are running a job that requires GPU, you need an AMI that has the NVIDIA driver installed.</p> 
<p>Choose <strong>Launch Stack</strong> to launch the CloudFormation template in us-east-1 in your account: <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Batch&amp;templateURL=https://s3.amazonaws.com/aws-batch-examples/create-ami.yaml"> <img src="https://s3.amazonaws.com/aws-batch-blog/2017-05-09_mxnet-batch/launch_stack_icon.png" /> </a></p> 
<p>As shown below, take note of the <strong>AMI</strong> value in the <strong>Outputs</strong> tab of the CloudFormation stack. You use this as the <em>imageId</em> value when creating the compute environment in the next section.</p> 
<p><img src="https://s3.amazonaws.com/aws-batch-blog/2017-05-09_mxnet-batch/ami_cloud_formation_screenshot.png" /></p> 
<p>Alternatively, you may follow the AWS Batch documentation to <a href="http://docs.aws.amazon.com/batch/latest/userguide/batch-gpu-ami.html">create a GPU-enabled AMI</a>.</p> 
<h3>Create AWS Batch resources</h3> 
<p>After you have built the AMI, create the following resources:</p> 
<ul> 
<li><a href="http://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html">compute environment</a></li> 
<li><a href="http://docs.aws.amazon.com/batch/latest/userguide/job_queues.html">job queue</a></li> 
<li><a href="http://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html">job definition</a></li> 
</ul> 
<p>A compute environment, is a collection of instances (compute resources) of the same or different instance types. In this case, you create a managed compute environment in which the instances are of type p2.xlarge. For imageId, specify the AMI you built in the previous section.</p> 
<p>Then, create a job queue. In AWS Batch, jobs are submitted to a job queue that are associated to an ordered list of compute environments. After a lower order compute environment is filled, jobs spill over to the next compute environment. For this example, you associate a single compute environment to the job queue.</p> 
<p>Finally, create a job definition, which is a template for a job specification. For those familiar with Amazon ECS, this is analogous to <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html">task definitions</a>. You mount the directory containing the NVIDIA driver on the host to <strong>/usr/local/nvidia</strong> on the container. You also need to set the <em>privileged</em> flag on the container properties.</p> 
<p>The following code creates the aforementioned resources in AWS Batch. For more information, see the <a href="http://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html">AWS Batch User Guide</a>.</p> 
<pre style="padding-left: 30px">git clone https://github.com/awslabs/aws-batch-helpers
cd aws-batch-helpers/gpu-example
python create-batch-entities.py\
--subnets &lt;subnet1,subnet2,…&gt;\
--security-groups &lt;sg1,sg2,…&gt;\
--key-pair &lt;ec2-key-pair&gt;\
--instance-role &lt;instance-role&gt;\
--image-id &lt;custom-AMI-image-id&gt;\
--service-role &lt;service-role-arn&gt; 
</pre> 
<h3>Submit a training job</h3> 
<p>Now you submit a job that trains a convolutional neural network model for handwritten digit recognition. Much like Amazon ECS tasks, jobs in AWS Batch are run as commands in a Docker container. To use MXNet as your deep learning library, you need a Docker image containing MXNet. For this example, use <a href="https://hub.docker.com/r/mxnet/python/">mxnet/python:gpu</a>.</p> 
<p>The submit-job.py script submits the job, and tails the output from CloudWatch Logs.</p> 
<pre style="padding-left: 30px"># cd aws-batch-helpers/gpu-example
python submit-job.py --wait</pre> 
<p>You should see an output that looks like the following:</p> 
<pre>Submitted job [train_imagenet - e1bccebc-76d9-4cd1-885b-667ef93eb1f5] to the job queue [gpu_queue]
Job [train_imagenet - e1bccebc-76d9-4cd1-885b-667ef93eb1f5] is RUNNING.
Output [train_imagenet/e1bccebc-76d9-4cd1-885b-667ef93eb1f5/12030dd3-0734-42bf-a3d1-d99118b401eb]:
================================================================================
[2017-04-25T19:02:57.076Z] INFO:root:Epoch[0] Batch [100]	Speed: 15554.63 samples/sec Train-accuracy=0.861077
[2017-04-25T19:02:57.428Z] INFO:root:Epoch[0] Batch [200]	Speed: 18224.89 samples/sec Train-accuracy=0.954688
[2017-04-25T19:02:57.755Z] INFO:root:Epoch[0] Batch [300]	Speed: 19551.42 samples/sec Train-accuracy=0.965313
[2017-04-25T19:02:58.080Z] INFO:root:Epoch[0] Batch [400]	Speed: 19697.65 samples/sec Train-accuracy=0.969531
[2017-04-25T19:02:58.405Z] INFO:root:Epoch[0] Batch [500]	Speed: 19705.82 samples/sec Train-accuracy=0.968281
[2017-04-25T19:02:58.734Z] INFO:root:Epoch[0] Batch [600]	Speed: 19486.54 samples/sec Train-accuracy=0.971719
[2017-04-25T19:02:59.058Z] INFO:root:Epoch[0] Batch [700]	Speed: 19735.59 samples/sec Train-accuracy=0.973281
[2017-04-25T19:02:59.384Z] INFO:root:Epoch[0] Batch [800]	Speed: 19631.17 samples/sec Train-accuracy=0.976562
[2017-04-25T19:02:59.713Z] INFO:root:Epoch[0] Batch [900]	Speed: 19490.74 samples/sec Train-accuracy=0.979062
[2017-04-25T19:02:59.834Z] INFO:root:Epoch[0] Train-accuracy=0.976774
[2017-04-25T19:02:59.834Z] INFO:root:Epoch[0] Time cost=3.190
[2017-04-25T19:02:59.850Z] INFO:root:Saved checkpoint to &quot;/mnt/model/mnist-0001.params&quot;
[2017-04-25T19:03:00.079Z] INFO:root:Epoch[0] Validation-accuracy=0.969148
================================================================================
Job [train_imagenet - e1bccebc-76d9-4cd1-885b-667ef93eb1f5] SUCCEEDED</pre> 
<p>In reality, you may want to modify the job command to save the trained model artifact to Amazon S3 so that subsequent prediction jobs can generate predictions against the model. For information about how to reference objects in Amazon S3 in your jobs, see the <a href="https://aws.amazon.com/blogs/compute/creating-a-simple-fetch-and-run-aws-batch-job/">Creating a Simple “Fetch &amp; Run” AWS Batch Job</a> post.</p> 
<b>Conclusion</b> 
<p>In this post, I walked you through an example of running a GPU-enabled job in AWS Batch, using MXNet as the deep learning library. AWS Batch exposes primitives to allow you to focus on implementing the most efficient algorithm for your workload. It enables you to manage the lifecycle of submitted jobs and dynamically adapt the infrastructure requirements of your jobs within the specified bounds. It’s easy to take advantage of the horizontal scalability of compute instances provided by AWS in a cost-efficient manner.</p> 
<p>MXNet, on the other hand, provides a rich set of highly optimized and scalable building blocks to start implementing your own deep learning algorithms. Together, you can not only solve problems requiring large neural network models, but also cut down on iteration time by harnessing the seemingly unlimited compute resources in Amazon EC2.</p> 
<p>With AWS Batch managing the resources on your behalf, you can easily implement workloads such as hyper-parameter optimization to fan out tens or even hundreds of searches in parallel to find the best set of model parameters for your problem space. Moreover, because your jobs are run inside Docker containers, you may choose the tools and libraries that best fit your needs, build a Docker image, and submit your jobs using the image of your choice.</p> 
<p>We encourage you to try it yourself and let us know what you think!</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2058');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b><a href="https://aws.amazon.com/blogs/compute/serverless-conference-and-more/" property="url" rel="bookmark"><span property="name headline">ServerlessConf and More!</span></a></b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Bryan Liston</span></span> | on 
<time property="datePublished" datetime="2017-04-17T12:01:57+00:00">17 APR 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/serverless-conference-and-more/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2036" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2036&amp;disqus_title=ServerlessConf+and+More%21&amp;disqus_url=https://aws.amazon.com/blogs/compute/serverless-conference-and-more/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2036');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<b>ServerlessConf Austin</b> 
<p><a href="https://austin.serverlessconf.io/"><strong>ServerlessConf Austin</strong></a> is just around the corner! April 26-28th come join us in Austin at the&nbsp;Zach Topfer Theater. Our very own Tim Wagner, Chris Munns and Randall Hunt will be giving some great talks.</p> 
<blockquote> 
<p>Serverlessconf is a community led conference focused on sharing experiences building applications using serverless architectures. Serverless architectures enable developers to express their creativity and focus on user needs instead of spending time managing infrastructure and servers.</p> 
</blockquote> 
<p><strong>Tim Wagner, GM Serverless Applications</strong>, will be giving a keynote on Friday the 28th, <strong>do not miss this</strong>!!!<br /> <strong>Chris Munns, Sr. Developer Advocate</strong>, will be giving an excellent talk on CI/CD for Serverless Applications.</p> 
<p>Check out the full agenda <a href="https://austin.serverlessconf.io/agenda.html">here</a>!</p> 
<b>AWS Serverless Updates and More!</b> 
<p>Incase you’ve missed out lately on some of our new content such as our new YouTube series “Coding with Sam”, or our new Serverless Special AWS Podcast Series, check them out!</p> 
<ul> 
<li>Coding with Sam Episode 1:&nbsp;<a href="https://www.youtube.com/watch?v=P7i01eqmzrs">How to Do Continuous Integration and Continuous Deployment with AWS Lambda and AWS CodePipeline</a></li> 
<li>Serverless Podcast Special Episode #1 (Serverless Architectures): <a href="https://soundcloud.com/amazon-web-services-306355661/171-serverless-special">AWS Podcast #171</a></li> 
<li>Serverless Podcast Special Episode #2 (Serverless Security): <a href="https://soundcloud.com/amazon-web-services-306355661/178-security-in-serverless-architectures">AWS Podcast #178</a></li> 
</ul> 
<b>Meet SAM!</b> 
<p>We’ve recently come out with a new branding for <a href="https://github.com/awslabs/serverless-application-model">AWS SAM</a> (Serverless Application Model), so please join me in welcoming SAM the Squirrel!</p> 
<p>The goal of AWS SAM is to define a standard application model for serverless applications.</p> 
<p><img class="alignleft" src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/aws_sam_introduction.png" /></p> 
<p>Once again, don’t hesitate to reach out if you have questions, comments, or general feedback.</p> 
<p>Thanks,<br /> <a href="https://twitter.com/listonb">@listonb</a></p> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">How to remove boilerplate validation logic in your REST APIs with Amazon API Gateway request validation</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Bryan Liston</span></span> | on 
<time property="datePublished" datetime="2017-04-11T10:54:55+00:00">11 APR 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-api-gateway/" title="View all posts in Amazon API Gateway"><span property="articleSection">Amazon API Gateway</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/how-to-remove-boilerplate-validation-logic-in-your-rest-apis-with-amazon-api-gateway-request-validation/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2026" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2026&amp;disqus_title=How+to+remove+boilerplate+validation+logic+in+your+REST+APIs+with+Amazon+API+Gateway+request+validation&amp;disqus_url=https://aws.amazon.com/blogs/compute/how-to-remove-boilerplate-validation-logic-in-your-rest-apis-with-amazon-api-gateway-request-validation/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2026');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><img style="width: 15%" src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/logging_rgreen.jpg" alt="" /><br /> <strong>Ryan Green, Software Development Engineer</strong></p> 
<p>Does your API suffer from code bloat or wasted developer time due to implementation of simple input validation rules? One of the necessary but least exciting aspects of building a robust REST API involves implementing basic validation of input data to your API. In addition to increasing the size of the code base, validation logic may require taking on extra dependencies and requires diligence in ensuring the API implementation doesn’t get out of sync with API request/response models and SDKs.</p> 
<p>Amazon API Gateway recently announced the release of <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-request-validation.html">request validators</a>, a simple but powerful new feature that should help to liberate API developers from the undifferentiated effort of implementing basic request validation in their API backends.</p> 
<p>This feature leverages API Gateway <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-create-model.html">models</a> to enable the validation of request payloads against the specified schema, including validation rules as defined in the <a href="http://json-schema.org/latest/json-schema-validation.html">JSON-Schema Validation</a> specification. Request validators also support basic validation of required HTTP request parameters in the URI, query string, and headers.</p> 
<p>When a validation failure occurs, API Gateway fails the API request with an HTTP 400 error, skips the request to the backend integration, and publishes detailed error results in <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch Logs</a>.</p> 
<p>In this post, I show two examples using request validators, validating the request body and the request parameters.<span id="more-2026"></span></p> 
<b id="toc_0">Example: Validating the request body</b> 
<p>For this example, you build a simple API for a simulated stock trading system. This API has a resource, “/orders”, that represents stock purchase orders. An HTTP POST to this resource allows the client to initiate one or more orders.</p> 
<p>A sample request might look like this:</p> 
<p>POST /orders</p> 
<pre><code class="language-javascript">[
&nbsp; {
&nbsp; &nbsp; &quot;account-id&quot;: &quot;abcdef123456&quot;,
&nbsp; &nbsp; &quot;type&quot;: &quot;STOCK&quot;,
&nbsp; &nbsp; &quot;symbol&quot;: &quot;AMZN&quot;,
&nbsp; &nbsp; &quot;shares&quot;: 100,
&nbsp; &nbsp; &quot;details&quot;: {
&nbsp; &nbsp; &nbsp; &quot;limit&quot;: 1000
&nbsp; &nbsp; }
&nbsp; },
&nbsp; {
&nbsp; &nbsp; &quot;account-id&quot;: &quot;zyxwvut987654&quot;,
&nbsp; &nbsp; &quot;type&quot;: &quot;STOCK&quot;,
&nbsp; &nbsp; &quot;symbol&quot;: &quot;BA&quot;,
&nbsp; &nbsp; &quot;shares&quot;: 250,
&nbsp; &nbsp; &quot;details&quot;: {
&nbsp; &nbsp; &nbsp; &quot;limit&quot;: 200
&nbsp; &nbsp; }
&nbsp; }
]</code></pre> 
<p>The JSON-Schema for this request body might look something like this:</p> 
<pre><code class="language-javascript">{
&nbsp; &quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&nbsp; &quot;title&quot;: &quot;Create Orders Schema&quot;,
&nbsp; &quot;type&quot;: &quot;array&quot;,
&nbsp; &quot;minItems&quot;: 1,
&nbsp; &quot;items&quot;: {
&nbsp; &nbsp; &quot;type&quot;: &quot;object&quot;,
&nbsp; &nbsp; &quot;required&quot;: [
&nbsp; &nbsp; &nbsp; &quot;account-id&quot;,
&nbsp; &nbsp; &nbsp; &quot;type&quot;,
&nbsp; &nbsp; &nbsp; &quot;symbol&quot;,
&nbsp; &nbsp; &nbsp; &quot;shares&quot;,
&nbsp; &nbsp; &nbsp; &quot;details&quot;
&nbsp; &nbsp; ],
&nbsp; &nbsp; &quot;properties&quot;: {
&nbsp; &nbsp; &nbsp; &quot;account_id&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;string&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;pattern&quot;: &quot;[A-Za-z]{6}[0-9]{6}&quot;
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;type&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;string&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;enum&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;STOCK&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;BOND&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;CASH&quot;
&nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;symbol&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;string&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;minLength&quot;: 1,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;maxLength&quot;: 4
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;shares&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;number&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;minimum&quot;: 1,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;maximum&quot;: 1000
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;details&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;object&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;required&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;limit&quot;
&nbsp; &nbsp; &nbsp; &nbsp; ],
&nbsp; &nbsp; &nbsp; &nbsp; &quot;properties&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;limit&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;number&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; }
}</code></pre> 
<p>This schema defines the “shape” of the request model but also defines several constraints on the various properties. Here are the validation rules for this schema:</p> 
<ul> 
<li>The root array must have at least 1 item</li> 
<li>All properties are required</li> 
<li>Account ID must match the regular expression format “[A-Za-z]{6}[0-9]{6}”</li> 
<li>Type must be one of STOCK, BOND, or CASH</li> 
<li>Symbol must be a string between 1 and 4 characters</li> 
<li>Shares must be a number between 1 and 1000</li> 
</ul> 
<p>I’m sure you can imagine how this would look in your validation library of choice, or at worst, in a hand-coded implementation.</p> 
<p>Now, try this out with API Gateway request validators. The Swagger definition below defines the REST API, models, and request validators. Its two operations define simple mock integrations to simulate behavior of the stock trading API.</p> 
<p>Note the request validator definitions under the “<em>x-amazon-apigateway-request-validators</em>” extension, and the references to these validators defined on the operation and on the API.</p> 
<pre><code class="language-javascript">{
&quot;swagger&quot;: &quot;2.0&quot;,
&quot;info&quot;: {
&quot;title&quot;: &quot;API Gateway - Request Validation Demo - rpgreen@amazon.com&quot;
},
&quot;schemes&quot;: [
&quot;https&quot;
],
&quot;produces&quot;: [
&quot;application/json&quot;
],
&quot;x-amazon-apigateway-request-validators&quot; : {
&quot;full&quot; : {
&quot;validateRequestBody&quot; : true,
&quot;validateRequestParameters&quot; : true
},
&quot;body-only&quot; : {
&quot;validateRequestBody&quot; : true,
&quot;validateRequestParameters&quot; : false
}
},
&quot;x-amazon-apigateway-request-validator&quot; : &quot;full&quot;,
&quot;paths&quot;: {
&quot;/orders&quot;: {
&quot;post&quot;: {
&quot;x-amazon-apigateway-request-validator&quot;: &quot;body-only&quot;,
&quot;parameters&quot;: [
{
&quot;in&quot;: &quot;body&quot;,
&quot;name&quot;: &quot;CreateOrders&quot;,
&quot;required&quot;: true,
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/CreateOrders&quot;
}
}
],
&quot;responses&quot;: {
&quot;200&quot;: {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Message&quot;
}
},
&quot;400&quot; : {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Message&quot;
}
}
},
&quot;x-amazon-apigateway-integration&quot;: {
&quot;responses&quot;: {
&quot;default&quot;: {
&quot;statusCode&quot;: &quot;200&quot;,
&quot;responseTemplates&quot;: {
&quot;application/json&quot;: &quot;{\&quot;message\&quot; : \&quot;Orders successfully created\&quot;}&quot;
}
}
},
&quot;requestTemplates&quot;: {
&quot;application/json&quot;: &quot;{\&quot;statusCode\&quot;: 200}&quot;
},
&quot;passthroughBehavior&quot;: &quot;never&quot;,
&quot;type&quot;: &quot;mock&quot;
}
},
&quot;get&quot;: {
&quot;parameters&quot;: [
{
&quot;in&quot;: &quot;header&quot;,
&quot;name&quot;: &quot;Account-Id&quot;,
&quot;required&quot;: true
},
{
&quot;in&quot;: &quot;query&quot;,
&quot;name&quot;: &quot;type&quot;,
&quot;required&quot;: false
}
],
&quot;responses&quot;: {
&quot;200&quot; : {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Orders&quot;
}
},
&quot;400&quot; : {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Message&quot;
}
}
},
&quot;x-amazon-apigateway-integration&quot;: {
&quot;responses&quot;: {
&quot;default&quot;: {
&quot;statusCode&quot;: &quot;200&quot;,
&quot;responseTemplates&quot;: {
&quot;application/json&quot;: &quot;[{\&quot;order-id\&quot; : \&quot;qrx987\&quot;,\n   \&quot;type\&quot; : \&quot;STOCK\&quot;,\n   \&quot;symbol\&quot; : \&quot;AMZN\&quot;,\n   \&quot;shares\&quot; : 100,\n   \&quot;time\&quot; : \&quot;1488217405\&quot;,\n   \&quot;state\&quot; : \&quot;COMPLETED\&quot;\n},\n{\n   \&quot;order-id\&quot; : \&quot;foo123\&quot;,\n   \&quot;type\&quot; : \&quot;STOCK\&quot;,\n   \&quot;symbol\&quot; : \&quot;BA\&quot;,\n   \&quot;shares\&quot; : 100,\n   \&quot;time\&quot; : \&quot;1488213043\&quot;,\n   \&quot;state\&quot; : \&quot;COMPLETED\&quot;\n}\n]&quot;
}
}
},
&quot;requestTemplates&quot;: {
&quot;application/json&quot;: &quot;{\&quot;statusCode\&quot;: 200}&quot;
},
&quot;passthroughBehavior&quot;: &quot;never&quot;,
&quot;type&quot;: &quot;mock&quot;
}
}
}
},
&quot;definitions&quot;: {
&quot;CreateOrders&quot;: {
&quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&quot;title&quot;: &quot;Create Orders Schema&quot;,
&quot;type&quot;: &quot;array&quot;,
&quot;minItems&quot; : 1,
&quot;items&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;$ref&quot; : &quot;#/definitions/Order&quot;
}
},
&quot;Orders&quot; : {
&quot;type&quot;: &quot;array&quot;,
&quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&quot;title&quot;: &quot;Get Orders Schema&quot;,
&quot;items&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;order_id&quot;: { &quot;type&quot;: &quot;string&quot; },
&quot;time&quot; : { &quot;type&quot;: &quot;string&quot; },
&quot;state&quot; : {
&quot;type&quot;: &quot;string&quot;,
&quot;enum&quot;: [
&quot;PENDING&quot;,
&quot;COMPLETED&quot;
]
},
&quot;order&quot; : {
&quot;$ref&quot; : &quot;#/definitions/Order&quot;
}
}
}
},
&quot;Order&quot; : {
&quot;type&quot;: &quot;object&quot;,
&quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&quot;title&quot;: &quot;Schema for a single Order&quot;,
&quot;required&quot;: [
&quot;account-id&quot;,
&quot;type&quot;,
&quot;symbol&quot;,
&quot;shares&quot;,
&quot;details&quot;
],
&quot;properties&quot; : {
&quot;account-id&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;pattern&quot;: &quot;[A-Za-z]{6}[0-9]{6}&quot;
},
&quot;type&quot;: {
&quot;type&quot; : &quot;string&quot;,
&quot;enum&quot; : [
&quot;STOCK&quot;,
&quot;BOND&quot;,
&quot;CASH&quot;]
},
&quot;symbol&quot; : {
&quot;type&quot;: &quot;string&quot;,
&quot;minLength&quot;: 1,
&quot;maxLength&quot;: 4
},
&quot;shares&quot;: {
&quot;type&quot;: &quot;number&quot;,
&quot;minimum&quot;: 1,
&quot;maximum&quot;: 1000
},
&quot;details&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;required&quot;: [
&quot;limit&quot;
],
&quot;properties&quot;: {
&quot;limit&quot;: {
&quot;type&quot;: &quot;number&quot;
}
}
}
}
},
&quot;Message&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;message&quot; : {
&quot;type&quot; : &quot;string&quot;
}
}
}
}
}</code></pre> 
<p>To create the demo API, run the following commands (requires the <a href="https://aws.amazon.com/cli/">AWS CLI</a>):</p> 
<pre><code class="language-bash">git clone https://github.com/rpgreen/apigateway-validation-demo.git
cd apigateway-validation-demo
aws apigateway import-rest-api --body &quot;file://validation-swagger.json&quot; --region us-east-1
export API_ID=[API ID from last step]
aws apigateway create-deployment --rest-api-id $API_ID --stage-name test --region us-east-1</code></pre> 
<p>Make some requests to this API. Here’s the happy path with valid request body:</p> 
<pre><code class="language-none">curl -v -H &quot;Content-Type: application/json&quot; -X POST -d ' [&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;{&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;account-id&quot;:&quot;abcdef123456&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;:&quot;STOCK&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;symbol&quot;:&quot;AMZN&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;shares&quot;:100,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;details&quot;:{&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;limit&quot;:1000
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;}
]' https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 200 OK</p> 
<pre><code class="language-none">{&quot;message&quot; : &quot;Orders successfully created&quot;}</code></pre> 
<p>Put the request validator to the test. Notice the errors in the payload:</p> 
<pre><code class="language-none">curl -v -H &quot;Content-Type: application/json&quot; -X POST -d '[
{
&quot;account-id&quot;: &quot;abcdef123456&quot;,
&quot;type&quot;: &quot;foobar&quot;,
&quot;symbol&quot;: &quot;thisstringistoolong&quot;,
&quot;shares&quot;: 999999,
&quot;details&quot;: {
&quot;limit&quot;: 1000
}
}
]' https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 400 Bad Request</p> 
<pre><code class="language-none">{&quot;message&quot;: &quot;Invalid request body&quot;}</code></pre> 
<p>When you inspect the CloudWatch Logs entries for this API, you see the detailed error messages for this payload. Run the following command:</p> 
<pre><code class="language-none">pip install apilogs
apilogs get --api-id $API_ID --stage test --watch --region us-east-1`</code></pre> 
<p>The CloudWatch Logs entry for this request reveals the specific validation errors:</p> 
<p>“Request body does not match model schema for content type application/json: [numeric instance is greater than the required maximum (maximum: 1000, found: 999999), string “thisstringistoolong” is too long (length: 19, maximum allowed: 4), instance value (“foobar”) not found in enum (possible values: [“STOCK”,”BOND”,”CASH”])]”</p> 
<p>Note on Content-Type:</p> 
<p>Request body validation is performed according to the configured request Model which is selected by the value of the request ‘Content-Type’ header. In order to enforce validation and restrict requests to explicitly-defined content types, it’s a good idea to use strict request passthrough behavior (‘”passthroughBehavior”: “never”‘), so that unsupported content types fail with 415 “Unsupported Media Type” response.</p> 
<b id="toc_1">Example: Validating the request parameters</b> 
<p>For the next example, add a GET method to the /orders resource that returns the list of purchase orders. This method has an optional query string parameter (type) and a required header parameter (Account-Id).</p> 
<p>The request validator configured for the GET method is set to validate incoming request parameters. This performs basic validation on the required parameters, ensuring that the request parameters are <em>present and non-blank</em>.</p> 
<p>Here are some example requests.</p> 
<p><strong>Happy path:</strong></p> 
<pre><code class="language-none">curl -v -H &quot;Account-Id: abcdef123456&quot; &quot;https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders?type=STOCK&quot;</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 200 OK</p> 
<pre><code class="language-none">[{&quot;order-id&quot; : &quot;qrx987&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;AMZN&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488217405&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
},
{
&quot;order-id&quot; : &quot;foo123&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;BA&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488213043&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
}]</code></pre> 
<p><strong>Omitting optional type parameter:</strong></p> 
<pre><code class="language-none">curl -v -H &quot;Account-Id: abcdef123456&quot; &quot;https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders&quot;</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 200 OK</p> 
<pre><code class="language-none">[{&quot;order-id&quot; : &quot;qrx987&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;AMZN&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488217405&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
},
{
&quot;order-id&quot; : &quot;foo123&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;BA&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488213043&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
}]</code></pre> 
<p><strong>Omitting required Account-Id parameter:</strong></p> 
<pre><code class="language-none">curl -v &quot;https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders?type=STOCK&quot;</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 400 Bad Request</p> 
<pre><code class="language-none">{&quot;message&quot;: &quot;Missing required request parameters: [Account-Id]&quot;}</code></pre> 
<b id="toc_2">Conclusion</b> 
<p>Request validators should help API developers to build better APIs by allowing them to remove boilerplate validation logic from backend implementations and focus on actual business logic and deep validation. This should further reduce the size of the API codebase and also help to ensure that API models and validation logic are kept in sync.</p> 
<p>Please forward any questions or feedback to the API Gateway team through <a href="https://aws.amazon.com/contact-us/">AWS Support</a> or on the <a href="https://forums.aws.amazon.com/forum.jspa?forumID=199">AWS Forums</a>.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2026');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Scaling Your Desktop Application Streams with Amazon AppStream 2.0</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Bryan Liston</span></span> | on 
<time property="datePublished" datetime="2017-04-04T06:20:28+00:00">04 APR 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/amazon-appstream-2-0/" title="View all posts in Amazon AppStream 2.0"><span property="articleSection">Amazon AppStream 2.0</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/scaling-your-desktop-application-streams-with-amazon-appstream-2-0/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2016" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2016&amp;disqus_title=Scaling+Your+Desktop+Application+Streams+with+Amazon+AppStream+2.0&amp;disqus_url=https://aws.amazon.com/blogs/compute/scaling-your-desktop-application-streams-with-amazon-appstream-2-0/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2016');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/dsuryan.png" /><br /> <strong>Deepak Sury, Principal Product Manager – Amazon AppStream 2.0</strong></p> 
<p>Want to stream desktop applications to a web browser, without rewriting them? <a href="https://aws.amazon.com/appstream2/">Amazon AppStream 2.0</a> is a fully managed, secure, application streaming service. An easy way to learn what the service does is to <a href="https://console.aws.amazon.com/appstream2/tryitnow/home">try out the end-user experience, at no cost</a>.</p> 
<p>In this post, I describe how you can scale your AppStream 2.0 environment, and achieve some cost optimizations. I also add some setup and monitoring tips.<span id="more-2016"></span></p> 
<h4 id="toc_0">AppStream 2.0 workflow</h4> 
<p>You import your applications into AppStream 2.0 using an image builder. The image builder allows you to connect to a desktop experience from within the AWS Management Console, and then install and test your apps. Then, create an image that is a snapshot of the image builder.</p> 
<p>After you have an image containing your applications, select an instance type and launch a fleet of streaming instances. Each instance in the fleet is used by only one user, and you match the instance type used in the fleet to match the needed application performance. Finally, attach the fleet to a stack to set up user access. The following diagram shows the role of each resource in the workflow.</p> 
<p style="text-align: center"><em>Figure 1: Describing an AppStream 2.0 workflow</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstream_figure11.png" alt="appstreamscaling_1.png" /></p> 
<b id="toc_1">Setting up AppStream 2.0</b> 
<p>To get started, <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/getting-started.html">set up an example AppStream 2.0 stack</a> or use the Quick Links on the console. For this example, I named my stack ds-sample, selected a sample image, and chose the stream.standard.medium instance type. You can explore the resources that you set up in the AWS console, or use the describe-stacks and describe-fleets commands as follows:</p> 
<p style="text-align: center"><em>Figure 2: Describing an AppStream 2.0 stack</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_1.png" alt="appstreamscaling_1.png" /></p> 
<p style="text-align: center"><em>Figure 3: Describing an AppStream 2.0 fleet</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/figure2.png" alt="appstreamscaling_2.43%20AM" /></p> 
<p>To set up user access to your streaming environment, you can use your existing <a title="undefined" href="https://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers.html" target="null">SAML 2.0 compliant directory</a>. Your users can then use their existing credentials to log in. Alternatively, to quickly test a streaming connection, or to start a streaming session from your own website, you can create a streaming URL. In the console, choose <strong>Stacks</strong>, <strong>Actions</strong>, <strong>Create URL</strong>, or call create-streaming-url as follows:</p> 
<p style="text-align: center"><em>Figure 4: Creating a streaming URL</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_3.png" alt="appstreamscaling_3.png" /></p> 
<p>You can paste the streaming URL into a browser, and open any of the displayed applications.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/figure3_part2.png" alt="appstreamscaling_4.30%20PM" /></p> 
<p>Now that you have a sample environment set up, here are a few tips on scaling.</p> 
<b id="toc_2">Scaling and cost optimization for AppStream 2.0</b> 
<p>To provide an instant-on streaming connection, the instances in an AppStream 2.0 fleet are always running. You are charged for running instances, and each running instance can serve exactly one user at any time. To optimize your costs, match the number of running instances to the number of users who want to stream apps concurrently. This section walks through three options for doing this:</p> 
<ul> 
<li>Fleet Auto Scaling</li> 
<li>Fixed fleets based on a schedule</li> 
<li>Fleet Auto Scaling with schedules</li> 
</ul> 
<b id="toc_3">Fleet Auto Scaling</b> 
<p>To dynamically update the number of running instances, you can use Fleet Auto Scaling. This feature allows you to scale the size of the fleet automatically between a minimum and maximum value based on demand. This is useful if you have user demand that changes constantly, and you want to scale your fleet automatically to match this demand. For examples about setting up and managing scaling policies, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/autoscaling.html">Fleet Auto Scaling</a>.</p> 
<p>You can trigger changes to the fleet through the available Amazon CloudWatch metrics:</p> 
<ul> 
<li>CapacityUtilization – the percentage of running instances already used.</li> 
<li>AvailableCapacity – the number of instances that are unused and can receive connections from users.</li> 
<li>InsufficientCapacityError – an error that is triggered when there is no available running instance to match a user’s request.</li> 
</ul> 
<p>You can create and attach scaling policies using the AWS SDK or AWS Management Console. I find it convenient to set up the policies using the console. Use the following steps:</p> 
<ol> 
<li>In the AWS Management Console, open AppStream 2.0.</li> 
<li>Choose <strong>Fleets</strong>, select a fleet, and choose <strong>Scaling Policies</strong>.</li> 
<li>For <strong>Minimum capacity</strong> and <strong>Maximum capacity</strong>, enter values for the fleet.</li> 
</ol> 
<p style="text-align: center"><em>Figure 5: Fleets tab for setting scaling policies</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_5.png" alt="appstreamscaling_5.png" /></p> 
<ol> 
<li>Create scale out and scale in policies by choosing <strong>Add Policy</strong> in each section.</li> 
</ol> 
<p style="text-align: center"><em>Figure 6: Adding a scale out policy</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_6.png" alt="appstreamscaling_6.png" /></p> 
<p style="text-align: center"><em>Figure 7: Adding a scale in policy</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_7.png" alt="appstreamscaling_7.png" /></p> 
<p>After you create the policies, they are displayed as part of your fleet details.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_8.png" alt="appstreamscaling_8.png" /></p> 
<p>The scaling policies are triggered by CloudWatch alarms. These alarms are automatically created on your behalf when you create the scaling policies using the console. You can view and modify the alarms via the CloudWatch console.</p> 
<p style="text-align: center"><em>Figure 8: CloudWatch alarms for triggering fleet scaling</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_9.png" alt="appstreamscaling_9.png" /></p> 
<b id="toc_4">Fixed fleets based on a schedule</b> 
<p>An alternative option to optimize costs and respond to predictable demand is to fix the number of running instances based on the time of day or day of the week. This is useful if you have a fixed number of users signing in at different times of the day― scenarios such as a training classes, call center shifts, or school computer labs. You can easily set the number of instances that are running using the AppStream 2.0 update-fleet command. Update the Desired value for the compute capacity of your fleet. The number of Running instances changes to match the Desired value that you set, as follows:</p> 
<p style="text-align: center"><em>Figure 9: Updating desired capacity for your fleet</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_10.png" alt="appstreamscaling_10.png" /></p> 
<p>Set up a Lambda function to update your fleet size automatically. Follow the example below to set up your own functions. If you haven’t used Lambda before, see <a href="http://docs.aws.amazon.com/lambda/latest/dg/getting-started-create-function.html">Step 2: Create a HelloWorld Lambda Function and Explore the Console</a>.</p> 
<p><strong>To create a function to change the fleet size</strong></p> 
<ol> 
<li>In the Lambda console, choose <strong>Create a Lambda function</strong>.</li> 
<li>Choose the <strong>Blank Function</strong> blueprint. This gives you an empty blueprint to which you can add your code.</li> 
<li>Skip the trigger section for now. Later on, you can add a trigger based on time, or any other input.</li> 
<li>In the <strong>Configure function</strong> section: 
<ol> 
<li>Provide a name and description.</li> 
<li>For <strong>Runtime</strong>, choose Node.js 4.3.</li> 
<li>Under <strong>Lambda function handler and role</strong>, choose <strong>Create a custom role</strong>.</li> 
<li>In the IAM wizard, enter a role name, for example Lambda-AppStream-Admin. Leave the defaults as is.</li> 
<li>After the IAM role is created, attach an AppStream 2.0 managed policy “AmazonAppStreamFullAccess” to the role. For more information, see <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-using.html">Working with Managed Policies</a>. This allows Lambda to call the AppStream 2.0 API on your behalf. You can edit and attach your own IAM policy, to limit access to only actions you would like to permit. To learn more, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/controlling-access.html">Controlling Access to Amazon AppStream 2.0</a>.</li> 
<li>Leave the default values for the rest of the fields, and choose <strong>Next</strong>, <strong>Create function</strong>.</li> 
</ol> </li> 
<li>To change the AppStream 2.0 fleet size, choose <strong>Code</strong> and add some sample code, as follows: 
<pre><code class="language-javascript">'use strict';
/**
This AppStream2 Update-Fleet blueprint sets up a schedule for a streaming fleet
**/
const AWS = require('aws-sdk');
const appstream = new AWS.AppStream();
const fleetParams = {
Name: 'ds-sample-fleet', /* required */
ComputeCapacity: {
DesiredInstances: 1 /* required */
}
};
exports.handler = (event, context, callback) =&gt; {
console.log('Received event:', JSON.stringify(event, null, 2));
var resource = event.resources[0];
var increase = resource.includes('weekday-9am-increase-capacity')
try {
if (increase) {
fleetParams.ComputeCapacity.DesiredInstances = 3
} else {
fleetParams.ComputeCapacity.DesiredInstances = 1
}
appstream.updateFleet(fleetParams, (error, data) =&gt; {
if (error) {
console.log(error, error.stack);
return callback(error);
}
console.log(data);
return callback(null, data);
});
} catch (error) {
console.log('Caught Error: ', error);
callback(error);
}
};</code></pre> 
<li>Test the code. Choose <strong>Test</strong> and use the “Hello World” test template. The first time you do this, choose <strong>Save and Test</strong>. Create a test input like the following to trigger the scaling update.<img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_11.png" alt="appstreamscaling_11.png" /></li> 
<li>You see output text showing the result of the update-fleet call. You can also use the CLI to check the effect of executing the Lambda function.</li> 
</ol> 
<p>Next, to set up a time-based schedule, set a trigger for invoking the Lambda function.</p> 
<p><strong>To set a trigger for the Lambda function</strong></p> 
<ol> 
<li>Choose <strong>Triggers</strong>, <strong>Add trigger</strong>.</li> 
<li>Choose <strong>CloudWatch Events – Schedule</strong>.</li> 
<li>Enter a rule name, such as “weekday-9am-increase-capacity”, and a description. For <strong>Schedule expression</strong>, choose <strong>cron</strong>. You can edit the value for the cron later.</li> 
<li>After the trigger is created, open the event <strong>weekday-9am-increase-capacity</strong>.</li> 
<li>In the CloudWatch console, edit the event details. To scale out the fleet at 9 am on a weekday, you can adjust the time to be: 00 17 ? * MON-FRI *. (If you’re not in Seattle (Pacific Time Zone), change this to another specific time zone).</li> 
<li>You can also add another event that triggers at the end of a weekday.</li> 
</ol> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_12.png" alt="appstreamscaling_12.png" /></p> 
<p>This setup now triggers scale-out and scale-in automatically, based on the time schedule that you set.</p> 
<b id="toc_5">Fleet Auto Scaling with schedules</b> 
<p>You can choose to combine both the fleet scaling and time-based schedule approaches to manage more complex scenarios. This is useful to manage the number of running instances based on business and non-business hours, and still respond to changes in demand. You could programmatically change the minimum and maximum sizes for your fleet based on time of day or day of week, and apply the default scale-out or scale-in policies. This allows you to respond to predictable minimum demand based on a schedule.</p> 
<p>For example, at the start of a work day, you might expect a certain number of users to request streaming connections at one time. You wouldn’t want to wait for the fleet to scale out and meet this requirement. However, during the course of the day, you might expect the demand to scale in or out, and would want to match the fleet size to this demand.</p> 
<p>To achieve this, set up the scaling polices via the console, and create a Lambda function to trigger changes to the minimum, maximum, and desired capacity for your fleet based on a schedule. Replace the code for the Lambda function that you created earlier with the following code:</p> 
<pre><code class="language-javascript">'use strict';
/**
This AppStream2 Update-Fleet function sets up a schedule for a streaming fleet
**/
const AWS = require('aws-sdk');
const appstream = new AWS.AppStream();
const applicationAutoScaling = new AWS.ApplicationAutoScaling();
const fleetParams = {
Name: 'ds-sample-fleet', /* required */
ComputeCapacity: {
DesiredInstances: 1 /* required */
}
};
var scalingParams = {
ResourceId: 'fleet/ds-sample-fleet', /* required - fleet name*/
ScalableDimension: 'appstream:fleet:DesiredCapacity', /* required */
ServiceNamespace: 'appstream', /* required */
MaxCapacity: 1,
MinCapacity: 6,
RoleARN: 'arn:aws:iam::659382443255:role/service-role/ApplicationAutoScalingForAmazonAppStreamAccess'
};
exports.handler = (event, context, callback) =&gt; {
console.log('Received this event now:', JSON.stringify(event, null, 2));
var resource = event.resources[0];
var increase = resource.includes('weekday-9am-increase-capacity')
try {
if (increase) {
//usage during business hours - start at capacity of 10 and scale
//if required. This implies at least 10 users can connect instantly. 
//More users can connect as the scaling policy triggers addition of
//more instances. Maximum cap is 20 instances - fleet will not scale
//beyond 20. This is the cap for number of users.
fleetParams.ComputeCapacity.DesiredInstances = 10
scalingParams.MinCapacity = 10
scalingParams.MaxCapacity = 20
} else {
//usage during non-business hours - start at capacity of 1 and scale
//if required. This implies only 1 user can connect instantly. 
//More users can connect as the scaling policy triggers addition of
//more instances. 
fleetParams.ComputeCapacity.DesiredInstances = 1
scalingParams.MinCapacity = 1
scalingParams.MaxCapacity = 10
}
//Update minimum and maximum capacity used by the scaling policies
applicationAutoScaling.registerScalableTarget(scalingParams, (error, data) =&gt; {
if (error) console.log(error, error.stack); 
else console.log(data);                     
});
//Update the desired capacity for the fleet. This sets 
//the number of running instances to desired number of instances
appstream.updateFleet(fleetParams, (error, data) =&gt; {
if (error) {
console.log(error, error.stack);
return callback(error);
}
console.log(data);
return callback(null, data);
});
} catch (error) {
console.log('Caught Error: ', error);
callback(error);
}
};
</code></pre> 
<p>Note: To successfully execute this code, you need to add IAM policies to the role used by the Lambda function. The policies allow Lambda to call the Application Auto Scaling service on your behalf.</p> 
<p style="text-align: center"><em>Figure 10: Inline policies for using Application Auto Scaling with Lambda</em></p> 
<pre><code class="language-none">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Effect&quot;: &quot;Allow&quot;,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Action&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;iam:PassRole&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Resource&quot;: &quot;*&quot;
&nbsp;&nbsp; }
]
}
{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Effect&quot;: &quot;Allow&quot;,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Action&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;application-autoscaling:*&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Resource&quot;: &quot;*&quot;
&nbsp;&nbsp; }
]
}</code></pre> 
<b id="toc_6">Monitoring usage</b> 
<p>After you have set up scaling for your fleet, you can use CloudWatch metrics with AppStream 2.0, and create a dashboard for monitoring. This helps optimize your scaling policies over time based on the amount of usage that you see.</p> 
<p>For example, if you were very conservative with your initial set up and over-provisioned resources, you might see long periods of low fleet utilization. On the other hand, if you set the fleet size too low, you would see high utilization or errors from insufficient capacity, which would block users’ connections. You can view CloudWatch metrics for up to 15 months, and drive adjustments to your fleet scaling policy.</p> 
<p style="text-align: center"><em>Figure 11: Dashboard with custom Amazon CloudWatch metrics</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/figure6.png" alt="appstreamscaling_13.53%20PM" /></p> 
<b id="toc_7">Summary</b> 
<p>These are just a few ideas for scaling AppStream 2.0 and optimizing your costs. Let us know if these are useful, and if you would like to see similar posts. If you have comments about the service, please post your feedback on the <a href="https://forums.aws.amazon.com/forum.jspa?forumID=233">AWS forum for AppStream 2.0</a>.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2016');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">A Serverless Authentication System by Jumia</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Bryan Liston</span></span> | on 
<time property="datePublished" datetime="2017-03-31T09:24:26+00:00">31 MAR 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-api-gateway/" title="View all posts in Amazon API Gateway"><span property="articleSection">Amazon API Gateway</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/a-serverless-authentication-system-by-jumia/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2013" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2013&amp;disqus_title=A+Serverless+Authentication+System+by+Jumia&amp;disqus_url=https://aws.amazon.com/blogs/compute/a-serverless-authentication-system-by-jumia/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2013');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><strong>Jumia is an ecosystem of nine different companies operating in 22 different countries in Africa. Jumia employs 3000 people and serves 15 million users/month.</strong></p> 
<p>Want to secure and centralize millions of user accounts across Africa? Shut down your servers! Jumia Group unified and centralized customer authentication on nine digital services platforms, operating in 22 (and counting) countries in Africa, totaling over 120 customer and merchant facing applications. All were unified into a custom Jumia Central Authentication System (JCAS), built in a timely fashion and designed using a serverless architecture.</p> 
<p>In this post, we give you our solution overview. For the full technical implementation, see the <a href="https://tech.jumia.com/jumia-central-authentication-system/">Jumia Central Authentication System</a> post on the Jumia Tech blog.<span id="more-2013"></span></p> 
<b id="toc_0">The challenge</b> 
<p>A group-level initiative was started to centralize authentication for all Jumia users in all countries for all companies. But it was impossible to unify the operational databases for the different companies. Each company had its own user database with its own technological implementation. Each company alone had yet to unify the logins for their own countries. The effects of deduplicating all user accounts were yet to be determined but were considered to be large. Finally, there was no team responsible for manage this new project, given that a new dedicated infrastructure would be needed.</p> 
<p>With these factors in mind, we decided to design this project as a serverless architecture to eliminate the need for infrastructure and a dedicated team. AWS was immediately considered as the best option for its level of<br /> service, intelligent pricing model, and excellent serverless services.</p> 
<p>The goal was simple. For all user accounts on all Jumia Group websites:</p> 
<ul> 
<li>Merge duplicates that might exist in different companies and countries</li> 
<li>Create a unique login (username and password)</li> 
<li>Enrich the user profile in this centralized system</li> 
<li>Share the profile with all Jumia companies</li> 
</ul> 
<b id="toc_1">Requirements</b> 
<p>We had the following initial requirements while designing this solution on the AWS platform:</p> 
<ul> 
<li><a href="https://en.wikipedia.org/wiki/Secure_by_design">Secure by design</a></li> 
<li>Highly available via multimaster replication</li> 
<li>Single login for all platforms/countries</li> 
<li>Minimal performance impact</li> 
<li>No admin overhead</li> 
</ul> 
<b id="toc_2">Chosen technologies</b> 
<p>We chose the following AWS services and technologies to implement our solution.</p> 
<h3 id="toc_3">Amazon API Gateway</h3> 
<p><a href="https://aws.amazon.com/api-gateway/">Amazon API Gateway</a> is a fully managed service, making it really simple to set up an API. It integrates directly with AWS Lambda, which was chosen as our endpoint. It can be easily replicated to other regions, using Swagger import/export.</p> 
<h3 id="toc_4">AWS Lambda</h3> 
<p><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> is the base of serverless computing, allowing you to run code without worrying about infrastructure. All our code runs on Lambda functions using Python; some functions are<br /> called from the API Gateway, others are <a href="https://docs.aws.amazon.com/lambda/latest/dg/tutorial-scheduled-events-schedule-expressions.html">scheduled like cron jobs</a>.</p> 
<h3 id="toc_5">Amazon DynamoDB</h3> 
<p><a href="https://docs.aws.amazon.com/amazondynamodb/latest/gettingstartedguide/quick-intro.html">Amazon DynamoDB</a> is a highly scalable, NoSQL database with a good API and a clean pricing model. It has great scalability as well as high availability, and fits the serverless model we aimed for with JCAS.</p> 
<h3 id="toc_6">AWS KMS</h3> 
<p><a href="https://aws.amazon.com/kms/">AWS KMS</a> was chosen as a key manager to perform envelope encryption. It’s a simple and secure key manager with multiple encryption functionalities.</p> 
<h4 id="toc_7">Envelope encryption</h4> 
<p>Oversimplifying envelope encryption is when you encrypt a <em>key</em> rather than the data itself. That encrypted key, which was used to encrypt the data, may now be stored with the data itself on your persistence layer since it doesn’t decrypt the data if compromised. For more information, see <a href="https://docs.aws.amazon.com/kms/latest/developerguide/workflow.html">How Envelope Encryption Works with Supported AWS Services</a>.</p> 
<p>Envelope encryption was chosen given that master keys have a 4 KB limit for data to be encrypted or decrypted.</p> 
<h3 id="toc_8">Amazon SQS</h3> 
<p><a href="https://aws.amazon.com/sqs/">Amazon SQS</a> is an inexpensive queuing service with dynamic scaling, 14-day retention availability, and easy management. It’s the perfect choice for our needs, as we use queuing systems only as a<br /> fallback when saving data to remote regions fails. All the features needed for those fallback cases were covered.</p> 
<h3 id="toc_9">JWT</h3> 
<p>We also use <a href="https://jwt.io/">JSON web tokens</a> for encoding and signing communications between JCAS and company servers. It’s another layer of security for data in transit.</p> 
<b id="toc_10">Data structure</b> 
<p>Our database design was pretty straightforward. DynamoDB records can be accessed by the primary key and allow access using secondary indexes as well. We created a UUID for each user on JCAS, which is used as a primary key<br /> on DynamoDB.</p> 
<p>Most data must be encrypted so we use a single field for that data. On the other hand, there’s data that needs to be stored in separate fields as they need to be accessed from the code without decryption for lookups or basic checks. This indexed data was also stored, as a hash or plain, outside the main ciphered blob store.</p> 
<p>We used a field for each searchable data piece:</p> 
<ul> 
<li><code>Id</code> (primary key)</li> 
<li><code>main phone</code> (indexed)</li> 
<li><code>main email</code> (indexed)</li> 
<li><code>account status</code></li> 
<li><code>document timestamp</code></li> 
</ul> 
<p>To hold the encrypted data we use a <code>data</code> dictionary with two main dictionaries, <code>info</code> with the user’s encrypted data and <code>keys</code> with the key for each AWS KMS region to decrypt the <code>info</code> blob.<br /> Passwords are stored in two dictionaries, <code>old_hashes</code> contains the legacy hashes from the origin systems and <code>secret</code> holds the user’s JCAS password.</p> 
<p>Here’s an example:<br /> <img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/data_at_rest.png" alt="data_at_rest" /></p> 
<b id="toc_11">Security</b> 
<p>Security is like an onion, it needs to be layered. That’s what we did when designing this solution. Our design makes all of our data unreadable at each layer of this solution, while easing our compliance needs.</p> 
<p>A field called <code>data</code> stores all personal information from customers. It’s encrypted using <strong>AES256-CBC</strong> with a key generated and managed by AWS KMS. A new data key is used for each transaction. For communication between companies and the API, we use API Keys, TLS and <a href="https://jwt.io/">JWT</a>, in the body to ensure that the post is signed and verified.</p> 
<b id="toc_12">Data flow</b> 
<p>Our second requirement on JCAS was system availability, so we designed some data pipelines. These pipelines allow multi-region replication, which evades collision using idempotent operations on all endpoints. The only technology added to the stack was Amazon SQS. On SQS queues, we place all the items we aren’t able to replicate at the time of the client’s request.</p> 
<p>JCAS may have inconsistencies between regions, caused by network or region availability; by using SQS, we have workers that synchronize all regions as soon as possible.</p> 
<p><strong>Example with two regions</strong></p> 
<p>We have three Lambda functions and two SQS queues, where:</p> 
<p>1) A trigger Lambda function is called for the DynamoDB stream. Upon changes to the user’s table, it tries to write directly to another DynamoDB table in the second region, falling back to writing to two SQS queues.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/fig1.png" alt="fig1" /></p> 
<p>2) A scheduled Lambda function (cron-style) checks a SQS queue for items and tries writing them to the DynamoDB table that potentially failed.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/fig2.png" alt="fig2" /></p> 
<p>3) A cron-style Lambda function checks the SQS queue, calling KMS for any items, and fixes the issue.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/fig3.png" alt="fig3" /></p> 
<p>The following diagram shows the full infrastrucure (for clarity, this diagram leaves out KMS recovery).</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/CAS.png" alt="CAS Full Diagram" /></p> 
<h3 id="toc_13">Results</h3> 
<p>Upon going live, we noticed a minor impact in our response times. Note the <strong>brown</strong> legend in the images below.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/lat_1.png" alt="lat_1" /></p> 
<p>This was a cold start, as the infrastructure started to get hot, response time started to converge. On 27 April, we were almost at the initial 500ms.<br /> <img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/lat_2.png" alt="lat_2" /></p> 
<p>It kept steady on values before JCAS went live (≈500ms).<br /> <img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/lat_3.png" alt="lat_3" /></p> 
<p>As of the writing of this post, our response time kept improving (dev changed the method name and we changed the subdomain name).<br /> <img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/jumia/lat_4.png" alt="lat_4" /></p> 
<p>Customer login used to take ≈500ms and it still takes ≈500ms with JCAS. Those times have improved as other components changed inside our code.</p> 
<b id="toc_14">Lessons learned</b> 
<ul> 
<li>Instead of the <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.CrossRegionRepl.html">standard cross-region replication</a>, creating your own DynamoDB cross-region replication might be a better fit for you, if your data and applications allow it.</li> 
<li>Take some time to tweak the Lambda runtime memory. Remember that it’s billed per 100ms, so it saves you money if you have it run near a round number.</li> 
<li>KMS takes away the problem of key management with great <a href="https://aws.amazon.com/kms/details/#secured">security by design</a>. It really simplifies your life.</li> 
<li>Always check the timestamp before operating on data. If it’s invalid, save the money by skipping further KMS, DynamoDB, and Lambda calls. You’ll love your systems even more.</li> 
<li>Embrace the serverless paradigm, even if it looks complicated at first. It will save your life further down the road when your traffic bursts or you want to find an engineer who knows your whole system.</li> 
</ul> 
<b id="toc_15">Next steps</b> 
<p>We are going to leverage everything we’ve done so far to implement SSO in Jumia. For a future project, we are already testing OpenID connect with DynamoDB as a backend.</p> 
<b id="toc_16">Conclusion</b> 
<p>We did a mindset revolution in many ways.<br /> Not only we went completely serverless as we started storing critical info on the cloud.<br /> On top of this we also architectured a system where all user data is decoupled between local systems and our central auth silo.<br /> Managing all of these critical systems became far more predictable and less cumbersome than we thought possible.<br /> For us this is the proof that good and simple designs are the best features to look out for when sketching new systems.</p> 
<p>If we were to do this again, we would do it in exactly the same way.</p> 
<p><strong><a href="https://twitter.com/DMLoureiro">Daniel Loureiro</a> (SecOps) &amp; <a href="https://twitter.com/tcx__">Tiago Caxias</a> (DBA) – Jumia Group</strong></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/dynamodb/" rel="tag">DynamoDB</a>, <a href="https://aws.amazon.com/blogs/compute/tag/jwt/" rel="tag">JWT</a>, <a href="https://aws.amazon.com/blogs/compute/tag/kms/" rel="tag">kms</a>, <a href="https://aws.amazon.com/blogs/compute/tag/sqs/" rel="tag">sqs</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2013');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Disabling Intel Hyper-Threading Technology on Amazon Linux</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Bryan Liston</span></span> | on 
<time property="datePublished" datetime="2017-03-30T11:30:38+00:00">30 MAR 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2"><span property="articleSection">Amazon EC2</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-linux/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2009" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2009&amp;disqus_title=Disabling+Intel+Hyper-Threading+Technology+on+Amazon+Linux&amp;disqus_url=https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-linux/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2009');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/brian_beach.jpg" /><br /> <strong>Brian Beach, Solutions Architect</strong></p> 
<p>Customers running high performance computing (HPC) workloads on Amazon Linux occasionally ask to disable the Intel Hyper-Threading Technology (HT Technology) that is enabled by default. In the pre-cloud world, this was usually performed by modifying the BIOS. That turned off HT Technology for all users, regardless of any possible benefits obtainable, for example, on I/O intensive workloads. With the cloud, HT Technology can be turned on or off, as is best for a particular application.</p> 
<p>This post discusses methods for disabling HT Technology.<span id="more-2009"></span></p> 
<b id="toc_0">What is HT Technology?</b> 
<p>According to Intel:</p> 
<blockquote> 
<p>Hyper-Threading Technology makes a single physical processor appear as multiple logical processors.&nbsp;To do this, there is one copy of the architecture state for each logical processor, and the logical processors share a single set of physical execution resources.</p> 
</blockquote> 
<p>For more information, see <a href="https://www.cs.sfu.ca/%7Efedorova/Teaching/CMPT886/Spring2007/papers/hyper-threading.pdf">Hyper-Threading Technology Architecture and Microarchitecture</a>.</p> 
<p>This is best explained with an analogy. Imagine that you own a small business assembling crafts for sale on <a href="https://www.amazon.com/Handmade">Amazon Handmade</a>. You have a building where the crafts are assembled. Six identical workbenches are inside. Each workbench includes a hammer, a wrench, a screwdriver, a paintbrush, etc. The craftspeople work independently, to produce approximately six times the output as a single craftsperson. On rare occasions, they contend for resources (for example, waiting in a short line to pull some raw materials off the shelf).</p> 
<p>As it turns out, business is booming and it’s time to expand production, but there is no more space to add workbenches. The decision is made to have two people work at each workbench.&nbsp;Overall, this is effective, because often there is still little contention between the workers—when one is using the hammer, the other doesn’t need it and is using the wrench. Occasionally one of the workers has to wait because the other worker is using the hammer. Contention for resources is somewhat higher in the single workbench case versus workbench-to-workbench.</p> 
<p>HT Technology works in a way analogous to that small business. The building is like the processor, the workbenches like cores, and the workers like threads. Within the Intel Xeon processor, there are two threads of execution in each core. The core is designed to allow progress to be made on one thread (execution state) while the other thread is waiting for a relatively slow operation to occur. In this context, even cached memory is slow, much less main memory (many dozens or hundreds of clock cycles away). Operations involving things like disk I/O are orders of magnitude worse. However, in cases where both threads are operating primarily on very close (e.g., registers) or relatively close (first-level cache) instructions or data, the overall throughput occasionally decreases compared to non-interleaved, serial execution of the two lines of execution.</p> 
<p>A good example of contention that makes HT Technology slower is an HPC job that relies heavily on floating point calculations. In this case, the two threads in each core share a single floating point unit (FPU) and are often blocked by one another.</p> 
<p>In the case of LINPACK, the benchmark used to measure supercomputers on the TOP500 list, many studies have shown that you get better performance by disabling HT Technology. But FPUs are not the only example. Other, very specific workloads can be shown by experimentation to be slower in some cases when HT Technology is used.</p> 
<p>Speaking of experimentation… before I discuss how to disable HT Technology, I’d like to discuss whether to disable it. Disabling HT Technology may help some workloads, but it is much more likely that it hinders others. In a traditional HPC environment, you have to decide to enable or disable HT Technology because you only have one cluster.</p> 
<p>In the cloud, you are not limited to one cluster. You can run multiple clusters and disable HT Technology in some cases while leaving it enabled in others. In addition, you can easily test both configurations and decide which is best based on empirical evidence. With that said, for the overwhelming majority of workloads, you should leave HT Technology enabled.</p> 
<b id="toc_1">Exploring HT Technology on Amazon Linux</b> 
<p>Look at the configuration on an Amazon Linux instance. I ran the examples below on an m4.2xlarge, which has eight vCPUs. &nbsp;Note that each vCPU is a thread of an Intel Xeon core. Therefore, the m4.2xlarge has four cores, each of which run two threads, resulting in eight vCPUs. You can see this by running <strong>lscpu</strong> (note that I have removed some of the output for brevity).</p> 
<pre><code class="language-bash">[root@ip-172-31-1-32 ~]# lscpu 
CPU(s):               8 
On-line CPU(s) list:  0-7 
Thread(s) per core:   2 
Core(s) per socket:   4 
Socket(s):            1 
NUMA node(s):         1 
NUMA node0 CPU(s):    0-7 </code></pre> 
<p>You can see from the output that there are four cores, and each core has two threads, resulting in eight CPUs. Exactly as expected. &nbsp;Furthermore, you can run <strong>lscpu –extended</strong> to see the details of the individual CPUs.</p> 
<pre><code class="language-bash">[root@ip-172-31-1-32 ~]# lscpu --extended 
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE 
0   0    0      0    0:0:0:0       yes 
1   0    0      1    1:1:1:0       yes 
2   0    0      2    2:2:2:0       yes 
3   0    0      3    3:3:3:0       yes 
4   0    0      0    0:0:0:0       yes 
5   0    0      1    1:1:1:0       yes 
6   0    0      2    2:2:2:0       yes 
7   0    0      3    3:3:3:0       yes </code></pre> 
<p>Notice that the eight CPUs are numbered 0–7. You can see that the first set of four CPUs are each associated with a different core, and the cores are repeated for the second set of four CPUs. Therefore, CPU 0 and CPU 4 are the two threads in core 0, CPU 1 and CPU 5 are the two threads in core 1 and so on.</p> 
<p>The Linux kernel, while generally agnostic on the topic and happy to schedule any thread on any logical CPU, is smart enough to know the difference between cores and threads. It gives preference to scheduling the first operating system threads per core. Only when all cores are busy does the kernel add OS threads to cores using the second hardware thread.</p> 
<b id="toc_2">Disabling HT Technology at runtime</b> 
<p>Now that you understand the relationship between CPUs and cores, you can disable the second set of CPUs using a process called “hotplugging” (in this case, you might say “hot-unplugging”). For more information, see <a href="https://www.kernel.org/doc/html/latest/core-api/cpu_hotplug.html">CPU hotplug in the Kernel</a>.</p> 
<p>Each logical CPU is represented in the Linux “filesystem” at&nbsp;an entirely virtual location under /sys/devices/system/cpu/. This isn’t really part of the literal filesystem, of course. Linux designers unified the actual filesystem, as well as devices and kernel objects, into a single namespace with many common operations, including the handy ability to control the state of kernel objects by changing the contents of the “files” that represent them. Here you see directory entries for each CPU, and under each directory you find an “online” file used to enable/disable&nbsp;the CPU. Now you can simply write a 0 to that file to disable the CPU.</p> 
<p>The kernel is smart enough to allow any scheduled operations to continue to completion on that CPU. The kernel then saves its state at the next scheduling event, and resumes those operations on another CPU when appropriate. Note the nice safety feature that Linux provides: you cannot disable CPU 0 and get an access-denied error if you try. Imagine what would happen if you accidentally took all the CPUs offline!</p> 
<p>Disable CPU4 (the second thread in the first core in the m4.2xlarge).&nbsp; You must be the root user to do this.</p> 
<pre><code class="language-bash">[root@ip-172-31-1-32 ~]# echo 0 &gt; /sys/devices/system/cpu/cpu4/online</code></pre> 
<p>Now you can run <strong>lscpu –extended</strong> again and see that CPU 4 is offline.</p> 
<pre><code class="language-bash">[root@ip-172-31-1-32 ~]# lscpu --extended 
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE 
0   0    0      0    0:0:0:0       yes 
1   0    0      1    1:1:1:0       yes 
2   0    0      2    2:2:2:0       yes 
3   0    0      3    3:3:3:0       yes 
4   -    -      -    :::           no 
5   0    0      1    1:1:1:0       yes 
6   0    0      2    2:2:2:0       yes 
7   0    0      3    3:3:3:0       yes </code></pre> 
<p>You can repeat this process for CPUs 5–7 to disable them all. However,&nbsp;this would be tedious on an x1.32xlarge with 128 vCPUs. You can use a script to disable them all.</p> 
<pre><code class="language-bash">#!/usr/bin/env bash
for cpunum in $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\n' | sort -un)
do
echo 0 &gt; /sys/devices/system/cpu/cpu$cpunum/online
done</code></pre> 
<p>Let me explain what that script does. &nbsp;First, it finds the total number of CPUs. Then, it loops over the second half of the CPUs. Finally, it disables each CPU in turn.</p> 
<p>After running the script, you can run&nbsp;<strong>lscpu –extended</strong>&nbsp;and see that second half of your CPUs are disabled, leaving only one thread per core. Perfect!</p> 
<pre><code class="language-bash">[root@ip-172-31-1-32 ~]# lscpu --extended 
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE 
0   0    0      0    0:0:0:0       yes 
1   0    0      1    1:1:1:0       yes 
2   0    0      2    2:2:2:0       yes 
3   0    0      3    3:3:3:0       yes 
4   -    -      -    :::           no 
5   -    -      -    :::           no 
6   -    -      -    :::           no 
7   -    -      -    :::           no </code></pre> 
<b id="toc_3">Disabling HT Technology at boot</b> 
<p>At this point, you have a script to disable HT Technology, but the change is only persisted until the next reboot. To apply the script on every boot, you can add the script to rc.local or use cloud-init bootcmd.</p> 
<p>Cloud-init is available on Amazon Linux (as well as most other Linux distributions available in EC2). It executes commands it finds in EC2 system metadata within the user-data part of the metadata namespace. Cloud-init includes a bootcmd module that executes a script each time the instance is booted. The following modified script runs on every boot:</p> 
<pre><code class="language-none">#cloud-config
bootcmd:
- for cpunum in $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\n' | sort -un); do echo 0 &gt; /sys/devices/system/cpu/cpu$cpunum/online; done </code></pre> 
<p>When you launch an instance, add this for <strong>User data</strong> in step 3 of the wizard. The instance starts with HT Technology disabled and disables it on subsequent reboots.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/linuxhyperthreadintel/linuxhyperthreadintel_1.png" alt="linuxhyperthreadintel_1.png" /></p> 
<p>An alternative approach is to edit grub.conf. Open /boot/grub/grub.conf in your favorite editor. Find the kernel directive and a maxcpus=X parameter where X is half the total number CPUs. Remember that Linux enumerates the first thread in each core followed by the second thread in each core. Therefore, limiting the number of CPUs to half of the total disables the second thread in each core just like you did with hotplugging earlier. You must reboot for this change to take effect.</p> 
<b id="toc_4">Conclusion</b> 
<p>Customers running HPC workloads on Amazon Linux cannot access the BIOS to disable HT Technology. This post described a process to disable the second thread in each core at runtime and during boot. In a future post, we examine similar solutions for configuring CPUs in Windows.</p> 
<p>Note: this technical approach has nothing to do with control over software licensing, or licensing rights, which are sometimes linked to the number of “CPUs” or “cores.” For licensing purposes, those are legal terms, not technical terms. This post did not cover anything about software licensing or licensing rights.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2009');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">Automating AWS Lambda Function Error Handling with AWS Step Functions</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-03-16T13:20:03+00:00">16 MAR 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions"><span property="articleSection">AWS Step Functions</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/automating-aws-lambda-function-error-handling-with-aws-step-functions/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-1975" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=1975&amp;disqus_title=Automating+AWS+Lambda+Function+Error+Handling+with+AWS+Step+Functions&amp;disqus_url=https://aws.amazon.com/blogs/compute/automating-aws-lambda-function-error-handling-with-aws-step-functions/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-1975');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/aaron_rehaag.jpg" alt="Aaron Rehaag" /><br /> <strong>Aaron Rehaag, Senior Software Engineer, Amazon Web Services</strong></p> 
<p><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> makes it easy to coordinate the components of distributed applications and microservices using visual workflows. You can scale and modify your applications quickly by building applications from individual components, each of which performs a discrete function.</p> 
<p>You can use Step Functions to create state machines, which orchestrate multiple <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> functions to build multi-step serverless applications. In certain cases, a Lambda function returns an error. Regardless of whether the error is a function exception created by the developer (e.g., <strong>file not found</strong>), or unpredicted (e.g., <strong>out of memory</strong>), Step Functions allows you to respond with conditional logic based on the type of error message in the form of <em>function error handling</em>.<span id="more-1975"></span></p> 
<b id="toc_0">Function error handling</b> 
<p>The function error handling feature of <em>Task</em> states allows you to raise an exception directly from a Lambda function and handle it (using Retry or Catch) directly within a Step Functions state machine.</p> 
<p>Consider the following state machine designed to sign up new customers for an account:</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/stepFunctionsErrorHandle/stepFunctionsErrorHandle_1.png" alt="stepFunctionsErrorHandle_1.png" /></p> 
<p>CreateAccount is a <em>Task</em> state, which writes a customer’s account details to a database using a Lambda function.</p> 
<p>If the task succeeds, an account is created, and the state machine progresses from the CreateAccount <em>Task</em> state to the SendWelcomeEmail <em>Task</em> state to send an email to welcome the customer.</p> 
<p>However, if a customer tries to register an account with a username already in use, the state machine suggests a different name to the user and retries the account creation process. The Lambda function raises an error, triggering the <a href="http://docs.aws.amazon.com/step-functions/latest/dg/awl-ref-errors.html">Catch</a> clause. This causes the state machine to transition to the SuggestAccount task state, suggesting a new name before transitioning back to the CreateAccountState.</p> 
<p>You can implement this scenario using any of the programming languages that Step Functions and Lambda support, which currently include Node.js, Java, C#, and Python. The following sections show how to implement a Catch clause in each language.</p> 
<b id="toc_1">Node.js</b> 
<p>Function errors in Node.js must extend from the Error prototype:</p> 
<pre><code class="language-javascript">exports.handler = function(event, context, callback) {
function AccountAlreadyExistsError(message) {
this.name = &quot;AccountAlreadyExistsError&quot;;
this.message = message;
}
AccountAlreadyExistsError.prototype = new Error();
const error = new AccountAlreadyExistsError(&quot;Account is in use!&quot;);
callback(error);
};</code></pre> 
<p>You can configure Step Functions to catch the error using a Catch rule:</p> 
<pre><code class="language-none">{
&quot;StartAt&quot;: &quot;CreateAccount&quot;,
&quot;States&quot;: {
&quot;CreateAccount&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:123456789012:function:CreateAccount&quot;,
&quot;Next&quot;: &quot;SendWelcomeEmail&quot;,
&quot;Catch&quot;: [
{
&quot;ErrorEquals&quot;: [&quot;AccountAlreadyExistsError&quot;],
&quot;Next&quot;: &quot;SuggestAccountName&quot;
}
]
},
…
}
}</code></pre> 
<p>At runtime, Step Functions catches the error, transitioning to the SuggestAccountName state as specified in the Next transition.</p> 
<p>Note: The name property of the Error object must match the ErrorEquals value.</p> 
<b id="toc_2">Java</b> 
<p>You can apply the same scenario to a Lambda Java function by extending the Exception class:</p> 
<pre><code class="language-java">package com.example;
public static class AccountAlreadyExistsException extends Exception {
public AccountAlreadyExistsException(String message) {
super(message);
}
}</code></pre> 
<pre><code class="language-java">package com.example;
import com.amazonaws.services.lambda.runtime.Context; 
public class Handler {
public static void CreateAccount(String name, Context context) throws AccountAlreadyExistsException {
throw new AccountAlreadyExistsException (&quot;Account is in use!&quot;);
}
}</code></pre> 
<p>Lambda automatically sets the error name to the fully-qualified class name of the exception at runtime:</p> 
<pre><code class="language-none">{
&quot;StartAt&quot;: &quot;CreateAccount&quot;,
&quot;States&quot;: {
&quot;CreateAccount&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:123456789012:function:CreateAccount&quot;,
&quot;Next&quot;: &quot;SendWelcomeEmail&quot;,
&quot;Catch&quot;: [
{
&quot;ErrorEquals&quot;: [&quot;com.example.AccountAlreadyExistsException&quot;],
&quot;Next&quot;: &quot;SuggestAccountName&quot;
}
]
},
…
}
}</code></pre> 
<b id="toc_3">C-Sharp</b> 
<p>In C#, specify function errors by extending the Exception class:</p> 
<pre><code class="language-csharp">namespace Example {
public class AccountAlreadyExistsException : Exception {
public AccountAlreadyExistsException(String message) :
base(message) {
}
}
}</code></pre> 
<pre><code class="language-csharp">namespace Example {
public class Handler {
&nbsp;    public static void CreateAccount() {
&nbsp;&nbsp;&nbsp;    throw new AccountAlreadyExistsException(&quot;Account is in use!&quot;);
}
}
}&nbsp;</code></pre> 
<p>Lambda automatically sets the error name to the simple class name of the exception at runtime:</p> 
<pre><code class="language-none">{
&quot;StartAt&quot;: &quot;CreateAccount&quot;,
&quot;States&quot;: {
&quot;CreateAccount&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:123456789012:function:CreateAccount&quot;,
&quot;Next&quot;: &quot;SendWelcomeEmail&quot;,
&quot;Catch&quot;: [
{
&quot;ErrorEquals&quot;: [&quot;AccountAlreadyExistsException&quot;],
&quot;Next&quot;: &quot;SuggestAccountName&quot;
}
]
},
…
}
}</code></pre> 
<b id="toc_4">Python</b> 
<p>In Python, specify function errors by extending the Exception class:</p> 
<pre><code class="language-python">def create_account(event, context):
class AccountAlreadyExistsException(Exception):
pass
raise AccountAlreadyExistsException('Account is in use!')</code></pre> 
<p>Lambda automatically sets the error name to the simple class name of the exception at runtime:</p> 
<pre><code class="language-none">{
&quot;StartAt&quot;: &quot;CreateAccount&quot;,
&quot;States&quot;: {
&quot;CreateAccount&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:123456789012:function:CreateAccount&quot;,
&quot;Next&quot;: &quot;SendWelcomeEmail&quot;,
&quot;Catch&quot;: [
{
&quot;ErrorEquals&quot;: [&quot;AccountAlreadyExistsException&quot;],
&quot;Next&quot;: &quot;SuggestAccountName&quot;
}
]
},
…
}
}</code></pre> 
<b id="toc_5">Getting started with error handling</b> 
<p>The function error handling feature of Step Functions makes it easier to create serverless applications. In addition to the Catch clause shown in this post, you can apply the same pattern to <a href="http://docs.aws.amazon.com/step-functions/latest/dg/awl-ref-errors.html">Retry</a> of failed Lambda functions.</p> 
<p>While you can create Catch and Retry patterns using a <a href="http://docs.aws.amazon.com/step-functions/latest/dg/awl-ref-states-choice.html">Choice</a> state, using Catch and Retry in your Task states allows you to separate exceptions from branching logic associated with the common happy paths through your state machines. Function error handling integrates with all supported Lambda programming models, so you are free to design your application in the programming languages of your choice, mixing and matching as you go.</p> 
<p>To create your own serverless applications using Step Functions, see the <a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> product page.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-1975');
});
</script> 
</article> 
<article class="post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="b post-title" property="name headline">SAML for Your Serverless JavaScript Application: Part II</b> 
<footer class="meta">
by 
<span property="author" typeof="Person"><span property="name">Bryan Liston</span></span> | on 
<time property="datePublished" datetime="2017-03-16T11:31:13+00:00">16 MAR 2017</time> | in 
<span class="categories"><a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-api-gateway/" title="View all posts in Amazon API Gateway"><span property="articleSection">Amazon API Gateway</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda"><span property="articleSection">AWS Lambda</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/saml-for-your-serverless-javascript-application-part-ii/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-1960" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=1960&amp;disqus_title=SAML+for+Your+Serverless+JavaScript+Application%3A+Part+II&amp;disqus_url=https://aws.amazon.com/blogs/compute/saml-for-your-serverless-javascript-application-part-ii/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-1960');
});
</script> | 
<a href="#" role="button" data-share-dialog=""><span class="span icon-share"></span>&nbsp;Share</a> 
<ul> 
</ul> 
</footer> 
<p><strong>Contributors: Richard Threlkeld, Gene Ting, Stefano Buliani</strong></p> 
<p>The full code for both scenarios—including <a href="http://docs.aws.amazon.com/lambda/latest/dg/deploying-lambda-apps.html">SAM templates</a>—can be found at the <a href="https://github.com/awslabs/samljs-serverless-sample">samljs-serverless-sample</a> GitHub repository. We highly recommend you use the SAM templates in the GitHub repository to create the resources, opitonally you can manually create them.</p> 
<hr /> 
<p>This is the second part of a two part series for using SAML providers in your application and receiving short-term credentials to access AWS Services. These credentials can be limited with IAM roles so the users of the applications can perform actions like fetching data from databases or uploading files based on their level of authorization. For example, you may want to build a JavaScript application that allows a user to authenticate against Active Directory Federation Services (ADFS). The user can be granted scoped AWS credentials to invoke an API to display information in the application or write to an <a href="https://aws.amazon.com/dynamodb">Amazon DynamoDB</a> table.</p> 
<p><a href="https://aws.amazon.com/blogs/compute/saml-for-your-serverless-javascript-application-part-i/">Part I of this series walked through a client-side flow of retrieving SAML claims</a> and passing them to <a href="https://aws.amazon.com/cognito">Amazon Cognito</a> to retrieve credentials. This blog post will take you through a more advanced scenario where logic can be moved to the backend for a more comprehensive and flexible solution.<span id="more-1960"></span></p> 
<b id="toc_0">Prerequisites</b> 
<p>As in Part I of this series, you need ADFS running in your environment. The following configurations are used for reference:</p> 
<ol> 
<li>ADFS federated with the AWS console. For a walkthrough with an AWS CloudFormation template, see <a href="https://aws.amazon.com/blogs/security/enabling-federation-to-aws-using-windows-active-directory-adfs-and-saml-2-0/">Enabling Federation to AWS Using Windows Active Directory, ADFS, and SAML 2.0</a>.</li> 
<li>Verify that you can authenticate with user example\bob for both the ADFS-Dev and ADFS-Production groups via the <a href="https://localhost/adfs/IdpInitiatedSignOn.aspx">sign-in page</a>.</li> 
<li>Create an <a href="http://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html">Amazon Cognito identity pool</a>.</li> 
</ol> 
<p> 
<!--more--></p> 
<b id="toc_1">Scenario Overview</b> 
<p>The scenario in the last blog post may be sufficient for many organizations but, due to size restrictions, some browsers may drop part or all of a query string when sending a large number of claims in the SAMLResponse. Additionally, for auditing and logging reasons, you may wish to relay SAML assertions via POST only and perform parsing in the backend before sending credentials to the client. This scenario allows you to perform custom business logic and validation as well as putting tracking controls in place.</p> 
<p>In this post, we want to show you how these requirements can be achieved in a Serverless application. We also show how different challenges (like XML parsing and JWT exchange) can be done in a Serverless application design. Feel free to mix and match, or swap pieces around to suit your needs.</p> 
<p>This scenario uses the following services and features:</p> 
<ul> 
<li>Cognito for unique ID generation and default role mapping</li> 
<li>S3 for static website hosting</li> 
<li>API Gateway for receiving the SAMLResponse POST from ADFS</li> 
<li>Lambda for processing the SAML assertion using a native XML parser</li> 
<li>DynamoDB conditional writes for session tracking exceptions</li> 
<li>STS for credentials via Lambda</li> 
<li>KMS for signing JWT tokens</li> 
<li>API Gateway custom authorizers for controlling per-session access to credentials, using JWT tokens that were signed with KMS keys</li> 
<li>JavaScript-generated SDK from API Gateway using a service proxy to DynamoDB</li> 
<li><a href="https://technet.microsoft.com/en-us/library/jj127245(v=ws.10).aspx">RelayState</a> in the SAMLRequest to ADFS to transmit the CognitoID and a short code from the client to your AWS backend</li> 
</ul> 
<p>At a high level, this solution is similar to that of Scenario 1; however, most of the work is done in the infrastructure rather than on the client.</p> 
<ul> 
<li>ADFS still uses a POST binding to redirect the SAMLResponse to API Gateway; however, the Lambda function does not immediately redirect.</li> 
<li>The Lambda function decodes and uses an XML parser to read the properties of the SAML assertion.</li> 
<li>If the user’s assertion shows that they belong to a certain group matching a specified string (“Prod” in the sample), then you assign a role that they can assume (“ADFS-Production”).</li> 
<li>Lambda then gets the credentials on behalf of the user and stores them in DynamoDB as well as logging an audit record in a separate table.</li> 
<li>Lambda then returns a short-lived, signed JSON Web Token (JWT) to the JavaScript application.</li> 
<li>The application uses the JWT to get their stored credentials from DynamoDB through an API Gateway custom authorizer.</li> 
</ul> 
<p>The architecture you build in this tutorial is outlined in the following diagram.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_1.png" alt="lambdasamltwo_1.png" /></p> 
<p>First, a user visits your static website hosted on S3. They generate an ephemeral random code that is transmitted during redirection to ADFS, where they are prompted for their Active Directory credentials.</p> 
<p>Upon successful authentication, the ADFS server redirects the SAMLResponse assertion, along with the code (as the RelayState) via POST to API Gateway.</p> 
<p>The Lambda function parses the SAMLResponse. If the user is part of the appropriate Active Directory group (AWS-Production in this tutorial), it retrieves credentials from STS on behalf of the user.</p> 
<p>The credentials are stored in a DynamoDB table called SAMLSessions, along with the short code. The user login is stored in a tracking table called SAMLUsers.</p> 
<p>The Lambda function generates a JWT token, with a 30-second expiration time signed with KMS, then redirects the client back to the static website along with this token.</p> 
<p>The client then makes a call to an API Gateway resource acting as a DynamoDB service proxy that retrieves the credentials via a DeleteItem call. To make this call, the client passes the JWT in the authorization header.</p> 
<p>A custom authorizer runs to validate the token using the KMS key again as well as the original random code.</p> 
<p>Now that the client has credentials, it can use these to access AWS resources.</p> 
<b id="toc_2">Tutorial: Backend processing and audit tracking</b> 
<p>Before you walk through this tutorial you will need the source code from the <a href="https://github.com/awslabs/samljs-serverless-sample">samljs-serverless-sample Github Repository</a>. You should use the SAM template provided in order to streamline the process but we’ll outline how you you would manually create resources too. There is a readme in the repository with instructions for using the SAM template. Either way you will still perform the manual steps of KMS key configuration, ADFS enablement of RelayState, and Amazon Cognito Identity Pool creation. The template will automate the process in creating the S3 website, Lambda functions, API Gateway resources and DynamoDB tables.</p> 
<p>We walk through the details of all the steps and configuration below for illustrative purposes in this tutorial calling out the sections that can be omitted if you used the SAM template.</p> 
<h3 id="toc_3">KMS key configuration</h3> 
<p>To sign JWT tokens, you need an encrypted plaintext key, to be stored in KMS. You will need to complete this step even if you use the SAM template.</p> 
<ol> 
<li>In the IAM console, choose <strong>Encryption Keys</strong>, <strong>Create Key</strong>.</li> 
<li>For <strong>Alias</strong>, type <strong>sessionMaster</strong>.</li> 
<li>For <strong>Advanced Options</strong>, choose <strong>KMS</strong>, <strong>Next Step</strong>.</li> 
<li>For <strong>Key Administrative Permissions</strong>, select your administrative role or user account.</li> 
<li>For <strong>Key Usage Permissions</strong>, you can leave this blank as the IAM Role (next section) will have individual key actions configured. This allows you to perform administrative actions on the set of keys while the Lambda functions have rights to just create data keys for encryption/decryption and use them to sign JWTs.</li> 
<li>Take note of the <strong>Key ID,</strong> which is needed for the Lambda functions.</li> 
</ol> 
<h3 id="toc_4">IAM role configuration</h3> 
<p>You will need an IAM role for executing your Lambda functions. If you are using the SAM template this can be skipped. The sample code in the GitHub repository under <strong>Scenario2</strong> creates separate roles for each function, with limited permissions on individual resources when you use the SAM template. We recommend separate roles scoped to individual resources for production deployments. Your Lambda functions need the following permissions:</p> 
<pre><code class="language-none">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Sid&quot;: &quot;Stmt1432927122000&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;dynamodb:PutItem&quot;,
“dynamodb:GetItem”,
“dynamodb:DeleteItem”,
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;,
&quot;logs:PutLogEvents&quot;,
&quot;kms:GenerateDataKey*&quot;,
“kms:Decrypt”
],
&quot;Resource&quot;: [
&quot;*&quot;
]
}
]
}</code></pre> 
<h3 id="toc_5">Lambda function configuration</h3> 
<p>If you are <strong>not</strong> using the SAM template, create the following three Lambda functions from the GitHub repository in /Scenario2/lambda using the following names and environment variables. The Lambda functions are written in Node.js.</p> 
<ul> 
<li><strong>GenerateKey_awslabs_samldemo</strong></li> 
<li><strong>ProcessSAML_awslabs_samldemo</strong></li> 
<li><strong>SAMLCustomAuth_awslabs_samldemo</strong></li> 
</ul> 
<p>The functions above are built, packaged, and uploaded to Lambda. For two of the functions, this can be done from your workstation (the sample commands for each function assume OSX or Linux). The third will need to be built on an AWS EC2 instance running the current Lambda AMI.</p> 
<h4 id="toc_6">GenerateKey_awslabs_samldemo</h4> 
<p>This function is only used one time to create keys in KMS for signing JWT tokens. The function calls <a href="http://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html">GenerateDataKey</a> and stores the encrypted CipherText blob as Base64 in DynamoDB. This is used by the other two functions for getting the PlainTextKey for signing with a <a href="http://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html">Decrypt</a> operation.</p> 
<p>This function only requires a single file. It has the following environment variables:</p> 
<ul> 
<li>KMS_KEY_ID: Unique identifier from KMS for your sessionMaster Key</li> 
<li>SESSION_DDB_TABLE: <strong>SAMLSessions</strong></li> 
<li>ENC_CONTEXT: <strong>ADFS</strong> (or something unique to your organization)</li> 
<li>RAND_HASH: us-east-1:XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</li> 
</ul> 
<p>Navigate into /Scenario2/lambda/GenerateKey and run the following commands:</p> 
<pre><code class="language-bash">zip –r generateKey.zip .
aws lambda create-function --function-name GenerateKey_awslabs_samldemo --runtime nodejs4.3 --role LAMBDA_ROLE_ARN --handler index.handler --timeout 10 --memory-size 512 --zip-file fileb://generateKey.zip --environment Variables={SESSION_DDB_TABLE=SAMLSessions,ENC_CONTEXT=ADFS,RAND_HASH=us-east-1:XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX,KMS_KEY_ID=&lt;kms key=&quot;KEY&quot; id=&quot;ID&quot;&gt;}</code></pre> 
<h4 id="toc_7">SAMLCustomAuth_awslabs_samldemo</h4> 
<p>This is an API Gateway custom authorizer called after the client has been redirected to the website as part of the login workflow. This function calls a GET against the service proxy to DynamoDB, retrieving credentials. The function uses the KMS key signing validation of the JWT created in the ProcessSAML_awslabs_samldemo function and also validates the <em>random code</em> that was generated at the beginning of the login workflow.</p> 
<p>You must install the dependencies before zipping this function up. It has the following environment variables:</p> 
<ul> 
<li>SESSION_DDB_TABLE: <strong>SAMLSessions</strong></li> 
<li>ENC_CONTEXT: <strong>ADFS</strong> (or whatever was used in GenerateKey_awslabs_samldemo)</li> 
<li>ID_HASH: us-east-1:XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</li> 
</ul> 
<p>Navigate into /Scenario2/lambda/CustomAuth and run:</p> 
<pre><code class="language-bash">npm install
zip –r custom_auth.zip .
aws lambda create-function --function-name SAMLCustomAuth_awslabs_samldemo --runtime nodejs4.3 --role LAMBDA_ROLE_ARN --handler CustomAuth.handler --timeout 10 --memory-size 512 --zip-file fileb://custom_auth.zip --environment Variables={SESSION_DDB_TABLE=SAMLSessions,ENC_CONTEXT=ADFS,ID_HASH= us-east-1:XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX }</code></pre> 
<h4 id="toc_8">ProcessSAML_awslabs_samldemo</h4> 
<p>This function is called when ADFS sends the SAMLResponse to API Gateway. The function parses the SAML assertion to select a role (based on a simple string search) and extract user information. It then uses this data to get short-term credentials from STS via AssumeRoleWithSAML and stores this information in a <strong>SAMLSessions</strong> table and tracks the user login via a <strong>SAMLUsers</strong> table. Both of these are DynamoDB tables but you could also store the user information in another AWS database type, as this is for auditing purposes. Finally, this function creates a JWT (signed with the KMS key) which is only valid for 30 seconds and is returned to the client as part of a 302 redirect from API Gateway.</p> 
<p>This function needs to be built on an EC2 server running Amazon Linux. This function leverages two main external libraries:</p> 
<ul> 
<li><a href="https://github.com/jwtk/njwt">nJwt</a>: Used for secure JWT creation for individual client sessions to get access to their records</li> 
<li><a href="https://github.com/libxmljs/libxmljs">libxmljs</a>: Used for XML XPath queries of the decoded SAMLResponse from AD FS</li> 
</ul> 
<p>Libxmljs uses native build tools and you should run this on EC2 running the same AMI as Lambda and with Node.js v4.3.2; otherwise, you might see errors. For more information about current Lambda AMI information, see <a href="http://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html">Lambda Execution Environment and Available Libraries</a>.</p> 
<p>After you have the correct AMI launched in EC2 and have SSH open to that host, install Node.js. Ensure that the Node.js version on EC2 is 4.3.2, to match Lambda. If your version is off, you can <a href="http://stackoverflow.com/questions/7718313/how-to-change-to-an-older-version-of-node-js">roll back with NVM</a>.</p> 
<p>After you have set up Node.js, run the following command:</p> 
<pre><code class="language-bash">yum install -y make gcc*</code></pre> 
<p>Now, create a <strong><em>/saml</em></strong> folder on your EC2 server and copy up ProcessSAML.js and package.json from /Scenario2/lambda/ProcessSAML to the EC2 server. Here is a sample SCP command:</p> 
<pre><code class="language-bash">cd ProcessSAML/
ls
package.json    ProcessSAML.js
scp -i ~/path/yourpemfile.pem ./* ec2-user@ec2-XX-XXX-XXX-X.compute-1.amazonaws.com:/home/ec2-user/saml/</code></pre> 
<p>Then you can SSH to your server, cd into the /saml directory, and run:</p> 
<pre><code class="language-bash">npm install</code></pre> 
<p>A successful build should look similar to the following:</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_2.png" alt="lambdasamltwo_2.png" /></p> 
<p>Finally, zip up the package and create the function using the following AWS CLI command and these environment variables. Configure the CLI with your credentials as needed.</p> 
<ul> 
<li>SESSION_DDB_TABLE: <strong>SAMLSessions</strong></li> 
<li>ENC_CONTEXT: <strong>ADFS</strong> (or whatever was used in GenerateKey<em>awslabs</em>samldemo)</li> 
<li>PRINCIPAL_ARN: Full ARN of the AD FS IdP created in the IAM console</li> 
<li>USER_DDB_TABLE: <strong>SAMLUsers</strong></li> 
<li>REDIRECT_URL: Endpoint URL of your static S3 website (or CloudFront distribution domain name if you did that optional step)</li> 
<li>ID_HASH: us-east-1:XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</li> 
</ul> 
<pre><code class="language-bash">zip –r saml.zip .
aws lambda create-function --function-name ProcessSAML_awslabs_samldemo --runtime nodejs4.3 --role LAMBDA_ROLE_ARN --handler ProcessSAML.handler --timeout 10 --memory-size 512 --zip-file fileb://saml.zip –environment Variables={USER_DDB_TABLE=SAMLUsers,SESSION_DDB_TABLE= SAMLSessions,REDIRECT_URL=&lt;your S3 bucket and test page path&gt;,ID_HASH=us-east-1:XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX,ENC_CONTEXT=ADFS,PRINCIPAL_ARN=&lt;your ADFS IdP ARN&gt;}</code></pre> 
<p>If you built the first two functions on your workstation and created the ProcessSAML_awslabs_samldemo function separately in the Lambda console before building on EC2, you can update the code after building on with the following command:</p> 
<pre><code class="language-none">aws lambda update-function-code --function-name ProcessSAML_awslabs_samldemo --zip-file fileb://saml.zip</code></pre> 
<h3 id="toc_9">Role trust policy configuration</h3> 
<p>This scenario uses STS directly to assume a role. You will need to complete this step even if you use the SAM template. Modify the trust policy, as you did before when Amazon Cognito was assuming the role. In the GitHub repository sample code, <em>ProcessSAML.js</em> is preconfigured to filter and select a role with <strong>“Prod”</strong> in the name via the <strong>selectedRole</strong> variable.</p> 
<p>This is an example of business logic you can alter in your organization later, such as a callout to an external mapping database for other rules matching. In this tutorial, it corresponds to the ADFS-Production role that was created.</p> 
<ol> 
<li>In the IAM console, choose <strong>Roles</strong> and open the ADFS-Production Role.</li> 
<li>Edit the <strong>Trust Permissions</strong> field and replace the content with the following: 
<pre><code class="language-none">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Federated&quot;: [
&quot;arn:aws:iam::ACCOUNTNUMBER:saml-provider/ADFS&quot;
]
},
&quot;Action&quot;: &quot;sts:AssumeRoleWithSAML&quot;
}
]
}</code></pre> 
</ol> 
<p>If you end up using another role (or add more complex filtering/selection logic), ensure that those roles have similar trust policy configurations. Also note that the sample policy above purposely uses an array for the federated provider matching the IdP ARN that you added. If your environment has multiple SAML providers, you could list them here and modify the code in <em>ProcessSAML.js</em> to process requests from different IdPs and grant or revoke credentials accordingly.</p> 
<h3 id="toc_10">DynamoDB table creation</h3> 
<p>If you are <strong>not</strong> using the SAM template, create two DynamoDB tables:</p> 
<ul> 
<li><strong>SAMLSessions</strong>: Temporarily stores credentials from STS. Credentials are removed by an API Gateway Service Proxy to the DynamoDB <strong><em>DeleteItem</em></strong> call that simultaneously returns the credentials to the client.</li> 
<li><strong>SAMLUsers</strong>: This table is for tracking user information and the last time they authenticated in the system via ADFS.</li> 
</ul> 
<p>The following AWS CLI commands creates the tables (indexed only with a primary key hash, called <em>identityHash</em> and <em>CognitoID</em> respectively):</p> 
<pre><code class="language-none">aws dynamodb create-table \
--table-name SAMLSessions \
--attribute-definitions \
AttributeName=group,AttributeType=S \
--key-schema AttributeName=identityhash,KeyType=HASH \
--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5</code></pre> 
<pre><code class="language-none">aws dynamodb create-table \
--table-name SAMLUsers \
--attribute-definitions \
AttributeName=CognitoID,AttributeType=S \
--key-schema AttributeName=CognitoID,KeyType=HASH \
--provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5</code></pre> 
<p>After the tables are created, you should be able to run the <strong>GenerateKey_awslabs_samldemo</strong> Lambda function and see a CipherText key stored in <strong>SAMLSessions</strong>. This is only for convenience of this post, to demonstrate that you should persist CipherText keys in a data store and never persist plaintext keys that have been decrypted. You should also never log plaintext keys in your code.</p> 
<h3 id="toc_11">API Gateway configuration</h3> 
<p>If you are <strong>not</strong> using the SAM template, you will need to create API Gateway resources. If you have created resources for Scenario 1 in Part I, then the naming of these resources may be similar. If that is the case, then simply create an API with a different name (SAMLAutb or similar) and follow these steps accordingly.</p> 
<ol> 
<li>In the API Gateway console for your API, choose <strong>Authorizers</strong>, <strong>Custom</strong> <strong>Authorizer</strong>.</li> 
<li>Select your region and enter <strong>SAMLCustomAuth_awslabs_samldemo</strong> for the Lambda function. Choose a friendly name like <strong>JWTParser</strong> and ensure that <strong>Identity token source</strong> is <strong>method.request.header.Authorization</strong>. This tells the custom authorizer to look for the JWT in the Authorization header of the HTTP request, which is specified in the JavaScript code on your S3 webpage. Save the changes. <p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_3.png" alt="lambdasamltwo_3.png" /></p></li> 
</ol> 
<p>Now it’s time to wire up the Lambda functions to API Gateway.</p> 
<ol> 
<li style="list-style-type: none"> 
<ol> 
<li>In the API Gateway console, choose <strong>Resources</strong>, select your API, and then create a <strong>Child Resource</strong> called SAML. This includes a POST and a GET method. The POST method uses the <strong>ProcessSAML_awslabs_samldemo</strong> Lambda function and a 302 redirect, while the GET method uses the JWTParser custom authorizer with a service proxy to DynamoDB to retrieve credentials upon successful authorization.</li> 
</ol> </li> 
</ol> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_4.png" alt="lambdasamltwo_4.png" /></p> 
<ol> 
<li>Create a <strong>POST</strong> method. For <strong>Integration Type</strong>, choose <strong>Lambda</strong> and add the <strong>ProcessSAML_awslabs_samldemo</strong> Lambda function. For <strong>Method Request</strong>, add headers called <strong>RelayState</strong> and <strong>SAMLResponse</strong>. <p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_5.png" alt="lambdasamltwo_5.png" /></p></li> 
<li>Delete the <strong>Method Response</strong> code for 200 and add a 302. Create a response header called <strong>Location.</strong> In the <strong>Response Models</strong> section, for <strong>Content-Type</strong>, choose <strong>application/json</strong> and for <strong>Models</strong>, choose <strong>Empty</strong>. <p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_6.png" alt="lambdasamltwo_6.png" /></p></li> 
<li>Delete the <strong>Integration Response</strong> section for 200 and add one for 302 that has a <strong>Method response status</strong> of 302. Edit the response header for <strong>Location</strong> to add a <strong>Mapping</strong> value of <strong>integration.response.body.location</strong>. <p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_7.png" alt="lambdasamltwo_7.png" /></p></li> 
<li>Finally, in order for Lambda to capture the <strong>SAMLResponse</strong> and <strong>RelayState</strong> values, choose <strong>Integration Request</strong>.</li> 
<li>In the <strong>Body Mapping Template</strong> section, for Content-Type, enter <strong>application/x-www-form-urlencoded</strong> and add the following template: 
<pre><code class="language-none">{
&quot;SAMLResponse&quot; :&quot;$input.params('SAMLResponse')&quot;,
&quot;RelayState&quot; :&quot;$input.params('RelayState')&quot;,
&quot;formparams&quot; : $input.json('$')
}</code></pre> 
<li>Create a <strong>GET</strong> method with an <strong>Integration Type</strong> of Service Proxy. Select the region and DynamoDB as the AWS Service. Use <strong>POST</strong> for the HTTP method and <strong>DeleteItem</strong> for the Action. This is important as you leverage a DynamoDB feature to return the current records when you perform deletion. This simultaneously allows credentials in this system to not be stored long term and also allows clients to retrieve them. For <strong>Execution role</strong>, use the Lambda role from earlier or a new role that only has IAM scoped permissions for <strong>DeleteItem</strong> on the <strong>SAMLSessions</strong> table. <p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_8.png" alt="lambdasamltwo_8.png" /></p></li> 
<li>Save this and open <strong>Method Request</strong>.</li> 
<li>For <strong>Authorization</strong>, select your custom authorizer JWTParser. Add in a header called COGNITO_ID and save the changes. <p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_9.png" alt="lambdasamltwo_9.png" /></p></li> 
<li>In the Integration Request, add in a header name of <strong>Content-Type</strong> and a value for <strong>Mapped</strong> of ‘<strong>application/x-amzn-json-1.0</strong>‘ (you need the single quotes surrounding the entry).</li> 
<li>Next, in the <strong>Body Mapping Template</strong> section, for <strong>Content-Type</strong>, enter <strong>application/json</strong> and add the following template: 
<pre><code class="language-none">{
&quot;TableName&quot;: &quot;SAMLSessions&quot;,
&quot;Key&quot;: {
&quot;identityhash&quot;: {
&quot;S&quot;: &quot;$input.params('COGNITO_ID')&quot;
}
},
&quot;ReturnValues&quot;: &quot;ALL_OLD&quot;
}</code></pre> 
</ol> 
<p>Inspect this closely for a moment. When your client passes the JWT in an Authorization Header to this GET method, the JWTParser Custom Authorizer grants/denies executing a DeleteItem call on the SAMLSessions table.</p> 
<p>ADF</p> 
<p>If it is granted, then there needs to be an item to delete the reference as a primary key to the table. The client JavaScript (seen in a moment) passes its CognitoID through as a header called COGNITO_ID that is mapped above. DeleteItem executes to remove the credentials that were placed there via a call to STS by the <strong>ProcessSAML_awslabs_samldemo</strong> Lambda function. Because the above action specifies <strong>ALL_OLD</strong> under the <strong>ReturnValues</strong> mapping, DynamoDB returns these credentials at the same time.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_10.png" alt="lambdasamltwo_10.png" /></p> 
<ol> 
<li>Save the changes and open your <strong>/saml</strong> resource root.</li> 
<li>Choose <strong>Actions</strong>, <strong>Enable CORS</strong>.</li> 
<li>In the <strong>Access-Control-Allow-Headers</strong> section, add <strong>COGNITO_ID</strong> into the end (inside the quotes and separated from other headers by a comma), then choose <strong>Enable CORS and replace existing CORS headers</strong>.</li> 
<li>When completed, choose <strong>Actions</strong>, <strong>Deploy API</strong>. Use the <strong>Prod</strong> stage or another stage.</li> 
<li>In the Stage Editor, choose <strong>SDK Generation</strong>. For <strong>Platform</strong>, choose JavaScript and then choose <strong>Generate SDK.</strong> Save the folder someplace close. Take note of the <strong>Invoke URL</strong> value at the top, as you need this for ADFS configuration later.</li> 
</ol> 
<h3 id="toc_12">Website configuration</h3> 
<p>If you are <strong>not</strong> using the SAM template, create an S3 bucket and configure it as a static website in the same way that you did for Part I.</p> 
<p>If you <strong>are</strong> using the SAM template this will automatically be created for you however the steps below will still need to be completed:</p> 
<p>In the source code repository, edit /Scenario2/website/<strong>configs.js</strong>.</p> 
<ol> 
<li>Ensure that the <strong>identityPool</strong> value matches your Amazon Cognito Pool ID and the region is correct.</li> 
<li>Leave <strong>adfsUrl</strong> the same if you’re testing on your lab server; otherwise, update with the AD FS DNS entries as appropriate.</li> 
<li>Update the <strong>relayingPartyId</strong> value as well if you used something different from the prerequisite blog post.</li> 
</ol> 
<p>Next, download the minified version of the <a href="https://aws.amazon.com/sdk-for-browser/">AWS SDK for JavaScript in the Browser</a> (aws-sdk.min.js) and place it along with the other files in /Scenario2/website into the S3 bucket.</p> 
<p>Copy the files from the API Gateway Generated SDK in the last section to this bucket so that the <strong>apigClient.js</strong> is in the root directory and <strong>lib</strong> folder is as well. The imports for these scripts (which do things like sign API requests and configure headers for the JWT in the Authorization header) are already included in the index.html file. Consult the latest API Gateway documentation if the SDK generation process updates in the future</p> 
<h3 id="toc_13">ADFS configuration</h3> 
<p>Now that the AWS setup is complete, modify your ADFS setup to capture <em>RelayState</em> information about the client and to send the POST response to API Gateway for processing. <strong>You will need to complete this step even if you use the SAM template.</strong></p> 
<p>If you’re using Windows Server 2008 with ADFS 2.0, ensure that Update Rollup 2 is installed before enabling <em>RelayState</em>. Please see official Microsoft documentation for specific download information.</p> 
<ol> 
<li>After Update Rollup 2 is installed, modify <strong>%systemroot%\inetpub\adfs\ls\web.config</strong>. If you’re on a newer version of Windows Server running AD FS 3.0, modify <strong>%systemroot%\ADFS\Microsoft.IdentityServer.Servicehost.exe.config</strong>.</li> 
<li>Find the section in the XML marked <code>&lt;Microsoft.identityServer.web&gt;</code> and add an entry for <code>&lt;useRelayStateForIdpInitiatedSignOn enabled=&quot;true&quot;&gt;</code>. If you have the proper ADFS rollup or version installed, this should allow the <em>RelayState</em> parameter to be accepted by the service provider.</li> 
<li>In the ADFS console, open <strong>Relaying Party Trusts</strong> for Amazon Web Services and choose <strong>Endpoints</strong>.</li> 
<li>For <strong>Binding</strong>, choose <strong>POST</strong> and for <strong>Invoke URL</strong>,enter the URL to your API Gateway from the stage that you noted earlier.</li> 
</ol> 
<p>At this point, you are ready to test out your webpage. Navigate to the S3 static website Endpoint URL and it should redirect you to the ADFS login screen. If the user login has been recent enough to have a valid SAML cookie, then you should see the login pass-through; otherwise, a login prompt appears. After the authentication has taken place, you should quickly end up back at your original webpage. Using the browser debugging tools, you see “Successful DDB call” followed by the results of a call to STS that were stored in DynamoDB.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_11.png" alt="lambdasamltwo_11.png" /></p> 
<p>As in Scenario 1, the sample code under /scenario2/website/index.html has a button that allows you to “ping” an endpoint to test if the federated credentials are working. If you have used the SAM template this should already be working and you can test it out (it will fail at first – keep reading to find out how to set the IAM permissions!). If not go to API Gateway and create a new Resource called <strong>/users</strong> at the same level of <strong>/saml</strong> in your API with a <strong>GET</strong> method.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_12.png" alt="lambdasamltwo_12.png" /></p> 
<p>For <strong>Integration type</strong>, choose <strong>Mock</strong>.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_13.png" alt="lambdasamltwo_13.png" /></p> 
<p>In the <strong>Method Request</strong>, for <strong>Authorization,</strong> choose <strong>AWS_IAM</strong>. In the <strong>Integration Response</strong>, in the <strong>Body Mapping Template</strong> section, for <strong>Content-Type</strong>, choose <strong>application/json</strong> and add the following JSON:</p> 
<pre><code class="language-none">{
&quot;status&quot;: &quot;Success&quot;,
&quot;agent&quot;: &quot;${context.identity.userAgent}&quot;
}</code></pre> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/lambdasamltwo/lambdasamltwo_14.png" alt="lambdasamltwo_14.png" /></p> 
<p>Before using this new Mock API as a test, configure CORS and re-generate the JavaScript SDK so that the browser knows about the new methods.</p> 
<ol> 
<li>On the <strong>/saml</strong> resource root and choose <strong>Actions,</strong> <strong>Enable CORS</strong>.</li> 
<li>In the <strong>Access-Control-Allow-Headers</strong> section, add <strong>COGNITO_ID</strong> into the endpoint and then choose <strong>Enable CORS and replace existing CORS headers</strong>.</li> 
<li>Choose <strong>Actions</strong>, <strong>Deploy API</strong>. Use the stage that you configured earlier.</li> 
<li>In the Stage Editor, choose <strong>SDK Generation</strong> and select JavaScript as your platform. Choose <strong>Generate SDK.</strong></li> 
<li>Upload the new <strong>apigClient.js</strong> and <strong>lib</strong> directory to the S3 bucket of your static website.</li> 
</ol> 
<p>One last thing must be completed before testing (You will need to complete this step even if you use the SAM template) if the credentials can invoke this mock endpoint with AWS_IAM credentials. The ADFS-Production Role needs <strong>execute-api:Invoke</strong> permissions for this API Gateway resource.</p> 
<ol> 
<li>In the IAM console, choose <strong>Roles</strong>, and open the ADFS-Production Role.</li> 
<li>For testing, you can attach the <strong>AmazonAPIGatewayInvokeFullAccess</strong> policy; however, for production, you should scope this down to the resource as documented in <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html">Control Access to API Gateway with IAM Permissions</a>.</li> 
<li>After you have attached a policy with invocation rights and authenticated with AD FS to finish the redirect process, choose <strong>PING</strong>.</li> 
</ol> 
<p>If everything has been set up successfully you should see an alert with information about the user agent.</p> 
<b id="toc_14">Final Thoughts</b> 
<p>We hope these scenarios and sample code help you to not only begin to build comprehensive enterprise applications on AWS but also to enhance your understanding of different AuthN and AuthZ mechanisms. Consider some ways that you might be able to evolve this solution to meet the needs of your own customers and innovate in this space. For example:</p> 
<ul> 
<li>Completing the CloudFront configuration and leveraging SSL termination for site identification. See if this can be incorporated into the Lambda processing pipeline.</li> 
<li>Attaching a scope-down IAM policy if the business rules are matched. For example, the default role could be more permissive for a group but if the user is a contractor (username with –C appended) they get extra restrictions applied when <a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html">assumeRoleWithSaml</a> is called in the ProcessSAML_awslabs_samldemo Lambda function.</li> 
<li>Changing the time duration before credentials expire on a per-role basis. Perhaps if the SAMLResponse parsing determines the user is an Administrator, they get a longer duration.</li> 
<li>Passing through additional user claims in SAMLResponse for further logical decisions or auditing by adding more claim rules in the ADFS console. This could also be a mechanism to synchronize some Active Directory schema attributes with AWS services.</li> 
<li>Granting different sets of credentials if a user has accounts with multiple SAML providers. While this tutorial was made with ADFS, you could also leverage it with other solutions such as Shibboleth and modify the ProcessSAML_awslabs_samldemo Lambda function to be aware of the different IdP ARN values. Perhaps your solution grants different IAM roles for the same user depending on if they initiated a login from Shibboleth rather than ADFS?</li> 
</ul> 
<p>The Lambda functions can be altered to take advantage of these options which you can <a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html">read more about here</a>. For more information about ADFS claim rule language manipulation, see <a href="https://technet.microsoft.com/en-us/library/dd807118(v=ws.11).aspx">The Role of the Claim Rule Language</a> on Microsoft TechNet.</p> 
<p>We would love to hear feedback from our customers on these designs and see different secure application designs that you’re implementing on the AWS platform.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/adfs/" rel="tag">adfs</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cognito/" rel="tag">cognito</a>, <a href="https://aws.amazon.com/blogs/compute/tag/dynamodb/" rel="tag">DynamoDB</a>, <a href="https://aws.amazon.com/blogs/compute/tag/jwt/" rel="tag">JWT</a>, <a href="https://aws.amazon.com/blogs/compute/tag/kms/" rel="tag">kms</a>, <a href="https://aws.amazon.com/blogs/compute/tag/s3/" rel="tag">S3</a>, <a href="https://aws.amazon.com/blogs/compute/tag/sts/" rel="tag">sts</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-1960');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
