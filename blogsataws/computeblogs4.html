<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Powering your Amazon ECS Cluster with Amazon EC2 Spot Instances</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> and 
<span property="author" typeof="Person"><span property="name">Sebastian Dreisch</span></span> | on 
<time property="datePublished" datetime="2017-06-06T16:21:58+00:00">06 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ec2-spot/" title="View all posts in Amazon EC2 Spot"><span property="articleSection">Amazon EC2 Spot</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2071" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2071&amp;disqus_title=Powering+your+Amazon+ECS+Cluster+with+Amazon+EC2+Spot+Instances&amp;disqus_url=https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2071');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post was graciously contributed by:</p> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 15px 0px 0px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ChadS.jpeg"><img class="size-full wp-image-2197" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ChadS.jpeg" alt="Chad Schmutzer, Solutions Architect" width="120" height="160" /></a></td> 
<td style="padding: 0px 0px 0px 15px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ShawnOConner.jpeg"><img class="wp-image-2198 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ShawnOConner.jpeg" alt="Shawn O'Conner, Enterprise Solutions Architect" width="119" height="160" /></a></td> 
</tr> 
<tr> 
<td style="padding: 0px 15px 0px 0px"><b>Chad Schmutzer</b><br /> Solutions Architect</td> 
<td style="padding: 0px 0px 0px 15px"><b>Shawn O’Connor</b><br /> Solutions Architect</td> 
</tr> 
</tbody> 
</table> 
<p>Today <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-ecs-adds-console-support-for-spot-fleet-creation/">we are excited to announce</a> that Amazon EC2 Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>) now supports the ability to launch your ECS cluster on <a href="https://aws.amazon.com/ec2/spot">Amazon EC2 Spot Instances</a> directly from the ECS console.</p> 
<p>Spot Instances allow you to bid on spare Amazon EC2 compute capacity. Spot Instances typically cost 50-90% less than On-Demand Instances. Powering your ECS cluster with Spot Instances lets you reduce the cost of running your existing containerized workloads, or increase your compute capacity by two to ten times while keeping the same budget. Or you could do a combination of both!</p> 
<p><span id="more-2071"></span></p> 
<p>Using Spot Instances, you specify the price you are willing to pay per instance-hour. Your Spot Instance runs whenever your bid exceeds the current Spot&nbsp;price. If your instance is reclaimed due to an increase in the Spot price, you are not charged for the partial hour that your instance has run.</p> 
<p>The ECS console uses <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html">Spot Fleet</a> to deploy your Spot Instances. Spot Fleet attempts to deploy the target capacity you request (expressed in terms of instances or a vCPU count) for your containerized application by launching Spot Instances that result in the best prices for you. If your Spot Instances are reclaimed due to a change in Spot prices or available capacity, Spot Fleet also attempts to maintain its target capacity.</p> 
<p>Containers are a natural fit for the diverse pool of resources that Spot Fleet thrives on. Spot Fleet enable you to provision capacity across multiple Spot Instance pools (combinations of instance types and Availability Zones), which helps improve your application’s availability and reduce operating costs of the fleet over time. Combining the extensible and flexible container placement system provided by ECS with Spot Fleet allows you to efficiently deploy containerized workloads and easily manage clusters at any scale for a fraction of the cost.</p> 
<p>Previously, deploying your ECS cluster on Spot Instances was a manual process. In this post, we show you how to achieve high availability, scalability, and cost savings for your container workloads by using the new Spot Fleet integration in the ECS console. We also show you how to build your own ECS cluster on Spot Instances using AWS CloudFormation.</p> 
<b>Creating an ECS cluster running on Spot Instances</b> 
<p>You can create an ECS cluster using the AWS Management Console.</p> 
<ol> 
<li>Open the Amazon ECS console at <a href="https://console.aws.amazon.com/ecs/">https://console.aws.amazon.com/ecs/</a>.</li> 
<li>In the navigation pane, choose&nbsp;<strong>Clusters</strong>.</li> 
<li>On the&nbsp;<strong>Clusters</strong>&nbsp;page, choose&nbsp;<strong>Create Cluster</strong>.</li> 
<li>For&nbsp;<strong>Cluster name</strong>, enter a name.</li> 
<li>In <strong>Instance configuration</strong>, for <strong>Provisioning model</strong>, choose <strong>Spot</strong>.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-1.png"><img class="aligncenter wp-image-2186" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-1.png" alt="ECS Create Cluster - Spot Fleet" width="700" height="345" /></a></p> 
<b>Choosing an allocation strategy</b> 
<p>The two available Spot Fleet allocation strategies are&nbsp;Diversified and Lowest price.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-2.png"><img class="aligncenter size-full wp-image-2187" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-2.png" alt="ECS Spot Allocation Strategies" width="572" height="122" /></a></p> 
<p>The allocation strategy you choose for your Spot Fleet determines how it fulfills your Spot Fleet request from the possible Spot Instance pools. When you use the diversified strategy, the Spot Instances are distributed across all pools. When you use the lowest price strategy, the Spot Instances come from the pool with the lowest price specified in your request.</p> 
<p>Remember that each instance type (the instance size within each instance family, e.g., c4.4xlarge), in each Availability Zone, in every region, is a separate pool of capacity, and therefore a separate Spot market. By diversifying across as many different instance types and Availability Zones as possible, you can improve the availability of your fleet. You also make your fleet less sensitive to increases in the Spot price in any one pool over time.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-3.png"><img class="aligncenter size-full wp-image-2188" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-3.png" alt="Spot Fleet Market" width="366" height="448" /></a></p> 
<p>You can select up to six EC2 instance types to use for your Spot Fleet. In this example, we’ve selected m3, m4, c3, c4, r3, and r4 instance types of size xlarge.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-4.png"><img class="aligncenter wp-image-2189" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-4.png" alt="Spot Instance Selection" width="400" height="302" /></a></p> 
<p>You need to enter a bid for your instances. Typically, bidding at or near the On-Demand Instance price is a good starting point. Your bid is the maximum price that you are willing to pay for instance types in that Spot pool. While the Spot&nbsp;price is at or below your bid, you pay the Spot&nbsp;price. Bidding lower ensures that you have lower costs, while bidding higher reduces the probability of interruption.</p> 
<p>Configure the number of instances to have in your cluster. Spot Fleet attempts to launch the number of Spot Instances that are required to meet the target capacity specified in your request. The Spot Fleet also attempts to maintain its target capacity if your Spot Instances are reclaimed due to a change in Spot prices or available capacity.</p> 
<p>The latest <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html">ECS–optimized AMI</a> is used for the instances when they are launched.</p> 
<p>Configure your storage and network settings. To enable diversification and high availability, be sure to select subnets in multiple Availability Zones. You can’t select multiple subnets in the same Availability Zone in a single Spot Fleet.</p> 
<p>The ECS container agent makes calls to the ECS API actions on your behalf. Container instances that run the agent require the ecsInstanceRole IAM policy and role for the service to know that the agent belongs to you. If you don’t have the ecsInstanceRole already, you can create one using the ECS console.</p> 
<p>If you create a managed compute environment that uses Spot Fleet, you must create a role that grants the Spot Fleet permission to bid on, launch, and terminate instances on your behalf. You can also create this role using the ECS console.</p> 
<p>That’s it! In the ECS console, choose <strong>Create</strong> to spin up your new ECS cluster running on Spot Instances.</p> 
<b>Using AWS CloudFormation to deploy your ECS cluster on Spot Instances</b> 
<p>We have also published a reference architecture AWS CloudFormation template that demonstrates how to easily launch a CloudFormation stack and deploy your ECS cluster on Spot Instances.</p> 
<p>The CloudFormation template includes the Spot Instance termination notice script mentioned earlier, as well as some additional logging and other example features to get you started quickly. You can find the CloudFormation template in the <a href="https://github.com/awslabs/ec2-spot-labs/tree/master/ecs-ec2-spot-fleet">Amazon EC2 Spot Instances GitHub repo</a>.</p> 
<p>Give it a try and customize it as needed for your environment!</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/0606-Spot-5.jpg"><img class="size-large wp-image-2194 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/0606-Spot-5-1024x709.jpg" alt="Spot Fleet Architecture" width="640" height="443" /></a></p> 
<b>Handling termination</b> 
<p>With Spot Instances, you never pay more than the price you specified. If the Spot price exceeds your bid price for a given instance, it is terminated automatically for you.</p> 
<p>The best way to protect against Spot Instance interruption is to architect your containerized application to be fault-tolerant. In addition, you can take advantage of a feature called Spot Instance termination notices, which provides a two-minute warning before EC2 must terminate your Spot Instance.</p> 
<p>This warning is made available to the applications on your Spot Instance using an item in the instance metadata. When you deploy your ECS cluster on Spot Instances using the console, AWS installs a script that checks every 5 seconds for the Spot Instance termination notice. If the notice is detected, the script immediately updates the container instance state to <strong>DRAINING</strong>.</p> 
<p>A simplified version of the Spot Instance termination notice script is as follows:</p> 
<pre><code class="lang-bash">#!/bin/bash
while sleep 5; do
if [ -z $(curl -Isf http://169.254.169.254/latest/meta-data/spot/termination-time) ]; then
/bin/false
else
ECS_CLUSTER=$(curl -s http://localhost:51678/v1/metadata | jq .Cluster | tr -d \&quot;)
CONTAINER_INSTANCE=$(curl -s http://localhost:51678/v1/metadata \
| jq .ContainerInstanceArn | tr -d \&quot;)
aws ecs update-container-instances-state --cluster $ECS_CLUSTER \
--container-instances $CONTAINER_INSTANCE --status DRAINING
fi
done</code></pre> 
<p>When you set a container instance to <strong>DRAINING</strong>, ECS prevents new tasks from being scheduled for placement on the container instance. If the resources are available, replacement service tasks are started on other container instances in the cluster. Container instance draining enables you to remove a container instance from a cluster without impacting tasks in your cluster. Service tasks on the container instance that are in the <strong>PENDING</strong> state are stopped immediately.</p> 
<p>Service tasks on the container instance that are in the <strong>RUNNING</strong> state are stopped and replaced according to the service’s deployment configuration parameters, minimumHealthyPercent and maximumPercent.</p> 
<b>ECS on Spot Instances in action<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/mapboxv1.jpg"><img class="alignright wp-image-2190 size-medium" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/mapboxv1-300x150.jpg" alt="" width="300" height="150" /></a></b> 
<p>Want to see how customers are already powering their ECS clusters on Spot Instances? Our friends at <a href="https://www.mapbox.com/">Mapbox</a> are doing just that.</p> 
<p>Mapbox&nbsp;is a platform for designing and publishing custom maps. The company uses ECS to power their entire batch processing architecture to collect and process over 100 million miles of sensor data per day that they use for powering their maps. They also optimize their batch processing architecture on ECS using Spot Instances.</p> 
<p>The Mapbox platform powers over 5,000 apps and reaches more than 200 million users each month. Its backend runs on ECS, allowing it to serve more than 1.3 billion requests per day. To learn more about their recent migration to ECS, read their recent blog post,&nbsp;<a href="https://www.mapbox.com/blog/switch-to-ecs/">We Switched to Amazon ECS, and You Won’t Believe What Happened Next</a>. Then, in their follow-up blog post, <a href="https://www.mapbox.com/blog/caches-to-cash/">Caches to Cash</a>, learn how they are running their entire platform on Spot Instances, allowing them to save upwards of 50–90% on their EC2 costs.</p> 
<b>Conclusion</b> 
<p>We hope that you are as excited as we are about running your containerized applications at scale and cost effectively using Spot Instances. For more information, see the following pages:</p> 
<li><a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_GetStarted.html">Getting Started with Amazon ECS</a></li> 
<li><a href="https://aws.amazon.com/ec2/spot/getting-started/">Getting Started with Amazon EC2 Spot Instances</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.html">Getting Started with AWS CloudFormation</a></li> 
<p>If you have comments or suggestions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ec2/" rel="tag">EC2</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/spot/" rel="tag">spot</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2071');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Workflow Layer (Part 4 of 4)</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-06-01T17:43:43+00:00">01 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-batch/" title="View all posts in AWS Batch*"><span property="articleSection">AWS Batch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2161" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2161&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Workflow+Layer+%28Part+4+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2161');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the fourth in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<li>Job</li> 
<li><a href="https://aws.amazon.com/batch/">Batch</a></li> 
<li>Workflow</li> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2</a>, you built a Docker container for each job that needed to run as part of your workflow, and stored them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>.</p> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3</a>, you tackled the batch layer and built a scalable, elastic, and easily maintainable batch engine using <a href="https://aws.amazon.com/batch">AWS Batch</a>. This solution took care of dynamically scaling your compute resources in response to the number of runnable jobs in your job queue length as well as managed job placement.<span id="more-2161"></span></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In part 4, you build out the workflow layer of your solution using <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> and <a href="https://aws.amazon.com/lambda">AWS Lambda</a>. You then run an end-to-end genomic analysis―specifically known as exome secondary analysis―for many times at a cost of less than $1 per exome.</p> 
<p>Step Functions makes it easy to coordinate the components of your applications using visual workflows. Building applications from individual components that each perform a single function lets you scale and change your workflow quickly. You can use the graphical console to arrange and visualize the components of your application as a series of steps, which simplify building and running multi-step applications. You can change and add steps without writing code, so you can easily evolve your application and innovate faster.</p> 
<p>An added benefit of using Step Functions to define your workflows is that the state machines you create are immutable. While you can delete a state machine, you cannot alter it after it is created. For regulated workloads where auditing is important, you can be assured that state machines you used in production cannot be altered.</p> 
<p>In this blog post, you will create a Lambda state machine to orchestrate your batch workflow. For more information on how to create a basic state machine, please see this <a href="https://docs.aws.amazon.com/step-functions/latest/dg/hello-lambda.html">Step Functions tutorial</a>.</p> 
<p>All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<b id="toc_0">Build a state machine building block</b> 
<p>To skip the following steps, we have provided an <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/workflow/deploy_state_machine.yaml">AWS CloudFormation template</a> that can deploy your Step Functions state machine. You can use this in combination with the setup you did in part 3 to quickly set up the environment in which to run your analysis.</p> 
<p>The state machine is composed of smaller state machines that submit a job to AWS Batch, and then poll and check its execution.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/Polling-State-Machine.png"><img class="aligncenter size-medium wp-image-2174" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/Polling-State-Machine-300x265.png" alt="" width="300" height="265" /></a></p> 
<p>The steps in this building block state machine are as follows:</p> 
<ol> 
<li><strong>A job is submitted.</strong><br /> Each analytical module/job has its own Lambda function for submission and calls the <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/runner/batch_submit_job.py">batchSubmitJob Lambda function</a> that you built in the previous blog post. You will build these specialized Lambda functions in the following section.</li> 
<li><strong>The state machine queries the AWS Batch API for the job status.</strong><br /> This is also a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/runner/batch_get_job_status.py">Lambda function</a>.</li> 
<li><strong>The job status is checked to see if the job has completed.</strong><br /> If the job status equals SUCCESS, proceed to log the final job status. If the job status equals FAILED, end the execution of the state machine. In all other cases, wait 30 seconds and go back to Step 2.</li> 
</ol> 
<p>Here is the JSON representing this state machine.</p> 
<pre><code class="language-none">{
&quot;Comment&quot;: &quot;A simple example that submits a Job to AWS Batch&quot;,
&quot;StartAt&quot;: &quot;SubmitJob&quot;,
&quot;States&quot;: {
&quot;SubmitJob&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:&lt;account-id&gt;::function:batchSubmitJob&quot;,
&quot;Next&quot;: &quot;GetJobStatus&quot;
},
&quot;GetJobStatus&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:&lt;account-id&gt;:function:batchGetJobStatus&quot;,
&quot;Next&quot;: &quot;CheckJobStatus&quot;,
&quot;InputPath&quot;: &quot;$&quot;,
&quot;ResultPath&quot;: &quot;$.status&quot;
},
&quot;CheckJobStatus&quot;: {
&quot;Type&quot;: &quot;Choice&quot;,
&quot;Choices&quot;: [
{
&quot;Variable&quot;: &quot;$.status&quot;,
&quot;StringEquals&quot;: &quot;FAILED&quot;,
&quot;End&quot;: true
},
{
&quot;Variable&quot;: &quot;$.status&quot;,
&quot;StringEquals&quot;: &quot;SUCCEEDED&quot;,
&quot;Next&quot;: &quot;GetFinalJobStatus&quot;
}
],
&quot;Default&quot;: &quot;Wait30Seconds&quot;
},
&quot;Wait30Seconds&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 30,
&quot;Next&quot;: &quot;GetJobStatus&quot;
},
&quot;GetFinalJobStatus&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-east-1:&lt;account-id&gt;:function:batchGetJobStatus&quot;,
&quot;End&quot;: true
}
}
}</code></pre> 
<b id="toc_1">Building the Lambda functions for the state machine</b> 
<p>You need two basic Lambda functions for this state machine. The first one submits a job to AWS Batch and the second checks the status of the AWS Batch job that was submitted.</p> 
<p>In AWS Step Functions, you specify an input as JSON that is read into your state machine. Each state receives the aggregate of the steps immediately preceding it, and you can specify which components a state passes on to its children. Because you are using Lambda functions to execute tasks, one of the easiest routes to take is to modify the input JSON, represented as a Python dictionary, within the Lambda function and return the entire dictionary back for the next state to consume.</p> 
<b id="toc_2">Building the batchSubmitIsaacJob Lambda function</b> 
<p>For Step 1 above, you need a Lambda function for each of the steps in your analysis workflow. As you created a generic Lambda function in the previous post to submit a batch job (batchSubmitJob), you can use that function as the basis for the specialized functions you’ll include in this state machine. Here is such a Lambda function for the Isaac aligner.</p> 
<pre><code class="language-none">from __future__ import print_function
import boto3
import json
import traceback
lambda_client = boto3.client('lambda')
def lambda_handler(event, context):
try:
# Generate output put
bam_s3_path = '/'.join([event['resultsS3Path'], event['sampleId'], 'bam/'])
depends_on = event['dependsOn'] if 'dependsOn' in event else []
# Generate run command
command = [
'--bam_s3_folder_path', bam_s3_path,
'--fastq1_s3_path', event['fastq1S3Path'],
'--fastq2_s3_path', event['fastq2S3Path'],
'--reference_s3_path', event['isaac']['referenceS3Path'],
'--working_dir', event['workingDir']
]
if 'cmdArgs' in event['isaac']:
command.extend(['--cmd_args', event['isaac']['cmdArgs']])
if 'memory' in event['isaac']:
command.extend(['--memory', event['isaac']['memory']])
# Submit Payload
response = lambda_client.invoke(
FunctionName='batchSubmitJob',
InvocationType='RequestResponse',
LogType='Tail',
Payload=json.dumps(dict(
dependsOn=depends_on,
containerOverrides={
'command': command,
},
jobDefinition=event['isaac']['jobDefinition'],
jobName='-'.join(['isaac', event['sampleId']]),
jobQueue=event['isaac']['jobQueue']
)))
response_payload = response['Payload'].read()
# Update event
event['bamS3Path'] = bam_s3_path
event['jobId'] = json.loads(response_payload)['jobId']
return event
except Exception as e:
traceback.print_exc()
raise e</code></pre> 
<p>In the Lambda console, create a Python 2.7 Lambda function named batchSubmitIsaacJob and paste in the above code. Use the LambdaBatchExecutionRole that you created in the previous post. For more information, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html">Step 2.1: Create a Hello World Lambda Function</a>.</p> 
<p>This Lambda function reads in the inputs passed to the state machine it is part of, formats the data for the batchSubmitJob Lambda function, invokes that Lambda function, and then modifies the event dictionary to pass onto the subsequent states. You can repeat these for each of the other tools, which can be found in the tools//lambda/lambda_function.py script in the GitHub repo.</p> 
<b id="toc_3">Building the batchGetJobStatus Lambda function</b> 
<p>For Step 2 above, the process queries the AWS Batch DescribeJobs API action with jobId to identify the state that the job is in. You can put this into a Lambda function to integrate it with Step Functions.</p> 
<p>In the Lambda console, create a new Python 2.7 function with the LambdaBatchExecutionRole IAM role. Name your function batchGetJobStatus and paste in the following code. This is similar to the <code>batch-get-job-python27</code> Lambda blueprint.</p> 
<pre><code class="language-none">from __future__ import print_function
import boto3
import json
print('Loading function')
batch_client = boto3.client('batch')
def lambda_handler(event, context):
# Log the received event
print(&quot;Received event: &quot; + json.dumps(event, indent=2))
# Get jobId from the event
job_id = event['jobId']
try:
response = batch_client.describe_jobs(
jobs=[job_id]
)
job_status = response['jobs'][0]['status']
return job_status
except Exception as e:
print(e)
message = 'Error getting Batch Job status'
print(message)
raise Exception(message)</code></pre> 
<b id="toc_4">Structuring state machine input</b> 
<p>You have structured the state machine input so that general file references are included at the top-level of the JSON object, and any job-specific items are contained within a nested JSON object. At a high level, this is what the input structure looks like:</p> 
<pre><code class="language-none">{
&quot;general_field_1&quot;: &quot;value1&quot;,
&quot;general_field_2&quot;: &quot;value2&quot;,
&quot;general_field_3&quot;: &quot;value3&quot;,
&quot;job1&quot;: {},
&quot;job2&quot;: {},
&quot;job3&quot;: {}
}</code></pre> 
<b id="toc_5">Building the full state machine</b> 
<p>By chaining these state machine components together, you can quickly build flexible workflows that can process genomes in multiple ways. The development of the larger state machine that defines the entire workflow uses four of the above building blocks. You use the Lambda functions that you built in the previous section. Rename each building block submission to match the tool name.</p> 
<p>We have provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/workflow/deploy_state_machine.yaml">CloudFormation template</a> to deploy your state machine and the associated IAM roles. In the <a href="https://console.aws.amazon.com/cloudformation">CloudFormation console</a>, select Create Stack, choose your template (deploy_state_machine.yaml), and enter in the ARNs for the Lambda functions you created.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/cfn_parameters.png"><img class="aligncenter size-full wp-image-2167" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/cfn_parameters.png" alt="" width="1898" height="1232" /></a></p> 
<p>Continue through the rest of the steps and deploy your stack. Be sure to check the box next to <strong>“I acknowledge that AWS CloudFormation might create IAM resources.”</strong></p> 
<p>Once the CloudFormation stack is finished deploying, you should see the following image of your state machine.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/statemachine_workflow.png"><img class="aligncenter size-full wp-image-2168" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/statemachine_workflow.png" alt="" width="1795" height="1691" /></a></p> 
<p>In short, you first submit a job for Isaac, which is the aligner you are using for the analysis. Next, you use parallel state to split your output from “GetFinalIsaacJobStatus” and send it to both your variant calling step, Strelka, and your QC step, Samtools Stats. These then are run in parallel and you annotate the results from your Strelka step with snpEff.</p> 
<b id="toc_6">Putting it all together</b> 
<p>Now that you have built all of the components for a genomics secondary analysis workflow, test the entire process.</p> 
<p>We have provided sequences from an Illumina sequencer that cover a region of the genome known as the exome. Most of the positions in the genome that we have currently associated with disease or human traits reside in this region, which is 1–2% of the entire genome. The workflow that you have built works for both analyzing an exome, as well as an entire genome.</p> 
<p>Additionally, we have provided prebuilt reference genomes for Isaac, located at:</p> 
<pre><code class="language-none">s3://aws-batch-genomics-resources/reference/</code></pre> 
<p>If you are interested, we have provided a script that sets up all of that data. To execute that script, run the following command on a large EC2 instance:</p> 
<pre><code class="language-none">make reference REGISTRY=&lt;your-ecr-registry&gt;</code></pre> 
<p>Indexing and preparing this reference takes many hours on a large-memory EC2 instance. Be careful about the costs involved and note that the data is available through the prebuilt reference genomes.</p> 
<b id="toc_7">Starting the execution</b> 
<p>In a previous section, you established a provenance for the JSON that is fed into your state machine. For ease, we have auto-populated the input JSON for you to the state machine. You can also find this in the GitHub repo under <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/workflow/test.input.json">workflow/test.input.json</a>:</p> 
<pre><code class="language-none">{
&quot;fastq1S3Path&quot;: &quot;s3://aws-batch-genomics-resources/fastq/SRR1919605_1.fastq.gz&quot;,
&quot;fastq2S3Path&quot;: &quot;s3://aws-batch-genomics-resources/fastq/SRR1919605_2.fastq.gz&quot;,
&quot;referenceS3Path&quot;: &quot;s3://aws-batch-genomics-resources/reference/hg38.fa&quot;,
&quot;resultsS3Path&quot;: &quot;s3://&lt;bucket&gt;/genomic-workflow/results&quot;,
&quot;sampleId&quot;: &quot;NA12878_states_1&quot;,
&quot;workingDir&quot;: &quot;/scratch&quot;,
&quot;isaac&quot;: {
&quot;jobDefinition&quot;: &quot;isaac-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/highPriority-myenv&quot;,
&quot;referenceS3Path&quot;: &quot;s3://aws-batch-genomics-resources/reference/isaac/&quot;
},
&quot;samtoolsStats&quot;: {
&quot;jobDefinition&quot;: &quot;samtools_stats-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/lowPriority-myenv&quot;
},
&quot;strelka&quot;: {
&quot;jobDefinition&quot;: &quot;strelka-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/highPriority-myenv&quot;,
&quot;cmdArgs&quot;: &quot; --exome &quot;
},
&quot;snpEff&quot;: {
&quot;jobDefinition&quot;: &quot;snpeff-myenv:1&quot;,
&quot;jobQueue&quot;: &quot;arn:aws:batch:us-east-1:&lt;account-id&gt;:job-queue/lowPriority-myenv&quot;,
&quot;cmdArgs&quot;: &quot; -t hg38 &quot;
}
}</code></pre> 
<p>You are now at the stage to run your full genomic analysis. Copy the above to a new text file, change paths and ARNs to the ones that you created previously, and save your JSON input as input.states.json.</p> 
<p>In the CLI, execute the following command. You need the ARN of the state machine that you created in the previous post:</p> 
<pre><code class="language-none">aws stepfunctions start-execution --state-machine-arn &lt;your-state-machine-arn&gt; --input file://input.states.json</code></pre> 
<p>Your analysis has now started. By using Spot Instances with AWS Batch, you can quickly scale out your workflows while concurrently optimizing for cost. While this is not guaranteed, most executions of the workflows presented here should cost under $1 for a full analysis.</p> 
<b id="toc_8">Monitoring the execution</b> 
<p>The output from the above CLI command gives you the ARN that describes the specific execution. Copy that and navigate to the Step Functions console. Select the state machine that you created previously and paste the ARN into the search bar.</p> 
<p>The screen shows information about your specific execution. On the left, you see where your execution currently is in the workflow.</p> 
<p>In the following screenshot, you can see that your workflow has successfully completed the alignment job and moved onto the subsequent steps, which are variant calling and generating quality information about your sample.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/stepfunctions_midworkflow.png"><img class="aligncenter size-full wp-image-2170" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/stepfunctions_midworkflow.png" alt="" width="1945" height="1170" /></a></p> 
<p>You can also navigate to the AWS Batch console and see that progress of all of your jobs reflected there as well.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/batch_dashboard.png"><img class="aligncenter size-full wp-image-2171" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/02/batch_dashboard.png" alt="" width="1945" height="687" /></a></p> 
<p>Finally, after your workflow has completed successfully, check out the S3 path to which you wrote all of your files. If you run a <code>ls –recursive</code> command on the S3 results path, specified in the input to your state machine execution, you should see something similar to the following:</p> 
<pre><code class="language-none">2017-05-02 13:46:32 6475144340 genomic-workflow/results/NA12878_run1/bam/sorted.bam
2017-05-02 13:46:34    7552576 genomic-workflow/results/NA12878_run1/bam/sorted.bam.bai
2017-05-02 13:46:32         45 genomic-workflow/results/NA12878_run1/bam/sorted.bam.md5
2017-05-02 13:53:20      68769 genomic-workflow/results/NA12878_run1/stats/bam_stats.dat
2017-05-02 14:05:12        100 genomic-workflow/results/NA12878_run1/vcf/stats/runStats.tsv
2017-05-02 14:05:12        359 genomic-workflow/results/NA12878_run1/vcf/stats/runStats.xml
2017-05-02 14:05:12  507577928 genomic-workflow/results/NA12878_run1/vcf/variants/genome.S1.vcf.gz
2017-05-02 14:05:12     723144 genomic-workflow/results/NA12878_run1/vcf/variants/genome.S1.vcf.gz.tbi
2017-05-02 14:05:12  507577928 genomic-workflow/results/NA12878_run1/vcf/variants/genome.vcf.gz
2017-05-02 14:05:12     723144 genomic-workflow/results/NA12878_run1/vcf/variants/genome.vcf.gz.tbi
2017-05-02 14:05:12   30783484 genomic-workflow/results/NA12878_run1/vcf/variants/variants.vcf.gz
2017-05-02 14:05:12    1566596 genomic-workflow/results/NA12878_run1/vcf/variants/variants.vcf.gz.tbi</code></pre> 
<b id="toc_9">Modifications to the workflow</b> 
<p>You have now built and run your genomics workflow. While diving deep into modifications to this architecture are beyond the scope of these posts, we wanted to leave you with several suggestions of how you might modify this workflow to satisfy additional business requirements.</p> 
<li><strong>Job tracking with Amazon DynamoDB</strong><br /> In many cases, such as if you are offering Genomics-as-a-Service, you might want to track the state of your jobs with DynamoDB to get fine-grained records of how your jobs are running. This way, you can easily identify the cost of individual jobs and workflows that you run.</li> 
<li><strong>Resuming from failure</strong><br /> Both AWS Batch and Step Functions natively support job retries and can cover many of the standard cases where a job might be interrupted. There may be cases, however, where your workflow might fail in a way that is unpredictable. In this case, you can use <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html">custom error handling with AWS Step Functions</a> to build out a workflow that is even more resilient. Also, you can build in fail states into your state machine to fail at any point, such as if a batch job fails after a certain number of retries.</li> 
<li><strong>Invoking Step Functions from Amazon API Gateway</strong><br /> You can use API Gateway to build an API that acts as a “front door” to Step Functions. You can create a POST method that contains the input JSON to feed into the state machine you built. For more information, see the <a href="https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/">Implementing Serverless Manual Approval Steps in AWS Step Functions and Amazon API Gateway</a> blog post.</li> 
<b id="toc_10">Conclusion</b> 
<p>While the approach we have demonstrated in this series has been focused on genomics, it is important to note that this can be generalized to nearly any high-throughput batch workload. We hope that you have found the information useful and that it can serve as a jump-start to building your own batch workloads on AWS with native AWS services.</p> 
<p>For more information about how AWS can enable your genomics workloads, be sure to check out the <a href="https://aws.amazon.com/health/genomics/">AWS Genomics</a> page.</p> 
<p>Other posts in this four-part series:</p> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
<p>Please leave any questions and comments below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/batch/" rel="tag">Batch</a>, <a href="https://aws.amazon.com/blogs/compute/tag/genomics/" rel="tag">genomics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/lambda/" rel="tag">lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a>, <a href="https://aws.amazon.com/blogs/compute/tag/workflow/" rel="tag">workflow</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2161');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building High-Throughput Genomic Batch Workflows on AWS: Batch Layer (Part 3 of 4)</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-31T17:45:09+00:00">31 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-batch/" title="View all posts in AWS Batch*"><span property="articleSection">AWS Batch*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2142" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2142&amp;disqus_title=Building+High-Throughput+Genomic+Batch+Workflows+on+AWS%3A++Batch+Layer+%28Part+3+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2142');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the third in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<li>Job</li> 
<li><a href="https://aws.amazon.com/batch/">Batch</a></li> 
<li>Workflow</li> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2</a>, you built a Docker container for each job that needed to run as part of your workflow, and stored them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>.<span id="more-2142"></span></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In Part 3, you tackle the batch layer and build a scalable, elastic, and easily maintainable batch engine using <a href="https://aws.amazon.com/batch">AWS Batch</a>.</p> 
<p>AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. It dynamically provisions the optimal quantity and type of compute resources (for example, CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs that you submit. With AWS Batch, you do not need to install and manage your own batch computing software or server clusters, which allows you to focus on analyzing results, such as those of your genomic analysis.</p> 
<b id="toc_0">Integrating applications into AWS Batch</b> 
<p>If you are new to AWS Batch, we recommend reading <a href="http://docs.aws.amazon.com/batch/latest/userguide/get-set-up-for-aws-batch.html">Setting Up AWS Batch</a> to ensure that you have the proper permissions and AWS environment.</p> 
<p>After you have a working environment, you define several types of resources:</p> 
<li>IAM roles that provide service permissions</li> 
<li>A compute environment that launches and terminates compute resources for jobs</li> 
<li>A custom Amazon Machine Image (AMI)</li> 
<li>A job queue to submit the units of work and to schedule the appropriate resources within the compute environment to execute those jobs</li> 
<li>Job definitions that define how to execute an application</li> 
<p>After the resources are created, you’ll test the environment and create an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function to send generic jobs to the queue.</p> 
<p>This genomics workflow covers the basic steps. For more information, see <a href="http://docs.aws.amazon.com/batch/latest/userguide/Batch_GetStarted.html">Getting Started with AWS Batch</a>.</p> 
<b id="toc_1">Creating the necessary IAM roles</b> 
<p>AWS Batch simplifies batch processing by managing a number of underlying AWS services so that you can focus on your applications. As a result, you create IAM roles that give the service permissions to act on your behalf. In this section, deploy the <a href="https://aws.amazon.com/cloudformation">AWS CloudFormation</a> template <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/setup/iam.template.yaml">included in the GitHub repository</a> and extract the ARNs for later use.</p> 
<p>To deploy the stack, go to the top level in the <a href="https://github.com/awslabs/aws-batch-genomics">repo</a> with the following command:</p> 
<pre><code class="language-none">aws cloudformation create-stack --template-body file://batch/setup/iam.template.yaml --stack-name iam --capabilities CAPABILITY_NAMED_IAM</code></pre> 
<p>You can capture the output from this stack in the <strong>Outputs</strong> tab in the CloudFormation console:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/iam_cfn_output.png"><img class="aligncenter size-full wp-image-2145" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/iam_cfn_output.png" alt="" width="1945" height="450" /></a></p> 
<b id="toc_2">Creating the compute environment</b> 
<p>In AWS Batch, you will set up a managed compute environments. Managed compute environments automatically launch and terminate compute resources on your behalf based on the aggregate resources needed by your jobs, such as vCPU and memory, and simple boundaries that you define.</p> 
<p>When defining your compute environment, specify the following:</p> 
<li>Desired instance types in your environment</li> 
<li>Min and max vCPUs in the environment</li> 
<li>The Amazon Machine Image (AMI) to use</li> 
<li>Percentage value for bids on the <a href="https://aws.amazon.com/ec2/spot/">Spot Market</a> and VPC subnets that can be used.</li> 
<p>AWS Batch then provisions an elastic and heterogeneous pool of <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances based on the aggregate resource requirements of jobs sitting in the RUNNABLE state. If a mix of CPU and memory-intensive jobs are ready to run, AWS Batch provisions the appropriate ratio and size of CPU and memory-optimized instances within your environment. For this post, you will use the simplest configuration, in which instance types are set to “optimal” allowing AWS Batch to choose from the latest C, M, and R EC2 instance families.</p> 
<p>While you could create this compute environment in the console, we provide the following CLI commands. Replace the subnet IDs and key name with your own private subnets and key, and the image-id with the image you will build in the next section.</p> 
<pre><code class="language-none">ACCOUNTID=&lt;your account id&gt;
SERVICEROLE=&lt;from output in CloudFormation template&gt;
IAMFLEETROLE=&lt;from output in CloudFormation template&gt;
JOBROLEARN=&lt;from output in CloudFormation template&gt;
SUBNETS=&lt;comma delimited list of subnets&gt;
SECGROUPS=&lt;your security groups&gt;
SPOTPER=50 # percentage of on demand
IMAGEID=&lt;ami-id corresponding to the one you created&gt;
INSTANCEROLE=&lt;from output in CloudFormation template&gt;
REGISTRY=${ACCOUNTID}.dkr.ecr.us-east-1.amazonaws.com
KEYNAME=&lt;your key name&gt;
MAXCPU=1024 # max vCPUs in compute environment
ENV=myenv
# Creates the compute environment
aws batch create-compute-environment --compute-environment-name genomicsEnv-$ENV --type MANAGED --state ENABLED --service-role ${SERVICEROLE} --compute-resources type=SPOT,minvCpus=0,maxvCpus=$MAXCPU,desiredvCpus=0,instanceTypes=optimal,imageId=$IMAGEID,subnets=$SUBNETS,securityGroupIds=$SECGROUPS,ec2KeyPair=$KEYNAME,instanceRole=$INSTANCEROLE,bidPercentage=$SPOTPER,spotIamFleetRole=$IAMFLEETROLE</code></pre> 
<b id="toc_3">Creating the custom AMI for AWS Batch</b> 
<p>While you can use default Amazon ECS-optimized AMIs with AWS Batch, you can also provide your own image in managed compute environments. Use this feature to provision additional scratch EBS storage on each of the instances that AWS Batch launches, and to encrypt both the Docker and scratch EBS volumes.</p> 
<p>The process for <a href="http://docs.aws.amazon.com/batch/latest/userguide/create-batch-ami.html">creating a custom AMI for AWS Batch</a> is well-documented. Because AWS Batch has the <a href="https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec">same requirements for your AMI</a> as Amazon ECS, use the default <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=Amazon+ECS-Optimized+Amazon+Linux+AMI&amp;page=1&amp;ref_=nav_search_box">Amazon ECS-optimized Amazon Linux AMI</a> as a base and change it in the following ways:</p> 
<li>Attach a 1 TB scratch volume to <code>/dev/sdb</code></li> 
<li>Set the EBS encryption options on both the Docker volume and the new scratch volume</li> 
<li>Modify the <code>/etc/fstab</code> file so that the scratch volume is mounted to <code>/docker_scratch</code> on system start</li> 
<p>The first two tasks can be addressed in the console. Use the standard EC2 instance launch process to spin up a small t2.micro instance of the ECS-optimized AMI. In the <strong>Add Storage</strong> step, add the scratch volume and make sure that you select the <strong>Encrypted</strong> boxes for both the Docker and scratch volume.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_ecs_setup.png"><img class="aligncenter size-full wp-image-2146" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_ecs_setup.png" alt="" width="1950" height="554" /></a></p> 
<p>After your instance has launched, record the IP address and then SSH into the instance. Copy and paste the following code:</p> 
<pre><code class="language-none">
sudo yum -y update
sudo mkfs -t ext4 /dev/xvdb
sudo mkdir /docker_scratch
sudo echo -e '/dev/xvdb\t/docker_scratch\text4\tdefaults\t0\t0' | sudo tee -a /etc/fstab
sudo mount –a
sudo stop ecs
sudo rm -rf /var/lib/ecs/data/ecs_agent_data.json
</code></pre> 
<p>This auto-mounts your scratch volume to <code>/docker_scratch</code>, which is your scratch directory for batch processing. The last two commands stop the ECS agent and remove any persistent data checkpoint files. Next, <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-ami-ebs.html">create your new AMI</a> and record the image ID.</p> 
<b id="toc_4">Creating the job queues</b> 
<p>AWS Batch job queues are used to coordinate the submission of batch jobs. Your jobs are submitted to job queues, which can be mapped to one or more compute environments. Job queues have priority relative to each other. You can also specify the order in which they consume resources from your compute environments.</p> 
<p>In this solution, use two job queues. The first is for high priority jobs, such as alignment or variant calling. Set this with a high priority (1000) and map back to the previously created compute environment. Next, set a second job queue for low priority jobs, such as quality statistics generation. To create these compute environments, enter the following CLI commands:</p> 
<pre><code class="language-none">aws batch create-job-queue --job-queue-name highPriority-${ENV} --compute-environment-order order=0,computeEnvironment=genomicsEnv-${ENV}  --priority 1000 --state ENABLED
aws batch create-job-queue --job-queue-name lowPriority-${ENV} --compute-environment-order order=0,computeEnvironment=genomicsEnv-${ENV}  --priority 1 --state ENABLED</code></pre> 
<b id="toc_5">Creating the job definitions</b> 
<p>To run the Isaac aligner container image locally, supply the Amazon S3 locations for the FASTQ input sequences, the reference genome to align to, and the output BAM file. For more information, see <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/README.md">tools/isaac/README.md</a>.</p> 
<p>The Docker container itself also requires some information on a suitable mountable volume so that it can read and write files temporary files without running out of space.</p> 
<p>Note: In the following example, the FASTQ files as well as the reference files to run are in a publicly available bucket.</p> 
<pre><code class="language-none">FASTQ1=s3://aws-batch-genomics-resources/fastq/SRR1919605_1.fastq.gz
FASTQ2=s3://aws-batch-genomics-resources/fastq/SRR1919605_2.fastq.gz
REF=s3://aws-batch-genomics-resources/reference/isaac/
BAM=s3://mybucket/genomic-workflow/test_results/bam/
mkdir ~/scratch
docker run --rm -ti -v $(HOME)/scratch:/scratch $REPO_URI --bam_s3_folder_path $BAM \
--fastq1_s3_path $FASTQ1 \
--fastq2_s3_path $FASTQ2 \
--reference_s3_path $REF \
--working_dir /scratch </code></pre> 
<p>Locally running containers can typically expand their CPU and memory resource headroom. In AWS Batch, the CPU and memory requirements are hard limits and are allocated to the container image at runtime.</p> 
<p>Isaac is a fairly resource-intensive algorithm, as it creates an uncompressed index of the reference genome in memory to match the query DNA sequences. The large memory space is shared across multiple CPU threads, and Isaac can scale almost linearly with the number of CPU threads given to it as a parameter.</p> 
<p>To fit these characteristics, choose an optimal instance size to maximize the number of CPU threads based on a given large memory footprint, and deploy a Docker container that uses all of the instance resources. In this case, we chose a host instance with 80+ GB of memory and 32+ vCPUs. The following code is example JSON that you can pass to the AWS CLI to create a job definition for Isaac.</p> 
<pre><code class="language-none">aws batch register-job-definition --job-definition-name isaac-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/isaac&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:80000,
&quot;vcpus&quot;:32,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'</code></pre> 
<p>You can copy and paste the following code for the other three job definitions:</p> 
<pre><code class="language-none">aws batch register-job-definition --job-definition-name strelka-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/strelka&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:32000,
&quot;vcpus&quot;:32,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'
aws batch register-job-definition --job-definition-name snpeff-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/snpeff&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:10000,
&quot;vcpus&quot;:4,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'
aws batch register-job-definition --job-definition-name samtoolsStats-${ENV} --type container --retry-strategy attempts=3 --container-properties '
{&quot;image&quot;: &quot;'${REGISTRY}'/samtools_stats&quot;,
&quot;jobRoleArn&quot;:&quot;'${JOBROLEARN}'&quot;,
&quot;memory&quot;:10000,
&quot;vcpus&quot;:4,
&quot;mountPoints&quot;: [{&quot;containerPath&quot;: &quot;/scratch&quot;, &quot;readOnly&quot;: false, &quot;sourceVolume&quot;: &quot;docker_scratch&quot;}],
&quot;volumes&quot;: [{&quot;name&quot;: &quot;docker_scratch&quot;, &quot;host&quot;: {&quot;sourcePath&quot;: &quot;/docker_scratch&quot;}}]
}'</code></pre> 
<p>The value for “image” comes from the previous post on creating a Docker image and publishing to ECR. The value for jobRoleArn you can find from the output of the CloudFormation template that you deployed earlier. In addition to providing the number of CPU cores and memory required by Isaac, you also give it a storage volume for scratch and staging. The volume comes from the previously defined custom AMI.</p> 
<b id="toc_6">Testing the environment</b> 
<p>After you have created the Isaac job definition, you can submit the job using the <a href="http://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html">AWS Batch submitJob</a> API action. While the base mappings for Docker run are taken care of in the job definition that you just built, the specific job parameters should be specified in the container overrides section of the API call. Here’s what this would look like in the CLI, using the same parameters as in the bash commands shown earlier:</p> 
<pre><code class="language-none">aws batch submit-job --job-name testisaac --job-queue highPriority-${ENV} --job-definition isaac-${ENV}:1 --container-overrides '{
&quot;command&quot;: [
&quot;--bam_s3_folder_path&quot;, &quot;s3://mybucket/genomic-workflow/test_batch/bam/&quot;,
&quot;--fastq1_s3_path&quot;, &quot;s3://aws-batch-genomics-resources/fastq/ SRR1919605_1.fastq.gz&quot;,
&quot;--fastq2_s3_path&quot;, &quot;s3://aws-batch-genomics-resources/fastq/SRR1919605_2.fastq.gz&quot;,
&quot;--reference_s3_path&quot;, &quot;s3://aws-batch-genomics-resources/reference/isaac/&quot;,
&quot;--working_dir&quot;, &quot;/scratch&quot;,
&quot;—cmd_args&quot;, &quot; --exome &quot;,]
}'</code></pre> 
<p>When you execute a submitJob call, jobId is returned. You can then track the progress of your job using the describeJobs API action:</p> 
<pre><code class="language-none">aws batch describe-jobs –jobs &lt;jobId returned from submitJob&gt;</code></pre> 
<p>You can also track the progress of all of your jobs in the AWS Batch console dashboard.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_dashboard.png"><img class="aligncenter size-full wp-image-2147" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_dashboard.png" alt="" width="1945" height="687" /></a></p> 
<p>To see exactly where a RUNNING job is at, use the link in the AWS Batch console to direct you to the appropriate location in CloudWatch logs.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_cwl.png"><img class="aligncenter size-full wp-image-2148" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/01/batch_cwl.png" alt="" width="1945" height="1016" /></a></p> 
<b id="toc_7">Completing the batch environment setup</b> 
<p>To finish, create a Lambda function to submit a generic AWS Batch job.</p> 
<p>In the Lambda console, create a Python 2.7 Lambda function named batchSubmitJob. Copy and paste the following code. This is similar to the <code>batch-submit-job-python27</code> Lambda blueprint. Use the LambdaBatchExecutionRole that you created earlier. For more information about creating functions, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html">Step 2.1: Create a Hello World Lambda Function</a>.</p> 
<pre><code class="language-none">from __future__ import print_function
import json
import boto3
batch_client = boto3.client('batch')
def lambda_handler(event, context):
# Log the received event
print(&quot;Received event: &quot; + json.dumps(event, indent=2))
# Get parameters for the SubmitJob call
# http://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html
job_name = event['jobName']
job_queue = event['jobQueue']
job_definition = event['jobDefinition']
# containerOverrides, dependsOn, and parameters are optional
container_overrides = event['containerOverrides'] if event.get('containerOverrides') else {}
parameters = event['parameters'] if event.get('parameters') else {}
depends_on = event['dependsOn'] if event.get('dependsOn') else []
try:
response = batch_client.submit_job(
dependsOn=depends_on,
containerOverrides=container_overrides,
jobDefinition=job_definition,
jobName=job_name,
jobQueue=job_queue,
parameters=parameters
)
# Log response from AWS Batch
print(&quot;Response: &quot; + json.dumps(response, indent=2))
# Return the jobId
event['jobId'] = response['jobId']
return event
except Exception as e:
print(e)
message = 'Error getting Batch Job status'
print(message)
raise Exception(message)</code></pre> 
<b id="toc_8">Conclusion</b> 
<p>In part 3 of this series, you successfully set up your data processing, or batch, environment in AWS Batch. We also provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/batch/setup/create_batch_env.py">Python script</a> in the corresponding GitHub repo that takes care of all of the above CLI arguments for you, as well as building out the job definitions for all of the jobs in the workflow: Isaac, Strelka, SAMtools, and snpEff. You can check the script’s README for additional documentation.</p> 
<p>In Part 4, you’ll cover the workflow layer using <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> and <a href="https://aws.amazon.com/lambda">AWS Lambda</a>.</p> 
<p>Other posts in this four-part series:</p> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
<p>Please leave any questions and comments below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/batch/" rel="tag">Batch</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecr/" rel="tag">ECR</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2142');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2017-05-31T11:13:18+00:00">31 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2091" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2091&amp;disqus_title=Building+Scalable+Applications+and+Microservices%3A+Adding+Messaging+to+Your+Toolbox&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2091');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/Jakub.jpeg"><img class="alignnone size-full wp-image-2082" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/Jakub.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Jakub Wojciak, Senior Software Development Engineer</strong></p> 
<p>Throughout our careers, we developers keep adding new tools to our development toolboxes. These range from the programming languages we learn, use, and become experts in, to architectural components such as HTTP servers, load balancers, and databases (both relational and NoSQL).</p> 
<p>I’d like to kick off a series of posts to introduce you to the architectural components of messaging solutions. Expand your toolbox with this indispensable tool for building modern, scalable services and applications. In the coming months, I will update this post with links that dive deeper into each topic and illustrate messaging use cases using <a href="http://aws.amazon.com/sqs/">Amazon Simple Queue Service (SQS)</a> and<a href="http://aws.amazon.com/sns/"> Amazon Simple Notification Service (SNS)</a>.<span id="more-2091"></span></p> 
<p><strong>What is messaging?</strong><br /> Messaging involves passing messages around, but it’s different from email or text messages, because it is intended for communication between software components, not between people. Enterprise messaging happens at a higher level than that of UDP packets or direct TCP connections (although it does frequently use these protocols).</p> 
<p>A message typically contains the payload — whatever information your application sends: XML, JSON, binary data, and so on. You can also add optional attributes and metadata to a message.</p> 
<p>A SQL or NoSQL database often has a server that stores data. Similarly, a messaging server or service allows a place for your messages to be stored temporarily and transmitted.</p> 
<p><strong>The queue and the topic</strong><br /> For a database service, the main resource is a table. In a messaging service, the two main resources are the queue and the topic.</p> 
<p>A queue is like a buffer. You can put messages into a queue, and you can retrieve messages from a queue. The software that puts messages into a queue is called a message producer and the software that retrieves messages is called a message consumer.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageQueueDiagramFINAL.png"><img class="size-full wp-image-2080 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageQueueDiagramFINAL.png" alt="" width="950" height="396" /></a></p> 
<p>A topic is like a broadcasting station. You can publish messages to a topic, and anyone interested in these messages can subscribe to the topic. Then, the interested parties are notified about the published messages. The software that broadcasts topics is called a topic publisher and the software that subscribes to broadcasts is called a topic subscriber.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageTopicDiagramFINAL.png"><img class="size-full wp-image-2079 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/24/MessageTopicDiagramFINAL.png" alt="" width="694" height="390" /></a><strong>When should you use messaging?</strong><br /> There are some common use cases that might instantly make you think “I should use messaging for that!” Here are some of these use cases (to be discussed in greater detail in future posts).</p> 
<li><strong>Service-to-service communication</strong><br /> You have two services or systems that need to communicate with each other. Let’s say a website (the frontend) has to update customer’s delivery address in a customer relationship management (CRM) system (the backend). Alternatively, you can set up a load balancer in front of the backend CRM service and call its API actions directly from the frontend website. You can also set up a queue and have the frontend website code send messages to the queue and have the backend CRM service to consume them.</li> 
<li><strong>Asynchronous work item backlogs</strong><br /> You have a service that has to track a backlog of actions to be executed. Let’s say a hotel booking system needs to cancel a booking and this process takes a long time (from a few seconds to a minute). You can execute the cancellation synchronously, but then you risk annoying the customer who has to wait for the webpage to load. You can also track all pending cancellations in your database and keep polling and executing cancellations. Alternatively, you can put a message into a queue and have the same hotel booking system consume messages from that queue and perform asynchronous cancellations.</li> 
<li><strong>State change notifications</strong><br /> You have a service that manages some resource and other services that receive updates about changes to those resources. Let’s say an inventory tracking system tracks products stocked in a warehouse. Whenever the stock is sold out, the website must stop offering that product. Whenever the stock is close to being depleted, the purchasing system must place an order for more items. Those systems can keep querying the inventory system to learn about these changes (or even directly examine the database—yuck!). Alternatively, the inventory system can publish notifications about stock changes to a topic and any interested program can subscribe to learn about those changes.</li> 
<p><strong>When should you not use messaging?</strong><br /> According to the law of the instrument, <a href="https://en.wikipedia.org/wiki/Law_of_the_instrument">“If all you have is a hammer, everything looks like a nail.”</a> In other words, it’s important to know when a particular technology won’t fit well with your use case. For example, you have a relational database that you can store large binary files in… but you probably shouldn’t.</p> 
<p>Messaging has its own set of commonly encountered anti-patterns (also to be discussed in greater detail in future posts).</p> 
<li><strong>Message selection</strong><br /> It’s tempting to have the ability to receive messages selectively from a queue —that match a particular set of attributes, or even match an ad-hoc logical query. For example, a service requests a message with a particular attribute because it contains a response to another message that the service sent out. This can lead to a scenario where there are messages in the queue that no one is polling for and are never consumed. (Note: This problem doesn’t exist for message routing or filtering, which are evaluated when messages are&nbsp;sent&nbsp;to a destination queue or topic.)</li> 
<li><strong>Very large messages or files</strong><br /> Most messaging protocols and implementations work best with reasonably sized messages (in the tens or hundreds of KBs). As message sizes grow, it’s best to use a dedicated file (or blob) storage system, such as Amazon S3, and pass a reference to an object in that store in the message itself. A dedicated file (or blob) store typically has much better support for uploading data in chunks with the ability to retry or resume downloads from a particular fragment.</li> 
<p><strong>Key features of messaging systems</strong><br /> Messaging servers and services offer much more than just produce/consume or publish/subscribe functionality. Thus, although it might seem easy to create your own message passing implementation on top of your own data store, consider all the extra features that a full-fledged messaging service provides. Here’s a list of a few but not—by any means—all messaging features:</p> 
<li><strong>Push or pull delivery</strong><br /> Most messaging services provide both options for consuming messages. Pull means continuously querying whether the messaging service has any new messages. Push means that the messaging service notifies you when a message is available. The notification about the new message might be a special packet sent over the messaging protocol. It might also be an HTTP call that the messaging service makes to your API endpoint. You can also use long-polling, which combines both push and pull functionality.</li> 
<li><strong>Dead letter queues</strong><br /> What can your application do if a queue contains a message that you can’t process? Most messaging services allow you to configure a dead-letter queue for messages that you fail to process a certain number of times. This makes it easy to set them aside for further inspection without blocking the queue processing or spending CPU cycles on a message that can never be consumed successfully.</li> 
<li><strong>Delay queues and scheduled messages</strong><br /> What if you want to postpone the processing of a particular message until a specific time? Many messaging services support setting a specific delivery time for a message. If you need to have a common delay for all messages, you can set up a delay queue.</li> 
<li><strong>Ordering, priorities, duplicates</strong><br /> Messaging services provide you with a variety of options that affect the delivery of messages:<p></p> 
<li>A choice between ordered delivery with limited maximum throughput or unordered delivery with virtually unlimited throughput</li> 
<li>Message priorities, where a higher priority message can skip over other messages in the queue</li> 
<li>Transactionality or best-effort acknowledgments of messages</li> 
</ul> </li> 
<p>When designing your system with messaging in mind, ask yourself the following questions:</p> 
<li>Do you need to process messages exactly in the order in which they were sent?</li> 
<li>Could your application parallelize the workload and process messages out of order?</li> 
<li>Do you want your application to consume certain messages at a higher priority than other messages?</li> 
<li>What happens if your application fails to process a message midway? Can you handle processing the same message again?</li> 
<p><strong>How can you get started?</strong></p> 
<p>If you have to configure and start a messaging server, it might take an extra effort to start using messaging. Instead, you can start to use message queues and topics today, using Amazon SQS and Amazon SNS. For more information, visit the following resources, and get started creating message queues and topics with just a few API actions:</p> 
<li><a href="https://aws.amazon.com/sqs/getting-started/">Getting started with Amazon SQS</a></li> 
<li><a href="https://aws.amazon.com/sns/getting-started/">Getting started with Amazon SNS</a></li> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-notification-service/" rel="tag">Amazon Simple Notification Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-queue-service/" rel="tag">Amazon Simple Queue Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sns/" rel="tag">Amazon SNS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sqs/" rel="tag">Amazon SQS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloud-messaging/" rel="tag">cloud messaging</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-queues/" rel="tag">message queues</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-topics/" rel="tag">message topics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/notifications/" rel="tag">notifications</a>, <a href="https://aws.amazon.com/blogs/compute/tag/pubsub/" rel="tag">pub/sub</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2091');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Job Layer (Part 2 of 4)</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-31T09:45:02+00:00">31 MAY 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2123" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2123&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Job+Layer+%28Part+2+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2123');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>This post is the second in a series on how to build a genomics workflow on AWS. In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1</a>, we introduced a general architecture, shown below, and highlighted the three common layers in a batch workflow:</p> 
<li>Job</li> 
<li><a href="https://aws.amazon.com/batch/">Batch</a></li> 
<li>Workflow</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p>In Part 2, you tackle the job layer and package a set of bioinformatics applications into Docker containers and store them in <a href="https://aws.amazon.com/ecr">Amazon ECR</a>. We illustrate some common patterns and best practices for these containers, such as how you can effectively use <a href="https://aws.amazon.com/s3">Amazon S3</a> to exchange data between jobs.<span id="more-2123"></span></p> 
<p>ECR is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, simplifying your development to a production workflow. ECR eliminates the need to operate your own container repositories or worry about scaling the underlying infrastructure by hosting your images in a highly available and scalable architecture. You can integrate with <a href="https://aws.amazon.com/iam">IAM</a> to provide resource-level control for each repository.</p> 
<p>All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<b id="toc_0">Packaging an application in a Docker container</b> 
<p>Genomic analysis often relies on open source software that is developed by academic groups or open-sourced by industry leaders. These applications have a range of requirements for libraries and reference data, and are typically executed using a Linux command line interface.</p> 
<p>Docker containers provide a consistent, reproducible run-time environment for applications to run in, which results in reproducible results. Consequently, containerization of the applications using Docker has received much attention from the bioinformatics community, resulting in the development of application registries such as <a href="https://dockstore.org/">Dockstore.org</a>, <a href="http://biocontainers.pro/">BioContainers</a>, and the <a href="https://galaxyproject.org/admin/tools/docker/">Galaxy Tool Shed</a>. In this post, we cover several good practices for packing genomics applications in Docker containers, including:</p> 
<li>Building your Dockerfile</li> 
<li>Dealing with job multitenancy</li> 
<li>Sharing data between jobs</li> 
<b id="toc_1">Building your Dockerfile</b> 
<p>Your Dockerfile contains all of the commands that you use to package your Docker container. In it, you pick a base image to build from, include any metadata to attribute to the image, describe how to build and configure the environment, and how to access the code running within it.</p> 
<p>We recommend that you adopt a standard set of conventions for your Dockerfiles. Add metadata to describe the contained application, and have an order set of sections for application packaging needs. Here is an example from the provided <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/docker/Dockerfile">Isaac Dockerfile</a>:</p> 
<pre><code class="language-none">FROM python:2.7
# Metadata
LABEL container.base.image=&quot;python:2.7&quot;
LABEL software.name=&quot;iSAAC&quot;
LABEL software.version=&quot;03.17.03.01&quot;
LABEL software.description=&quot;Aligner for sequencing data&quot;
LABEL software.website=&quot;https://github.com/Illumina/Isaac3&quot;
LABEL software.documentation=&quot;https://github.com/Illumina/Isaac3/blob/master/src/markdown/manual.md&quot;
LABEL software.license=&quot;GPLv3&quot;
LABEL tags=&quot;Genomics&quot;
RUN apt-get -y update &amp;&amp; \
apt-get -y install zlib1g-dev gnuplot &amp;&amp; \
apt-get clean
RUN pip install boto3 awscli
RUN git clone https://github.com/Illumina/Isaac3.git &amp;&amp; cd /Isaac3 &amp;&amp; git checkout 6f0191a4e0d4b332e8f34b7ced57dc6e6eb4f2f1
RUN mkdir /iSAAC-build /isaac
WORKDIR /iSAAC-build
RUN /Isaac3/src/configure --prefix=/isaac
RUN make
RUN make install
WORKDIR /
RUN rm -rf /Isaac3 /iSAAC-build
RUN chmod -R +x /isaac/bin/
ENV PATH=&quot;/isaac/bin:$PATH&quot;
ENV LD_LIBRARY_PATH=&quot;/usr/local/lib:/usr/lib:/isaac/libexec:${LD_LIBRARY_PATH}&quot;
COPY isaac/src/run_isaac.py /
COPY common_utils /common_utils
ENTRYPOINT [&quot;python&quot;, &quot;/run_isaac.py&quot;]</code></pre> 
<p>There are many ways to architect Docker containers, but we wanted to consolidate some recommendations that we have observed our customers successfully using:</p> 
<li>Work off of a base image that satisfies most dependencies across a set of applications. In the code provided, we often use the <a href="https://github.com/docker-library/python/blob/a248f4583c5f788e1af02016f762cdc323ee5765/2.7/Dockerfile">official <code>python:2.7</code> image</a> from <a href="https://hub.docker.com/_/python/">Docker Hub</a>.</li> 
<li>In the above Dockerfile, you can see that we have separated out the metadata, underlying system and library dependencies, application-specific dependencies, and the installation requirements into a logical ordered set for easier maintenance. It’s worth noting that if you are building a production application, you would traditionally have a golden set of images to build from and application artifacts to install that have been validated with internal processes.</li> 
<li>Instead of building large datasets into the container itself, we recommend that you download reference data at runtime instead. This allows the decoupling of the algorithm from reference data updates, which could happen nightly. It also allows you to download a subset of all reference data for an algorithm that is specific to the running analysis, for example the particular species under study.</li> 
<li>Provide an application entry point to both expose and limit the default functionality of the container image. In this example, we created a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/src/run_isaac.py">simple Python script</a> that takes care of downloading the dependencies from S3, such as reference data for your analysis, on the fly from a set of provided runtime parameters and stage results back into S3. We dive deeper into this script in the following section.</li> 
<p>Often these shared dependencies are a mix of packaged code that is easily installable (in this case via pip) or private modules usually shared as part of internal source repositories, as is the case here.</p> 
<p>When you build the Docker images, you should take care to include both the necessary private modules within the context of the build. The example below shows how you would accomplish that given a directory context with some dependencies a few levels higher that the Dockerfile. In the project, we provided a <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/isaac/docker/Makefile">Makefile</a> for taking care of some of these items, but for clarity’s sake we issue the necessary Docker commands.</p> 
<pre><code class="language-none"># Given the following partial directory tree structure
# .
# └── tools
#     ├── common\_utils
#     │   ├── __init__.py
#     │   ├── job_utils.py
#     │   └── s3_utils.py
#     └── isaac
#         └── docker
#             ├── Dockerfile
#             └── Makefile
# cd &lt;git repository&gt;/tools/isaac/docker
$ docker build -t isaac:03.17.03.01 \
-t isaac:latest \
-f Dockerfile ../..</code></pre> 
<b id="toc_2">Job multitenancy and sharing data between jobs</b> 
<p>Many bioinformatics tools have been developed to run in any Linux environment, and not necessarily optimized for cloud computing or multitenancy. To overcome these challenges, you can use a simple Python wrapper script for each tool that facilitates the deployment of a job.</p> 
<p>Our tools have several of the same requirements, such as the need to read and write from S3 and deal with job multitenancy. For these common utilities, we built a separate <a href="https://github.com/awslabs/aws-batch-genomics/tree/master/tools/common_utils"><code>common_utils</code></a> package to import during the creation of the Docker image. These utilities deal with the previously mentioned common requirements, such as:</p> 
<li style="list-style-type: none"> 
<li><strong>Container placement</strong></li> 
</ul> </li> 
<p>To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute. The <a href="https://github.com/awslabs/aws-batch-genomics/blob/master/tools/common_utils/s3_utils.py"><code>tools/common_utils/s3_utils.py</code></a> script contains the functions required to leverage S3.</p> 
<li style="list-style-type: none"> 
<li><strong>Multitenancy</strong></li> 
</ul> </li> 
<p>Multiple container jobs may run concurrently on the same instance. In these situations, it’s essential that your job writes to a unique subdirectory. An easy way to do this is to create a subfolder using a UUID and have your application write all of your data there.</p> 
<li style="list-style-type: none"> 
<li><strong>Cleanup</strong></li> 
</ul> </li> 
<p>As your jobs complete and write the output back to S3, you can delete the scratch data on your instance generated by that job. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances. As you ensure that you’re writing to a unique subdirectory in the multitenancy solution, you can simply delete that subdirectory to minimize your storage footprint.</p> 
<p>Each of the Python wrappers takes in all of the requisite data dependencies, residing in S3, as command-line arguments and any other necessary commands to run the tool it wraps. It then handles all of the file downloading, running the bioinformatics tool, and uploading the files back to S3. For more information about each of these tools, see the READMEs for each individual tool.</p> 
<b id="toc_3">Deploying images to Amazon ECR</b> 
<p>Next, publish the Docker images to ECR. The first example below creates an ECR repository and collects the URI you provide to Docker in order to push the image to ECR. If a repository already exists for the container image, query for it, as shown in the second example.</p> 
<pre><code class="language-none"># Create an ECR repository for Isaac, then copy the `repositoryUri` into a variable
$ REPO\_URI=$(aws ecr create-repository \
--repository-name isaac \
--output text --query &quot;repository.repositoryUri&quot;)
# If the repository already exists, then
# query ECR for the `repositoryUri`
$ REPO\_URI=$(aws ecr describe-repositories \
--repository-names isaac \
--output text --query &quot;repositories[0].repositoryUri&quot;)</code></pre> 
<p>After you have a repository URI, you can tag the container image and push it to ECR.</p> 
<pre><code class="language-none">$ eval $(aws ecr get-login)
$ docker tag isaac:latest $(REPO\_URI):latest
$ docker push $(REPO\_URI):latest
$ docker tag isaac:03.17.03.01 $(REPO\_URI):03.17.03.01
$ docker push $(REPO\_URI):03.17.03.01</code></pre> 
<b id="toc_4">Conclusion</b> 
<p>In part 2 of this series, we showed a practical example of packaging up a bioinformatics application within a Docker container, and publishing the container image to an ECR repository. Along the way, we discussed some generally applicable best practices and design decisions specific to this demonstration project.</p> 
<p>Now that you have built one of the Docker containers, you can go ahead and build each of the containers. We have provided all of the necessary code within a GitHub repository and within that repository are specific instructions, as well as some helpers for building container images using GNU make.</p> 
<p>In <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3</a> of this series, you’ll dive deep into the batch processing layer and how to leverage the packaged applications within AWS Batch.</p> 
<p>Other posts in this four-part series:</p> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
<p>Please leave any questions and comments below.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2123');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building High-Throughput Genomics Batch Workflows on AWS: Introduction (Part 1 of 4)</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-05-30T15:03:34+00:00">30 MAY 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2102" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2102&amp;disqus_title=Building+High-Throughput+Genomics+Batch+Workflows+on+AWS%3A+Introduction+%28Part+1+of+4%29&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2102');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg"><img class="alignnone size-full wp-image-2114" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Aaron.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Aaron Friedman is a Healthcare and Life Sciences Partner Solutions Architect at AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg"><img class="alignnone size-full wp-image-2115" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Angel.jpeg" alt="" width="119" height="160" /></a></p> 
<p><strong>Angel Pizarro is a Scientific Computing Technical Business Development Manager at AWS</strong></p> 
<p>Deriving insights from data is foundational to nearly every organization, and many customers process high volumes of data every day. One common requirement of customers in life sciences is the need to analyze these data in a high-throughput fashion without sacrificing time-to-insight.</p> 
<p>Such analyses, which tend to be composed of a series of massively parallel processes (MPP) are well suited to the AWS Cloud. Many AWS customers and partners today, such as <a href="https://aws.amazon.com/solutions/case-studies/illumina/">Illumina</a>, <a href="https://aws.amazon.com/solutions/case-studies/dnanexus/">DNAnexus</a>, <a href="https://aws.amazon.com/solutions/case-studies/seven-bridges-genomics/">Seven Bridges Genomics</a>, <a href="https://www.youtube.com/watch?v=AW_JylyCZLM">AstraZeneca</a>, <a href="https://aws.amazon.com/solutions/case-studies/uc-santa-cruz-genomics-institute/">UCSC Genomics Institute</a>, and <a href="https://www.youtube.com/watch?v=ooMA9J7suh8&amp;index=2&amp;list=PLhr1KZpdzukdeX8mQ2qO73bg6UKQHYsHb">Human Longevity, Inc.</a>, have built scalable and elastic genomics processing solutions on AWS.</p> 
<p>One such use case is genomic sequencing. Modern DNA sequencers, such as Illumina’s <a href="https://www.illumina.com/systems/sequencing-platforms/novaseq.html">NovaSeq</a>, can produce multiple terabytes of raw data each day. The data must then be processed into meaningful information that clinicians and research can act on in a timely fashion. This processing of genomic data is commonly referred to as “secondary analysis”.</p> 
<p>Most common secondary analysis workflows take raw reads generated from sequencers and then process them in a multi-step workflow to identify the variation in a biological sample compared to a standard genome reference. The individual steps are normally similar to the following:<span id="more-2102"></span></p> 
<ol> 
<li>DNA sequences are mapped to a standard genome reference by use of an alignment algorithm, such as Smith-Waterman or Burrows-Wheeler.</li> 
<li>After the sequences are mapped to the reference, the differences are identified as single nucleotide variations, insertions, deletions, or complex structural variation in a process known as variant calling.</li> 
<li>The resulting variants are often combined with other information to identify genomic variants highly correlated with disease or drug response. They might also be analyzed in the context of clinical data such as to identify disease susceptibility or state for a patient.</li> 
<li>Along the way, quality metrics are collected or computed to ensure that the generated data is of the appropriate quality for use in requisite research or clinical settings.</li> 
</ol> 
<p>In this post series, you can build a secondary analysis workflow similar to the one just described. Here is a diagram of the workflow:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/secondary_analysis.png"><img class="aligncenter size-full wp-image-2112" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/secondary_analysis.png" alt="" width="2241" height="554" /></a></p> 
<hr /> 
<b id="toc_1">Secondary analysis is a batch workflow</b> 
<p>At its core, a genomics pipeline is similar to a series of Extract Transform and Load (ETL) steps that convert raw files from a DNA sequencer to a list of variants for one or more individuals. Each step extracts a set of input files from a data source, processes them as a compute-intensive workload (transform), and then loads the output into another location for subsequent storage or analysis.</p> 
<p>These steps are often chained together to build a flexible genomics processing workflow. The files can then be used for downstream analysis, such as population scale analytics with <a href="https://aws.amazon.com/blogs/big-data/interactive-analysis-of-genomic-datasets-using-amazon-athena/">Amazon Athena</a> or <a href="https://blogs.aws.amazon.com/bigdata/post/Tx1GE3J0NATVJ39/Will-Spark-Power-the-Data-behind-Precision-Medicine">Spark on Amazon EMR</a>. These ETL processes can be represented as individual batch steps in an overall workflow.</p> 
<p>When we discuss batch processing with customers, we often focus on the following three layers:</p> 
<p><strong>Jobs (analytical modules):</strong> These jobs are individual units of work that take a set of inputs, run a single process, and produce a set of outputs. In this series, you use Docker containers to define these analytical modules. For genomics, these commonly include alignment, variant calling, quality control, or another module in your workflow. <a href="https://aws.amazon.com/ecs">Amazon ECS</a> is an AWS service that orchestrates and runs these Docker containerized modules on top of <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances.</p> 
<p><strong><a href="https://aws.amazon.com/batch/">Batch engine</a>:</strong> This is a framework for submitting individual analytical modules with the appropriate requirements for computational resources, such as memory and number of CPUs. Each step of the analysis pipeline requires a definition of how to run a job:</p> 
<li>Computational resources (disk, memory, CPU)</li> 
<li>The compute environment to run it in (Docker container, runtime parameters)</li> 
<li>Information about the priority of different jobs</li> 
<li>Any dependencies between jobs</li> 
<p>You can leverage concepts such as container placement and bin packing to maximize performance of your genomic pipeline while concurrently optimizing for cost. We will use <a href="https://aws.amazon.com/batch/">AWS Batch</a> for this layer. AWS Batch dynamically provisions the optimal quantity and type of compute resources (for example, CPU or memory optimized instances) based on the volume and specific resource requirements of the submitted batch jobs.</p> 
<p><strong>Workflow orchestration:</strong> This layer sits on top of the batch engine and allows you to decouple your workflow definition from the execution of the individual jobs. You can envision this layer as a state machine where you define a series of steps and pass appropriate metadata between states. We will use <a href="https://aws.amazon.com/lambda">AWS Lambda</a> to submit the jobs to AWS Batch and use <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a> to define and orchestrate the workflow.</p> 
<b id="toc_2">Architecture</b> 
<p>In the next three posts, you build a genome analysis pipeline using the following architecture. You don’t explicitly build the grayed out section, but we wanted to include them in the diagram as they are natural extensions to the core architecture. We discuss these and other extensions, briefly, in the concluding post. All code related to this blog series can be found in the associated GitHub repository <a href="https://github.com/awslabs/aws-batch-genomics">here</a>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png"><img class="aligncenter size-full wp-image-2106" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/05/30/Batch-overview.png" alt="" width="3237" height="1795" /></a></p> 
<p><strong><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2 covers the job layer</a></strong>. We demonstrate how you can package bioinformatics applications in Docker containers, and discuss best practices when developing these containers for use in a multitenant batch environment.</p> 
<p><strong><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3 dives deep into the batch, or data processing layer</a></strong>. We discuss common considerations for deploying Docker containers to be used in batch analysis as well as demonstrate how you can use AWS Batch for a scalable and elastic batch engine.</p> 
<p><strong>Part 4 dives into workflow layer orchestration</strong>. We show how you might architect that layer with AWS services. You take the components built in parts 2 and 3 and combine them into an entire secondary analysis workflow. This workflow manages dependencies as well as continually checking the progress of existing jobs. We conclude by running a secondary analysis end-to-end for under $1 and discuss some extensions you can build on top of this core workflow.</p> 
<b id="toc_3">What you can expect to learn</b> 
<p>At the end of this series, you will have built a scalable and elastic solution to process genomes on AWS, as well as gained a general understanding of architectural choices available to you. The solution is generally applicable to other workloads, such as image processing. Even non-life science workloads such as trade analytics in financial services can benefit.</p> 
<p>In your solution, you use <a href="https://aws.amazon.com/ec2/spot/">Amazon EC2 Spot Instances</a> to optimize for cost. Spot Instances allow you to bid on spare EC2 compute capacity, which can save up to 90% off of traditional On-Demand prices. In many cases, this translates into the ability to analyze genomes at scale for as low as $1 per analysis.</p> 
<p>Let’s get building!</p> 
<p>Other posts in this four-part series:</p> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Part 1: Introduction</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-job-layer-part-2-of-4/">Part 2: Job Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/">Part 3: Batch Layer</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-workflow-layer-part-4-of-4/">Part 4: Workflow Layer</a></li> 
<p>Please leave any questions and comments below.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2102');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deep Learning on AWS Batch</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Barclay</span></span> | on 
<time property="datePublished" datetime="2017-05-09T15:53:31+00:00">09 MAY 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-batch/" title="View all posts in AWS Batch*"><span property="articleSection">AWS Batch*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/deep-learning-on-aws-batch/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2058" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2058&amp;disqus_title=Deep+Learning+on+AWS+Batch&amp;disqus_url=https://aws.amazon.com/blogs/compute/deep-learning-on-aws-batch/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2058');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Thanks to my colleague&nbsp;Kiuk Chung for this great post on Deep Learning using AWS Batch.</p> 
<p>—-</p> 
<p>GPU instances naturally pair with deep learning as neural network algorithms can take advantage of their massive parallel processing power. AWS provides GPU instance families, such as g2 and p2, which allow customers to run scalable GPU workloads. You can leverage such scalability efficiently with AWS Batch.</p> 
<p><a href="https://aws.amazon.com/batch">AWS Batch</a> manages the underlying compute resources on-your behalf, allowing you to focus on modeling tasks without the overhead of resource management. Compute environments (that is, clusters) in AWS Batch are pools of instances in your account, which AWS Batch dynamically scales up and down, provisioning and terminating instances with respect to the numbers of jobs. This minimizes idle instances, which in turn optimizes cost.</p> 
<p>Moreover, AWS Batch ensures that submitted jobs are scheduled and placed onto the appropriate instance, hence managing the lifecycle of the jobs. With the addition of <a href="https://aws.amazon.com/about-aws/whats-new/2017/04/aws-batch-managed-compute-environments-support-customer-provided-amis/">customer-provided AMIs</a>, AWS Batch users can now take advantage of this elasticity and convenience for jobs that require GPU.</p> 
<p>This post illustrates how you can run GPU-based deep learning workloads on AWS Batch. I walk you through an <a href="http://mxnet.io/tutorials/python/mnist.html">example</a> of training a convolutional neural network (the <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a> architecture), using Apache MXNet to recognize handwritten digits using the MNIST dataset.<span id="more-2058"></span></p> 
<b>Running an MXNet job in AWS Batch</b> 
<p>Apache MXNet is a full-featured, flexibly programmable, and highly scalable deep learning framework that supports state-of-the-art deep models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs).</p> 
<p>There are three steps to running an AWS Batch job:</p> 
<li>Create a custom AMI</li> 
<li>Create AWS Batch entities</li> 
<li>Submit a training job</li> 
<h3>Create a custom AMI</h3> 
<p>Start by creating an AMI that includes the NVIDIA driver and the Amazon ECS agent. In AWS Batch, instances can be launched with the specific AMI of your choice by specifying <em>imageId</em> when you create your compute environment. Because you are running a job that requires GPU, you need an AMI that has the NVIDIA driver installed.</p> 
<p>Choose <strong>Launch Stack</strong> to launch the CloudFormation template in us-east-1 in your account: <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Batch&amp;templateURL=https://s3.amazonaws.com/aws-batch-examples/create-ami.yaml"> <img src="https://s3.amazonaws.com/aws-batch-blog/2017-05-09_mxnet-batch/launch_stack_icon.png" /> </a></p> 
<p>As shown below, take note of the <strong>AMI</strong> value in the <strong>Outputs</strong> tab of the CloudFormation stack. You use this as the <em>imageId</em> value when creating the compute environment in the next section.</p> 
<p><img src="https://s3.amazonaws.com/aws-batch-blog/2017-05-09_mxnet-batch/ami_cloud_formation_screenshot.png" /></p> 
<p>Alternatively, you may follow the AWS Batch documentation to <a href="http://docs.aws.amazon.com/batch/latest/userguide/batch-gpu-ami.html">create a GPU-enabled AMI</a>.</p> 
<h3>Create AWS Batch resources</h3> 
<p>After you have built the AMI, create the following resources:</p> 
<li><a href="http://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html">compute environment</a></li> 
<li><a href="http://docs.aws.amazon.com/batch/latest/userguide/job_queues.html">job queue</a></li> 
<li><a href="http://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html">job definition</a></li> 
<p>A compute environment, is a collection of instances (compute resources) of the same or different instance types. In this case, you create a managed compute environment in which the instances are of type p2.xlarge. For imageId, specify the AMI you built in the previous section.</p> 
<p>Then, create a job queue. In AWS Batch, jobs are submitted to a job queue that are associated to an ordered list of compute environments. After a lower order compute environment is filled, jobs spill over to the next compute environment. For this example, you associate a single compute environment to the job queue.</p> 
<p>Finally, create a job definition, which is a template for a job specification. For those familiar with Amazon ECS, this is analogous to <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html">task definitions</a>. You mount the directory containing the NVIDIA driver on the host to <strong>/usr/local/nvidia</strong> on the container. You also need to set the <em>privileged</em> flag on the container properties.</p> 
<p>The following code creates the aforementioned resources in AWS Batch. For more information, see the <a href="http://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html">AWS Batch User Guide</a>.</p> 
<pre style="padding-left: 30px">git clone https://github.com/awslabs/aws-batch-helpers
cd aws-batch-helpers/gpu-example
python create-batch-entities.py\
--subnets &lt;subnet1,subnet2,…&gt;\
--security-groups &lt;sg1,sg2,…&gt;\
--key-pair &lt;ec2-key-pair&gt;\
--instance-role &lt;instance-role&gt;\
--image-id &lt;custom-AMI-image-id&gt;\
--service-role &lt;service-role-arn&gt; 
</pre> 
<h3>Submit a training job</h3> 
<p>Now you submit a job that trains a convolutional neural network model for handwritten digit recognition. Much like Amazon ECS tasks, jobs in AWS Batch are run as commands in a Docker container. To use MXNet as your deep learning library, you need a Docker image containing MXNet. For this example, use <a href="https://hub.docker.com/r/mxnet/python/">mxnet/python:gpu</a>.</p> 
<p>The submit-job.py script submits the job, and tails the output from CloudWatch Logs.</p> 
<pre style="padding-left: 30px"># cd aws-batch-helpers/gpu-example
python submit-job.py --wait</pre> 
<p>You should see an output that looks like the following:</p> 
<pre>Submitted job [train_imagenet - e1bccebc-76d9-4cd1-885b-667ef93eb1f5] to the job queue [gpu_queue]
Job [train_imagenet - e1bccebc-76d9-4cd1-885b-667ef93eb1f5] is RUNNING.
Output [train_imagenet/e1bccebc-76d9-4cd1-885b-667ef93eb1f5/12030dd3-0734-42bf-a3d1-d99118b401eb]:
================================================================================
[2017-04-25T19:02:57.076Z] INFO:root:Epoch[0] Batch [100]	Speed: 15554.63 samples/sec Train-accuracy=0.861077
[2017-04-25T19:02:57.428Z] INFO:root:Epoch[0] Batch [200]	Speed: 18224.89 samples/sec Train-accuracy=0.954688
[2017-04-25T19:02:57.755Z] INFO:root:Epoch[0] Batch [300]	Speed: 19551.42 samples/sec Train-accuracy=0.965313
[2017-04-25T19:02:58.080Z] INFO:root:Epoch[0] Batch [400]	Speed: 19697.65 samples/sec Train-accuracy=0.969531
[2017-04-25T19:02:58.405Z] INFO:root:Epoch[0] Batch [500]	Speed: 19705.82 samples/sec Train-accuracy=0.968281
[2017-04-25T19:02:58.734Z] INFO:root:Epoch[0] Batch [600]	Speed: 19486.54 samples/sec Train-accuracy=0.971719
[2017-04-25T19:02:59.058Z] INFO:root:Epoch[0] Batch [700]	Speed: 19735.59 samples/sec Train-accuracy=0.973281
[2017-04-25T19:02:59.384Z] INFO:root:Epoch[0] Batch [800]	Speed: 19631.17 samples/sec Train-accuracy=0.976562
[2017-04-25T19:02:59.713Z] INFO:root:Epoch[0] Batch [900]	Speed: 19490.74 samples/sec Train-accuracy=0.979062
[2017-04-25T19:02:59.834Z] INFO:root:Epoch[0] Train-accuracy=0.976774
[2017-04-25T19:02:59.834Z] INFO:root:Epoch[0] Time cost=3.190
[2017-04-25T19:02:59.850Z] INFO:root:Saved checkpoint to &quot;/mnt/model/mnist-0001.params&quot;
[2017-04-25T19:03:00.079Z] INFO:root:Epoch[0] Validation-accuracy=0.969148
================================================================================
Job [train_imagenet - e1bccebc-76d9-4cd1-885b-667ef93eb1f5] SUCCEEDED</pre> 
<p>In reality, you may want to modify the job command to save the trained model artifact to Amazon S3 so that subsequent prediction jobs can generate predictions against the model. For information about how to reference objects in Amazon S3 in your jobs, see the <a href="https://aws.amazon.com/blogs/compute/creating-a-simple-fetch-and-run-aws-batch-job/">Creating a Simple “Fetch &amp; Run” AWS Batch Job</a> post.</p> 
<b>Conclusion</b> 
<p>In this post, I walked you through an example of running a GPU-enabled job in AWS Batch, using MXNet as the deep learning library. AWS Batch exposes primitives to allow you to focus on implementing the most efficient algorithm for your workload. It enables you to manage the lifecycle of submitted jobs and dynamically adapt the infrastructure requirements of your jobs within the specified bounds. It’s easy to take advantage of the horizontal scalability of compute instances provided by AWS in a cost-efficient manner.</p> 
<p>MXNet, on the other hand, provides a rich set of highly optimized and scalable building blocks to start implementing your own deep learning algorithms. Together, you can not only solve problems requiring large neural network models, but also cut down on iteration time by harnessing the seemingly unlimited compute resources in Amazon EC2.</p> 
<p>With AWS Batch managing the resources on your behalf, you can easily implement workloads such as hyper-parameter optimization to fan out tens or even hundreds of searches in parallel to find the best set of model parameters for your problem space. Moreover, because your jobs are run inside Docker containers, you may choose the tools and libraries that best fit your needs, build a Docker image, and submit your jobs using the image of your choice.</p> 
<p>We encourage you to try it yourself and let us know what you think!</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2058');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">ServerlessConf and More!</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><a href="https://aws.amazon.com/blogs/compute/author/bryan-liston/" title="Posts by Bryan Liston" property="name">Bryan Liston</a></span> | on 
<time property="datePublished" datetime="2017-04-17T12:01:57+00:00">17 APR 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/serverless-conference-and-more/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2036" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2036&amp;disqus_title=ServerlessConf+and+More%21&amp;disqus_url=https://aws.amazon.com/blogs/compute/serverless-conference-and-more/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2036');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<b>ServerlessConf Austin</b> 
<p><a href="https://austin.serverlessconf.io/"><strong>ServerlessConf Austin</strong></a> is just around the corner! April 26-28th come join us in Austin at the&nbsp;Zach Topfer Theater. Our very own Tim Wagner, Chris Munns and Randall Hunt will be giving some great talks.</p> 
<blockquote> 
<p>Serverlessconf is a community led conference focused on sharing experiences building applications using serverless architectures. Serverless architectures enable developers to express their creativity and focus on user needs instead of spending time managing infrastructure and servers.</p> 
</blockquote> 
<p><strong>Tim Wagner, GM Serverless Applications</strong>, will be giving a keynote on Friday the 28th, <strong>do not miss this</strong>!!!<br /> <strong>Chris Munns, Sr. Developer Advocate</strong>, will be giving an excellent talk on CI/CD for Serverless Applications.</p> 
<p>Check out the full agenda <a href="https://austin.serverlessconf.io/agenda.html">here</a>!</p> 
<b>AWS Serverless Updates and More!</b> 
<p>Incase you’ve missed out lately on some of our new content such as our new YouTube series “Coding with Sam”, or our new Serverless Special AWS Podcast Series, check them out!</p> 
<li>Coding with Sam Episode 1:&nbsp;<a href="https://www.youtube.com/watch?v=P7i01eqmzrs">How to Do Continuous Integration and Continuous Deployment with AWS Lambda and AWS CodePipeline</a></li> 
<li>Serverless Podcast Special Episode #1 (Serverless Architectures): <a href="https://soundcloud.com/amazon-web-services-306355661/171-serverless-special">AWS Podcast #171</a></li> 
<li>Serverless Podcast Special Episode #2 (Serverless Security): <a href="https://soundcloud.com/amazon-web-services-306355661/178-security-in-serverless-architectures">AWS Podcast #178</a></li> 
<b>Meet SAM!</b> 
<p>We’ve recently come out with a new branding for <a href="https://github.com/awslabs/serverless-application-model">AWS SAM</a> (Serverless Application Model), so please join me in welcoming SAM the Squirrel!</p> 
<p>The goal of AWS SAM is to define a standard application model for serverless applications.</p> 
<p><img class="alignleft" src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/aws_sam_introduction.png" /></p> 
<p>Once again, don’t hesitate to reach out if you have questions, comments, or general feedback.</p> 
<p>Thanks,<br /> <a href="https://twitter.com/listonb">@listonb</a></p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2036');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">How to remove boilerplate validation logic in your REST APIs with Amazon API Gateway request validation</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><a href="https://aws.amazon.com/blogs/compute/author/bryan-liston/" title="Posts by Bryan Liston" property="name">Bryan Liston</a></span> | on 
<time property="datePublished" datetime="2017-04-11T10:54:55+00:00">11 APR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-api-gateway/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/how-to-remove-boilerplate-validation-logic-in-your-rest-apis-with-amazon-api-gateway-request-validation/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2026" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2026&amp;disqus_title=How+to+remove+boilerplate+validation+logic+in+your+REST+APIs+with+Amazon+API+Gateway+request+validation&amp;disqus_url=https://aws.amazon.com/blogs/compute/how-to-remove-boilerplate-validation-logic-in-your-rest-apis-with-amazon-api-gateway-request-validation/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2026');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><img style="width: 15%" src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/logging_rgreen.jpg" alt="" /><br /> <strong>Ryan Green, Software Development Engineer</strong></p> 
<p>Does your API suffer from code bloat or wasted developer time due to implementation of simple input validation rules? One of the necessary but least exciting aspects of building a robust REST API involves implementing basic validation of input data to your API. In addition to increasing the size of the code base, validation logic may require taking on extra dependencies and requires diligence in ensuring the API implementation doesn’t get out of sync with API request/response models and SDKs.</p> 
<p>Amazon API Gateway recently announced the release of <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-request-validation.html">request validators</a>, a simple but powerful new feature that should help to liberate API developers from the undifferentiated effort of implementing basic request validation in their API backends.</p> 
<p>This feature leverages API Gateway <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-create-model.html">models</a> to enable the validation of request payloads against the specified schema, including validation rules as defined in the <a href="http://json-schema.org/latest/json-schema-validation.html">JSON-Schema Validation</a> specification. Request validators also support basic validation of required HTTP request parameters in the URI, query string, and headers.</p> 
<p>When a validation failure occurs, API Gateway fails the API request with an HTTP 400 error, skips the request to the backend integration, and publishes detailed error results in <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch Logs</a>.</p> 
<p>In this post, I show two examples using request validators, validating the request body and the request parameters.<span id="more-2026"></span></p> 
<b id="toc_0">Example: Validating the request body</b> 
<p>For this example, you build a simple API for a simulated stock trading system. This API has a resource, “/orders”, that represents stock purchase orders. An HTTP POST to this resource allows the client to initiate one or more orders.</p> 
<p>A sample request might look like this:</p> 
<p>POST /orders</p> 
<pre><code class="language-javascript">[
&nbsp; {
&nbsp; &nbsp; &quot;account-id&quot;: &quot;abcdef123456&quot;,
&nbsp; &nbsp; &quot;type&quot;: &quot;STOCK&quot;,
&nbsp; &nbsp; &quot;symbol&quot;: &quot;AMZN&quot;,
&nbsp; &nbsp; &quot;shares&quot;: 100,
&nbsp; &nbsp; &quot;details&quot;: {
&nbsp; &nbsp; &nbsp; &quot;limit&quot;: 1000
&nbsp; &nbsp; }
&nbsp; },
&nbsp; {
&nbsp; &nbsp; &quot;account-id&quot;: &quot;zyxwvut987654&quot;,
&nbsp; &nbsp; &quot;type&quot;: &quot;STOCK&quot;,
&nbsp; &nbsp; &quot;symbol&quot;: &quot;BA&quot;,
&nbsp; &nbsp; &quot;shares&quot;: 250,
&nbsp; &nbsp; &quot;details&quot;: {
&nbsp; &nbsp; &nbsp; &quot;limit&quot;: 200
&nbsp; &nbsp; }
&nbsp; }
]</code></pre> 
<p>The JSON-Schema for this request body might look something like this:</p> 
<pre><code class="language-javascript">{
&nbsp; &quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&nbsp; &quot;title&quot;: &quot;Create Orders Schema&quot;,
&nbsp; &quot;type&quot;: &quot;array&quot;,
&nbsp; &quot;minItems&quot;: 1,
&nbsp; &quot;items&quot;: {
&nbsp; &nbsp; &quot;type&quot;: &quot;object&quot;,
&nbsp; &nbsp; &quot;required&quot;: [
&nbsp; &nbsp; &nbsp; &quot;account-id&quot;,
&nbsp; &nbsp; &nbsp; &quot;type&quot;,
&nbsp; &nbsp; &nbsp; &quot;symbol&quot;,
&nbsp; &nbsp; &nbsp; &quot;shares&quot;,
&nbsp; &nbsp; &nbsp; &quot;details&quot;
&nbsp; &nbsp; ],
&nbsp; &nbsp; &quot;properties&quot;: {
&nbsp; &nbsp; &nbsp; &quot;account_id&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;string&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;pattern&quot;: &quot;[A-Za-z]{6}[0-9]{6}&quot;
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;type&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;string&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;enum&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;STOCK&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;BOND&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;CASH&quot;
&nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;symbol&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;string&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;minLength&quot;: 1,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;maxLength&quot;: 4
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;shares&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;number&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;minimum&quot;: 1,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;maximum&quot;: 1000
&nbsp; &nbsp; &nbsp; },
&nbsp; &nbsp; &nbsp; &quot;details&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;object&quot;,
&nbsp; &nbsp; &nbsp; &nbsp; &quot;required&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;limit&quot;
&nbsp; &nbsp; &nbsp; &nbsp; ],
&nbsp; &nbsp; &nbsp; &nbsp; &quot;properties&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;limit&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;type&quot;: &quot;number&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; }
}</code></pre> 
<p>This schema defines the “shape” of the request model but also defines several constraints on the various properties. Here are the validation rules for this schema:</p> 
<li>The root array must have at least 1 item</li> 
<li>All properties are required</li> 
<li>Account ID must match the regular expression format “[A-Za-z]{6}[0-9]{6}”</li> 
<li>Type must be one of STOCK, BOND, or CASH</li> 
<li>Symbol must be a string between 1 and 4 characters</li> 
<li>Shares must be a number between 1 and 1000</li> 
<p>I’m sure you can imagine how this would look in your validation library of choice, or at worst, in a hand-coded implementation.</p> 
<p>Now, try this out with API Gateway request validators. The Swagger definition below defines the REST API, models, and request validators. Its two operations define simple mock integrations to simulate behavior of the stock trading API.</p> 
<p>Note the request validator definitions under the “<em>x-amazon-apigateway-request-validators</em>” extension, and the references to these validators defined on the operation and on the API.</p> 
<pre><code class="language-javascript">{
&quot;swagger&quot;: &quot;2.0&quot;,
&quot;info&quot;: {
&quot;title&quot;: &quot;API Gateway - Request Validation Demo - rpgreen@amazon.com&quot;
},
&quot;schemes&quot;: [
&quot;https&quot;
],
&quot;produces&quot;: [
&quot;application/json&quot;
],
&quot;x-amazon-apigateway-request-validators&quot; : {
&quot;full&quot; : {
&quot;validateRequestBody&quot; : true,
&quot;validateRequestParameters&quot; : true
},
&quot;body-only&quot; : {
&quot;validateRequestBody&quot; : true,
&quot;validateRequestParameters&quot; : false
}
},
&quot;x-amazon-apigateway-request-validator&quot; : &quot;full&quot;,
&quot;paths&quot;: {
&quot;/orders&quot;: {
&quot;post&quot;: {
&quot;x-amazon-apigateway-request-validator&quot;: &quot;body-only&quot;,
&quot;parameters&quot;: [
{
&quot;in&quot;: &quot;body&quot;,
&quot;name&quot;: &quot;CreateOrders&quot;,
&quot;required&quot;: true,
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/CreateOrders&quot;
}
}
],
&quot;responses&quot;: {
&quot;200&quot;: {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Message&quot;
}
},
&quot;400&quot; : {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Message&quot;
}
}
},
&quot;x-amazon-apigateway-integration&quot;: {
&quot;responses&quot;: {
&quot;default&quot;: {
&quot;statusCode&quot;: &quot;200&quot;,
&quot;responseTemplates&quot;: {
&quot;application/json&quot;: &quot;{\&quot;message\&quot; : \&quot;Orders successfully created\&quot;}&quot;
}
}
},
&quot;requestTemplates&quot;: {
&quot;application/json&quot;: &quot;{\&quot;statusCode\&quot;: 200}&quot;
},
&quot;passthroughBehavior&quot;: &quot;never&quot;,
&quot;type&quot;: &quot;mock&quot;
}
},
&quot;get&quot;: {
&quot;parameters&quot;: [
{
&quot;in&quot;: &quot;header&quot;,
&quot;name&quot;: &quot;Account-Id&quot;,
&quot;required&quot;: true
},
{
&quot;in&quot;: &quot;query&quot;,
&quot;name&quot;: &quot;type&quot;,
&quot;required&quot;: false
}
],
&quot;responses&quot;: {
&quot;200&quot; : {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Orders&quot;
}
},
&quot;400&quot; : {
&quot;schema&quot;: {
&quot;$ref&quot;: &quot;#/definitions/Message&quot;
}
}
},
&quot;x-amazon-apigateway-integration&quot;: {
&quot;responses&quot;: {
&quot;default&quot;: {
&quot;statusCode&quot;: &quot;200&quot;,
&quot;responseTemplates&quot;: {
&quot;application/json&quot;: &quot;[{\&quot;order-id\&quot; : \&quot;qrx987\&quot;,\n   \&quot;type\&quot; : \&quot;STOCK\&quot;,\n   \&quot;symbol\&quot; : \&quot;AMZN\&quot;,\n   \&quot;shares\&quot; : 100,\n   \&quot;time\&quot; : \&quot;1488217405\&quot;,\n   \&quot;state\&quot; : \&quot;COMPLETED\&quot;\n},\n{\n   \&quot;order-id\&quot; : \&quot;foo123\&quot;,\n   \&quot;type\&quot; : \&quot;STOCK\&quot;,\n   \&quot;symbol\&quot; : \&quot;BA\&quot;,\n   \&quot;shares\&quot; : 100,\n   \&quot;time\&quot; : \&quot;1488213043\&quot;,\n   \&quot;state\&quot; : \&quot;COMPLETED\&quot;\n}\n]&quot;
}
}
},
&quot;requestTemplates&quot;: {
&quot;application/json&quot;: &quot;{\&quot;statusCode\&quot;: 200}&quot;
},
&quot;passthroughBehavior&quot;: &quot;never&quot;,
&quot;type&quot;: &quot;mock&quot;
}
}
}
},
&quot;definitions&quot;: {
&quot;CreateOrders&quot;: {
&quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&quot;title&quot;: &quot;Create Orders Schema&quot;,
&quot;type&quot;: &quot;array&quot;,
&quot;minItems&quot; : 1,
&quot;items&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;$ref&quot; : &quot;#/definitions/Order&quot;
}
},
&quot;Orders&quot; : {
&quot;type&quot;: &quot;array&quot;,
&quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&quot;title&quot;: &quot;Get Orders Schema&quot;,
&quot;items&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;order_id&quot;: { &quot;type&quot;: &quot;string&quot; },
&quot;time&quot; : { &quot;type&quot;: &quot;string&quot; },
&quot;state&quot; : {
&quot;type&quot;: &quot;string&quot;,
&quot;enum&quot;: [
&quot;PENDING&quot;,
&quot;COMPLETED&quot;
]
},
&quot;order&quot; : {
&quot;$ref&quot; : &quot;#/definitions/Order&quot;
}
}
}
},
&quot;Order&quot; : {
&quot;type&quot;: &quot;object&quot;,
&quot;$schema&quot;: &quot;http://json-schema.org/draft-04/schema#&quot;,
&quot;title&quot;: &quot;Schema for a single Order&quot;,
&quot;required&quot;: [
&quot;account-id&quot;,
&quot;type&quot;,
&quot;symbol&quot;,
&quot;shares&quot;,
&quot;details&quot;
],
&quot;properties&quot; : {
&quot;account-id&quot;: {
&quot;type&quot;: &quot;string&quot;,
&quot;pattern&quot;: &quot;[A-Za-z]{6}[0-9]{6}&quot;
},
&quot;type&quot;: {
&quot;type&quot; : &quot;string&quot;,
&quot;enum&quot; : [
&quot;STOCK&quot;,
&quot;BOND&quot;,
&quot;CASH&quot;]
},
&quot;symbol&quot; : {
&quot;type&quot;: &quot;string&quot;,
&quot;minLength&quot;: 1,
&quot;maxLength&quot;: 4
},
&quot;shares&quot;: {
&quot;type&quot;: &quot;number&quot;,
&quot;minimum&quot;: 1,
&quot;maximum&quot;: 1000
},
&quot;details&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;required&quot;: [
&quot;limit&quot;
],
&quot;properties&quot;: {
&quot;limit&quot;: {
&quot;type&quot;: &quot;number&quot;
}
}
}
}
},
&quot;Message&quot;: {
&quot;type&quot;: &quot;object&quot;,
&quot;properties&quot;: {
&quot;message&quot; : {
&quot;type&quot; : &quot;string&quot;
}
}
}
}
}</code></pre> 
<p>To create the demo API, run the following commands (requires the <a href="https://aws.amazon.com/cli/">AWS CLI</a>):</p> 
<pre><code class="language-bash">git clone https://github.com/rpgreen/apigateway-validation-demo.git
cd apigateway-validation-demo
aws apigateway import-rest-api --body &quot;file://validation-swagger.json&quot; --region us-east-1
export API_ID=[API ID from last step]
aws apigateway create-deployment --rest-api-id $API_ID --stage-name test --region us-east-1</code></pre> 
<p>Make some requests to this API. Here’s the happy path with valid request body:</p> 
<pre><code class="language-none">curl -v -H &quot;Content-Type: application/json&quot; -X POST -d ' [&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;{&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;account-id&quot;:&quot;abcdef123456&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;:&quot;STOCK&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;symbol&quot;:&quot;AMZN&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;shares&quot;:100,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;details&quot;:{&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;limit&quot;:1000
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;}
]' https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 200 OK</p> 
<pre><code class="language-none">{&quot;message&quot; : &quot;Orders successfully created&quot;}</code></pre> 
<p>Put the request validator to the test. Notice the errors in the payload:</p> 
<pre><code class="language-none">curl -v -H &quot;Content-Type: application/json&quot; -X POST -d '[
{
&quot;account-id&quot;: &quot;abcdef123456&quot;,
&quot;type&quot;: &quot;foobar&quot;,
&quot;symbol&quot;: &quot;thisstringistoolong&quot;,
&quot;shares&quot;: 999999,
&quot;details&quot;: {
&quot;limit&quot;: 1000
}
}
]' https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 400 Bad Request</p> 
<pre><code class="language-none">{&quot;message&quot;: &quot;Invalid request body&quot;}</code></pre> 
<p>When you inspect the CloudWatch Logs entries for this API, you see the detailed error messages for this payload. Run the following command:</p> 
<pre><code class="language-none">pip install apilogs
apilogs get --api-id $API_ID --stage test --watch --region us-east-1`</code></pre> 
<p>The CloudWatch Logs entry for this request reveals the specific validation errors:</p> 
<p>“Request body does not match model schema for content type application/json: [numeric instance is greater than the required maximum (maximum: 1000, found: 999999), string “thisstringistoolong” is too long (length: 19, maximum allowed: 4), instance value (“foobar”) not found in enum (possible values: [“STOCK”,”BOND”,”CASH”])]”</p> 
<p>Note on Content-Type:</p> 
<p>Request body validation is performed according to the configured request Model which is selected by the value of the request ‘Content-Type’ header. In order to enforce validation and restrict requests to explicitly-defined content types, it’s a good idea to use strict request passthrough behavior (‘”passthroughBehavior”: “never”‘), so that unsupported content types fail with 415 “Unsupported Media Type” response.</p> 
<b id="toc_1">Example: Validating the request parameters</b> 
<p>For the next example, add a GET method to the /orders resource that returns the list of purchase orders. This method has an optional query string parameter (type) and a required header parameter (Account-Id).</p> 
<p>The request validator configured for the GET method is set to validate incoming request parameters. This performs basic validation on the required parameters, ensuring that the request parameters are <em>present and non-blank</em>.</p> 
<p>Here are some example requests.</p> 
<p><strong>Happy path:</strong></p> 
<pre><code class="language-none">curl -v -H &quot;Account-Id: abcdef123456&quot; &quot;https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders?type=STOCK&quot;</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 200 OK</p> 
<pre><code class="language-none">[{&quot;order-id&quot; : &quot;qrx987&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;AMZN&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488217405&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
},
{
&quot;order-id&quot; : &quot;foo123&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;BA&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488213043&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
}]</code></pre> 
<p><strong>Omitting optional type parameter:</strong></p> 
<pre><code class="language-none">curl -v -H &quot;Account-Id: abcdef123456&quot; &quot;https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders&quot;</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 200 OK</p> 
<pre><code class="language-none">[{&quot;order-id&quot; : &quot;qrx987&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;AMZN&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488217405&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
},
{
&quot;order-id&quot; : &quot;foo123&quot;,
&quot;type&quot; : &quot;STOCK&quot;,
&quot;symbol&quot; : &quot;BA&quot;,
&quot;shares&quot; : 100,
&quot;time&quot; : &quot;1488213043&quot;,
&quot;state&quot; : &quot;COMPLETED&quot;
}]</code></pre> 
<p><strong>Omitting required Account-Id parameter:</strong></p> 
<pre><code class="language-none">curl -v &quot;https://$API_ID.execute-api.us-east-1.amazonaws.com/test/orders?type=STOCK&quot;</code></pre> 
<p>Response:</p> 
<p>HTTP/1.1 400 Bad Request</p> 
<pre><code class="language-none">{&quot;message&quot;: &quot;Missing required request parameters: [Account-Id]&quot;}</code></pre> 
<b id="toc_2">Conclusion</b> 
<p>Request validators should help API developers to build better APIs by allowing them to remove boilerplate validation logic from backend implementations and focus on actual business logic and deep validation. This should further reduce the size of the API codebase and also help to ensure that API models and validation logic are kept in sync.</p> 
<p>Please forward any questions or feedback to the API Gateway team through <a href="https://aws.amazon.com/contact-us/">AWS Support</a> or on the <a href="https://forums.aws.amazon.com/forum.jspa?forumID=199">AWS Forums</a>.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2026');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Scaling Your Desktop Application Streams with Amazon AppStream 2.0</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><a href="https://aws.amazon.com/blogs/compute/author/bryan-liston/" title="Posts by Bryan Liston" property="name">Bryan Liston</a></span> | on 
<time property="datePublished" datetime="2017-04-04T06:20:28+00:00">04 APR 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/desktop-app-streaming/amazon-appstream-2-0/" title="View all posts in Amazon AppStream 2.0*"><span property="articleSection">Amazon AppStream 2.0*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/scaling-your-desktop-application-streams-with-amazon-appstream-2-0/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2016" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2016&amp;disqus_title=Scaling+Your+Desktop+Application+Streams+with+Amazon+AppStream+2.0&amp;disqus_url=https://aws.amazon.com/blogs/compute/scaling-your-desktop-application-streams-with-amazon-appstream-2-0/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2016');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/dsuryan.png" /><br /> <strong>Deepak Sury, Principal Product Manager – Amazon AppStream 2.0</strong></p> 
<p>Want to stream desktop applications to a web browser, without rewriting them? <a href="https://aws.amazon.com/appstream2/">Amazon AppStream 2.0</a> is a fully managed, secure, application streaming service. An easy way to learn what the service does is to <a href="https://console.aws.amazon.com/appstream2/tryitnow/home">try out the end-user experience, at no cost</a>.</p> 
<p>In this post, I describe how you can scale your AppStream 2.0 environment, and achieve some cost optimizations. I also add some setup and monitoring tips.<span id="more-2016"></span></p> 
<h4 id="toc_0">AppStream 2.0 workflow</h4> 
<p>You import your applications into AppStream 2.0 using an image builder. The image builder allows you to connect to a desktop experience from within the AWS Management Console, and then install and test your apps. Then, create an image that is a snapshot of the image builder.</p> 
<p>After you have an image containing your applications, select an instance type and launch a fleet of streaming instances. Each instance in the fleet is used by only one user, and you match the instance type used in the fleet to match the needed application performance. Finally, attach the fleet to a stack to set up user access. The following diagram shows the role of each resource in the workflow.</p> 
<p style="text-align: center"><em>Figure 1: Describing an AppStream 2.0 workflow</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstream_figure11.png" alt="appstreamscaling_1.png" /></p> 
<b id="toc_1">Setting up AppStream 2.0</b> 
<p>To get started, <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/getting-started.html">set up an example AppStream 2.0 stack</a> or use the Quick Links on the console. For this example, I named my stack ds-sample, selected a sample image, and chose the stream.standard.medium instance type. You can explore the resources that you set up in the AWS console, or use the describe-stacks and describe-fleets commands as follows:</p> 
<p style="text-align: center"><em>Figure 2: Describing an AppStream 2.0 stack</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_1.png" alt="appstreamscaling_1.png" /></p> 
<p style="text-align: center"><em>Figure 3: Describing an AppStream 2.0 fleet</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/figure2.png" alt="appstreamscaling_2.43%20AM" /></p> 
<p>To set up user access to your streaming environment, you can use your existing <a title="undefined" href="https://docs.aws.amazon.com/appstream2/latest/developerguide/external-identity-providers.html" target="null">SAML 2.0 compliant directory</a>. Your users can then use their existing credentials to log in. Alternatively, to quickly test a streaming connection, or to start a streaming session from your own website, you can create a streaming URL. In the console, choose <strong>Stacks</strong>, <strong>Actions</strong>, <strong>Create URL</strong>, or call create-streaming-url as follows:</p> 
<p style="text-align: center"><em>Figure 4: Creating a streaming URL</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_3.png" alt="appstreamscaling_3.png" /></p> 
<p>You can paste the streaming URL into a browser, and open any of the displayed applications.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/figure3_part2.png" alt="appstreamscaling_4.30%20PM" /></p> 
<p>Now that you have a sample environment set up, here are a few tips on scaling.</p> 
<b id="toc_2">Scaling and cost optimization for AppStream 2.0</b> 
<p>To provide an instant-on streaming connection, the instances in an AppStream 2.0 fleet are always running. You are charged for running instances, and each running instance can serve exactly one user at any time. To optimize your costs, match the number of running instances to the number of users who want to stream apps concurrently. This section walks through three options for doing this:</p> 
<li>Fleet Auto Scaling</li> 
<li>Fixed fleets based on a schedule</li> 
<li>Fleet Auto Scaling with schedules</li> 
<b id="toc_3">Fleet Auto Scaling</b> 
<p>To dynamically update the number of running instances, you can use Fleet Auto Scaling. This feature allows you to scale the size of the fleet automatically between a minimum and maximum value based on demand. This is useful if you have user demand that changes constantly, and you want to scale your fleet automatically to match this demand. For examples about setting up and managing scaling policies, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/autoscaling.html">Fleet Auto Scaling</a>.</p> 
<p>You can trigger changes to the fleet through the available Amazon CloudWatch metrics:</p> 
<li>CapacityUtilization – the percentage of running instances already used.</li> 
<li>AvailableCapacity – the number of instances that are unused and can receive connections from users.</li> 
<li>InsufficientCapacityError – an error that is triggered when there is no available running instance to match a user’s request.</li> 
<p>You can create and attach scaling policies using the AWS SDK or AWS Management Console. I find it convenient to set up the policies using the console. Use the following steps:</p> 
<ol> 
<li>In the AWS Management Console, open AppStream 2.0.</li> 
<li>Choose <strong>Fleets</strong>, select a fleet, and choose <strong>Scaling Policies</strong>.</li> 
<li>For <strong>Minimum capacity</strong> and <strong>Maximum capacity</strong>, enter values for the fleet.</li> 
</ol> 
<p style="text-align: center"><em>Figure 5: Fleets tab for setting scaling policies</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_5.png" alt="appstreamscaling_5.png" /></p> 
<ol> 
<li>Create scale out and scale in policies by choosing <strong>Add Policy</strong> in each section.</li> 
</ol> 
<p style="text-align: center"><em>Figure 6: Adding a scale out policy</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_6.png" alt="appstreamscaling_6.png" /></p> 
<p style="text-align: center"><em>Figure 7: Adding a scale in policy</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_7.png" alt="appstreamscaling_7.png" /></p> 
<p>After you create the policies, they are displayed as part of your fleet details.</p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_8.png" alt="appstreamscaling_8.png" /></p> 
<p>The scaling policies are triggered by CloudWatch alarms. These alarms are automatically created on your behalf when you create the scaling policies using the console. You can view and modify the alarms via the CloudWatch console.</p> 
<p style="text-align: center"><em>Figure 8: CloudWatch alarms for triggering fleet scaling</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_9.png" alt="appstreamscaling_9.png" /></p> 
<b id="toc_4">Fixed fleets based on a schedule</b> 
<p>An alternative option to optimize costs and respond to predictable demand is to fix the number of running instances based on the time of day or day of the week. This is useful if you have a fixed number of users signing in at different times of the day― scenarios such as a training classes, call center shifts, or school computer labs. You can easily set the number of instances that are running using the AppStream 2.0 update-fleet command. Update the Desired value for the compute capacity of your fleet. The number of Running instances changes to match the Desired value that you set, as follows:</p> 
<p style="text-align: center"><em>Figure 9: Updating desired capacity for your fleet</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_10.png" alt="appstreamscaling_10.png" /></p> 
<p>Set up a Lambda function to update your fleet size automatically. Follow the example below to set up your own functions. If you haven’t used Lambda before, see <a href="http://docs.aws.amazon.com/lambda/latest/dg/getting-started-create-function.html">Step 2: Create a HelloWorld Lambda Function and Explore the Console</a>.</p> 
<p><strong>To create a function to change the fleet size</strong></p> 
<ol> 
<li>In the Lambda console, choose <strong>Create a Lambda function</strong>.</li> 
<li>Choose the <strong>Blank Function</strong> blueprint. This gives you an empty blueprint to which you can add your code.</li> 
<li>Skip the trigger section for now. Later on, you can add a trigger based on time, or any other input.</li> 
<li>In the <strong>Configure function</strong> section: 
<ol> 
<li>Provide a name and description.</li> 
<li>For <strong>Runtime</strong>, choose Node.js 4.3.</li> 
<li>Under <strong>Lambda function handler and role</strong>, choose <strong>Create a custom role</strong>.</li> 
<li>In the IAM wizard, enter a role name, for example Lambda-AppStream-Admin. Leave the defaults as is.</li> 
<li>After the IAM role is created, attach an AppStream 2.0 managed policy “AmazonAppStreamFullAccess” to the role. For more information, see <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-using.html">Working with Managed Policies</a>. This allows Lambda to call the AppStream 2.0 API on your behalf. You can edit and attach your own IAM policy, to limit access to only actions you would like to permit. To learn more, see <a href="http://docs.aws.amazon.com/appstream2/latest/developerguide/controlling-access.html">Controlling Access to Amazon AppStream 2.0</a>.</li> 
<li>Leave the default values for the rest of the fields, and choose <strong>Next</strong>, <strong>Create function</strong>.</li> 
</ol> </li> 
<li>To change the AppStream 2.0 fleet size, choose <strong>Code</strong> and add some sample code, as follows: 
<pre><code class="language-javascript">'use strict';
/**
This AppStream2 Update-Fleet blueprint sets up a schedule for a streaming fleet
**/
const AWS = require('aws-sdk');
const appstream = new AWS.AppStream();
const fleetParams = {
Name: 'ds-sample-fleet', /* required */
ComputeCapacity: {
DesiredInstances: 1 /* required */
}
};
exports.handler = (event, context, callback) =&gt; {
console.log('Received event:', JSON.stringify(event, null, 2));
var resource = event.resources[0];
var increase = resource.includes('weekday-9am-increase-capacity')
try {
if (increase) {
fleetParams.ComputeCapacity.DesiredInstances = 3
} else {
fleetParams.ComputeCapacity.DesiredInstances = 1
}
appstream.updateFleet(fleetParams, (error, data) =&gt; {
if (error) {
console.log(error, error.stack);
return callback(error);
}
console.log(data);
return callback(null, data);
});
} catch (error) {
console.log('Caught Error: ', error);
callback(error);
}
};</code></pre> 
<li>Test the code. Choose <strong>Test</strong> and use the “Hello World” test template. The first time you do this, choose <strong>Save and Test</strong>. Create a test input like the following to trigger the scaling update.<img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_11.png" alt="appstreamscaling_11.png" /></li> 
<li>You see output text showing the result of the update-fleet call. You can also use the CLI to check the effect of executing the Lambda function.</li> 
</ol> 
<p>Next, to set up a time-based schedule, set a trigger for invoking the Lambda function.</p> 
<p><strong>To set a trigger for the Lambda function</strong></p> 
<ol> 
<li>Choose <strong>Triggers</strong>, <strong>Add trigger</strong>.</li> 
<li>Choose <strong>CloudWatch Events – Schedule</strong>.</li> 
<li>Enter a rule name, such as “weekday-9am-increase-capacity”, and a description. For <strong>Schedule expression</strong>, choose <strong>cron</strong>. You can edit the value for the cron later.</li> 
<li>After the trigger is created, open the event <strong>weekday-9am-increase-capacity</strong>.</li> 
<li>In the CloudWatch console, edit the event details. To scale out the fleet at 9 am on a weekday, you can adjust the time to be: 00 17 ? * MON-FRI *. (If you’re not in Seattle (Pacific Time Zone), change this to another specific time zone).</li> 
<li>You can also add another event that triggers at the end of a weekday.</li> 
</ol> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/appstreamscaling_12.png" alt="appstreamscaling_12.png" /></p> 
<p>This setup now triggers scale-out and scale-in automatically, based on the time schedule that you set.</p> 
<b id="toc_5">Fleet Auto Scaling with schedules</b> 
<p>You can choose to combine both the fleet scaling and time-based schedule approaches to manage more complex scenarios. This is useful to manage the number of running instances based on business and non-business hours, and still respond to changes in demand. You could programmatically change the minimum and maximum sizes for your fleet based on time of day or day of week, and apply the default scale-out or scale-in policies. This allows you to respond to predictable minimum demand based on a schedule.</p> 
<p>For example, at the start of a work day, you might expect a certain number of users to request streaming connections at one time. You wouldn’t want to wait for the fleet to scale out and meet this requirement. However, during the course of the day, you might expect the demand to scale in or out, and would want to match the fleet size to this demand.</p> 
<p>To achieve this, set up the scaling polices via the console, and create a Lambda function to trigger changes to the minimum, maximum, and desired capacity for your fleet based on a schedule. Replace the code for the Lambda function that you created earlier with the following code:</p> 
<pre><code class="language-javascript">'use strict';
/**
This AppStream2 Update-Fleet function sets up a schedule for a streaming fleet
**/
const AWS = require('aws-sdk');
const appstream = new AWS.AppStream();
const applicationAutoScaling = new AWS.ApplicationAutoScaling();
const fleetParams = {
Name: 'ds-sample-fleet', /* required */
ComputeCapacity: {
DesiredInstances: 1 /* required */
}
};
var scalingParams = {
ResourceId: 'fleet/ds-sample-fleet', /* required - fleet name*/
ScalableDimension: 'appstream:fleet:DesiredCapacity', /* required */
ServiceNamespace: 'appstream', /* required */
MaxCapacity: 1,
MinCapacity: 6,
RoleARN: 'arn:aws:iam::659382443255:role/service-role/ApplicationAutoScalingForAmazonAppStreamAccess'
};
exports.handler = (event, context, callback) =&gt; {
console.log('Received this event now:', JSON.stringify(event, null, 2));
var resource = event.resources[0];
var increase = resource.includes('weekday-9am-increase-capacity')
try {
if (increase) {
//usage during business hours - start at capacity of 10 and scale
//if required. This implies at least 10 users can connect instantly. 
//More users can connect as the scaling policy triggers addition of
//more instances. Maximum cap is 20 instances - fleet will not scale
//beyond 20. This is the cap for number of users.
fleetParams.ComputeCapacity.DesiredInstances = 10
scalingParams.MinCapacity = 10
scalingParams.MaxCapacity = 20
} else {
//usage during non-business hours - start at capacity of 1 and scale
//if required. This implies only 1 user can connect instantly. 
//More users can connect as the scaling policy triggers addition of
//more instances. 
fleetParams.ComputeCapacity.DesiredInstances = 1
scalingParams.MinCapacity = 1
scalingParams.MaxCapacity = 10
}
//Update minimum and maximum capacity used by the scaling policies
applicationAutoScaling.registerScalableTarget(scalingParams, (error, data) =&gt; {
if (error) console.log(error, error.stack); 
else console.log(data);                     
});
//Update the desired capacity for the fleet. This sets 
//the number of running instances to desired number of instances
appstream.updateFleet(fleetParams, (error, data) =&gt; {
if (error) {
console.log(error, error.stack);
return callback(error);
}
console.log(data);
return callback(null, data);
});
} catch (error) {
console.log('Caught Error: ', error);
callback(error);
}
};
</code></pre> 
<p>Note: To successfully execute this code, you need to add IAM policies to the role used by the Lambda function. The policies allow Lambda to call the Application Auto Scaling service on your behalf.</p> 
<p style="text-align: center"><em>Figure 10: Inline policies for using Application Auto Scaling with Lambda</em></p> 
<pre><code class="language-none">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Effect&quot;: &quot;Allow&quot;,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Action&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;iam:PassRole&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Resource&quot;: &quot;*&quot;
&nbsp;&nbsp; }
]
}
{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Effect&quot;: &quot;Allow&quot;,&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Action&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;application-autoscaling:*&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;Resource&quot;: &quot;*&quot;
&nbsp;&nbsp; }
]
}</code></pre> 
<b id="toc_6">Monitoring usage</b> 
<p>After you have set up scaling for your fleet, you can use CloudWatch metrics with AppStream 2.0, and create a dashboard for monitoring. This helps optimize your scaling policies over time based on the amount of usage that you see.</p> 
<p>For example, if you were very conservative with your initial set up and over-provisioned resources, you might see long periods of low fleet utilization. On the other hand, if you set the fleet size too low, you would see high utilization or errors from insufficient capacity, which would block users’ connections. You can view CloudWatch metrics for up to 15 months, and drive adjustments to your fleet scaling policy.</p> 
<p style="text-align: center"><em>Figure 11: Dashboard with custom Amazon CloudWatch metrics</em></p> 
<p><img src="https://awscomputeblogimages.s3-us-west-2.amazonaws.com/appstreamscaling/figure6.png" alt="appstreamscaling_13.53%20PM" /></p> 
<b id="toc_7">Summary</b> 
<p>These are just a few ideas for scaling AppStream 2.0 and optimizing your costs. Let us know if these are useful, and if you would like to see similar posts. If you have comments about the service, please post your feedback on the <a href="https://forums.aws.amazon.com/forum.jspa?forumID=233">AWS forum for AppStream 2.0</a>.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2016');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
