<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="aws-blogs" class="layout-inner aws-blogs">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Simplify Your Pub/Sub Messaging with Amazon SNS Message Filtering</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-22T11:56:01+00:00">22 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/simplify-pubsub-messaging-with-amazon-sns-message-filtering/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3362" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3362&amp;disqus_title=Simplify+Your+Pub%2FSub+Messaging+with+Amazon+SNS+Message+Filtering&amp;disqus_url=https://aws.amazon.com/blogs/compute/simplify-pubsub-messaging-with-amazon-sns-message-filtering/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3362');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by:&nbsp;Stephen Liedig, Senior Solutions Architect, ANZ Public Sector, and&nbsp;Otavio Ferreira, Manager, Amazon Simple Notification Service</em></p> 
<p>Want to make your cloud-native applications scalable, fault-tolerant, and highly available? Recently, we wrote a couple of posts about using AWS messaging services <a href="https://aws.amazon.com/sqs/">Amazon SQS</a>&nbsp;and&nbsp;<a href="https://aws.amazon.com/sns/">Amazon SNS</a> to address messaging patterns for loosely coupled communication between highly cohesive components. For more information, see:</p> 
<li><a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/">Messaging Fanout Pattern for Serverless Architectures Using Amazon SNS</a></li> 
<li><a href="https://aws.amazon.com/blogs/compute/event-driven-computing-with-amazon-sns-compute-storage-database-and-networking-services/">Event-Driven Computing with Amazon SNS and AWS Compute, Storage, Database, and Networking Services</a></li> 
<p>Today, AWS is releasing a new message filtering functionality for SNS.&nbsp;This new feature simplifies the pub/sub messaging architecture by offloading the filtering logic from subscribers, as well as the routing logic from publishers, to SNS.</p> 
<p>In this post, we walk you through the new message filtering feature, and how to use it to clean up unnecessary logic in your components, and reduce the number of topics in your architecture.<span id="more-3362"></span></p> 
<b>Topic-based filtering</b> 
<p>SNS is a fully managed <a href="https://aws.amazon.com/pub-sub-messaging/">pub/sub messaging</a> service that lets you fan out messages to large numbers of recipients at one time, using topics. SNS topics support a variety of subscription types, allowing you to push messages to SQS&nbsp;queues, <a href="https://aws.amazon.com/lambda">AWS Lambda</a> functions, HTTP endpoints, email addresses, and mobile devices (SMS, push).</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_1.png" /></a></p> 
<p>In the above scenario, every subscriber receives the same message published to the topic, allowing them to process the message independently. For many use cases, this is sufficient.</p> 
<p>However, in more complex scenarios, the subscriber may only be interested in a subset of the messages being published. The onus, in that case, is on each subscriber to ensure that they are filtering and only processing those messages in which they are actually interested.</p> 
<p>To avoid this additional filtering logic on each subscriber, many organizations have adopted a practice in which the publisher is now responsible for routing different types of messages to different topics. However, as depicted in the following diagram, this topic-based filtering practice can lead to overly complicated publishers, topic proliferation, and additional overhead in provisioning and&nbsp;managing your SNS topics.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_2.png" /></a></p> 
<b>Attribute-based filtering</b> 
<p>To leverage the new message filtering capability, SNS requires the publisher to set message attributes and each subscriber to set a subscription attribute (a&nbsp;subscription filter policy). When the publisher posts a new message to the topic, SNS attempts to match the incoming message attributes to the filter policy set on each subscription, to determine whether a particular subscriber is interested in that incoming event. If there is a match, SNS then pushes the message to the subscriber in question. The new attribute-based message filtering approach is depicted in the following diagram.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_3.png" /></a></p> 
<b>Message filtering in action</b> 
<p>Look at how message filtering works. The following example is based on a sports merchandise ecommerce website, which publishes a variety of events to an SNS topic. The events range from checkout events (triggered when orders are placed or canceled) to buyers’ navigation events (triggered when product pages are visited).&nbsp;The code below is based on the existing <a href="https://aws.amazon.com/tools/#sdk">AWS SDK for Python</a>.</p> 
<p>First, create the single SNS topic to which all shopping events are published.</p> 
<code class="lang-python">topic_arn = sns.create_topic(
Name='ShoppingEvents'
)['TopicArn']</code> 
<p>Next, subscribe the endpoints that will be listening to those shopping events. The first subscriber is an SQS queue that is processed by a payment gateway, while the second subscriber is a Lambda function that indexes the buyer’s shopping interests against a search engine.</p> 
<p>A subscription filter policy is set as a subscription attribute, by the subscription owner, as a simple JSON object, containing a set of key-value pairs. This object defines the kind of event in which the subscriber is interested.</p> 
<code class="lang-python">payment_gateway_subscription_arn = sns.subscribe(
TopicArn = topic_arn,
Protocol = 'sqs',
Endpoint = 'arn:aws:sqs:ap-southeast-2:123456789012:PaymentQueue'
)['SubscriptionArn']
sns.set_subscription_attributes(
SubscriptionArn = payment_gateway_subscription_arn, 
AttributeName = 'FilterPolicy', 
AttributeValue = '{&quot;event_type&quot;: [&quot;order_placed&quot;, &quot;order_cancelled&quot;]}'
)
search_engine_subscription_arn = sns.subscribe(
TopicArn = topic_arn,
Protocol = 'lambda',
Endpoint = 'arn:aws:lambda:ap-southeast-2:123456789012:function:SearchIndex'
)['SubscriptionArn']
sns.set_subscription_attributes(
SubscriptionArn = search_engine_subscription_arn,
AttributeName ='FilterPolicy', 
AttributeValue ='{&quot;event_type&quot;: [&quot;product_page_visited&quot;]}'
)</code> 
<p>You’re now ready to start publishing events with attributes!</p> 
<p>Message attributes allow you to provide structured metadata items (such as time stamps, geospatial data, event type, signatures, and identifiers) about the message. Message attributes are optional and separate from, but sent along with, the message body. You can include up to 10 message attributes with your message.</p> 
<p>The first message published in this example is related to an order that has been placed on the ecommerce website. The message attribute “<strong>event_type</strong>” with the value “<strong>order_placed</strong>” matches only the filter policy associated with the payment gateway subscription. Therefore, only the SQS queue subscribed to the SNS topic is notified about this checkout event.</p> 
<code class="lang-python">message = '{&quot;order&quot;: {&quot;id&quot;: 5678, &quot;status&quot;: &quot;confirmed&quot;, &quot;items&quot;: [' \
'{&quot;code&quot;: &quot;P-9012&quot;, &quot;product&quot;: &quot;Santos FC Jersey&quot;, &quot;units&quot;: 1},' \
'{&quot;code&quot;: &quot;P-3156&quot;, &quot;product&quot;: &quot;Soccer Ball&quot;, &quot;units&quot;: 2}]},' \
' &quot;buyer&quot;: {&quot;id&quot;: 4454}}'
sns.publish(
TopicArn = topic_arn,
Subject = 'Order Placed #5678',
Message = message,
MessageAttributes = {
'event_type': {
'DataType': 'String',
'StringValue': 'order_placed'
}
}
)</code> 
<p>The second message published is related to a buyer’s navigation activity on the ecommerce website. The message attribute “<strong>event_type</strong>” with the value “<strong>product_page_visited</strong>” matches only the filter policy associated with the search engine subscription. Therefore, only the Lambda function subscribed to the SNS topic is notified about this navigation event.</p> 
<code class="lang-python">message = '{&quot;product&quot;: {&quot;id&quot;: 1251, &quot;status&quot;: &quot;in_stock&quot;},' \
' &quot;buyer&quot;: {&quot;id&quot;: 4454}}'
sns.publish(
TopicArn = topic_arn,
Subject = 'Product Visited #1251',
Message = message,
MessageAttributes = {
'event_type': {
'DataType': 'String',
'StringValue': 'product_page_visited'
}
}
)</code> 
<p>The following diagram represents the architecture for this ecommerce website, with the message filtering mechanism in action. As described earlier, checkout events are pushed only to the SQS queue, whereas navigation events are pushed to the Lambda function only.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/20/introducing_sns_message_filtering_image_4.png" /></a></p> 
<b>Message filtering criteria</b> 
<p>It is important to remember the following things about subscription filter policy matching:</p> 
<li>A subscription filter policy either matches an incoming message, or it doesn’t. It’s Boolean logic.</li> 
<li>For a filter policy to match a message, the message must contain all the attribute keys listed in the policy.</li> 
<li>Attributes of the message not mentioned in the filtering policy are ignored.</li> 
<li>The value of each key in the filter policy is an array containing one or more values. The policy matches if any of the values in the array match the value in the corresponding message attribute.</li> 
<li>If the value in the message attribute is an array, then the filter policy matches if the intersection of the policy array and the message array is non-empty.</li> 
<li>The matching is exact (character-by-character), without case-folding or any other string normalization.</li> 
<li>The values being matched follow JSON rules: Strings enclosed in quotes, numbers, and the unquoted keywords true, false, and null.</li> 
<li>Number matching is at the string representation level. Example: 300, 300.0, and 3.0e2 aren’t considered equal.</li> 
<b>When should I use message filtering?</b> 
<p>We recommend using message filtering and grouping subscribers into a single topic only when all of the following is true:</p> 
<li>Subscribers are semantically related to each other</li> 
<li>Subscribers consume similar types of events</li> 
<li>Subscribers are supposed to share the same access permissions on the topic</li> 
<p>Technically, you could get away with creating a single topic for your entire domain to handle all event processing, even unrelated use cases, but this wouldn’t be recommended. This option could result in an unnecessarily large topic, which could potentially impact your message delivery latency. Also, you would lose the ability to implement fine-grained access control on your topics.</p> 
<p>Finally, if you already use SNS, but had to add filtering logic in your subscribers or routing logic in your publishers (topic-based filtering), you can now immediately benefit from message filtering. This new approach lets you clean up any unnecessary logic in your components, and reduce the number of topics in your architecture.</p> 
<b>Summary</b> 
<p>As we’ve shown in this post, the new message filtering capability in Amazon SNS gives you a great amount of flexibility in your messaging pattern. It allows you to really simplify your pub/sub infrastructure requirements.</p> 
<p>Message filtering can be implemented easily with existing <a href="https://aws.amazon.com/tools/#sdk">AWS SDKs</a>&nbsp;by applying message and subscription attributes across all SNS supported protocols (Amazon SQS, AWS Lambda, HTTP, SMS, email, and mobile push). It’s now available in all AWS commercial regions, at no extra charge.</p> 
<p>Here’s a few ideas for next steps to get you started:</p> 
<li>Add filter policies to your subscriptions on the <a href="https://console.aws.amazon.com/sns/">SNS console</a>,</li> 
<li>Try the 10-minute tutorial, <a href="https://aws.amazon.com/getting-started/tutorials/filter-messages-published-to-topics/">Filter Messages Published to Topics</a></li> 
<li>Read the <a href="http://docs.aws.amazon.com/sns/latest/dg/message-filtering.html">message filtering section of the SNS documentation</a></li> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3362');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/budget_lambda_iam_ver7.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Serverless Automated Cost Controls, Part1</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Shankar Ramachandran</span></span> | on 
<time property="datePublished" datetime="2017-11-22T09:34:50+00:00">22 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/" title="View all posts in Application Services*"><span property="articleSection">Application Services*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/aws-cost-management/aws-budgets/" title="View all posts in AWS Budgets*"><span property="articleSection">AWS Budgets*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/aws-cost-management/" title="View all posts in AWS Cost Management*"><span property="articleSection">AWS Cost Management*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/aws-identity-and-access-management-iam/" title="View all posts in AWS Identity and Access Management (IAM)*"><span property="articleSection">AWS Identity and Access Management (IAM)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/" title="View all posts in Security, Identity, &amp; Compliance*"><span property="articleSection">Security, Identity, &amp; Compliance*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3299" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3299&amp;disqus_title=Serverless+Automated+Cost+Controls%2C+Part1&amp;disqus_url=https://aws.amazon.com/blogs/compute/serverless-automated-cost-controls-part1/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3299');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Shankar Ramachandran, Pubali Sen, and George Mao</em></p> 
<p>In line with AWS’s continual efforts to reduce costs for customers, this series focuses on how customers can build serverless automated cost controls. This post provides an architecture blueprint and a sample implementation to prevent budget overruns.</p> 
<p>This solution uses the following AWS products:</p> 
<li><a href="https://aws.amazon.com/aws-cost-management/aws-budgets/">AWS Budgets</a> – An AWS Cost Management tool that helps customers define and track budgets for AWS costs, and forecast for up to three months.</li> 
<li><a href="https://aws.amazon.com/sns/">Amazon SNS</a> – An AWS service that makes it easy to set up, operate, and send notifications from&nbsp;the cloud.</li> 
<li><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> – An AWS service that lets you run code without provisioning or managing servers.</li> 
<p>You can fine-tune a budget for various parameters, for example filtering by service or tag. The Budgets tool lets you post notifications on an SNS topic. A Lambda function that subscribes to the SNS topic can act on the notification. Any programmatically implementable action can be taken.</p> 
<p>The diagram below describes the architecture blueprint.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/using_lambda_for_cost_control_ver4.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/using_lambda_for_cost_control_ver4.jpg" /></a></p> 
<p>In this post, we describe how to use this blueprint with AWS Step Functions and IAM to effectively revoke the ability of a user to start new Amazon EC2 instances, after a budget amount is exceeded.</p> 
<b>Freedom with guardrails</b> 
<p>AWS lets you quickly spin up resources as you need them, deploying hundreds or even thousands of servers in minutes.&nbsp;This means you can quickly develop and roll out new applications. Teams can experiment and innovate more quickly and frequently. If an experiment fails, you can always de-provision those servers without risk.</p> 
<p>This improved agility also brings in the need for effective cost controls. Your Finance and Accounting department must budget, monitor, and control the AWS spend. For example, this could be a budget per project. Further, Finance and Accounting must take appropriate actions if the budget for the project has been exceeded, for example. Call it “freedom with guardrails” – where Finance wants to give developers freedom, but with financial constraints.</p> 
<b>Architecture</b> 
<p>This section describes how to use the blueprint introduced earlier to implement a “freedom with guardrails” solution.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/budget_lambda_iam_ver7.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/budget_lambda_iam_ver7.jpg" /></a></p> 
<ol> 
<li>The budget for “Project Beta” is set up in Budgets. In this example, we focus on EC2 usage and identify the instances that belong to this project by filtering on the tag Project with the value Beta. For more information, see <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html">Creating a Budget</a>.</li> 
<li>The budget configuration also includes settings to send a notification on an SNS topic when the usage exceeds 100% of the budgeted amount. For more information, see <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-sns-policy.html">Creating an Amazon SNS Topic for Budget Notifications</a>.</li> 
<li>The master Lambda function receives the SNS notification.</li> 
<li>It triggers execution of a Step Functions state machine with the parameters for completing the configured action.</li> 
<li>The action Lambda function is triggered as a task in the state machine. The function interacts with IAM to effectively remove the user’s permissions to create an EC2 instance.</li> 
</ol> 
<p>This decoupled modular design allows for extensibility.&nbsp; New actions (serially or in parallel) can be added by simply adding new steps.</p> 
<b>Implementing the solution</b> 
<p>All the instructions and code needed to implement the architecture have been posted on the <a href="https://github.com/aws-samples/serverless-automated-cost-controls">Serverless Automated Cost Controls</a> GitHub repo. We recommend that you try this first in a Dev/Test environment.</p> 
<p>This implementation description can be broken down into two parts:</p> 
<ol> 
<li>Create a solution stack for serverless automated cost controls.</li> 
<li>Verify the solution by testing the EC2 fleet.</li> 
</ol> 
<p>To tie this back to the “freedom with guardrails” scenario, the Finance department performs a one-time implementation of the solution stack. To simulate resources for Project Beta, the developers spin up the test EC2 fleet.</p> 
<b>Prerequisites</b> 
<p>There are two prerequisites:</p> 
<li>Make sure that you have the necessary IAM permissions. For more information, see the section titled “Required IAM permissions” in the <a href="https://github.com/aws-samples/serverless-automated-cost-controls/blob/master/README.md">README</a>.</li> 
<li>Define and activate a cost allocation tag with the key Project. For more information, see <a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html">Using Cost Allocation Tags</a>. It can take up to 12 hours for the tags to propagate to Budgets.</li> 
<b>Create resources</b> 
<p>The solution stack includes creating the following resources:</p> 
<li>Three Lambda functions</li> 
<li>One Step Functions state machine</li> 
<li>One SNS topic</li> 
<li>One IAM group</li> 
<li>One IAM user</li> 
<li>IAM policies as needed</li> 
<li>One budget</li> 
<p>Two of the Lambda functions were described in the previous section, to a) receive the SNS notification and b) trigger the Step Functions state machine. Another Lambda function is used to create the budget, as a <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html">custom AWS CloudFormation resource</a>. The SNS topic connects Budgets with Lambda function A. Lambda function B is configured as a task in Step Functions. A budget for $2 is created which is filtered by Service: EC2 and Tag: Project, Beta. A test IAM group and user is created to enable you to validate this Cost Control Solution.</p> 
<p>To create the serverless automated cost control solution stack, choose the button below. It takes few minutes to spin up the stack. You can monitor the progress in the CloudFormation console.</p> 
<p><a href="https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=costControlStack&amp;templateURL=https://s3-us-west-2.amazonaws.com/computeblog-us-west-2/serverless-automated-cost-controls/cfn_budget_lambda_blog_post.json"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/cloudformation-launch-stack-button.png" /></a></p> 
<p>When you see the CREATE_COMPLETE status for the stack you had created, choose Outputs. Copy the following four values that you need later:</p> 
<li>TemplateURL</li> 
<li>UserName</li> 
<li>SignInURL</li> 
<li>Password</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_1.png" /></a></p> 
<b>Verify the stack</b> 
<p>The next step is to verify the serverless automated cost controls solution stack that you just created. To do this, spin up an EC2 fleet of t2.micro instances, representative of the resources needed for Project Beta, and tag them with <strong>Project, Beta</strong>.</p> 
<ol> 
<li>Browse to the <strong>SignInURL</strong>, and log in using the <strong>UserName</strong> and <strong>Password</strong> values copied on from the stack output.</li> 
<li>In the CloudFormation console, choose <strong>Create Stack</strong>.</li> 
<li>For <strong>Choose a template</strong>, select <strong>Choose an Amazon S3 template URL</strong> and paste the <strong>TemplateURL</strong> value from the preceding section. Choose <strong>Next</strong>.</li> 
<li>Give this stack a name, such as “testEc2FleetForProjectBeta”. Choose <strong>Next</strong>.</li> 
<li>On the Specify Details page, enter parameters such as the <strong>UserName</strong> and <strong>Password</strong> copied in the previous section. Choose Next.</li> 
<li>Ignore any errors related to listing IAM roles. The test user has a minimal set of permissions that is just sufficient to spin up this test stack (in line with security best practices).</li> 
<li>On the <strong>Options</strong> page, choose <strong>Next</strong>.</li> 
<li>On the <strong>Review</strong> page, choose <strong>Create</strong>. It takes a few minutes to spin up the stack, and you can monitor the progress in the CloudFormation console.&nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_2-1.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_2-1.jpg" /></a></li> 
<li>When you see the status “CREATE_COMPLETE”, open the EC2 console to verify that four t2.micro instances have been spun up, with the tag of Project, Beta.</li> 
</ol> 
<p>The hourly cost for these instances depends on the region in which they are running. On the average (irrespective of the region), you can expect the aggregate cost for this EC2 fleet to exceed the set $2 budget in 48 hours.</p> 
<b>Verify the solution</b> 
<p>The first step is to identify the test IAM group that was created in the previous section. The group should have “projectBeta” in the name, prepended with the CloudFormation stack name and appended with an alphanumeric string. Verify that the managed policy associated is: “EC2FullAccess”, which indicates that the users in this group have unrestricted access to EC2.</p> 
<p>There are two stages of verification for this serverless automated cost controls solution: simulating a notification and waiting for a breach.</p> 
<h3>Simulated notification</h3> 
<p>Because it takes at least a few hours for the aggregate cost of the EC2 fleet to breach the set budget, you can verify the solution by simulating the notification from Budgets.</p> 
<ol> 
<li>Log in to the SNS console (using your regular AWS credentials).</li> 
<li>Publish a message on the SNS topic that has “budgetNotificationTopic” in the name. The complete name is appended by the CloudFormation stack identifier. &nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_3.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_3-1024x488.jpg" /></a></li> 
<li>Copy the following text as the body of the notification: “This is a mock notification”.</li> 
<li>Choose Publish.</li> 
<li>Open the IAM console to verify that the policy for the test group has been switched to “EC2ReadOnly”. This prevents users in this group from creating new instances.</li> 
<li>Verify that the test user created in the previous section cannot spin up new EC2 instances. &nbsp;You can log in as the test user and try creating a new EC2 instance (via the same CloudFormation stack or the EC2 console). You should get an error message indicating that you do not have the necessary permissions.</li> 
<li>If you are proceeding to stage 2 of the verification, then you must switch the permissions back to “EC2FullAccess” for the test group, which can be done in the IAM console.</li> 
</ol> 
<h3>Automatic notification</h3> 
<p>Within 48 hours, the aggregate cost of the EC2 fleet spun up in the earlier section breaches the budget rule and triggers an automatic notification. This results in the permissions getting switched out, just as in the simulated notification.</p> 
<b>Clean up</b> 
<p>Use the following steps to delete your resources and stop incurring costs.</p> 
<ol> 
<li>Open the CloudFormation console.</li> 
<li>Delete the EC2 fleet by deleting the appropriate stack (for example, delete the stack named “testEc2FleetForProjectBeta”). &nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_4-1.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_4-1.jpg" /></a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<b>&nbsp;</b></li> 
<li>Next, delete the “costControlStack” stack. &nbsp; <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_5.jpg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/screen_shot_5-1024x465.jpg" /></a>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<strong>&nbsp;</strong></li> 
</ol> 
<b>Conclusion</b> 
<p>Using Lambda in tandem with Budgets, you can build Serverless automated cost controls on AWS. Find all the resources (instructions, code) for implementing the solution discussed in this post on the <a href="https://github.com/aws-samples/serverless-automated-cost-controls">Serverless Automated Cost Controls</a> GitHub repo.</p> 
<p>Stay tuned to this series for more tips about building serverless automated cost controls. In the next post, we discuss using smart lighting to influence developer behavior and describe a solution to encourage cost-aware development practices.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3299');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/social_ECR.png" /> 
<b class="lb-b blog-post-title" property="name headline">AWS re:Invent 2017 Guide to All Things Containers</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">tiffany jernigan (@tiffanyfayj)</span></span> | on 
<time property="datePublished" datetime="2017-11-17T13:05:07+00:00">17 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/aws-reinvent-2017-guide-to-all-things-containers/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3321" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3321&amp;disqus_title=AWS+re%3AInvent+2017+Guide+to+All+Things+Containers&amp;disqus_url=https://aws.amazon.com/blogs/compute/aws-reinvent-2017-guide-to-all-things-containers/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3321');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><i>Contributed</i><em>&nbsp;by <a href="https://twitter.com/tiffanyfayj">Tiffany Jernigan</a>, Developer Advocate for Amazon ECS</em></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/ecs-ship-orange_planet-1024x318.png" /></p> 
<b>Get ready for takeoff!</b> 
<p>We made sure that this year’s <a href="https://reinvent.awsevents.com/">re:Invent</a> is chock-full of containers: there are over 40 sessions! New to containers? No problem, we have several introductory sessions for you to dip your toes. Been using containers for years and know the ins and outs? Don’t miss our technical deep-dives and interactive chalk talks led by container experts.</p> 
<p>If you’re already registered for re:Invent, you can browse the session catalog <a href="https://www.portal.reinvent.awsevents.com/connect/search.ww#loadSearch-searchPhrase=&amp;searchType=session&amp;tc=0&amp;sortBy=abbreviationSort&amp;p=&amp;i(10042)=16546">here</a>. If you can’t make it to Las Vegas, you can catch the keynotes and session recaps from our <a href="https://reinvent.awsevents.com/live-stream/">livestream</a> and on <a href="http://www.twitch.tv/aws">Twitch</a>.</p> 
<h3>Session types</h3> 
<p>Not everyone learns the same way, so we have multiple types of breakout content:</p> 
<li><strong>Birds of a Feather</strong><br /> An interactive discussion with industry leaders about containers on AWS.</li> 
<li><strong>Breakout sessions</strong><br /> 60-minute presentations about building on AWS. Sessions are delivered by both AWS experts and customers and span all content levels.</li> 
<li><strong>Workshops</strong><br /> 2.5-hour, hands-on sessions that teach how to build on AWS. AWS credits are provided. Bring a laptop, and have an active AWS account.</li> 
<li><strong>Chalk Talks</strong><br /> 1-hour, highly interactive sessions with a smaller audience. They begin with a short lecture delivered by an AWS expert, followed by a discussion with the audience.</li> 
<h3>Session levels</h3> 
<p>Whether you’re new to containers or you’ve been using them for years, you’ll find useful information at every level.</p> 
<li><strong>Introductory</strong><br /> Sessions are focused on providing an overview of AWS services and features, with the assumption that attendees are new to the topic.</li> 
<li><strong>Advanced</strong><br /> Sessions dive deeper into the selected topic. Presenters assume that the audience has some familiarity with the topic, but may or may not have direct experience implementing a similar solution.</li> 
<li><strong>Expert</strong><br /> Sessions are for attendees who are deeply familiar with the topic, have implemented a solution on their own already, and are comfortable with how the technology works across multiple services, architectures, and implementations.</li> 
<h3>Session locations</h3> 
<p>All container sessions are located in the Aria Resort.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/17/reInvent_locations.png" /></p> 
<b>MONDAY 11/27</b> 
<h3>Breakout sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON202 –&nbsp;Getting Started with Docker and Amazon ECS<br /> </strong>By packaging software into standardized units, Docker gives code everything it needs to run, ensuring consistency from your laptop all the way into production. But once you have your code ready to ship, how do you run and scale it in the cloud? In this session, you become comfortable running containerized services in production using Amazon ECS. We cover container deployment, cluster management, service auto-scaling, service discovery, secrets management, logging, monitoring, security, and other core concepts. We also cover integrated AWS services and supplementary services that you can take advantage of to run and scale container-based services in the cloud.</p> 
<h3>Chalk talks</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON211 –&nbsp;Reducing your Compute Footprint with Containers and Amazon ECS<br /> </strong>Tomas Riha, platform architect for Volvo, shows how Volvo transitioned its WirelessCar platform from using Amazon EC2 virtual machines to containers running on Amazon ECS, significantly reducing cost. Tomas dives deep into the architecture that Volvo used to achieve the migration in under four months, including Amazon ECS, Amazon ECR, Elastic Load Balancing, and AWS CloudFormation.</p> 
<p><strong>CON212 –&nbsp;Anomaly Detection Using Amazon ECS, AWS Lambda, and Amazon EMR</strong><br /> Learn about the architecture that Cisco CloudLock uses to enable automated security and compliance checks throughout the entire development lifecycle, from the first line of code through runtime. It includes integration with IAM roles, Amazon VPC, and AWS KMS.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON410 –&nbsp;Advanced CICD with Amazon ECS Control Plane<br /> </strong>Mohit Gupta, product and engineering lead for Clever, demonstrates how to extend the Amazon ECS control plane to optimize management of container deployments and how the control plane can be broadly applied to take advantage of new AWS services. This includes ark—an AWS CLI-based deployment to Amazon ECS, Dapple—a slack-based automation system for deployments and notifications, and Kayvee—log and event routing libraries based on Amazon Kinesis.</p> 
<h3>Workshops</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON209 –&nbsp;Interstella 8888: Learn How to Use Docker on AWS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. &nbsp;Join this workshop to get hands-on experience with Docker as you containerize Interstella 8888’s aging monolithic application and deploy it using Amazon ECS.</p> 
<p><strong>CON213 –&nbsp;Hands-on Deployment of Kubernetes on AWS<br /> </strong>In this workshop, attendees get hands-on experience using Kubernetes and Kops (Kubernetes Operations), as described in our recent blog post. Attendees learn how to provision a cluster, assign role-based permissions and security, and launch a container. If you’re interested in learning best practices for running Kubernetes on AWS, don’t miss this workshop.</p> 
<b>TUESDAY 11/28</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON206 –&nbsp;Docker on AWS</strong><br /> In this session, Docker Technical Staff Member Patrick Chanezon discusses how Finnish Rail, the national train system for Finland, is using Docker on Amazon Web Services to modernize their customer facing applications, from ticket sales to reservations. Patrick also shares the state of Docker development and adoption on AWS, including explaining the opportunities and implications of efforts such as Project Moby, Docker EE, and how developers can use and contribute to Docker projects.</p> 
<p><strong>CON208 –&nbsp;Building Microservices on AWS</strong><br /> Increasingly, organizations are turning to microservices to help them empower autonomous teams, letting them innovate and ship software faster than ever before. But implementing a microservices architecture comes with a number of new challenges that need to be dealt with. Chief among these finding an appropriate platform to help manage a growing number of independently deployable services. In this session, Sam Newman, author of Building Microservices and a renowned expert in microservices strategy, discusses strategies for building scalable and robust microservices architectures. He also tells you how to choose the right platform for building microservices, and about common challenges and mistakes organizations make when they move to microservices architectures.</p> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON302 –&nbsp;Building a CICD Pipeline for Containers on AWS</strong><br /> Containers can make it easier to scale applications in the cloud, but how do you set up your CICD workflow to automatically test and deploy code to containerized apps? In this session, we explore how developers can build effective CICD workflows to manage their containerized code deployments on AWS.</p> 
<p>Ajit Zadgaonkar, Director of Engineering and Operations at Edmunds walks through best practices for CICD architectures used by his team to deploy containers. We also deep dive into topics such as how to create an accessible CICD platform and architect for safe blue/green deployments.</p> 
<p><strong>CON307 –&nbsp;Building Effective Container Images</strong><br /> Sick of getting paged at 2am and wondering “where did all my disk space go?” New Docker users often start with a stock image in order to get up and running quickly, but this can cause problems as your application matures and scales. Creating efficient container images is important to maximize resources, and deliver critical security benefits.</p> 
<p>In this session, AWS Sr. Technical Evangelist Abby Fuller covers how to create effective images to run containers in production. This includes an in-depth discussion of how Docker image layers work, things you should think about when creating your images, working with Amazon ECR, and mise-en-place for install dependencies. Prakash Janakiraman, Co-Founder and Chief Architect at Nextdoor discuss high-level and language-specific best practices for with building images and how Nextdoor uses these practices to successfully scale their containerized services with a small team.</p> 
<p><strong>CON309 –&nbsp;Containerized Machine Learning on AWS</strong><br /> Image recognition is a field of deep learning that uses neural networks to recognize the subject and traits for a given image. In Japan, Cookpad uses Amazon ECS to run an image recognition platform on clusters of GPU-enabled EC2 instances. In this session, hear from Cookpad about the challenges they faced building and scaling this advanced, user-friendly service to ensure high-availability and low-latency for tens of millions of users.</p> 
<p><strong>CON320 –&nbsp;Monitoring, Logging, and Debugging for Containerized Services</strong><br /> As containers become more embedded in the platform tools, debug tools, traces, and logs become increasingly important. Nare Hayrapetyan, Senior Software Engineer and Calvin French-Owen, Senior Technical Officer for Segment discuss the principals of monitoring and debugging containers and the tools Segment has implemented and built for logging, alerting, metric collection, and debugging of containerized services running on Amazon ECS.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON314 –&nbsp;Automating Zero-Downtime Production Cluster Upgrades for Amazon ECS</strong><br /> Containers make it easy to deploy new code into production to update the functionality of a service, but what happens when you need to update the Amazon EC2 compute instances that your containers are running on? In this talk, we’ll deep dive into how to upgrade the Amazon EC2 infrastructure underlying a live production Amazon ECS cluster without affecting service availability. Matt Callanan, Engineering Manager at Expedia walk through Expedia’s “PRISM” project that safely relocates hundreds of tasks onto new Amazon EC2 instances with zero-downtime to applications.</p> 
<p><strong>CON322 –&nbsp;Maximizing Amazon ECS for Large-Scale Workloads</strong><br /> Head of Mobfox DevOps, David Spitzer, shows how Mobfox used Docker and Amazon ECS to scale the Mobfox services and development teams to achieve low-latency networking and automatic scaling. This session covers Mobfox’s ecosystem architecture. It compares 2015 and today, the challenges Mobfox faced in growing their platform, and how they overcame them.</p> 
<p><strong>CON323 –&nbsp;Microservices Architectures for the Enterprise</strong><br /> Salva Jung, Principle Engineer for Samsung Mobile shares how Samsung Connect is architected as microservices running on Amazon ECS to securely, stably, and efficiently handle requests from millions of mobile and IoT devices around the world.</p> 
<p><strong>CON324 –&nbsp;Windows Containers on Amazon ECS</strong><br /> Docker containers are commonly regarded as powerful and portable runtime environments for Linux code, but Docker also offers API and toolchain support for running Windows Servers in containers. In this talk, we discuss the various options for running windows-based applications in containers on AWS.</p> 
<p><strong>CON326 –&nbsp;Remote Sensing and Image Processing on AWS</strong><br /> Learn how Encirca services by DuPont Pioneer uses Amazon ECS powered by GPU-instances and Amazon EC2 Spot Instances to run proprietary image-processing algorithms against satellite imagery. Mark Lanning and Ethan Harstad, engineers at DuPont Pioneer show how this architecture has allowed them to process satellite imagery multiple times a day for each agricultural field in the United States in order to identify crop health changes.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON317 –&nbsp;Advanced Container Management at Catsndogs.lol</strong><br /> Catsndogs.lol is a (fictional) company that needs help deploying and scaling its container-based application. During this workshop, attendees join the new DevOps team at CatsnDogs.lol, and help the company to manage their applications using Amazon ECS, and help release new features to make our customers happier than ever.Attendees get hands-on with service and container-instance auto-scaling, spot-fleet integration, container placement strategies, service discovery, secrets management with AWS Systems Manager Parameter Store, time-based and event-based scheduling, and automated deployment pipelines. If you are a developer interested in learning more about how Amazon ECS can accelerate your application development and deployment workflows, or if you are a systems administrator or DevOps person interested in understanding how Amazon ECS can simplify the operational model associated with running containers at scale, then this workshop is for you. You should have basic familiarity with Amazon ECS, Amazon EC2, and IAM.</p> 
<p>Additional requirements:</p> 
<li>The AWS CLI or AWS Tools for PowerShell installed</li> 
<li>An AWS account with administrative permissions (including the ability to create IAM roles and policies) created at least 24 hours in advance.</li> 
<b>WEDNESDAY 11/29</b> 
<h3>Birds of a Feather (BoF)</h3> 
<p><strong>CON205 –&nbsp;Birds of a Feather: Containers and Open Source at AWS</strong><br /> Cloud native architectures take advantage of on-demand delivery, global deployment, elasticity, and higher-level services to enable developer productivity and business agility. Open source is a core part of making cloud native possible for everyone. In this session, we welcome thought leaders from the CNCF, Docker, and AWS to discuss the cloud’s direction for growth and enablement of the open source community. We also discuss how AWS is integrating open source code into its container services and its contributions to open source projects.</p> 
<h3>Breakout Sessions</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON308 –&nbsp;Mastering Kubernetes on AWS</strong><br /> Much progress has been made on how to bootstrap a cluster since Kubernetes’ first commit and is now only a matter of minutes to go from zero to a running cluster on Amazon Web Services. However, evolving a simple Kubernetes architecture to be ready for production in a large enterprise can quickly become overwhelming with options for configuration and customization.</p> 
<p>In this session, Arun Gupta, Open Source Strategist for AWS and Raffaele Di Fazio, software engineer at leading European fashion platform Zalando, show the common practices for running Kubernetes on AWS and share insights from experience in operating tens of Kubernetes clusters in production on AWS. We cover options and recommendations on how to install and manage clusters, configure high availability, perform rolling upgrades and handle disaster recovery, as well as continuous integration and deployment of applications, logging, and security.</p> 
<p><strong>CON310 –&nbsp;Moving to Containers: Building with Docker and Amazon ECS</strong><br /> If you’ve ever considered moving part of your application stack to containers, don’t miss this session. We cover best practices for containerizing your code, implementing automated service scaling and monitoring, and setting up automated CI/CD pipelines with fail-safe deployments. Manjeeva Silva and Thilina Gunasinghe show how McDonalds implemented their home delivery platform in four months using Docker containers and Amazon ECS to serve tens of thousands of customers.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON402 –&nbsp;Advanced Patterns in Microservices Implementation with Amazon ECS</strong><br /> Scaling a microservice-based infrastructure can be challenging in terms of both technical implementation and developer workflow. In this talk, AWS Solutions Architect Pierre Steckmeyer is joined by Will McCutchen, Architect at BuzzFeed, to discuss Amazon ECS as a platform for building a robust infrastructure for microservices. We look at the key attributes of microservice architectures and how Amazon ECS supports these requirements in production, from configuration to sophisticated workload scheduling to networking capabilities to resource optimization. We also examine what it takes to build an end-to-end platform on top of the wider AWS ecosystem, and what it’s like to migrate a large engineering organization from a monolithic approach to microservices.</p> 
<p><strong>CON404 –&nbsp;Deep Dive into Container Scheduling with Amazon ECS</strong><br /> As your application’s infrastructure grows and scales, well-managed container scheduling is critical to ensuring high availability and resource optimization. In this session, we deep dive into the challenges and opportunities around container scheduling, as well as the different tools available within Amazon ECS and AWS to carry out efficient container scheduling. We discuss patterns for container scheduling available with Amazon ECS, the Blox scheduling framework, and how you can customize and integrate third-party scheduler frameworks to manage container scheduling on Amazon ECS.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON312 –&nbsp;Building a Selenium Fleet on the Cheap with Amazon ECS with Spot Fleet</strong><br /> Roberto Rivera&nbsp;and Matthew Wedgwood, engineers at RetailMeNot, give a practical overview of setting up a fleet of Selenium nodes running on Amazon ECS with Spot Fleet. Discuss the challenges of running Selenium with high availability at minimum cost using Amazon ECS container introspection to connect the Selenium Hub with its nodes.</p> 
<p><strong>CON315 –&nbsp;Virtually There: Building a Render Farm with Amazon ECS</strong><br /> Learn how 8i Corp scales its multi-tenanted, volumetric render farm up to thousands of instances using AWS, Docker, and an API-driven infrastructure. This render farm enables them to turn the video footage from an array of synchronized cameras into a photo-realistic hologram capable of playback on a range of devices, from mobile phones to high-end head mounted displays. Join Owen Evans, VP of Engineering for 8i, as they dive deep into how 8i’s rendering infrastructure is built and maintained by just a handful of people and powered by Amazon ECS.</p> 
<p><strong>CON325 –&nbsp;Developing Microservices – from Your Laptop to the Cloud</strong><br /> Wesley Chow, Staff Engineer at Adroll, shows how his team extends Amazon ECS by enabling local development capabilities. Hologram, Adroll’s local development program, brings the capabilities of the Amazon EC2 instance metadata service to non-EC2 hosts, so that developers can run the same software on local machines with the same credentials source as in production.</p> 
<p><strong>CON327 –&nbsp;Patterns and Considerations for Service Discovery</strong><br /> Roven Drabo, head of cloud operations at Kaplan Test Prep, illustrates Kaplan’s complete container automation solution using Amazon ECS along with how his team uses NGINX and HashiCorp Consul to provide an automated approach to service discovery and container provisioning.</p> 
<p><strong>CON328 –&nbsp;Building a Development Platform on Amazon ECS</strong><br /> Quinton Anderson, Head of Engineering for Commonwealth Bank of Australia, walks through&nbsp;how they migrated their internal development and deployment platform from Mesos/Marathon to Amazon ECS. The platform uses a custom DSL to abstract a layered application architecture, in a way that makes it easy to plug or replace new implementations into each layer in the stack.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON318 –&nbsp;Interstella 8888: Monolith to Microservices with Amazon ECS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. Join this workshop to get hands-on experience deploying Docker containers as you break Interstella 8888’s aging monolithic application into containerized microservices. Using Amazon ECS and an Application Load Balancer, you create API-based microservices and deploy them leveraging integrations with other AWS services.</p> 
<p><strong>CON332 –&nbsp;Build a Java Spring Application on Amazon ECS</strong><br /> This workshop teaches you how to lift and shift existing Spring and Spring Cloud applications onto the AWS platform. Learn how to build a Spring application container, understand bootstrap secrets, push container images to Amazon ECR, and deploy the application to Amazon ECS. Then, learn how to configure the deployment for production.</p> 
<b>THURSDAY 11/30</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 200 (Introductory)</h4> 
<p><strong>CON201 –&nbsp;Containers on AWS – State of the Union</strong><br /> Just over four years after the first public release of Docker, and three years to the day after the launch of Amazon ECS, the use of containers has surged to run a significant percentage of production workloads at startups and enterprise organizations. Join Deepak Singh, General Manager of Amazon Container Services, as he covers the state of containerized application development and deployment trends, new container capabilities on AWS that are available now, options for running containerized applications on AWS, and how AWS customers successfully run container workloads in production.</p> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON304 –&nbsp;Batch Processing with Containers on AWS</strong><br /> Batch processing is useful to analyze large amounts of data. But configuring and scaling a cluster of virtual machines to process complex batch jobs can be difficult. In this talk, we show how to use containers on AWS for batch processing jobs that can scale quickly and cost-effectively. We also discuss AWS Batch, our fully managed batch-processing service. You also hear from GoPro and Here about how they use AWS to run batch processing jobs at scale including best practices for ensuring efficient scheduling, fine-grained monitoring, compute resource automatic scaling, and security for your batch jobs.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON406 –&nbsp;Architecting Container Infrastructure for Security and Compliance</strong><br /> While organizations gain agility and scalability when they migrate to containers and microservices, they also benefit from compliance and security, advantages that are often overlooked. In this session, Kelvin Zhu, lead software engineer at Okta, joins Mitch Beaumont, enterprise solutions architect at AWS, to discuss security best practices for containerized infrastructure. Learn how Okta built their development workflow with an emphasis on security through testing and automation. Dive deep into how containers enable automated security and compliance checks throughout the development lifecycle. Also understand best practices for implementing AWS security and secrets management services for any containerized service architecture.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON329 –&nbsp;Full Software Lifecycle Management for Containers Running on Amazon ECS</strong><br /> Learn how The Washington Post uses Amazon ECS to run Arc Publishing, a digital journalism platform that powers The Washington Post and a growing number of major media websites. Amazon ECS enabled The Washington Post to containerize their existing microservices architecture, avoiding a complete rewrite that would have delayed the platform’s launch by several years. In this session, Jason Bartz, Technical Architect at The Washington Post, discusses the platform’s architecture. He addresses the challenges of optimizing Arc Publishing’s workload, and managing the application lifecycle to support 2,000 containers running on more than 50 Amazon ECS clusters.</p> 
<p><strong>CON330 –&nbsp;Running Containerized HIPAA Workloads on AWS</strong><br /> Nihar Pasala, Engineer at Aetion, discusses the Aetion Evidence Platform, a system for generating the real-world evidence used by healthcare decision makers to implement value-based care. This session discusses the architecture Aetion uses to run HIPAA workloads using containers on Amazon ECS, best practices, and learnings.</p> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON408 –&nbsp;Building a Machine Learning Platform Using Containers on AWS</strong><br /> DeepLearni.ng develops&nbsp;and implements machine learning models for complex enterprise applications. In this session, Thomas Rogers, Engineer for DeepLearni.ng discusses how they worked with Scotiabank to leverage&nbsp;Amazon ECS, Amazon ECR, Docker, GPU-accelerated Amazon EC2 instances, and TensorFlow to develop a retail risk model that helps manage payment collections for millions of Canadian credit card customers.</p> 
<h3>Workshops</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON319 –&nbsp;Interstella 8888: CICD for Containers on AWS</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. &nbsp;Join this workshop to learn how to set up a CI/CD pipeline for containerized microservices. You get hands-on experience&nbsp;deploying Docker container images using Amazon ECS, AWS CloudFormation, AWS CodeBuild, and AWS CodePipeline,&nbsp;automating&nbsp;everything from code check-in to production.</p> 
<b>FRIDAY 12/1</b> 
<h3>Breakout Sessions</h3> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON405 –&nbsp;Moving to Amazon ECS – the Not-So-Obvious Benefits</strong><br /> If you ask 10 teams why they migrated to containers, you will likely get answers like ‘developer productivity’, ‘cost reduction’, and ‘faster scaling’. But teams often find there are several other ‘hidden’ benefits to using containers for their services. In this talk, Franziska Schmidt, Platform Engineer at Mapbox and Yaniv Donenfeld from&nbsp;AWS will discuss the obvious, and not so obvious benefits of moving to containerized architecture. These include using Docker and Amazon ECS to achieve shared libraries for dev teams, separating private infrastructure from shareable code, and making it easier for non-ops engineers to run services.</p> 
<h3>Chalk Talks</h3> 
<h4>Level 300 (Advanced)</h4> 
<p><strong>CON331 –&nbsp;Deploying a Regulated Payments Application on Amazon ECS</strong><br /> Travelex discusses how they built an FCA-compliant international payments service using a microservices architecture on AWS. This chalk talk covers the challenges of designing and operating an Amazon ECS-based PaaS in a regulated environment using a DevOps model.</p> 
<h3>Workshops</h3> 
<h4>Level 400 (Expert)</h4> 
<p><strong>CON407 –&nbsp;Interstella 8888: Advanced Microservice Operations</strong><br /> Interstella 8888 is an intergalactic trading company that deals in rare resources, but their antiquated monolithic logistics systems are causing the business to lose money. In this workshop, you help Interstella 8888 build a modern microservices-based logistics system to save the company from financial ruin. We give you the hands-on experience you need to run microservices in the real world. This includes implementing advanced container scheduling and scaling to deal with variable service requests, implementing a service mesh, issue&nbsp;tracing with AWS X-Ray, container and instance-level logging with Amazon CloudWatch, and load testing.</p> 
<b>Know before you go</b> 
<p>Want to brush up on your container knowledge before re:Invent? Here are some helpful resources to get started:</p> 
<li><a href="https://aws.amazon.com/ecs/getting-started">Amazon ECS Getting Started</a></li> 
<li><a href="https://aws.amazon.com/ecs/resources">Amazon ECS Resources</a></li> 
<li><a href="https://github.com/nathanpeck/awesome-ecs">Nathan Peck’s AWSome ECS</a></li> 
<li>Docs 
<li><a href="https://aws.amazon.com/documentation/ec2/">Amazon EC2</a></li> 
<li><a href="https://aws.amazon.com/documentation/ecs/">Amazon ECS</a></li> 
</ul> </li> 
<li>Blogs 
<li><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/">AWS Compute Blog</a></li> 
<li><a href="https://aws.amazon.com/blogs/aws/category/ec2-container-service/">AWS Blog</a></li> 
</ul> </li> 
<p>– Tiffany<br /> <a href="https://twitter.com/tiffanyfayj">@tiffanyfayj</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3321');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Resume AWS Step Functions from Any State</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-11-16T15:31:27+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/resume-aws-step-functions-from-any-state/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3166" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3166&amp;disqus_title=Resume+AWS+Step+Functions+from+Any+State&amp;disqus_url=https://aws.amazon.com/blogs/compute/resume-aws-step-functions-from-any-state/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3166');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Yash.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Yash.jpeg" /></a><br /> <strong>Yash Pant, Solutions Architect, AWS</strong></p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Aaron.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/10/Aaron.jpeg" /></a><br /> <strong>Aaron Friedman, Partner Solutions Architect, AWS</strong></p> 
<p>When we discuss how to build applications with customers, we often align to the <a href="https://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf">Well Architected Framework</a> pillars of security, reliability, performance efficiency, cost optimization, and operational excellence. Designing for failure is an essential component to developing well architected applications that are resilient to spurious errors that may occur.</p> 
<p>There are many ways you can use AWS services to achieve high availability and resiliency of your applications. For example, you can couple Elastic Load Balancing with Auto Scaling and Amazon EC2 instances to build highly available applications. Or use Amazon API Gateway and AWS Lambda to rapidly scale out a microservices-based architecture. Many AWS services have built in solutions to help with the appropriate error handling, such as <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">Dead Letter Queues (DLQ)</a> for Amazon SQS or <a href="http://docs.aws.amazon.com/batch/latest/userguide/job_retries.html">retries in AWS Batch</a>.</p> 
<p><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> is an AWS service that makes it easy for you to coordinate the components of distributed applications and microservices. Step Functions allows you to easily design for failure, by incorporating features such as <a href="https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-errors.html#amazon-states-language-retrying-after-error">error retries</a> and <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-handling-error-conditions.html">custom error handling</a> from AWS Lambda exceptions. These features allow you to programmatically handle many common error modes and build robust, reliable applications.</p> 
<p>In some rare cases, however, your application may fail in an unexpected manner. In these situations, you might not want to duplicate in a repeat execution those portions of your state machine that have already run. This is especially true when orchestrating long-running jobs or executing a complex state machine as part of a microservice. Here, you need to know the last successful state in your state machine from which to resume, so that you don't duplicate previous work. In this post, we present a solution to enable you to resume from any given state in your state machine in the case of an unexpected failure.</p> 
<b id="toc_1">Resuming from a given state</b> 
<p>To resume a failed state machine execution from the state at which it failed, you first run a script that dynamically creates a new state machine. When the new state machine is executed, it resumes the failed execution from the point of failure. The script contains the following two primary steps:</p> 
<ol> 
<li>Parse the execution history of the failed execution to find the name of the state at which it failed, as well as the JSON input to that state.</li> 
<li>Create a new state machine, which adds an additional state to failed state machine, called &quot;GoToState&quot;. &quot;GoToState&quot; is a <em>choice</em> state at the beginning of the state machine that branches execution directly to the failed state, allowing you to skip states that had succeeded in the previous execution.</li> 
</ol> 
<p>The full script along with a CloudFormation template that creates a demo of this is available in the <a href="https://github.com/awslabs/aws-sfn-resume-from-any-state">aws-sfn-resume-from-any-state</a> GitHub repo.</p> 
<b id="toc_2">Diving into the script</b> 
<p>In this section, we walk you through the script and highlight the core components of its functionality. The script contains a main function, which adds a command line parameter for the <em>failedExecutionArn</em> so that you can easily call the script from the command line:</p> 
<code class="language-bash">python gotostate.py --failedExecutionArn '&lt;Failed_Execution_Arn&gt;'</code> 
<b id="toc_3">Identifying the failed state in your execution</b> 
<p>First, the script extracts the name of the failed state along with the input to that state. It does so by using the failed state machine execution history, which is identified by the Amazon Resource Name (ARN) of the execution. The failed state is marked in the execution history, along with the input to that state (which is also the output of the preceding successful state). The script is able to parse these values from the log.</p> 
<p>The script loops through the execution history of the failed state machine, and traces it backwards until it finds the failed state. If the state machine failed in a parallel state, then it must restart from the beginning of the parallel state. The script is able to capture the name of the parallel state that failed, rather than any substate within the parallel state that may have caused the failure. The following code is the Python function that does this.</p> 
<code class="language-python">
def parseFailureHistory(failedExecutionArn):
'''
Parses the execution history of a failed state machine to get the name of failed state and the input to the failed state:
Input failedExecutionArn = A string containing the execution ARN of a failed state machine y
Output = A list with two elements: [name of failed state, input to failed state]
'''
failedAtParallelState = False
try:
#Get the execution history
response = client.get\_execution\_history(
executionArn=failedExecutionArn,
reverseOrder=True
)
failedEvents = response['events']
except Exception as ex:
raise ex
#Confirm that the execution actually failed, raise exception if it didn't fail.
try:
failedEvents[0]['executionFailedEventDetails']
except:
raise('Execution did not fail')
'''
If you have a 'States.Runtime' error (for example, if a task state in your state machine attempts to execute a Lambda function in a different region than the state machine), get the ID of the failed state, and use it to determine the failed state name and input.
'''
if failedEvents[0]['executionFailedEventDetails']['error'] == 'States.Runtime':
failedId = int(filter(str.isdigit, str(failedEvents[0]['executionFailedEventDetails']['cause'].split()[13])))
failedState = failedEvents[-1 \* failedId]['stateEnteredEventDetails']['name']
failedInput = failedEvents[-1 \* failedId]['stateEnteredEventDetails']['input']
return (failedState, failedInput)
'''
You need to loop through the execution history, tracing back the executed steps.
The first state you encounter is the failed state. If you failed on a parallel state, you need the name of the parallel state rather than the name of a state within a parallel state that it failed on. This is because you can only attach goToState to the parallel state, but not a substate within the parallel state.
This loop starts with the ID of the latest event and uses the previous event IDs to trace back the execution to the beginning (id 0). However, it returns as soon it finds the name of the failed state.
'''
currentEventId = failedEvents[0]['id']
while currentEventId != 0:
#multiply event ID by -1 for indexing because you're looking at the reversed history
currentEvent = failedEvents[-1 \* currentEventId]
'''
You can determine if the failed state was a parallel state because it and an event with 'type'='ParallelStateFailed' appears in the execution history before the name of the failed state
'''
if currentEvent['type'] == 'ParallelStateFailed':
failedAtParallelState = True
'''
If the failed state is not a parallel state, then the name of failed state to return is the name of the state in the first 'TaskStateEntered' event type you run into when tracing back the execution history
'''
if currentEvent['type'] == 'TaskStateEntered' and failedAtParallelState == False:
failedState = currentEvent['stateEnteredEventDetails']['name']
failedInput = currentEvent['stateEnteredEventDetails']['input']
return (failedState, failedInput)
'''
If the failed state was a parallel state, then you need to trace execution back to the first event with 'type'='ParallelStateEntered', and return the name of the state
'''
if currentEvent['type'] == 'ParallelStateEntered' and failedAtParallelState:
failedState = failedState = currentEvent['stateEnteredEventDetails']['name']
failedInput = currentEvent['stateEnteredEventDetails']['input']
return (failedState, failedInput)
#Update the ID for the next execution of the loop
currentEventId = currentEvent['previousEventId']
</code> 
<b id="toc_4">Create the new state machine</b> 
<p>The script uses the name of the failed state to create the new state machine, with &quot;GoToState&quot; branching execution directly to the failed state.</p> 
<p>To do this, the script requires the <a href="http://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html">Amazon States Language (ASL)</a> definition of the failed state machine. It modifies the definition to append &quot;GoToState&quot;, and create a new state machine from it.</p> 
<p>The script gets the ARN of the failed state machine from the execution ARN of the failed state machine. This ARN allows it to get the ASL definition of the failed state machine by calling the <a href="http://docs.aws.amazon.com/step-functions/latest/apireference/API_DescribeStateMachine.html">DesribeStateMachine</a> API action. It creates a new state machine with &quot;GoToState&quot;.</p> 
<p>When the script creates the new state machine, it also adds an additional input variable called &quot;resuming&quot;. When you execute this new state machine, you specify this resuming variable as <em>true</em> in the input JSON. This tells &quot;GoToState&quot; to branch execution to the state that had previously failed. Here's the function that does this:</p> 
<code class="language-python">def attachGoToState(failedStateName, stateMachineArn):
'''
Given a state machine ARN and the name of a state in that state machine, create a new state machine that starts at a new choice state called 'GoToState'. &quot;GoToState&quot; branches to the named state, and sends the input of the state machine to that state, when a variable called &quot;resuming&quot; is set to True.
Input failedStateName = A string with the name of the failed state
stateMachineArn = A string with the ARN of the state machine
Output response from the create_state_machine call, which is the API call that creates a new state machine
'''
try:
response = client.describe\_state\_machine(
stateMachineArn=stateMachineArn
)
except:
raise('Could not get ASL definition of state machine')
roleArn = response['roleArn']
stateMachine = json.loads(response['definition'])
#Create a name for the new state machine
newName = response['name'] + '-with-GoToState'
#Get the StartAt state for the original state machine, because you point the 'GoToState' to this state
originalStartAt = stateMachine['StartAt']
'''
Create the GoToState with the variable $.resuming.
If new state machine is executed with $.resuming = True, then the state machine skips to the failed state.
Otherwise, it executes the state machine from the original start state.
'''
goToState = {'Type':'Choice', 'Choices':[{'Variable':'$.resuming', 'BooleanEquals':False, 'Next':originalStartAt}], 'Default':failedStateName}
#Add GoToState to the set of states in the new state machine
stateMachine['States']['GoToState'] = goToState
#Add StartAt
stateMachine['StartAt'] = 'GoToState'
#Create new state machine
try:
response = client.create_state_machine(
name=newName,
definition=json.dumps(stateMachine),
roleArn=roleArn
)
except:
raise('Failed to create new state machine with GoToState')
return response
</code> 
<b id="toc_5">Testing the script</b> 
<p>Now that you understand how the script works, you can test it out.</p> 
<p>The following screenshot shows an example state machine that has failed, called &quot;TestMachine&quot;. This state machine successfully completed &quot;FirstState&quot; and &quot;ChoiceState&quot;, but when it branched to &quot;FirstMatchState&quot;, it failed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_1.png" /></a></p> 
<p>Use the script to create a new state machine that allows you to rerun this state machine, but skip the &quot;FirstState&quot; and the &quot;ChoiceState&quot; steps that already succeeded. You can do this by calling the script as follows:</p> 
<code class="language-bash">python gotostate.py --failedExecutionArn 'arn:aws:states:us-west-2:&lt;AWS_ACCOUNT_ID&gt;:execution:TestMachine-with-GoToState:b2578403-f41d-a2c7-e70c-7500045288595</code> 
<p>This creates a new state machine called &quot;TestMachine-with-GoToState&quot;, and returns its ARN, along with the input that had been sent to &quot;FirstMatchState&quot;. You can then inspect the input to determine what caused the error. In this case, you notice that the input to &quot;FirstMachState&quot; was the following:</p> 
<code class="language-json">{
&quot;foo&quot;: 1,
&quot;Message&quot;: true
}</code> 
<p>However, this state machine expects the &quot;Message&quot; field of the JSON to be a string rather than a Boolean. Execute the new &quot;TestMachine-with-GoToState&quot; state machine, change the input to be a string, and add the &quot;resuming&quot; variable that &quot;GoToState&quot; requires:</p> 
<code class="language-json">{
&quot;foo&quot;: 1,
&quot;Message&quot;: &quot;Hello!&quot;,
&quot;resuming&quot;:true
}</code> 
<p>When you execute the new state machine, it skips &quot;FirstState&quot; and &quot;ChoiceState&quot;, and goes directly to &quot;FirstMatchState&quot;, which was the state that failed:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_2.png" /></a></p> 
<p>Look at what happens when you have a state machine with multiple parallel steps. This example is included in the <a href="https://github.com/awslabs/aws-sfn-resume-from-any-state">GitHub repository</a> associated with this post. The repo contains a CloudFormation template that sets up this state machine and provides instructions to replicate this solution.</p> 
<p>The following state machine, &quot;ParallelStateMachine&quot;, takes an input through two subsequent parallel states before doing some final processing and exiting, along with the JSON with the ASL definition of the state machine.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_3.png" /></a></p> 
<code class="language-json">{
&quot;Comment&quot;: &quot;An example of the Amazon States Language using a parallel state to execute two branches at the same time.&quot;,
&quot;StartAt&quot;: &quot;Parallel&quot;,
&quot;States&quot;: {
&quot;Parallel&quot;: {
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;ResultPath&quot;:&quot;$.output&quot;,
&quot;Next&quot;: &quot;Parallel 2&quot;,
&quot;Branches&quot;: [
{
&quot;StartAt&quot;: &quot;Parallel Step 1, Process 1&quot;,
&quot;States&quot;: {
&quot;Parallel Step 1, Process 1&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaA&quot;,
&quot;End&quot;: true
}
}
},
{
&quot;StartAt&quot;: &quot;Parallel Step 1, Process 2&quot;,
&quot;States&quot;: {
&quot;Parallel Step 1, Process 2&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaA&quot;,
&quot;End&quot;: true
}
}
}
]
},
&quot;Parallel 2&quot;: {
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;Next&quot;: &quot;Final Processing&quot;,
&quot;Branches&quot;: [
{
&quot;StartAt&quot;: &quot;Parallel Step 2, Process 1&quot;,
&quot;States&quot;: {
&quot;Parallel Step 2, Process 1&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;: true
}
}
},
{
&quot;StartAt&quot;: &quot;Parallel Step 2, Process 2&quot;,
&quot;States&quot;: {
&quot;Parallel Step 2, Process 2&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;: true
}
}
}
]
},
&quot;Final Processing&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaC&quot;,
&quot;End&quot;: true
}
}
}</code> 
<p>First, use an input that initially fails:</p> 
<code class="language-json">{
&quot;Message&quot;: &quot;Hello!&quot;
}</code> 
<p>This fails because the state machine expects you to have a variable in the input JSON called &quot;foo&quot; in the second parallel state to run &quot;Parallel Step 2, Process 1&quot; and &quot;Parallel Step 2, Process 2&quot;. Instead, the original input gets processed by the first parallel state and produces the following output to pass to the second parallel state:</p> 
<code class="language-json">{
&quot;output&quot;: [
{
&quot;Message&quot;: &quot;Hello!&quot;
},
{
&quot;Message&quot;: &quot;Hello!&quot;
}
],
}</code> 
<p>Run the script on the failed state machine to create a new state machine that allows it to resume directly at the second parallel state instead of having to redo the first parallel state. This creates a new state machine called &quot;ParallelStateMachine-with-GoToState&quot;. The following JSON was created by the script to define the new state machine in ASL. It contains the &quot;GoToState&quot; value that was attached by the script.</p> 
<code class="language-json">{
&quot;Comment&quot;:&quot;An example of the Amazon States Language using a parallel state to execute two branches at the same time.&quot;,
&quot;States&quot;:{
&quot;Final Processing&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaC&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
},
&quot;GoToState&quot;:{
&quot;Default&quot;:&quot;Parallel 2&quot;,
&quot;Type&quot;:&quot;Choice&quot;,
&quot;Choices&quot;:[
{
&quot;Variable&quot;:&quot;$.resuming&quot;,
&quot;BooleanEquals&quot;:false,
&quot;Next&quot;:&quot;Parallel&quot;
}
]
},
&quot;Parallel&quot;:{
&quot;Branches&quot;:[
{
&quot;States&quot;:{
&quot;Parallel Step 1, Process 1&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaA&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 1, Process 1&quot;
},
{
&quot;States&quot;:{
&quot;Parallel Step 1, Process 2&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:LambdaA&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 1, Process 2&quot;
}
],
&quot;ResultPath&quot;:&quot;$.output&quot;,
&quot;Type&quot;:&quot;Parallel&quot;,
&quot;Next&quot;:&quot;Parallel 2&quot;
},
&quot;Parallel 2&quot;:{
&quot;Branches&quot;:[
{
&quot;States&quot;:{
&quot;Parallel Step 2, Process 1&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 2, Process 1&quot;
},
{
&quot;States&quot;:{
&quot;Parallel Step 2, Process 2&quot;:{
&quot;Resource&quot;:&quot;arn:aws:lambda:us-west-2:XXXXXXXXXXXX:function:LambdaB&quot;,
&quot;End&quot;:true,
&quot;Type&quot;:&quot;Task&quot;
}
},
&quot;StartAt&quot;:&quot;Parallel Step 2, Process 2&quot;
}
],
&quot;Type&quot;:&quot;Parallel&quot;,
&quot;Next&quot;:&quot;Final Processing&quot;
}
},
&quot;StartAt&quot;:&quot;GoToState&quot;
}</code> 
<p>You can then execute this state machine with the correct input by adding the &quot;foo&quot; and &quot;resuming&quot; variables:</p> 
<code class="language-json">{
&quot;foo&quot;: 1,
&quot;output&quot;: [
{
&quot;Message&quot;: &quot;Hello!&quot;
},
{
&quot;Message&quot;: &quot;Hello!&quot;
}
],
&quot;resuming&quot;: true
}</code> 
<p>This yields the following result. Notice that this time, the state machine executed successfully to completion, and skipped the steps that had previously failed.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_4.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/Blog_Pic_4.png" /></a></p> 
<hr /> 
<b id="toc_6">Conclusion</b> 
<p>When you're building out complex workflows, it's important to be prepared for failure. You can do this by taking advantage of features such as automatic error retries in Step Functions and custom error handling of Lambda exceptions.</p> 
<p>Nevertheless, state machines still have the possibility of failing. With the methodology and script presented in this post, you can resume a failed state machine from its point of failure. This allows you to skip the execution of steps in the workflow that had already succeeded, and recover the process from the point of failure.</p> 
<p>To see more examples, please visit the <a href="https://aws.amazon.com/step-functions/getting-started/" target="_blank" rel="noopener noreferrer">Step Functions Getting Started</a> page.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3166');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Event-Driven Computing with Amazon SNS and AWS Compute, Storage, Database, and Networking Services</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-16T12:51:46+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-elastic-file-system-efs/" title="View all posts in Amazon Elastic File System (EFS)*"><span property="articleSection">Amazon Elastic File System (EFS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-elasticache/" title="View all posts in Amazon ElastiCache*"><span property="articleSection">Amazon ElastiCache*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/analytics/amazon-elasticsearch-service/" title="View all posts in Amazon Elasticsearch Service*"><span property="articleSection">Amazon Elasticsearch Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-glacier/" title="View all posts in Amazon Glacier*"><span property="articleSection">Amazon Glacier*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/analytics/amazon-kinesis/" title="View all posts in Amazon Kinesis*"><span property="articleSection">Amazon Kinesis*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-rds/" title="View all posts in Amazon RDS*"><span property="articleSection">Amazon RDS*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/database/amazon-redshift/" title="View all posts in Amazon Redshift*"><span property="articleSection">Amazon Redshift*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-route-53/" title="View all posts in Amazon Route 53*"><span property="articleSection">Amazon Route 53*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/auto-scaling/" title="View all posts in Auto Scaling*"><span property="articleSection">Auto Scaling*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-codepipeline/" title="View all posts in AWS CodePipeline*"><span property="articleSection">AWS CodePipeline*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/aws-direct-connect/" title="View all posts in AWS Direct Connect*"><span property="articleSection">AWS Direct Connect*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-elastic-beanstalk/" title="View all posts in AWS Elastic Beanstalk*"><span property="articleSection">AWS Elastic Beanstalk*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/analytics/aws-glue/" title="View all posts in AWS Glue*"><span property="articleSection">AWS Glue*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/aws-snowball/" title="View all posts in AWS Snowball*"><span property="articleSection">AWS Snowball*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/elastic-load-balancing/" title="View all posts in Elastic Load Balancing*"><span property="articleSection">Elastic Load Balancing*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/event-driven-computing-with-amazon-sns-compute-storage-database-and-networking-services/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3282" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3282&amp;disqus_title=Event-Driven+Computing+with+Amazon+SNS+and+AWS+Compute%2C+Storage%2C+Database%2C+and+Networking+Services&amp;disqus_url=https://aws.amazon.com/blogs/compute/event-driven-computing-with-amazon-sns-compute-storage-database-and-networking-services/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3282');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by Otavio&nbsp;Ferreira,&nbsp;Manager, Software Development, AWS Messaging</em></p> 
<p>Like other developers around the world, you may be tackling increasingly complex business problems. A key success factor, in that case, is the ability to break down a large project scope into smaller, more manageable components. A service-oriented architecture guides you toward designing systems as a collection of loosely coupled, independently scaled, and highly reusable services. Microservices take this even further. To improve performance and scalability, they promote fine-grained interfaces and lightweight protocols.</p> 
<p>However, the communication among isolated microservices can be challenging. Services are often deployed onto independent servers and don’t share any compute or storage resources. Also, you should avoid hard dependencies among microservices, to preserve maintainability and reusability.</p> 
<p>If you apply the <a href="https://aws.amazon.com/pub-sub-messaging/">pub/sub design pattern</a>, you can effortlessly decouple and independently scale out your microservices and serverless architectures. A pub/sub messaging service, such as <a href="https://aws.amazon.com/sns/">Amazon SNS</a>, promotes event-driven computing that statically decouples event publishers from subscribers, while dynamically allowing for the exchange of messages between them. An event-driven architecture also introduces the responsiveness needed to deal with complex problems, which are often unpredictable and asynchronous.<span id="more-3282"></span></p> 
<b>What is event-driven computing?</b> 
<p>Given the context of microservices, event-driven computing is a model in which subscriber services automatically perform work in response to events triggered by publisher services. This paradigm can be applied to automate workflows while decoupling the services that collectively and independently work to fulfil these workflows. <a href="https://aws.amazon.com/sns/">Amazon SNS</a> is an event-driven computing hub, in the AWS Cloud, that has native integration with several AWS publisher and subscriber services.</p> 
<b>Which AWS services publish events to SNS natively?</b> 
<p>Several AWS services have been integrated as SNS publishers and, therefore, can natively trigger event-driven computing for a variety of use cases. In this post, I specifically cover AWS compute, storage, database, and networking services, as depicted below.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide01.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide01.png" /></a></p> 
<b>Compute services</b> 
<li><a href="https://aws.amazon.com/autoscaling/"><strong>Auto Scaling:</strong></a> Helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You can configure Auto Scaling lifecycle hooks to trigger events, as Auto Scaling resizes your EC2 cluster.As an example, you may want to warm up the local cache store on newly launched EC2 instances, and also download log files from other EC2 instances that are about to be terminated. To make this happen, set an SNS topic as your Auto Scaling group’s notification target, then subscribe two Lambda functions to this SNS topic. The first function is responsible for handling scale-out events (to warm up cache upon provisioning), whereas the second is in charge of handling scale-in events (to download logs upon termination). 
<li><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/ASGettingNotifications.html">Getting SNS Notifications When Your Auto Scaling Group Scales</a></li> 
<li><a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/lifecycle-hooks.html">Auto Scaling Lifecycle Hooks</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide02.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide02.png" /></a></p> 
<li><strong><a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk:</a></strong> An easy-to-use service for deploying and scaling web applications and web services developed in a number of programming languages. You can configure event notifications for your Elastic Beanstalk environment so that notable events can be automatically published to an SNS topic, then pushed to topic subscribers.As an example, you may use this event-driven architecture to coordinate your continuous integration pipeline (such as Jenkins CI). That way, whenever an environment is created, Elastic Beanstalk publishes this event to an SNS topic, which triggers a subscribing Lambda function, which then kicks off a CI job against your newly created Elastic Beanstalk environment. 
<li><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html">Configuring Amazon SNS Notifications with Elastic Beanstalk</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide03.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide03.png" /></a></p> 
<li><a href="https://aws.amazon.com/elasticloadbalancing/"><strong>Elastic Load Balancing:</strong> </a>Automatically distributes incoming application traffic across Amazon EC2 instances, containers, or other resources identified by IP addresses.You can configure CloudWatch alarms on Elastic Load Balancing metrics, to automate the handling of events derived from Classic Load Balancers. As an example, you may leverage this event-driven design to automate latency profiling in an Amazon ECS cluster behind a Classic Load Balancer. In this example, whenever your ECS cluster breaches your load balancer latency threshold, an event is posted by CloudWatch to an SNS topic, which then triggers a subscribing Lambda function. This function runs a task on your ECS cluster to trigger a latency profiling tool, hosted on the cluster itself. This can enhance your latency troubleshooting exercise by making it timely. 
<li><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html">CloudWatch Metrics and SNS Notifications for Your Classic Load Balancer</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide04.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide04.png" /></a></p> 
<b>Storage services</b> 
<li><strong><a href="https://aws.amazon.com/s3/">Amazon S3:</a>&nbsp;</strong>Object storage built to store and retrieve any amount of data.You can enable S3 event notifications, and automatically get them posted to SNS topics, to automate a variety of workflows. For instance, imagine that you have an S3 bucket to store incoming resumes from candidates, and a fleet of EC2 instances to encode these resumes from their original format (such as Word or text) into a portable format (such as PDF).In this example, whenever new files are uploaded to your input bucket, S3 publishes these events to an SNS topic, which in turn pushes these messages into subscribing SQS queues. Then, encoding workers running on EC2 instances poll these messages from the SQS queues; retrieve the original files from the input S3 bucket; encode them into PDF; and&nbsp;finally store them in an output S3 bucket. 
<li><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">Configuring Amazon S3 Event Notifications</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html">Configuring Amazon S3 Buckets for Amazon SNS Notifications</a> (Walkthrough)</li> 
<li><a href="https://aws.amazon.com/blogs/compute/messaging-fanout-pattern-for-serverless-architectures-using-amazon-sns/">Messaging Fan-out Pattern for Serverless Architectures Using Amazon SNS</a> (Multimedia Encoding Example)</li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide05.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide05.png" /></a></p> 
<li><a href="https://aws.amazon.com/efs/"><strong>Amazon EFS:</strong></a> Provides simple and scalable file storage, for use with Amazon EC2 instances, in the AWS Cloud.You can configure CloudWatch alarms on EFS metrics, to automate the management of your EFS systems. For example, consider a highly parallelized genomics analysis application that runs against an EFS system. By default, this file system is instantiated on the “General Purpose” performance mode. Although this performance mode allows for lower latency, it might eventually impose a scaling bottleneck. Therefore, you may leverage an event-driven design to handle it automatically.Basically, as soon as the EFS metric “Percent I/O Limit” breaches 95%, CloudWatch could post this event to an SNS topic, which in turn would push this message into a subscribing Lambda function. This function automatically creates a new file system, this time on the “Max I/O” performance mode, then switches the genomics analysis application to this new file system. As a result, your application starts experiencing higher I/O throughput rates. 
<li><a href="http://docs.aws.amazon.com/efs/latest/ug/performance.html">Amazon EFS Performance</a></li> 
<li><a href="http://docs.aws.amazon.com/efs/latest/ug/monitoring_automated_manual.html">Amazon EFS Monitoring Tools</a></li> 
<li><a href="http://docs.aws.amazon.com/efs/latest/ug/creating_alarms.html">Creating CloudWatch Alarms and SNS Topics to Monitor Amazon EFS</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide06.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide06.png" /></a></p> 
<li><a href="https://aws.amazon.com/glacier/"><strong>Amazon Glacier:</strong></a> A secure, durable, and low-cost cloud storage service for data archiving and long-term backup.You can set a notification configuration on an Amazon Glacier vault so that when a job completes, a message is published to an SNS topic. Retrieving an archive from Amazon Glacier is a two-step asynchronous operation, in which you first initiate a job, and then download the output after the job completes. Therefore, SNS helps you eliminate polling your Amazon Glacier vault to check whether your job has been completed, or not. As usual, you may subscribe SQS queues, Lambda functions, and HTTP endpoints to your SNS topic, to be notified when your Amazon Glacier job is done. 
<li><a href="http://docs.aws.amazon.com/amazonglacier/latest/dev/configuring-notifications.html">Configuring Vault Notifications in Amazon Glacier</a></li> 
<li><a href="http://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html">Retrieving Amazon Glacier Archives</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide07.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide07.png" /></a></p> 
<li><a href="https://aws.amazon.com/snowball/"><strong>AWS Snowball:</strong></a> A petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data.You can leverage Snowball notifications to automate workflows related to importing data into and exporting data from AWS. More specifically, whenever your Snowball job status changes, Snowball can publish this event to an SNS topic, which in turn can broadcast the event to all its subscribers.As an example, imagine a Geographic Information System (GIS) that distributes high-resolution satellite images to users via Web browser. In this example, the GIS vendor could capture up to 80 TB of satellite images; create a Snowball job to import these files from an on-premises system to an S3 bucket; and provide an SNS topic ARN to be notified upon job status changes in Snowball. After Snowball changes the job status from “Importing” to “Completed”, Snowball publishes this event to the specified SNS topic, which delivers this message to a subscribing Lambda function, which finally creates a CloudFront web distribution for the target S3 bucket, to serve the images to end users. 
<li><a href="http://docs.aws.amazon.com/snowball/latest/ug/notifications.html">Snowball Notifications with Amazon SNS</a> (AWS Snowball User Guide)</li> 
<li><a href="http://docs.aws.amazon.com/snowball/latest/api-reference/API_Notification.html">Snowball Notifications with Amazon SNS</a> (AWS Snowball API Reference)</li> 
<li><a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.html">Getting Started with CloudFront</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide08.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide08.png" /></a></p> 
<b>Database services</b> 
<li><a href="https://aws.amazon.com/rds/"><strong>Amazon RDS:</strong></a> Makes it easy to set up, operate, and scale a relational database in the cloud.RDS leverages SNS to broadcast notifications when RDS events occur. As usual, these notifications can be delivered via any protocol supported by SNS, including SQS queues, Lambda functions, and HTTP endpoints.As an example, imagine that you own a social network website that has experienced organic growth, and needs to scale its compute and database resources on demand. In this case, you could provide an SNS topic to listen to RDS DB instance events. When the “Low Storage” event is published to the topic, SNS pushes this event to a subscribing Lambda function, which in turn leverages the RDS API to increase the storage capacity allocated to your DB instance. The provisioning itself takes place within the specified DB maintenance window. 
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html">Using Amazon RDS Event Notifications via Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ListEvents.html">Viewing Amazon RDS Events</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide09.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide09.png" /></a></p> 
<li><a href="https://aws.amazon.com/elasticache/"><strong>Amazon ElastiCache:</strong></a> A web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.ElastiCache can publish messages using Amazon SNS when significant events happen on your cache cluster. This feature can be used to refresh the list of servers on client machines connected to individual cache node endpoints of a cache cluster. For instance, an ecommerce website fetches product details from a cache cluster, with the goal of offloading a relational&nbsp;database and speeding up page load times. Ideally, you want to make sure that each web server always has an updated list of cache servers to which to connect.To automate this node discovery process, you can get your ElastiCache cluster to publish events to an SNS topic. Thus, when ElastiCache event “AddCacheNodeComplete” is published, your topic then pushes this event to all subscribing HTTP endpoints that serve your ecommerce website, so that these HTTP servers can update their list of cache nodes. 
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ElastiCacheSNS.html">ElastiCache Event Notifications via Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ECEvents.SNS.html">Managing ElastiCache Notifications with Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/ECEvents.Viewing.html">Viewing ElastiCache Events</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide10.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide10.png" /></a></p> 
<li><a href="https://aws.amazon.com/redshift/"><strong>Amazon Redshift:</strong></a> A fully managed data warehouse that makes it simple to analyze data using standard SQL and BI (Business Intelligence) tools.Amazon Redshift uses SNS to broadcast relevant events so that data warehouse workflows can be automated. As an example, imagine a news website that sends clickstream data to a Kinesis Firehose stream, which then loads the data into Amazon Redshift, so that popular news and reading preferences might be surfaced on a BI tool. At some point though, this Amazon Redshift cluster might need to be resized, and the cluster enters a&nbsp;ready-only mode. Hence, this Amazon Redshift event is published to an SNS topic, which delivers this event to a subscribing Lambda function, which finally deletes the corresponding Kinesis Firehose delivery stream, so that clickstream data uploads can be put on hold.At a later point, after Amazon Redshift publishes the event that the maintenance window has been closed, SNS notifies a subscribing Lambda function accordingly, so that this function can re-create the Kinesis Firehose delivery stream, and resume clickstream data uploads to Amazon Redshift. 
<li><a href="http://docs.aws.amazon.com/redshift/latest/mgmt/working-with-event-notifications.html">Amazon Redshift Event Notifications with Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/redshift/latest/mgmt/manage-event-notifications-console.html">Managing Event Notifications Using the Amazon Redshift Console</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide11.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide11.png" /></a></p> 
<li><a href="https://aws.amazon.com/dms/"><strong>AWS DMS:</strong></a> Helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.DMS also uses SNS to provide notifications when DMS events occur, which can automate database migration workflows. As an example, you might create data replication tasks to migrate an on-premises MS SQL database, composed of multiple tables, to MySQL. Thus, if replication tasks fail due to incompatible data encoding in the source tables, these events can be published to an SNS topic, which can push these messages into a subscribing SQS queue. Then, encoders running on EC2 can poll these messages from the SQS queue, encode the source tables into a compatible character set, and restart the corresponding replication tasks in DMS. This is an event-driven approach to a self-healing database migration process. 
<li><a href="http://docs.aws.amazon.com/dms/latest/userguide/CHAP_Events.html">Working with DMS Event Notifications via Amazon SNS</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide12.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide12.png" /></a></p> 
<b>Networking services</b> 
<li><a href="https://aws.amazon.com/route53/"><strong>Amazon Route 53:</strong></a> A highly available and scalable cloud-based DNS (Domain Name System). Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources.You can set CloudWatch alarms and get automated Amazon SNS notifications when the status of your Route 53 health check changes. As an example, imagine an online payment gateway that reports the health of its platform to merchants worldwide, via a status page. This page is hosted on EC2 and fetches platform health data from DynamoDB. In this case, you could configure a CloudWatch alarm for your Route 53 health check, so that when the alarm threshold is breached, and the payment gateway is no longer considered healthy, then CloudWatch publishes this event to an SNS topic, which pushes this message to a subscribing Lambda function, which finally updates the DynamoDB table that populates the status page. This event-driven approach avoids any kind of manual update to the status page visited by merchants. 
<li><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-monitor-view-status.html">Monitoring Route 53 Health Check Status and Getting Notifications via Amazon SNS</a></li> 
<li><a href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-creating.html">Creating and Updating Route 53 Health Checks with Notifications</a></li> 
</ul> </li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide13.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide13.png" /></a></p> 
<li><a href="https://aws.amazon.com/directconnect/"><strong>AWS Direct Connect (AWS DX):</strong></a> Makes it easy to establish a dedicated network connection from your premises to AWS, which can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.You can monitor physical DX connections using CloudWatch alarms, and send SNS messages when alarms change their status. As an example, when a DX connection state shifts to 0 (zero), indicating that the connection is down, this event can be published to an SNS topic, which can fan out this message to impacted servers through HTTP endpoints, so that they might reroute their traffic through a different connection instead. This is an event-driven approach to connectivity resilience. 
<li><a href="http://docs.aws.amazon.com/directconnect/latest/UserGuide/monitoring-cloudwatch.html">Monitoring Direct Connect with Amazon CloudWatch and Amazon SNS</a></li> 
</ul> </li> 
<b><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide14.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/event_driven_sns_compute_slide14.png" /></a>More event-driven computing on AWS</b> 
<p>In addition to SNS, event-driven computing is also addressed by <a href="https://aws.amazon.com/blogs/aws/new-cloudwatch-events-track-and-respond-to-changes-to-your-aws-resources/">Amazon CloudWatch Events</a>, which delivers a near real-time stream of system events that describe changes in AWS resources. With CloudWatch Events, you can route each event type to one or more targets, including:</p> 
<li>SNS topics</li> 
<li><a href="https://aws.amazon.com/sqs/">Amazon SQS</a> queues</li> 
<li><a href="https://aws.amazon.com/ec2/">Amazon EC2</a> instances</li> 
<li><a href="https://aws.amazon.com/ecs/">Amazon ECS</a> tasks</li> 
<li><a href="https://aws.amazon.com/kinesis/streams/">Amazon Kinesis Streams</a></li> 
<li><a href="https://aws.amazon.com/kinesis/firehose/">Amazon Kinesis Firehose</a> delivery streams</li> 
<li><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> functions</li> 
<li><a href="https://aws.amazon.com/step-functions/">AWS Step Functions</a> state machines</li> 
<li><a href="https://aws.amazon.com/codepipeline/">AWS CodePipeline</a> pipelines</li> 
<p>Many AWS services publish events to CloudWatch. As an example, you can get CloudWatch Events to capture events on your ETL (Extract, Transform, Load) jobs running on <a href="https://aws.amazon.com/glue/">AWS Glue</a> and push failed ones to an SQS queue, so that you can retry them later.</p> 
<b>Conclusion</b> 
<p><a href="https://aws.amazon.com/sns/">Amazon SNS</a> is a pub/sub messaging service that can be used as an event-driven computing hub to AWS customers worldwide. By capturing events natively triggered by AWS services, such as EC2, S3 and RDS, you can automate and optimize all kinds of workflows, namely scaling, testing, encoding, profiling, broadcasting, discovery, failover, and much more. Business use cases presented in this post ranged from recruiting websites, to scientific research, geographic systems, social networks, retail websites, and news portals.</p> 
<p>Start now by visiting Amazon SNS in the <a href="https://console.aws.amazon.com/sns/">AWS Management Console</a>, or by trying the AWS 10-Minute Tutorial, <a href="https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/">Send Fan-out Event Notifications with Amazon SNS and Amazon SQS</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3282');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/dragen-arch.png" /> 
<b class="lb-b blog-post-title" property="name headline">Accelerating Precision Medicine at Scale</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Chris Munns</span></span> | on 
<time property="datePublished" datetime="2017-11-16T12:29:52+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/accelerating-precision-medicine-at-scale/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3270" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3270&amp;disqus_title=Accelerating+Precision+Medicine+at+Scale&amp;disqus_url=https://aws.amazon.com/blogs/compute/accelerating-precision-medicine-at-scale/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3270');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of Aaron Friedman, Healthcare and Life Sciences Partner Solutions Architect, AWS and&nbsp;</em><em>Angel Pizarro, Genomics and Life Sciences Senior Solutions Architect, AWS</em></p> 
<p>Precision medicine is tailored to individuals based on quantitative signatures, including genomics, lifestyle, and environment. It is often considered to be the driving force behind the next wave of human health. Through new initiatives and technologies such as population-scale genomics sequencing and IoT-backed wearables, researchers and clinicians in both commercial and public sectors are gaining new, previously inaccessible insights.</p> 
<p>Many of these precision medicine initiatives are already happening on AWS. A few of these include:</p> 
<li><a href="https://precision.fda.gov/">PrecisionFDA</a> – This initiative is led by the US Food and Drug Administration. The goal is to define the next-generation standard of care for genomics in precision medicine.</li> 
<li><a href="https://precision.heart.org/">American Heart Association Precision Medicine Platform</a> – The platform gives researchers the ability to collaborate and analyze datasets, to better predict and intervene in cardiovascular disease and stroke.</li> 
<li><a href="https://www2.deloitte.com/us/en/pages/consulting/topics/convergehealth.html">Deloitte ConvergeHEALTH</a> – Gives healthcare and life sciences organizations the ability to analyze their disparate datasets on a singular real world evidence platform.</li> 
<p>Central to many of these initiatives is genomics, which gives healthcare organizations the ability to establish a baseline for longitudinal studies. Due to its wide applicability in precision medicine initiatives—from rare disease diagnosis to improving outcomes of clinical trials—genomics data is growing at a larger rate than Moore’s law across the globe. Many expect these datasets to grow to be in the range of tens of exabytes by 2025.</p> 
<p>Genomics data is also regularly re-analyzed by the community as researchers develop new computational methods or compare older data with newer genome references. These trends are driving innovations in data analysis methods and algorithms to address the massive increase of computational requirements.</p> 
<p><a href="http://www.edicogenome.com/">Edico Genome</a>, an AWS Partner Network (APN) Partner, has developed a novel solution that accelerates genomics analysis using field-programmable gate arrays, or FPGAs. Historically, Edico Genome deployed their FPGA appliances on-premises. When AWS announced the <a href="https://aws.amazon.com/ec2/instance-types/f1/">Amazon EC2 F1</a>&nbsp;FPGA-based instance family in December 2016, Edico Genome adopted a cloud-first strategy, became a <a href="https://aws.amazon.com/ec2/instance-types/f1/partners/">F1 launch partner</a>, and was one of the first partners to deploy FPGA-enabled applications on AWS.</p> 
<p>On October 19, 2017, Edico Genome partnered with the Children’s Hospital of Philadelphia (CHOP) to demonstrate their FPGA-accelerated genomic pipeline software, called DRAGEN. It can significantly reduce time-to-insight for patient genomes, and analyzed 1,000 genomes from the Center for Applied Genomics Biobank in the shortest time possible. This set a <a href="http://edicogenome.com/childrens-hospital-philadelphia-edico-genome-set-guinness-world-records/">Guinness World Record for the fastest analysis of 1000 whole human genomes</a>, and they did this using 1000 EC2 f1.2xlarge instances in a single AWS region. Not only were they able to analyze genomes at high throughput, they did so averaging approximately $3 per whole human genome of AWS compute for the analysis.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/minutes-dragen-run.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/minutes-dragen-run.png" /></a></p> 
<p>The version of DRAGEN that Edico Genome used for this analysis was also the same one used in the precisionFDA Hidden Treasures – Warm Up challenge, where they were one of the top performers in every assessment.</p> 
<p>In the remainder of this post, we walk through the architecture used by Edico Genome, combining EC2 F1 instances and AWS Batch to achieve this milestone.</p> 
<h3>EC2 F1 instances and Edico’s DRAGEN</h3> 
<p>EC2 F1 instances provide access to programmable hardware-acceleration using FPGAs at a cloud scale. AWS customers use F1 instances for a wide variety of applications, including big data, financial analytics and risk analysis, image and video processing, engineering simulations, AR/VR, and accelerated genomics.<br /> Edico Genome’s FPGA-backed DRAGEN Bio-IT Platform is now integrated with EC2 F1 instances. You can access the accuracy, speed, flexibility, and low compute cost of DRAGEN through a number of third-party platforms, <a href="https://aws.amazon.com/marketplace/seller-profile?id=0aad8c40-a94d-4a2a-82d1-eb5f95d165ae">AWS Marketplace</a>, and Edico Genome’s own platform. The DRAGEN platform offers a scalable, accelerated, and cost-efficient secondary analysis solution for a wide variety of genomics applications. Edico Genome also provides a highly optimized mechanism for the efficient storage of genomic data.</p> 
<h3>Scaling DRAGEN on AWS</h3> 
<p>Edico Genome used 1,000 EC2 F1 instances to help their customer, the Children’s Hospital of Philadelphia (CHOP), to process and analyze all 1,000 whole human genomes in parallel. They used AWS Batch to provision compute resources and orchestrate DRAGEN compute jobs across the 1,000 EC2 F1 instances. This solution successfully addressed the challenge of creating a scalable genomic processing pipeline that can easily scale to thousands of engines running in parallel.</p> 
<h4>Architecture</h4> 
<p>A simplified view of the architecture used for the analysis is shown in the following diagram:<br /> <a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/dragen-arch.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/dragen-arch.png" /></a></p> 
<ol> 
<li>DRAGEN’s portal uses Elastic Load Balancing and Auto Scaling groups to scale out EC2 instances that submitted jobs to AWS Batch.</li> 
<li>Job metadata is stored in their Workflow Management (WFM) database, built on top of Amazon Aurora.</li> 
<li>The DRAGEN Workflow Manager API submits jobs to AWS Batch.</li> 
<li>These jobs are executed on the AWS Batch managed compute environment that was responsible for launching the EC2 F1 instances.</li> 
<li>These jobs run as Docker containers that have the requisite DRAGEN binaries for whole genome analysis.</li> 
<li>As each job runs, it retrieves and stores genomics data that is staged in Amazon S3.</li> 
</ol> 
<p>The steps listed previously can also be bucketed into the following higher-level layers:</p> 
<li>Workflow: &nbsp;Edico Genome used their Workflow Management API to orchestrate the submission of AWS Batch jobs. Metadata for the jobs (such as the S3 locations of the genomes, etc.) resides in the Workflow Management Database backed by Amazon Aurora.</li> 
<li>Batch execution: &nbsp;AWS Batch launches EC2 F1 instances and coordinates the execution of DRAGEN jobs on these compute resources. AWS Batch enabled Edico to quickly and easily scale up to the full number of instances they needed as jobs were submitted. They also scaled back down as each job was completed, to optimize for both cost and performance.</li> 
<li>Compute/job: &nbsp;Edico Genome stored their binaries in a Docker container that AWS Batch deployed onto each of the F1 instances, giving each instance the ability to run DRAGEN without the need to pre-install the core executables. The AWS based DRAGEN solution streams all genomics data from S3 for local computation and then writes the results to a destination bucket. They used an AWS Batch job role that specified the IAM permissions. The role ensured that DRAGEN only had access to the buckets or S3 key space it needed for the analysis. Jobs didn’t need to embed AWS credentials.</li> 
<p>We also covered these concepts previously in the <a href="https://aws.amazon.com/blogs/compute/building-high-throughput-genomics-batch-workflows-on-aws-introduction-part-1-of-4/">Building High-Throughput Genomics Batch Workflows on AWS</a> series on the AWS Compute blog.</p> 
<h3>Walkthrough</h3> 
<p>In the following sections, we dive deeper into several tasks that enabled Edico Genome’s scalable FPGA genome analysis on AWS:</p> 
<ol> 
<li>Prepare your Amazon FPGA Image for AWS Batch</li> 
<li>Create a Dockerfile and build your Docker image</li> 
<li>Set up your AWS Batch FPGA compute environment</li> 
</ol> 
<h4>Prerequisites</h4> 
<p>In brief, you need a modern Linux distribution (3.10+), Amazon ECS Container Agent, awslogs driver, and Docker configured on your image. There are additional recommendations in the <a href="https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec">Compute Resource AMI specification</a>.</p> 
<h4>Preparing your Amazon FPGA Image for AWS Batch</h4> 
<p>You can use any Amazon Machine Image (AMI) or Amazon FPGA Image (AFI) with AWS Batch, provided that it meets the <a href="https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec">Compute Resource AMI specification</a>. This gives you the ability to customize any workload by increasing the size of root or data volumes, adding instance stores, and connecting with the FPGA (F) and GPU (G and P) instance families.</p> 
<p>Next, install the AWS CLI:</p> 
<p><code>pip install awscli</code></p> 
<p>Add any additional software required to interact with the FPGAs on the F1 instances.</p> 
<p>As a starting point, AWS publishes an <a href="https://aws.amazon.com/marketplace/pp/B06VVYBLZZ">FPGA Developer AMI</a> in the AWS Marketplace. It is based on a CentOS Linux image and includes pre-integrated FPGA development tools. It also includes the runtime tools required to develop and use custom FPGAs for hardware acceleration applications.</p> 
<p>For more information about how to set up custom AMIs for your AWS Batch managed compute environments, see <a href="https://docs.aws.amazon.com/batch/latest/userguide/create-batch-ami.html">Creating a Compute Resource AMI</a>.</p> 
<h4>Building your Dockerfile</h4> 
<p>There are two common methods for connecting to AWS Batch to run FPGA-enabled algorithms. The first method, which is the route Edico Genome took, involves storing your binaries in the Docker container itself and running that on top of an F1 instance with Docker installed. The following code example is what a Dockerfile to build your container might look like for this scenario.</p> 
<code class="lang-bash"># DRAGEN_EXEC Docker image generator --
# Run this Dockerfile from a local directory that contains the latest release of
# - Dragen RPM and Linux DMA Driver available from Edico
# - Edico's Dragen WFMS Wrapper files
FROM centos:centos7
RUN rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
# Install Basic packages needed for Dragen
RUN yum -y install \
perl \
sos \
coreutils \
gdb \
time \
systemd-libs \
bzip2-libs \
R \
ca-certificates \
ipmitool \
smartmontools \
rsync
# Install the Dragen RPM
RUN mkdir -m777 -p /var/log/dragen /var/run/dragen
ADD . /root
RUN rpm -Uvh /root/edico_driver*.rpm || true
RUN rpm -Uvh /root/dragen-aws*.rpm || true
# Auto generate the Dragen license
RUN /opt/edico/bin/dragen_lic -i auto
#########################################################
# Now install the Edico WFMS &quot;Wrapper&quot; functions
# Add development tools needed for some util
RUN yum groupinstall -y &quot;Development Tools&quot;
# Install necessary standard packages
RUN yum -y install \
dstat \
git \
python-devel \
python-pip \
time \
tree &amp;&amp; \
pip install --upgrade pip &amp;&amp; \
easy_install requests &amp;&amp; \
pip install psutil &amp;&amp; \
pip install python-dateutil &amp;&amp; \
pip install constants &amp;&amp; \
easy_install boto3
# Setup Python path used by the wrapper
RUN mkdir -p /opt/workflow/python/bin
RUN ln -s /usr/bin/python /opt/workflow/python/bin/python2.7
RUN ln -s /usr/bin/python /opt/workflow/python/bin/python
# Install d_haul and dragen_job_execute wrapper functions and associated packages
RUN mkdir -p /root/wfms/trunk/scheduler/scheduler
COPY scheduler/d_haul /root/wfms/trunk/scheduler/
COPY scheduler/dragen_job_execute /root/wfms/trunk/scheduler/
COPY scheduler/scheduler/aws_utils.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/constants.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/job_utils.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/logger.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/scheduler_utils.py&nbsp; /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/webapi.py /root/wfms/trunk/scheduler/scheduler/
COPY scheduler/scheduler/wfms_exception.py /root/wfms/trunk/scheduler/scheduler/
RUN touch /root/wfms/trunk/scheduler/scheduler/__init__.py
# Landing directory should be where DJX is located
WORKDIR &quot;/root/wfms/trunk/scheduler/&quot;
# Debug print of container's directories
RUN tree /root/wfms/trunk/scheduler
# Default behaviour. Over-ride with --entrypoint on docker run cmd line
ENTRYPOINT [&quot;/root/wfms/trunk/scheduler/dragen_job_execute&quot;]
CMD []</code> 
<p><em>Note: Edico Genome’s custom Python wrapper functions for its Workflow Management System (WFMS) in the latter part of this Dockerfile should be replaced with functions that are specific to your workflow.</em></p> 
<p>The second method is to install binaries and then use Docker as a lightweight connector between AWS Batch and the AFI. For example, this might be a route you would choose to use if you were provisioning <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=edico+genome+dragen&amp;page=1&amp;ref_=nav_search_box">DRAGEN from the AWS Marketplace</a>.</p> 
<p>In this case, the Dockerfile would not contain the installation of the binaries to run DRAGEN, but would contain any other packages necessary for job completion. When you run your Docker container, you enable Docker to access the underlying file system.</p> 
<h3>Connecting to AWS Batch</h3> 
<p>AWS Batch provisions compute resources and runs your jobs, choosing the right instance types based on your job requirements and scaling down resources as work is completed. AWS Batch users submit a job, based on a template or “job definition” to an AWS Batch job queue.</p> 
<p>Job queues are mapped to one or more compute environments that describe the quantity and types of resources that AWS Batch can provision. In this case, Edico created a managed compute environment that was able to launch 1,000 EC2 F1 instances across multiple Availability Zones in us-east-1. As jobs are submitted to a job queue, the service launches the required quantity and types of instances that are needed. As instances become available, AWS Batch then runs each job within appropriately sized Docker containers.</p> 
<p>The Edico Genome workflow manager API submits jobs to an AWS Batch job queue. This job queue maps to an AWS Batch managed compute environment containing On-Demand F1 instances. In this section, you can set this up yourself.</p> 
<p>To create the compute environment that DRAGEN can use:</p> 
<p><code>aws batch create-compute-environment --cli-input-json file://&lt;path_to_json_file&gt;/F1OnDemand.json</code></p> 
<p>Where your JSON file contains the following code (replace with your own resource IDs):</p> 
<code class="lang-json">
{
&quot;computeEnvironmentName&quot;: &quot;F1OnDemand&quot;,
&quot;type&quot;: &quot;MANAGED&quot;,
&quot;state&quot;: &quot;ENABLED&quot;,
&quot;computeResources&quot;: {
&quot;type&quot;: &quot;EC2&quot;,
&quot;minvCpus&quot;: 0,
&quot;maxvCpus&quot;: 128,
&quot;desiredvCpus&quot;: 0,
&quot;instanceTypes&quot;: [
&quot;f1.2xlarge&quot;,
&quot;f1.16xlarge&quot;
],
&quot;subnets&quot;: [
&quot;subnet-220c0e0a&quot;,
&quot;subnet-1a95556d&quot;,
&quot;subnet-978f6dce&quot;
],
&quot;securityGroupIds&quot;: [
&quot;sg-cf5093b2&quot;
],
&quot;ec2KeyPair&quot;: &quot;id_rsa&quot;,
&quot;instanceRole&quot;: &quot;ecsInstanceRole&quot;,
&quot;tags&quot;: {
&quot;Name&quot;: &quot;Batch Instance – F1OnDemand&quot;
}
},
&quot;serviceRole&quot;: &quot;arn:aws:iam::012345678910:role/service-role/AWSBatchServiceRole&quot;
}
</code> 
<p>And the corresponding job queue:</p> 
<p><code>aws batch create-job-queue --cli-input-json file://&lt;path_to_json_file&gt;/dragen.json</code></p> 
<p>Where dragen.json is as follows:</p> 
<code class="lang-json">
{
&quot;jobQueueName&quot;: &quot;DRAGEN-OnDemand&quot;,
&quot;state&quot;: &quot;ENABLED&quot;,
&quot;priority&quot;: 100,
&quot;computeEnvironmentOrder&quot;: 
[
{
&quot;order&quot;: 1,
&quot;computeEnvironment&quot;: &quot;F1OnDemand&quot;
}
]
}
</code> 
<p>An f1.2xlarge EC2 instance contains one FPGA, eight vCPUs, and 122-GiB RAM. As DRAGEN requires an entire FPGA to run, Edico Genome needed to ensure that only one analysis per time executed on an instance. By using the f1.2xlarge vCPUs and memory as a proxy in their AWS Batch job definition, Edico Genome could ensure that only one job runs on an instance at a time. Here’s what that looks like in the AWS CLI:</p> 
<p><code>aws batch register-job-definition --job-definition-name dragen-wgs --type container --container-properties '{ &quot;image&quot;: ${DRAGEN_IMAGE}, &quot;vcpus&quot;: 8, &quot;memory&quot;: 120000}'</code></p> 
<p>Now, you can submit jobs easily to your DRAGEN environment:</p> 
<p><code>aws batch submit-job --job-name dragen-run1 --job-queue DRAGEN-OnDemand --job-definition dragen-wgs --container-overrides command=${RUN_PARAMETERS}</code></p> 
<p>You can query the status of your DRAGEN job with the following command:</p> 
<p><code>aws batch describe-jobs --jobs &lt;the job ID from the above command&gt;</code></p> 
<p>The logs for your job are written to the /aws/batch/job CloudWatch log group.</p> 
<h3>Conclusion</h3> 
<p>In this post, we demonstrated how to set up an environment with AWS Batch that can run DRAGEN on EC2 F1 instances at scale. If you followed the walkthrough, you’ve replicated much of the architecture Edico Genome used to set the Guinness World Record.</p> 
<p>There are several ways in which you can harness the computational power of DRAGEN to analyze genomes at scale. First, DRAGEN is available through several different genomics platforms, such as the <a href="https://aws.amazon.com/blogs/apn/how-dnanexus-and-edico-genome-are-powering-precision-medicine-on-amazon-web-services-aws/">DNAnexus Platform</a>. DRAGEN is also available on the <a href="https://aws.amazon.com/marketplace/search/results?x=0&amp;y=0&amp;searchTerms=dragen&amp;page=1&amp;ref_=nav_search_box">AWS Marketplace</a>. You can apply the architecture presented in this post to build a scalable solution that is both performant and cost-optimized.</p> 
<p>For more information about how AWS Batch can facilitate genomics processing at scale, be sure to check out our <a href="https://github.com/awslabs/aws-batch-genomics">aws-batch-genomics</a> GitHub repo on high-throughput genomics on AWS.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3270');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/1_HRCaptureProcess.png" /> 
<b class="lb-b blog-post-title" property="name headline">Capturing Custom, High-Resolution Metrics from Containers Using AWS Step Functions and AWS Lambda</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-11-16T07:25:18+00:00">16 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/capturing-custom-high-resolution-metrics-from-containers-using-aws-step-functions-and-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3251" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3251&amp;disqus_title=Capturing+Custom%2C+High-Resolution+Metrics+from+Containers+Using+AWS+Step+Functions+and+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/capturing-custom-high-resolution-metrics-from-containers-using-aws-step-functions-and-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3251');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>Contributed by Trevor Sullivan, AWS Solutions Architect</em></p> 
<p>When&nbsp;you&nbsp;deploy containers&nbsp;with <a href="https://aws.amazon.com/ecs">Amazon ECS</a>, are you gathering all of the key metrics so that you can correctly monitor the overall health of your ECS cluster?</p> 
<p>By default, ECS writes metrics to <a href="https://aws.amazon.com/cloudwatch">Amazon CloudWatch</a> in 5-minute increments. For complex or large services, this may not be sufficient to make scaling decisions quickly. You may want to respond immediately to changes in workload or to identify application performance problems. Last July, CloudWatch announced <a href="https://aws.amazon.com/about-aws/whats-new/2017/07/amazon-cloudwatch-introduces-high-resolution-custom-metrics-and-alarms/">support for high-resolution metrics</a>, up to a per-second basis.</p> 
<p>These high-resolution metrics can be used to give you a clearer picture of the load and performance for your applications, containers, clusters, and hosts. In this post, I discuss how you can use <a href="https://aws.amazon.com/step-functions">AWS Step Functions</a>, along with <a href="https://aws.amazon.com/lambda">AWS Lambda</a>, to cost effectively record high-resolution metrics into CloudWatch. You implement this solution using a serverless architecture, which keeps your costs low and makes it easier to troubleshoot the solution.</p> 
<p>To show how this works, you retrieve some useful metric data from an ECS cluster running in the same AWS account and region (Oregon, us-west-2) as the Step Functions state machine and Lambda function. However, you can use this architecture to retrieve any custom application metrics from any resource in any AWS account and region.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/1_HRCaptureProcess.png" /></p> 
<b>Why Step Functions?</b> 
<p>Step Functions enables you to orchestrate multi-step tasks in the AWS Cloud that run for any period of time, up to a year. Effectively, you’re building a blueprint for an end-to-end process. After it’s built, you can execute the process as many times as you want.</p> 
<p>For this architecture, you gather metrics from an ECS cluster, every five seconds, and then write the metric data to CloudWatch. After your ECS cluster metrics are stored in CloudWatch, you can create CloudWatch alarms to notify you. An alarm can also trigger an automated remediation activity such as scaling ECS services, when a metric exceeds a threshold defined by you.</p> 
<p>When you build a Step Functions state machine, you define the different states inside it as JSON objects. The bulk of the work in Step Functions is handled by the common task state, which invokes Lambda functions or Step Functions activities. There is also a built-in library of other useful states that allow you to control the execution flow of your program.</p> 
<p>One of the most useful state types in Step Functions is the parallel state. Each parallel state in your state machine can have one or more branches, each of which is executed in parallel. Another useful state type is the wait state, which waits for a period of time before moving to the next state.</p> 
<p>In this walkthrough, you combine these three states (parallel, wait, and task) to create a state machine that triggers a Lambda function, which then gathers metrics from your ECS cluster.</p> 
<h3>Step Functions pricing</h3> 
<p>This state machine is executed every minute, resulting in 60 executions per hour, and 1,440 executions per day. Step Functions is billed per state transition,&nbsp;including&nbsp;the Start and End state transitions, and giving you approximately&nbsp;37,440 state transitions per day. To reach this number, I’m using this estimated math:</p> 
<blockquote> 
<p>26 state transitions per-execution x 60 minutes x 24 hours</p> 
</blockquote> 
<p>Based on current pricing, at $0.000025 per state transition, the daily cost of this metric gathering state machine would be <strong>$0.936</strong>.</p> 
<p>Step Functions offers an&nbsp;indefinite&nbsp;4,000 free state transitions every month. This benefit is available to all customers, not just customers who are still under the 12-month AWS Free Tier. For more information and cost example scenarios, see&nbsp;<a href="https://aws.amazon.com/step-functions/pricing/">Step Functions pricing</a>.</p> 
<b>Why Lambda?</b> 
<p>The goal is to capture metrics from an ECS cluster, and write the metric data to CloudWatch. This is a straightforward, short-running process that makes Lambda the perfect place to run your code. Lambda is one of the key services that makes up “Serverless” application architectures. It enables you to consume compute capacity only when your code is actually executing.</p> 
<p>The process of gathering metric data from ECS and writing it to CloudWatch takes a short period of time. In fact, my average Lambda function execution time, while developing this post, is only about 250 milliseconds on average. For every five-second interval that occurs, I’m only using 1/20th of the compute time that I’d otherwise be paying for.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/2_captureMetrics.png" /></p> 
<h3>Lambda pricing</h3> 
<p>For billing purposes, Lambda execution time is rounded up to the nearest 100-ms interval. In general, based on the metrics that I observed during development, a 250-ms runtime would be billed at 300 ms. Here, I calculate the cost of this Lambda function executing on a daily basis.</p> 
<p>Assuming 31 days in each month, there would be 535,680 five-second intervals (31 days x 24 hours x 60 minutes x 12 five-second intervals = 535,680). The Lambda function is invoked every five-second interval, by the Step Functions state machine, and runs for a 300-ms period. At current Lambda pricing, for a 128-MB function, you would be paying approximately the following:</p> 
<blockquote> 
<p><strong>Total compute</strong></p> 
<p>Total executions = 535,680<br /> Total compute = total executions x (3 x $0.000000208 per 100 ms) = <strong>$0.334 per day</strong></p> 
<p><strong>Total requests</strong></p> 
<p>Total requests = (535,680 / 1000000) * $0.20 per million requests = <strong>$0.11 per day</strong></p> 
<p><strong>Total Lambda Cost</strong></p> 
<p>$0.11 requests + $0.334 compute time = <strong>$0.444 per day</strong></p> 
</blockquote> 
<p>Similar to Step Functions, Lambda offers an indefinite free tier. For more information, see&nbsp;<a href="https://aws.amazon.com/lambda/pricing/">Lambda Pricing</a>.</p> 
<b>Walkthrough</b> 
<p>In the following sections, I step through the process of configuring the solution just discussed. If you follow along, at a high level, you will:</p> 
<li>Configure an IAM role and policy</li> 
<li>Create a Step Functions state machine to control metric gathering execution</li> 
<li>Create a metric-gathering Lambda function</li> 
<li>Configure a CloudWatch Events rule to trigger the state machine</li> 
<li>Validate the solution</li> 
<h3>Prerequisites</h3> 
<p>You should already have an AWS account with a running ECS cluster. If you don’t have one running, you can easily <a href="https://aws.amazon.com/getting-started/tutorials/deploy-docker-containers/">deploy a Docker container on an ECS cluster</a>&nbsp;using the AWS Management Console. In the example produced for this post, I use an ECS cluster running Windows Server (currently in beta), but either a Linux or Windows Server cluster works.</p> 
<h3>Create an IAM role and policy</h3> 
<p>First, create an IAM role and policy that enables Step Functions, Lambda, and CloudWatch to communicate with each other.</p> 
<li>The CloudWatch Events rule needs permissions to trigger the Step Functions state machine.</li> 
<li>The Step Functions state machine needs permissions to trigger the Lambda function.</li> 
<li>The Lambda function needs permissions to query ECS and then write to CloudWatch Logs and metrics.</li> 
<p>When you create the state machine, Lambda function, and CloudWatch Events rule, you assign this role to each of those resources. Upon execution, each of these resources assumes the specified role and executes using the role’s permissions.</p> 
<ol> 
<li>Open the IAM console.</li> 
<li>Choose <strong>Roles</strong>, create <strong>New Role</strong>.</li> 
<li>For Role Name, enter <code class="lang-bash">WriteMetricFromStepFunction</code>.</li> 
<li>Choose <strong>Save</strong>.</li> 
</ol> 
<p>Create the IAM role trust relationship<br /> The trust relationship (also known as the assume role policy document) for your IAM role looks like the following JSON document. As you can see from the document, your IAM role needs to trust the Lambda, CloudWatch Events, and Step Functions services. By configuring your role to trust these services, they can assume this role and inherit the role permissions.</p> 
<ol> 
<li>Open the IAM console.</li> 
<li>Choose <strong>Roles</strong> and select the IAM role previously created.</li> 
<li>Choose&nbsp;<strong>Trust Relationships</strong>,&nbsp;<strong>Edit Trust Relationships</strong>.</li> 
<li>Enter the following trust policy text and choose <strong>Save</strong>.</li> 
</ol> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;lambda.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;events.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;states.us-west-2.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}</code> 
<h3>Create an IAM policy</h3> 
<p>After you’ve finished configuring your role’s trust relationship, grant the role access to the other AWS resources that make up the solution.</p> 
<p>The IAM policy is what gives your IAM role permissions to access various resources. You must whitelist explicitly the specific resources to which your role has access, because the default IAM behavior is to deny access to any AWS resources.</p> 
<p>I’ve tried to keep this policy document as generic as possible, without allowing permissions to be too open. If the name of your ECS cluster is different than the one in the example policy below, make sure that you update the policy document before attaching it to your IAM role. You can attach this policy as an inline policy, instead of creating the policy separately first. However, either approach is valid.</p> 
<ol> 
<li>Open the IAM console.</li> 
<li>Select the IAM role, and choose <strong>Permissions</strong>.</li> 
<li>Choose&nbsp;<strong>Add in-line policy</strong>.</li> 
<li>Choose&nbsp;<strong>Custom Policy</strong>&nbsp;and then enter the following policy. The inline policy name does not matter.</li> 
</ol> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;logs:*&quot; ],
&quot;Resource&quot;: &quot;*&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;cloudwatch:PutMetricData&quot; ],
&quot;Resource&quot;: &quot;*&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;states:StartExecution&quot; ],
&quot;Resource&quot;: [
&quot;arn:aws:states:*:*:stateMachine:WriteMetricFromStepFunction&quot;
]
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;lambda:InvokeFunction&quot; ],
&quot;Resource&quot;: &quot;arn:aws:lambda:*:*:function:WriteMetricFromStepFunction&quot;
},
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [ &quot;ecs:Describe*&quot; ],
&quot;Resource&quot;: &quot;arn:aws:ecs:*:*:cluster/ECSEsgaroth&quot;
}
]
}</code> 
<h3>Create a Step Functions state machine</h3> 
<p>In this section, you create a Step Functions state machine that invokes the metric-gathering Lambda function every five (5) seconds, for a one-minute period. If you divide a minute (60) seconds into equal parts of five-second intervals, you get 12. Based on this math, you create 12 branches, in a single parallel state, in the state machine. Each branch triggers the metric-gathering Lambda function at a different five-second marker, throughout the one-minute period. After all of the parallel branches finish executing, the Step Functions execution completes and another begins.</p> 
<p>Follow these steps to create your Step Functions state machine:</p> 
<ol> 
<li>Open the Step Functions console.</li> 
<li>Choose&nbsp;<strong>Dashboard</strong>,&nbsp;<strong>Create State Machine</strong>.</li> 
<li>For&nbsp;State Machine Name,&nbsp;enter <code class="lang-bash">WriteMetricFromStepFunction</code>.</li> 
<li>Enter the state machine code below into the editor. Make sure that you insert your own AWS account ID for every instance of “676655494xxx”</li> 
<li>Choose&nbsp;<strong>Create State Machine</strong>.</li> 
<li>Select the&nbsp;<strong>WriteMetricFromStepFunction</strong>&nbsp;IAM role that you previously created.</li> 
</ol> 
<code class="lang-json">{
&quot;Comment&quot;: &quot;Writes ECS metrics to CloudWatch every five seconds, for a one-minute period.&quot;,
&quot;StartAt&quot;: &quot;ParallelMetric&quot;,
&quot;States&quot;: {
&quot;ParallelMetric&quot;: {
&quot;Type&quot;: &quot;Parallel&quot;,
&quot;Branches&quot;: [
{
&quot;StartAt&quot;: &quot;WriteMetricLambda&quot;,
&quot;States&quot;: {
&quot;WriteMetricLambda&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;WaitFive&quot;,
&quot;States&quot;: {
&quot;WaitFive&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 5,
&quot;Next&quot;: &quot;WriteMetricLambdaFive&quot;
},
&quot;WriteMetricLambdaFive&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;WaitTen&quot;,
&quot;States&quot;: {
&quot;WaitTen&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 10,
&quot;Next&quot;: &quot;WriteMetricLambda10&quot;
},
&quot;WriteMetricLambda10&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;WaitFifteen&quot;,
&quot;States&quot;: {
&quot;WaitFifteen&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 15,
&quot;Next&quot;: &quot;WriteMetricLambda15&quot;
},
&quot;WriteMetricLambda15&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait20&quot;,
&quot;States&quot;: {
&quot;Wait20&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 20,
&quot;Next&quot;: &quot;WriteMetricLambda20&quot;
},
&quot;WriteMetricLambda20&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait25&quot;,
&quot;States&quot;: {
&quot;Wait25&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 25,
&quot;Next&quot;: &quot;WriteMetricLambda25&quot;
},
&quot;WriteMetricLambda25&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait30&quot;,
&quot;States&quot;: {
&quot;Wait30&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 30,
&quot;Next&quot;: &quot;WriteMetricLambda30&quot;
},
&quot;WriteMetricLambda30&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait35&quot;,
&quot;States&quot;: {
&quot;Wait35&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 35,
&quot;Next&quot;: &quot;WriteMetricLambda35&quot;
},
&quot;WriteMetricLambda35&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait40&quot;,
&quot;States&quot;: {
&quot;Wait40&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 40,
&quot;Next&quot;: &quot;WriteMetricLambda40&quot;
},
&quot;WriteMetricLambda40&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait45&quot;,
&quot;States&quot;: {
&quot;Wait45&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 45,
&quot;Next&quot;: &quot;WriteMetricLambda45&quot;
},
&quot;WriteMetricLambda45&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait50&quot;,
&quot;States&quot;: {
&quot;Wait50&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 50,
&quot;Next&quot;: &quot;WriteMetricLambda50&quot;
},
&quot;WriteMetricLambda50&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
},
{
&quot;StartAt&quot;: &quot;Wait55&quot;,
&quot;States&quot;: {
&quot;Wait55&quot;: {
&quot;Type&quot;: &quot;Wait&quot;,
&quot;Seconds&quot;: 55,
&quot;Next&quot;: &quot;WriteMetricLambda55&quot;
},
&quot;WriteMetricLambda55&quot;: {
&quot;Type&quot;: &quot;Task&quot;,
&quot;Resource&quot;: &quot;arn:aws:lambda:us-west-2:676655494xxx:function:WriteMetricFromStepFunction&quot;,
&quot;End&quot;: true
} 
}
}
],
&quot;End&quot;: true
}
}
}</code> 
<p>Now you’ve got a shiny new Step Functions state machine! However, you might ask yourself, “After the state machine has been created, how does it get executed?” Before I answer that question, create the Lambda function that writes the custom metric, and then you get the end-to-end process moving.</p> 
<h3>Create a Lambda function</h3> 
<p>The meaty part of the solution is a Lambda function, written to consume the Python 3.6 runtime, that retrieves metric values from ECS, and then writes them to CloudWatch. This Lambda function is what the Step Functions state machine is triggering every five seconds, via the Task states. Key points to remember:</p> 
<p>The Lambda function needs permission to:</p> 
<li>Write CloudWatch metrics (PutMetricData API).</li> 
<li>Retrieve metrics from ECS clusters (DescribeCluster API).</li> 
<li>Write StdOut to CloudWatch Logs.</li> 
<p>Boto3, the AWS SDK for Python, is&nbsp;included in the Lambda execution environment&nbsp;for Python 2.x and 3.x.</p> 
<p>Because&nbsp;<a href="http://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html">Lambda includes the AWS SDK</a>, you don’t have to worry about packaging it up and uploading it to Lambda. You can focus on writing code and automatically take a dependency on boto3.</p> 
<p>As for permissions, you’ve already created the IAM role and attached a policy to it that enables your Lambda function to access the necessary API actions. When you create your Lambda function, make sure that you select the correct IAM role, to ensure it is invoked with the correct permissions.</p> 
<p>The following Lambda function code is generic. So how does the Lambda function know which ECS cluster to gather metrics for? Your Step Functions state machine automatically passes in its state to the Lambda function. When you create your CloudWatch Events rule, you specify a simple JSON object that passes the desired ECS cluster name into your Step Functions state machine, which then passes it to the Lambda function.</p> 
<p>Use the following property values as you create your Lambda function:</p> 
<blockquote> 
<p><strong>Function Name</strong>: <code class="lang-bash">WriteMetricFromStepFunction</code><br /> <strong>Description</strong>: This Lambda function retrieves metric values from an ECS cluster and writes them to Amazon CloudWatch.<br /> <strong>Runtime</strong>: Python3.6<br /> <strong>Memory</strong>: 128 MB<br /> <strong>IAM Role</strong>: WriteMetricFromStepFunction</p> 
</blockquote> 
<code class="lang-python">import boto3
def handler(event, context):
cw = boto3.client('cloudwatch')
ecs = boto3.client('ecs')
print('Got boto3 client objects')
Dimension = {
'Name': 'ClusterName',
'Value': event['ECSClusterName']
}
cluster = get_ecs_cluster(ecs, Dimension['Value'])
cw_args = {
'Namespace': 'ECS',
'MetricData': [
{
'MetricName': 'RunningTask',
'Dimensions': [ Dimension ],
'Value': cluster['runningTasksCount'],
'Unit': 'Count',
'StorageResolution': 1
},
{
'MetricName': 'PendingTask',
'Dimensions': [ Dimension ],
'Value': cluster['pendingTasksCount'],
'Unit': 'Count',
'StorageResolution': 1
},
{
'MetricName': 'ActiveServices',
'Dimensions': [ Dimension ],
'Value': cluster['activeServicesCount'],
'Unit': 'Count',
'StorageResolution': 1
},
{
'MetricName': 'RegisteredContainerInstances',
'Dimensions': [ Dimension ],
'Value': cluster['registeredContainerInstancesCount'],
'Unit': 'Count',
'StorageResolution': 1
}
]
}
cw.put_metric_data(**cw_args)
print('Finished writing metric data')
def get_ecs_cluster(client, cluster_name):
cluster = client.describe_clusters(clusters = [ cluster_name ])
print('Retrieved cluster details from ECS')
return cluster['clusters'][0]</code> 
<h3>Create the CloudWatch Events rule</h3> 
<p>Now you’ve created an IAM role and policy, Step Functions state machine, and Lambda function. How do these components actually start communicating with each other? The final step in this process is to set up a CloudWatch Events rule that triggers your metric-gathering Step Functions state machine every minute. You have two choices for your&nbsp;CloudWatch Events rule expression: rate or cron. In this example, use the cron expression.</p> 
<p>A couple key learning points from creating the CloudWatch Events rule:</p> 
<li>You can specify one or more targets, of different types (for example, Lambda function, Step Functions state machine, SNS topic, and so on).</li> 
<li>You’re required to specify an IAM role with permissions to trigger your target.<br /> <strong>NOTE</strong>: This applies only to certain types of targets, including Step Functions state machines.</li> 
<li>Each target that supports IAM roles can be triggered using a different IAM role, in the same CloudWatch Events rule.</li> 
<li><em>Optional:</em>&nbsp;You can provide custom JSON that is passed to your target Step Functions state machine as input.</li> 
<p>Follow these steps to create the CloudWatch Events rule:</p> 
<ol> 
<li>Open the CloudWatch console.</li> 
<li>Choose&nbsp;<strong>Events</strong>, <strong>Rules</strong>,&nbsp;<strong>Create Rule</strong>.</li> 
<li>Select Schedule, Cron Expression,&nbsp;and then enter the following rule:<br /> <code class="lang-bash">0/1 * * * ?&nbsp;*</code></li> 
<li>Choose&nbsp;<strong>Add Target</strong>, <strong>Step Functions State Machine</strong>,&nbsp;<strong>WriteMetricFromStepFunction</strong>.</li> 
<li>For&nbsp;<strong>Configure Input</strong>,&nbsp;select <strong>Constant (JSON Text)</strong>.</li> 
<li>Enter the following JSON input, which is passed to Step Functions, while changing the cluster name accordingly:<br /> <code class="lang-json">{ &quot;ECSClusterName&quot;: &quot;ECSEsgaroth&quot; }</code></li> 
<li>Choose&nbsp;Use Existing Role, WriteMetricFromStepFunction&nbsp;(the IAM role that you previously created).</li> 
</ol> 
<p>After you’ve completed with these steps, your screen should look similar to this:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/3_metricTargets-822x1024.png" /></p> 
<h3>Validate the solution</h3> 
<p>Now that you have finished implementing the solution to gather high-resolution metrics from ECS, validate that it’s working properly.</p> 
<ol> 
<li>Open the CloudWatch console.</li> 
<li>Choose&nbsp;<strong>Metrics</strong>.</li> 
<li>Choose <strong>custom</strong>&nbsp;and select the <strong>ECS</strong> namespace.</li> 
<li>Choose the&nbsp;<strong>ClusterName</strong>&nbsp;metric dimension.</li> 
</ol> 
<p>You should see your metrics listed below.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/16/4_metricList.png" /></p> 
<h3>Troubleshoot configuration issues</h3> 
<p>If you aren’t receiving the expected ECS cluster metrics in CloudWatch, check for the following common configuration issues. Review the earlier procedures to make sure that the resources were properly configured.</p> 
<li><strong>The IAM role’s trust relationship is incorrectly configured</strong>.<br /> Make sure that the IAM role trusts Lambda, CloudWatch Events, and Step Functions in the correct region.</li> 
<li><strong>The IAM role does not have the correct policies attached to it</strong>.<br /> Make sure that you have copied the IAM policy correctly as an inline policy on the IAM role.</li> 
<li><strong>The CloudWatch Events rule is not triggering new Step Functions executions.</strong><br /> Make sure that the target configuration on the rule has the correct Step Functions state machine and IAM role selected.</li> 
<li><strong>The Step Functions state machine is being executed, but failing part way through.</strong><br /> Examine the detailed error message on the failed state within the failed Step Functions execution. It’s possible that the</li> 
<li><strong>IAM role does not have permissions to trigger the target Lambda function, that the target Lambda function may not exist, or that the Lambda function failed to complete successfully due to invalid permissions.</strong><br /> Although the above list covers several different potential configuration issues, it is not comprehensive. Make sure that you understand how each service is connected to each other, how permissions are granted through IAM policies, and how IAM trust relationships work.</li> 
<b>Conclusion</b> 
<p>In this post, you implemented a Serverless solution to gather and record high-resolution application metrics from containers running on Amazon ECS into CloudWatch. The solution consists of a Step Functions state machine, Lambda function, CloudWatch Events rule, and an IAM role and policy. The data that you gather from this solution helps you rapidly identify issues with an ECS cluster.</p> 
<p>To gather high-resolution metrics from&nbsp;any&nbsp;service, modify your Lambda function to gather the correct metrics from your target. If you prefer not to use Python, you can implement a Lambda function using one of the other supported runtimes, including Node.js, Java, or .NET Core. However, this post should give you the fundamental basics about capturing high-resolution metrics in CloudWatch.</p> 
<p>If you found this post useful,&nbsp;or have questions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3251');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/ecs_tasknetworking.png" /> 
<b class="lb-b blog-post-title" property="name headline">Introducing Cloud Native Networking for Amazon ECS Containers</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-11-14T21:27:04+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3212" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3212&amp;disqus_title=Introducing+Cloud+Native+Networking+for+Amazon+ECS+Containers&amp;disqus_url=https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3212');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of ECS Sr. Software Dev Engineer Anirudh Aithal.</em></p> 
<p>Today, AWS announced <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-ecs-introduces-awsvpc-networking-mode-for-containers-to-support-full-networking-capabilities/">task networking</a> for Amazon ECS. This feature brings <a href="https://aws.amazon.com/ec2">Amazon EC2</a> networking capabilities to tasks using <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">elastic network interfaces</a>.</p> 
<p>An elastic network interface is a virtual network interface that you can attach to an instance in a <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html">VPC</a>. When you launch an EC2 virtual machine, an elastic network interface is automatically provisioned to provide networking capabilities for the instance.</p> 
<p>A task is a logical group of running containers. Previously, tasks running on <a href="https://aws.amazon.com/ecs">Amazon ECS</a> shared the elastic network interface of their EC2 host. Now, the new <code class="lang-bash">awsvpc</code> networking mode lets you attach an elastic network interface directly to a task.</p> 
<p>This simplifies network configuration, allowing you to treat each container just like an EC2 instance with full networking features, segmentation, and security controls in the VPC.</p> 
<p>In this post, I cover how <code class="lang-bash">awsvpc</code> mode works and show you how you can start using elastic network interfaces with your tasks running on ECS.<span id="more-3212"></span></p> 
<b>Background: &nbsp;Elastic network interfaces in EC2</b> 
<p>When you launch EC2 instances within a VPC, you don’t have to configure an additional overlay network for those instances to communicate with each other. By default, routing tables in the VPC enable seamless communication between instances and other endpoints. This is made possible by virtual network interfaces in VPCs called elastic network interfaces. Every EC2 instance that launches is automatically assigned an elastic network interface (the primary network interface). All networking parameters—such as subnets, security groups, and so on—are handled as properties of this primary network interface.</p> 
<p>Furthermore, an IPv4 address is allocated to every elastic network interface by the VPC at creation (the primary IPv4 address). This primary address is unique and routable within the VPC. This effectively makes your VPC a flat network, resulting in a simple networking topology.</p> 
<p>Elastic network interfaces can be treated as fundamental building blocks for connecting various endpoints in a VPC, upon which you can build higher-level abstractions. This allows elastic network interfaces to be leveraged for:</p> 
<li>VPC-native IPv4 addressing and routing (between instances and other endpoints in the VPC)</li> 
<li>Network traffic isolation</li> 
<li>Network policy enforcement using ACLs and firewall rules (security groups)</li> 
<li>IPv4 address range enforcement (via subnet CIDRs)</li> 
<b>Why use awsvpc?</b> 
<p>Previously, ECS relied on the networking capability provided by Docker’s default networking behavior to set up the network stack for containers. With the default bridge network mode, containers on an instance are connected to each other using the docker0 bridge. Containers use this bridge to communicate with endpoints outside of the instance, using the primary elastic network interface of the instance on which they are running. Containers share and rely on the networking properties of the primary elastic network interface, including the firewall rules (security group subscription) and IP addressing.</p> 
<p>This means you cannot address these containers with the IP address allocated by Docker (it’s allocated from a pool of locally scoped addresses), nor can you enforce finely grained network ACLs and firewall rules. Instead, containers are addressable in your VPC by the combination of the IP address of the primary elastic network interface of the instance, and the host port to which they are mapped (either via static or dynamic port mapping). Also, because a single elastic network interface is shared by multiple containers, it can be difficult to create easily understandable network policies for each container.</p> 
<p>The <code class="lang-bash">awsvpc</code> networking mode addresses these issues by provisioning elastic network interfaces on a per-task basis. Hence, containers no longer share or contend use these resources. This enables you to:</p> 
<li>Run multiple copies of the container on the same instance using the same container port without needing to do any port mapping or translation, simplifying the application architecture.</li> 
<li>Extract higher network performance from your applications as they no longer contend for bandwidth on a shared bridge.</li> 
<li>Enforce finer-grained access controls for your containerized applications by associating security group rules for each Amazon ECS task, thus improving the security for your applications.</li> 
<p>Associating security group rules with a container or containers in a task allows you to restrict the ports and IP addresses from which your application accepts network traffic. For example, you can enforce a policy allowing SSH access to your instance, but blocking the same for containers. Alternatively, you could also enforce a policy where you allow HTTP traffic on port 80 for your containers, but block the same for your instances. Enforcing such security group rules greatly reduces the surface area of attack for your instances and containers.</p> 
<p>ECS manages the lifecycle and provisioning of elastic network interfaces for your tasks, creating them on-demand and cleaning them up after your tasks stop. You can specify the same properties for the task as you would when launching an EC2 instance. This means that containers in such tasks are:</p> 
<li>Addressable by IP addresses and the DNS name of the elastic network interface</li> 
<li>Attachable as ‘IP’ targets to Application Load Balancers and Network Load Balancers</li> 
<li>Observable from VPC flow logs</li> 
<li>Access controlled by security groups</li> 
<p>&shy;This also enables you to run multiple copies of the same task definition on the same instance, without needing to worry about port conflicts. You benefit from higher performance because you don’t need to perform any port translations or contend for bandwidth on the shared docker0 bridge, as you do with the bridge networking mode.</p> 
<b>Getting started</b> 
<p>If you don’t already have an ECS cluster, you can create one using the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/create_cluster.html">create cluster wizard</a>. In this post, I use&nbsp;“<code class="lang-bash">awsvpc-demo</code>” as the cluster name. Also, if you are following along with the command line instructions, make sure that you have the latest version of the <a href="https://aws.amazon.com/cli/">AWS CLI</a>&nbsp;or <a href="https://aws.amazon.com/tools/">SDK</a>.</p> 
<h3>Registering the task definition</h3> 
<p>The only change to make in your task definition for task networking is to set the <code class="lang-bash">networkMode</code> parameter to <code class="lang-bash">awsvpc</code>. In the <a href="https://console.aws.amazon.com/ecs/">ECS console</a>, enter this value for <strong>Network Mode</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_1.png" /></p> 
<p>If you plan on registering a container in this task definition with an ECS service, also specify a container port in the task definition. This example specifies an NGINX container exposing port 80:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_2.png" /></p> 
<p>This creates a task definition named “<code class="lang-bash">nginx-awsvpc&quot;</code> with networking mode set to <code class="lang-bash">awsvpc</code>. The following commands illustrate registering the task definition from the command line:</p> 
<code class="lang-bash"><strong>$ cat nginx-awsvpc.json</strong>
{
&quot;family&quot;: &quot;nginx-awsvpc&quot;,
&quot;networkMode&quot;: &quot;awsvpc&quot;,
&quot;containerDefinitions&quot;: [
{
&quot;name&quot;: &quot;nginx&quot;,
&quot;image&quot;: &quot;nginx:latest&quot;,
&quot;cpu&quot;: 100,
&quot;memory&quot;: 512,
&quot;essential&quot;: true,
&quot;portMappings&quot;: [
{
&quot;containerPort&quot;: 80,
&quot;protocol&quot;: &quot;tcp&quot;
}
]
}
]
}
</code> 
<strong><code class="lang-bash">$ aws ecs register-task-definition --cli-input-json file://./nginx-awsvpc.json</code></strong> 
<h3>Running the task</h3> 
<p>To run a task with this task definition, navigate to the cluster in the Amazon ECS console and choose <strong>Run new task</strong>. Specify the task definition as “<code class="lang-bash">nginx-awsvpc</code>“. Next, specify the set of subnets in which to run this task. You must have instances registered with ECS in at least one of these subnets. Otherwise, ECS can’t find a candidate instance to attach the elastic network interface.</p> 
<p>You can use the console to narrow down the subnets by selecting a value for <strong>Cluster VPC</strong>:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_3.png" /></p> 
<p>Next, select a security group for the task. For the purposes of this example, create a new security group that allows ingress only on port 80. Alternatively, you can also select security groups that you’ve already created.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_4.png" /></p> 
<p>Next, run the task by choosing <strong>Run Task</strong>.</p> 
<p>You should have a running task now. If you look at the details of the task, you see that it has an elastic network interface allocated to it, along with the IP address of the elastic network interface:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_intro_5.png" /></p> 
<p>You can also use the command line to do this:</p> 
<code class="lang-bash"><strong>$ aws ecs run-task --cluster awsvpc-ecs-demo --network-configuration &quot;awsvpcConfiguration={subnets=[&quot;subnet-c070009b&quot;],securityGroups=[&quot;sg-9effe8e4&quot;]}&quot; nginx-awsvpc $ aws ecs describe-tasks --cluster awsvpc-ecs-demo --task $ECS_TASK_ARN --query tasks[0]</strong>
{
&quot;taskArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:task/f5xx-...&quot;,
&quot;group&quot;: &quot;family:nginx-awsvpc&quot;,
&quot;attachments&quot;: [
{
&quot;status&quot;: &quot;ATTACHED&quot;,
&quot;type&quot;: &quot;ElasticNetworkInterface&quot;,
&quot;id&quot;: &quot;xx..&quot;,
&quot;details&quot;: [
{
&quot;name&quot;: &quot;subnetId&quot;,
&quot;value&quot;: &quot;subnet-c070009b&quot;
},
{
&quot;name&quot;: &quot;networkInterfaceId&quot;,
&quot;value&quot;: &quot;eni-b0aaa4b2&quot;
},
{
&quot;name&quot;: &quot;macAddress&quot;,
&quot;value&quot;: &quot;0a:47:e4:7a:2b:02&quot;
},
{
&quot;name&quot;: &quot;privateIPv4Address&quot;,
&quot;value&quot;: &quot;10.0.0.35&quot;
}
]
}
],
...
&quot;desiredStatus&quot;: &quot;RUNNING&quot;,
&quot;taskDefinitionArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:task-definition/nginx-awsvpc:2&quot;,
&quot;containers&quot;: [
{
&quot;containerArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:container/62xx-...&quot;,
&quot;taskArn&quot;: &quot;arn:aws:ecs:us-west-2:xx..x:task/f5x-...&quot;,
&quot;name&quot;: &quot;nginx&quot;,
&quot;networkBindings&quot;: [],
&quot;lastStatus&quot;: &quot;RUNNING&quot;,
&quot;networkInterfaces&quot;: [
{
&quot;privateIpv4Address&quot;: &quot;10.0.0.35&quot;,
&quot;attachmentId&quot;: &quot;xx..&quot;
}
]
}
]
}</code> 
<p>When you describe an “awsvpc” task, details of the elastic network interface are returned via the “attachments” object. You can also get this information from the “containers” object. For example:</p> 
<code class="lang-bash"><strong>$ aws ecs describe-tasks --cluster awsvpc-ecs-demo --task $ECS_TASK_ARN --query tasks[0].containers[0].networkInterfaces[0].privateIpv4Address</strong>
&quot;10.0.0.35&quot;</code> 
<b>Conclusion</b> 
<p>The nginx container is now addressable in your VPC via the <code>10.0.0.35</code> IPv4 address. You did not have to modify the security group on the instance to allow requests on port 80, thus improving instance security. Also, you ensured that all ports apart from port 80 were blocked for this application without modifying the application itself, which makes it easier to manage your task on the network. You did not have to interact with any of the elastic network interface API operations, as ECS handled all of that for you.</p> 
<p>You can read more about the task networking feature in the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html">ECS documentation</a>. For a detailed look at how this new networking mode is implemented on an instance, see <a href="https://aws.amazon.com/blogs/compute/under-the-hood-task-networking-for-amazon-ecs/">Under the Hood: Task Networking for Amazon ECS</a>.</p> 
<p>Please use the comments section below to send your feedback.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3212');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/ecs_tasknetworking.png" /> 
<b class="lb-b blog-post-title" property="name headline">Under the Hood: Task Networking for Amazon ECS</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-11-14T21:27:01+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/under-the-hood-task-networking-for-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3199" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3199&amp;disqus_title=Under+the+Hood%3A+Task+Networking+for+Amazon+ECS&amp;disqus_url=https://aws.amazon.com/blogs/compute/under-the-hood-task-networking-for-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3199');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post courtesy of ECS&nbsp;Sr. Software Dev Engineer Anirudh Aithal.</em></p> 
<p>Today, AWS&nbsp;announced <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/amazon-ecs-introduces-awsvpc-networking-mode-for-containers-to-support-full-networking-capabilities/">task networking</a> for <a href="https://aws.amazon.com/ecs">Amazon ECS</a>, which enables <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">elastic network interfaces</a> to be attached to containers.</p> 
<p>In this post, I take a closer look at how this new container-native “awsvpc” network mode is implemented using <a href="https://github.com/containernetworking/cni">container networking interface</a> plugins on ECS managed instances (referred to as container instances).</p> 
<p>This post is a deep dive into how task networking works with Amazon ECS. If you want to learn more about how you can start using task networking for your containerized applications, see <a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">Introducing Cloud Native Networking for Amazon ECS Containers</a>. Cloud Native Computing Foundation (CNCF) hosts the <a href="https://github.com/containernetworking/cni">Container Networking Interface (CNI)</a> project, which consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers. For more about cloud native computing in AWS, see Adrian Cockcroft’s post on <a href="https://medium.com/@adrianco/cloud-native-computing-5f0f41a982bf">Cloud Native Computing</a>.</p> 
<p><span id="more-3199"></span></p> 
<b>Container instance setup</b> 
<p>Before I discuss the details of enabling task networking on container instances, look at how a typical instance looks in ECS.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_dd_1.png" /></p> 
<p>The diagram above shows a typical container instance. The <a href="https://github.com/aws/amazon-ecs-agent/">ECS agent</a>, which itself is running as a container, is responsible for:</p> 
<li>Registering the EC2 instance with the ECS backend</li> 
<li>Ensuring that task state changes communicated to it by the ECS backend are enacted on the container instance</li> 
<li>Interacting with the Docker daemon to create, start, stop, and monitor</li> 
<li>Relaying container state and task state transitions to the ECS backend</li> 
<p>Because the ECS agent is just acting as the supervisor for containers under its management, it offloads the problem of setting up networking for containers to either the Docker daemon (for containers configured with one of Docker’s default networking modes) or a set of CNI plugins (for containers in task with networking mode set to <code class="lang-bash">awsvpc</code>).</p> 
<p>In either case, network stacks of containers are configured via network namespaces. As per the <code class="lang-bash">ip-netns(8)</code> manual, “<em>A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices</em>.” The network namespace construct makes the partitioning of network stack between processes and containers running on a host possible.</p> 
<b>Network namespaces and CNI plugins</b> 
<p>CNI plugins are executable files that comply with the <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI specification</a> and configure the network connectivity of containers. The CNI project defines a specification for the plugins and provides a library for interacting with plugins, thus providing a consistent, reliable, and simple interface with which to interact with the plugins.</p> 
<p>You specify the container or its network namespace and invoke the plugin with the ADD command to add network interfaces to a container, and then the DEL command to tear them down. For example, the reference <a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/bridge">bridge plugin</a> adds all containers on the same host into a bridge that resides in the host network namespace.</p> 
<p>This plugin model fits in nicely with the ECS agent’s “<em>minimal intrusion in the container lifecycle</em>” model, as the agent doesn’t need to concern itself with the details of the network setup for containers. It’s also an extensible model, which allows the agent to switch to a different set of plugins if the need arises in future. Finally, the ECS agent doesn’t need to monitor the liveliness of these plugins as they are only invoked when required.</p> 
<b>Invoking CNI plugins from the ECS agent</b> 
<p>When ECS attaches an elastic network interface to the instance and sends the message to the agent to provision the elastic network interface for containers in a task, the elastic network interface (as with any network device) shows up in the global default network namespace of the host. The ECS agent invokes a chain of CNI plugins to ensure that the elastic network interface is configured appropriately in the container’s network namespace. You can review these plugins in the <a href="https://github.com/aws/amazon-ecs-cni-plugins">amazon-ecs-cni-plugins GitHub repo</a>.</p> 
<p>The first plugin invoked in this chain is the <a href="https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/eni">ecs-eni</a>&nbsp;plugin,&nbsp;which ensures that the elastic network interface is attached to container’s network namespace and configured with the VPC-allocated IP addresses and the default route to use the subnet gateway. The container also needs to make HTTP requests to the credentials endpoint (hosted by the ECS agent) for getting IAM role credentials. This is handled by the <a href="https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/ecs-bridge">ecs-bridge</a> and <a href="https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/ipam">ecs-ipam</a> plugins, which are invoked next. The CNI library provides mechanisms to interpret the results from the execution of these plugins, which results in an efficient error handling in the agent. The following diagram illustrates the different steps in this process:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/15/tn_dd_2.png" /></p> 
<p>To avoid the race condition between configuring the network stack and commands being invoked in application containers, the ECS agent creates an additional “pause” container for each task before starting the containers in the task definition. It then sets up the network namespace of the pause container by executing the previously mentioned CNI plugins. It also starts the rest of the containers in the task so that they share their network stack of the pause container. This means that all containers in a task are addressable by the IP addresses of the elastic network interface, and they can communicate with each other over the <code class="lang-bash">localhost</code> interface.</p> 
<p>In this example setup, you have two containers in a task behind an elastic network interface. The following commands show that they have a similar view of the network stack and can talk to each other over the localhost interface.</p> 
<h3>List the last three containers running on the host (you launched a task with two containers and the ECS agent launched the additional container to configure the network namespace):</h3> 
<code class="lang-bash"><b>$ docker ps -n 3 --format &quot;{{.ID}}\t{{.Names}}\t{{.Command}}\t{{.Status}}&quot;</b>
7d7b7fbc30b9	ecs-front-envoy-5-envoy-sds-ecs-ce8bd9eca6dd81a8d101	&quot;/bin/sh -c '/usr/...&quot;	Up 3 days
dfdcb2acfc91	ecs-front-envoy-5-front-envoy-faeae686adf9c1d91000	&quot;/bin/sh -c '/usr/...&quot;	Up 3 days
f731f6dbb81c	ecs-front-envoy-5-internalecspause-a8e6e19e909fa9c9e901	&quot;./pause&quot;	Up 3 days</code> 
<h3>List interfaces for these containers and make sure that they are the same:</h3> 
<code class="lang-bash"><b>$ for id in `docker ps -n 3 -q`; do pid=`docker inspect $id -f '{{.State.Pid}}'`; echo container $id; sudo nsenter -t $pid -n ip link show; done</b>
container 7d7b7fbc30b9
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: ecs-eth0@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
link/ether 0a:58:a9:fe:ac:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
27: etb2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:5a:a1:1a:43:42 brd ff:ff:ff:ff:ff:ff
container dfdcb2acfc91
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: ecs-eth0@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
link/ether 0a:58:a9:fe:ac:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
27: etb2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:5a:a1:1a:43:42 brd ff:ff:ff:ff:ff:ff
container f731f6dbb81c
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
3: ecs-eth0@if28: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
link/ether 0a:58:a9:fe:ac:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
27: etb2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 02:5a:a1:1a:43:42 brd ff:ff:ff:ff:ff:ff</code> 
<b>Conclusion</b> 
<p>All of this work means that you can use the new <code class="lang-bash">awsvpc</code> networking mode and benefit from native networking support for your containers. You can learn more about using <code class="lang-bash">awsvpc</code> mode in <a href="https://aws.amazon.com/blogs/compute/introducing-cloud-native-networking-for-ecs-containers/">Introducing Cloud Native Networking for Amazon ECS Containers</a> or the <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html">ECS documentation</a>.</p> 
<p>I&nbsp;appreciate your feedback in the comments section. You can also reach&nbsp;me on GitHub in either the <a href="https://github.com/aws/amazon-ecs-cni-plugins">ECS CNI Plugins</a> or the <a href="https://github.com/aws/amazon-ecs-agent">ECS Agent</a> repositories.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3199');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/18/cross-account-integration-with-amazon-sns_img1-1260x459.png" /> 
<b class="lb-b blog-post-title" property="name headline">Cross-Account Integration with Amazon SNS</b> 
<p style="margin: 0; padding:0;">=======================<p>
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-14T12:51:03+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/" title="View all posts in Messaging*"><span property="articleSection">Messaging*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/cross-account-integration-with-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2726" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2726&amp;disqus_title=Cross-Account+Integration+with+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/cross-account-integration-with-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2726');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h4>Contributed by Zak Islam, Senior Manager, Software Development, AWS Messaging</h4> 
<p><a href="https://aws.amazon.com/sns/">Amazon Simple Notification Service</a>&nbsp;(Amazon SNS)&nbsp;is a fully managed AWS service that makes it easy to decouple your application components and fan-out messages. SNS provides topics (similar to topics in message brokers such as RabbitMQ or ActiveMQ) that you can use to create 1:1, 1:N, or N:N producer/consumer design patterns. For more information about how to send messages from SNS to Amazon SQS, AWS Lambda, or HTTP(S) endpoints in the same account, see <a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.html">Sending Amazon SNS Messages to Amazon SQS Queues</a>.</p> 
<p>SNS can be used to send messages within a single account or to resources in different accounts to create administrative isolation. This enables administrators to grant only the minimum level of permissions required to process a workload (for example, limiting the scope of your application account to only send messages and to deny deletes). This approach is commonly known as the “principle of least privilege.” If you are interested, read more about AWS’s <a href="https://aws.amazon.com/answers/account-management/aws-multi-account-security-strategy/">multi-account security strategy</a>.</p> 
<p>This is great from a security perspective, but why would you want to share messages between accounts? It may sound scary, but it’s a common practice to isolate application components (such as producer and consumer) to operate using different AWS accounts to lock down privileges in case credentials are exposed. In this post, I go slightly deeper and explore how to set up your SNS topic so that it can route messages to SQS queues that are owned by a separate AWS account.</p> 
<b>Potential use cases</b> 
<p style="text-align: left">First, look at a common order processing design pattern:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_1.png" /></a></p> 
<p><span id="more-2726"></span>This is a simple architecture. A web server submits an order directly to an SNS topic, which then fans out messages to two SQS queues. One SQS queue is used to track all incoming orders for audits (such as anti-entropy, comparing the data of all replicas and updating each replica to the newest version). The other is used to pass the request to the order processing systems.</p> 
<p>Imagine now that a few years have passed, and your downstream processes no longer scale, so you are kicking around the idea of a re-architecture project. To thoroughly test your system, you need a way to replay your production messages in your development system. Sure, you can build a system to replicate and replay orders from your production environment in your development environment. Wouldn’t it be easier to subscribe your development queues to the production SNS topic so you can test your new system in real time? That’s exactly what you can do here.</p> 
<p>Here’s another use case. As your business grows, you recognize the need for more metrics from your order processing pipeline. The analytics team at your company has built a metrics aggregation service and ingests data via a central SQS queue. Their architecture is as follows:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_2.png" /></a></p> 
<p>Again, it’s a fairly simple architecture. All data is ingested via SQS queues (master_ingest_queue, in this case). You subscribe the master_ingest_queue, running under the analytics team’s AWS account, to the topic that is in the order management team’s account.</p> 
<b>Making it work</b> 
<p>Now that you’ve seen a few scenarios, let’s dig into the details. There are a couple of ways to link an SQS queue to an SNS topic (subscribe a queue to a topic):</p> 
<ol> 
<li>The queue owner can create a subscription to the topic.</li> 
<li>The topic owner can subscribe a queue in another account to the topic.</li> 
</ol> 
<b>Queue owner subscription</b> 
<p>What happens when the queue owner subscribes to a topic? In this case, assume that the topic owner has given permission to the subscriber’s account to call the Subscribe API action using the topic ARN (Amazon Resource Name). For the examples below, also assume the following:</p> 
<li>&nbsp;<em>Topic_Owner</em> is the identifier for the account that owns the topic <em>MainTopic</em></li> 
<li><em>Queue_Owner</em> is the identifier for the account that owns the queue subscribed to the main topic</li> 
<p>To enable the subscriber to subscribe to a topic, the topic owner must add the sns:Subscribe and topic ARN to the topic policy via the AWS Management Console, as follows:</p> 
<code class="lang-json">{
&quot;Version&quot;:&quot;2012-10-17&quot;,
&quot;Id&quot;:&quot;MyTopicSubscribePolicy&quot;,
&quot;Statement&quot;:[{
&quot;Sid&quot;:&quot;Allow-other-account-to-subscribe-to-topic&quot;,
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Principal&quot;:{
&quot;AWS&quot;:&quot;Topic_Owner&quot;
},
&quot;Action&quot;:&quot;sns:Subscribe&quot;,
&quot;Resource&quot;:&quot;arn:aws:sns:us-east-1:Queue_Owner:MainTopic&quot;
}
]
}</code> 
<p>After this has been set up, the subscriber (using account <em>Queue_Owner</em>) can call Subscribe to link the queue to the topic. After the queue has been successfully subscribed, SNS starts to publish notifications. In this case, neither the topic owner nor the subscriber have had to process any kind of confirmation message.</p> 
<b>Topic owner subscription</b> 
<p>The second way to subscribe an SQS queue to an SNS topic is to have the <em>Topic_Owner</em> account initiate the subscription for the queue from account <em>Queue_Owner</em>. In this case, SNS first sends a confirmation message to the queue. To confirm the subscription, a user who can read messages from the queue must visit the URL specified in the&nbsp;SubscribeURL&nbsp;value in the message. Until the subscription is confirmed, no notifications published to the topic are sent to the queue. To confirm a subscription, you can use the SQS console or the&nbsp;ReceiveMessage&nbsp;API action.</p> 
<b>What’s next?</b> 
<p>In this post, I covered a few simple use cases but the principles can be extended to complex systems as well. As you architect new systems and refactor existing ones, think about where you can leverage queues (SQS) and topics (SNS) to build a loosely coupled system that can be quickly and easily extended to meet your business need.</p> 
<p>For step by step instructions, see&nbsp;<a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.cross.account.html">Sending Amazon SNS messages to an Amazon SQS queue in a different account</a>. You can also visit the following resources to get started working with message queues and topics:</p> 
<li><span style="color: #000000">10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-messages-distributed-applications/">Send Messages Between Distributed Applications with Amazon SQS</a></span></li> 
<li><span style="color: #000000">10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/">Send&nbsp;Fanout Event Notifications with Amazon SNS and Amazon SQS</a></span></li> 
<li>Blog:&nbsp;<a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</a></li> 
<li>Blog:&nbsp;<a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a></li> 
<li>Other resources:&nbsp;<a href="https://aws.amazon.com/sns/getting-started/">Getting started with Amazon SNS</a></li> 
<li>Other resources:&nbsp;<a href="https://aws.amazon.com/sqs/getting-started/">Getting started with Amazon SQS</a></li> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2726');
});
</script> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
