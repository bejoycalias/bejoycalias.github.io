<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/18/cross-account-integration-with-amazon-sns_img1-1260x459.png" /> 
<b class="lb-b blog-post-title" property="name headline">Cross-Account Integration with Amazon SNS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Christie Gifrin</span></span> | on 
<time property="datePublished" datetime="2017-11-14T12:51:03+00:00">14 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-notification-service-sns/" title="View all posts in Amazon Simple Notification Service (SNS)*"><span property="articleSection">Amazon Simple Notification Service (SNS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/amazon-simple-queue-service-sqs/" title="View all posts in Amazon Simple Queue Service (SQS)*"><span property="articleSection">Amazon Simple Queue Service (SQS)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/messaging/" title="View all posts in Messaging*"><span property="articleSection">Messaging*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/cross-account-integration-with-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2726" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2726&amp;disqus_title=Cross-Account+Integration+with+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/cross-account-integration-with-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2726');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h4>Contributed by Zak Islam, Senior Manager, Software Development, AWS Messaging</h4> 
<p>&nbsp;</p> 
<p><a href="https://aws.amazon.com/sns/">Amazon Simple Notification Service</a>&nbsp;(Amazon SNS)&nbsp;is a fully managed AWS service that makes it easy to decouple your application components and fan-out messages. SNS provides topics (similar to topics in message brokers such as RabbitMQ or ActiveMQ) that you can use to create 1:1, 1:N, or N:N producer/consumer design patterns. For more information about how to send messages from SNS to Amazon SQS, AWS Lambda, or HTTP(S) endpoints in the same account, see <a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.html">Sending Amazon SNS Messages to Amazon SQS Queues</a>.</p> 
<p>SNS can be used to send messages within a single account or to resources in different accounts to create administrative isolation. This enables administrators to grant only the minimum level of permissions required to process a workload (for example, limiting the scope of your application account to only send messages and to deny deletes). This approach is commonly known as the “principle of least privilege.” If you are interested, read more about AWS’s <a href="https://aws.amazon.com/answers/account-management/aws-multi-account-security-strategy/">multi-account security strategy</a>.</p> 
<p>This is great from a security perspective, but why would you want to share messages between accounts? It may sound scary, but it’s a common practice to isolate application components (such as producer and consumer) to operate using different AWS accounts to lock down privileges in case credentials are exposed. In this post, I go slightly deeper and explore how to set up your SNS topic so that it can route messages to SQS queues that are owned by a separate AWS account.</p> 
<b>Potential use cases</b> 
<p style="text-align: left">First, look at a common order processing design pattern:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_1.png" /></a></p> 
<p><span id="more-2726"></span>This is a simple architecture. A web server submits an order directly to an SNS topic, which then fans out messages to two SQS queues. One SQS queue is used to track all incoming orders for audits (such as anti-entropy, comparing the data of all replicas and updating each replica to the newest version). The other is used to pass the request to the order processing systems.</p> 
<p>Imagine now that a few years have passed, and your downstream processes no longer scale, so you are kicking around the idea of a re-architecture project. To thoroughly test your system, you need a way to replay your production messages in your development system. Sure, you can build a system to replicate and replay orders from your production environment in your development environment. Wouldn’t it be easier to subscribe your development queues to the production SNS topic so you can test your new system in real time? That’s exactly what you can do here.</p> 
<p>Here’s another use case. As your business grows, you recognize the need for more metrics from your order processing pipeline. The analytics team at your company has built a metrics aggregation service and ingests data via a central SQS queue. Their architecture is as follows:<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/03/cross_acct_integration_sns_image_2.png" /></a></p> 
<p>Again, it’s a fairly simple architecture. All data is ingested via SQS queues (master_ingest_queue, in this case). You subscribe the master_ingest_queue, running under the analytics team’s AWS account, to the topic that is in the order management team’s account.</p> 
<b>Making it work</b> 
<p>Now that you’ve seen a few scenarios, let’s dig into the details. There are a couple of ways to link an SQS queue to an SNS topic (subscribe a queue to a topic):</p> 
<ol> 
<li>The queue owner can create a subscription to the topic.</li> 
<li>The topic owner can subscribe a queue in another account to the topic.</li> 
</ol> 
<b>Queue owner subscription</b> 
<p>What happens when the queue owner subscribes to a topic? In this case, assume that the topic owner has given permission to the subscriber’s account to call the Subscribe API action using the topic ARN (Amazon Resource Name). For the examples below, also assume the following:</p> 
<li>&nbsp;<em>Topic_Owner</em> is the identifier for the account that owns the topic <em>MainTopic</em></li> 
<li><em>Queue_Owner</em> is the identifier for the account that owns the queue subscribed to the main topic</li> 
<p>To enable the subscriber to subscribe to a topic, the topic owner must add the sns:Subscribe and topic ARN to the topic policy via the AWS Management Console, as follows:</p> 
<code class="lang-json">{
&quot;Version&quot;:&quot;2012-10-17&quot;,
&quot;Id&quot;:&quot;MyTopicSubscribePolicy&quot;,
&quot;Statement&quot;:[{
&quot;Sid&quot;:&quot;Allow-other-account-to-subscribe-to-topic&quot;,
&quot;Effect&quot;:&quot;Allow&quot;,
&quot;Principal&quot;:{
&quot;AWS&quot;:&quot;Topic_Owner&quot;
},
&quot;Action&quot;:&quot;sns:Subscribe&quot;,
&quot;Resource&quot;:&quot;arn:aws:sns:us-east-1:Queue_Owner:MainTopic&quot;
}
]
}</code> 
<p>After this has been set up, the subscriber (using account <em>Queue_Owner</em>) can call Subscribe to link the queue to the topic. After the queue has been successfully subscribed, SNS starts to publish notifications. In this case, neither the topic owner nor the subscriber have had to process any kind of confirmation message.</p> 
<b>Topic owner subscription</b> 
<p>The second way to subscribe an SQS queue to an SNS topic is to have the <em>Topic_Owner</em> account initiate the subscription for the queue from account <em>Queue_Owner</em>. In this case, SNS first sends a confirmation message to the queue. To confirm the subscription, a user who can read messages from the queue must visit the URL specified in the&nbsp;SubscribeURL&nbsp;value in the message. Until the subscription is confirmed, no notifications published to the topic are sent to the queue. To confirm a subscription, you can use the SQS console or the&nbsp;ReceiveMessage&nbsp;API action.</p> 
<b>What’s next?</b> 
<p>In this post, I covered a few simple use cases but the principles can be extended to complex systems as well. As you architect new systems and refactor existing ones, think about where you can leverage queues (SQS) and topics (SNS) to build a loosely coupled system that can be quickly and easily extended to meet your business need.</p> 
<p>For step by step instructions, see&nbsp;<a href="http://docs.aws.amazon.com/sns/latest/dg/SendMessageToSQS.cross.account.html">Sending Amazon SNS messages to an Amazon SQS queue in a different account</a>. You can also visit the following resources to get started working with message queues and topics:</p> 
<li><span style="color: #000000">10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-messages-distributed-applications/">Send Messages Between Distributed Applications with Amazon SQS</a></span></li> 
<li><span style="color: #000000">10-minute Tutorial: <a href="https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/">Send&nbsp;Fanout Event Notifications with Amazon SNS and Amazon SQS</a></span></li> 
<li>Blog:&nbsp;<a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</a></li> 
<li>Blog:&nbsp;<a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a></li> 
<li>Other resources:&nbsp;<a href="https://aws.amazon.com/sns/getting-started/">Getting started with Amazon SNS</a></li> 
<li>Other resources:&nbsp;<a href="https://aws.amazon.com/sqs/getting-started/">Getting started with Amazon SQS</a></li> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2726');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/route-53-healthcheck.png" /> 
<b class="lb-b blog-post-title" property="name headline">Building a Multi-region Serverless Application with Amazon API Gateway and AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Stefano Buliani</span></span> | on 
<time property="datePublished" datetime="2017-11-13T14:28:20+00:00">13 NOV 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-route-53/" title="View all posts in Amazon Route 53*"><span property="articleSection">Amazon Route 53*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3143" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3143&amp;disqus_title=Building+a+Multi-region+Serverless+Application+with+Amazon+API+Gateway+and+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-a-multi-region-serverless-application-with-amazon-api-gateway-and-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3143');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
This post written by:&nbsp;Magnus Bjorkman – Solutions Architect 
<p>Many customers are looking to run their services at global scale, deploying their backend to multiple regions. In this post, we describe how to deploy a Serverless API into multiple regions and how to leverage <a title="Amazon Route 53" href="https://aws.amazon.com/route53/" target="_blank" rel="noopener noreferrer">Amazon Route 53</a> to route the traffic between regions. We use latency-based routing and health checks to achieve an active-active setup that can fail over between regions in case of an issue. We leverage the new regional API endpoint feature in <a title="Amazon API Gateway" href="https://aws.amazon.com/api-gateway" target="_blank" rel="noopener noreferrer">Amazon API Gateway</a> to make this a seamless process for the API client making the requests. This post does not cover the replication of your data, which is another aspect to consider when deploying applications across regions.</p> 
<h3>Solution overview</h3> 
<p>Currently, the default API endpoint type in API Gateway is the edge-optimized API endpoint, which enables clients to access an API through an Amazon CloudFront distribution. This typically improves connection time for geographically diverse clients. By default, a custom domain name is globally unique and the edge-optimized API endpoint would invoke a Lambda function in a single region in the case of Lambda integration. You can’t use this type of endpoint with a Route 53 active-active setup and fail-over.</p> 
<p>The new regional API endpoint in API Gateway moves the API endpoint into the region and the custom domain name is unique per region. This makes it possible to run a full copy of an API in each region and then use Route 53 to use an active-active setup and failover. The following diagram shows how you do this:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/active-active-architecture.png" /></p> 
<li>Deploy your Rest API stack, consisting of API Gateway and Lambda, in two regions, such as us-east-1 and us-west-2.</li> 
<li>Choose the regional API endpoint type for your API.</li> 
<li>Create a custom domain name and choose the regional API endpoint type for that one as well. In both regions, you are configuring the custom domain name to be the same, for example, helloworldapi.replacewithyourcompanyname.com</li> 
<li>Use the host name of the custom domain names from each region, for example, xxxxxx.execute-api.us-east-1.amazonaws.com and xxxxxx.execute-api.us-west-2.amazonaws.com, to configure record sets in Route 53 for your client-facing domain name, for example, helloworldapi.replacewithyourcompanyname.com</li> 
<p>The above solution provides an active-active setup for your API across the two regions, but you are not doing failover yet. For that to work, set up a health check in Route 53:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/route-53-healthcheck.png" /></p> 
<p>A Route 53 health check must have an endpoint to call to check the health of a service. You could do a simple ping of your actual Rest API methods, but instead provide a specific method on your Rest API that does a deep ping. That is, it is a Lambda function that checks the status of all the dependencies.</p> 
<p>In the case of the Hello World API, you don’t have any other dependencies. In a real-world scenario, you could check on dependencies as databases, other APIs, and external dependencies. Route 53 health checks themselves cannot use your custom domain name endpoint’s DNS address, so you are going to directly call the API endpoints via their region unique endpoint’s DNS address.</p> 
<h3>Walkthrough</h3> 
<p>The following sections describe how to set up this solution. You can find the complete solution at the <a title="Multi region serverless app GitHub sample" href="https://github.com/aws-samples/blog-multi-region-serverless-service/" target="_blank" rel="noopener noreferrer">blog-multi-region-serverless-service GitHub repo</a>. Clone or download the repository locally to be able to do the setup as described.</p> 
<h4>Prerequisites</h4> 
<p>You need the following resources to set up the solution described in this post:</p> 
<li><a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html" target="_blank" rel="noopener noreferrer">AWS CLI</a></li> 
<li>An S3 bucket in each region in which to deploy the solution, which can be used by the AWS Serverless Application Model (SAM). You can use the following CloudFormation templates to create buckets in us-east-1 and us-west-2: 
<li>us-east-1: <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=SAMS3Bucket&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/building-a-multi-region-serverless-application/create-s3-bucket.yaml" target="_blank" rel="noopener noreferrer"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png" /></a></li> 
<li>us-west-2: <a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=SAMS3Bucket&amp;templateURL=https://s3.amazonaws.com/computeblog-us-west-2/building-a-multi-region-serverless-application/create-s3-bucket.yaml" target="_blank" rel="noopener noreferrer"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png" /></a></li> 
</ul> </li> 
<li>A hosted zone registered in Amazon Route 53. This is used for defining the domain name of your API endpoint, for example, <em>helloworldapi.replacewithyourcompanyname.com</em>. You can use a third-party domain name registrar and then <a title="Configure Route 53 DNS" href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/MigratingDNS.html" target="_blank" rel="noopener noreferrer">configure the DNS in Amazon Route 53</a>, or you can <a title="Purchase domain from Route 53" href="http://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html" target="_blank" rel="noopener noreferrer">purchase a domain directly from Amazon Route 53.</a></li> 
<h4>Deploy API with health checks in two regions</h4> 
<p>Start by creating a small “Hello World” Lambda function that sends back a message in the region in which it has been deployed.</p> 
<code class="lang-python">
&quot;&quot;&quot;Return message.&quot;&quot;&quot;
import logging
logging.basicConfig()
logger = logging.getLogger()
logger.setLevel(logging.INFO)
def lambda_handler(event, context):
&quot;&quot;&quot;Lambda handler for getting the hello world message.&quot;&quot;&quot;
region = context.invoked_function_arn.split(':')[3]
logger.info(&quot;message: &quot; + &quot;Hello from &quot; + region)
return {
&quot;message&quot;: &quot;Hello from &quot; + region
}
</code> 
<p>Also create a Lambda function for doing a health check that returns a value based on another environment variable (either “ok” or “fail”) to allow for ease of testing:</p> 
<code class="lang-python">
&quot;&quot;&quot;Return health.&quot;&quot;&quot;
import logging
import os
logging.basicConfig()
logger = logging.getLogger()
logger.setLevel(logging.INFO)
def lambda_handler(event, context):
&quot;&quot;&quot;Lambda handler for getting the health.&quot;&quot;&quot;
logger.info(&quot;status: &quot; + os.environ['STATUS'])
return {
&quot;status&quot;: os.environ['STATUS']
}
</code> 
<p>Deploy both of these using an AWS <a title="Serverless Application Model (SAM)" href="https://github.com/awslabs/serverless-application-model" target="null">Serverless Application Model (SAM)</a> template. SAM is a CloudFormation extension that is optimized for serverless, and provides a standard way to create a complete serverless application. You can find the full <a title="Hello World SAM template" href="https://github.com/aws-samples/blog-multi-region-serverless-service/blob/master/helloworld-api/helloworld-sam.yaml" target="null">helloworld-sam.yaml</a> template in the blog-multi-region-serverless-service GitHub repo.</p> 
<p>A few things to highlight:</p> 
<li>You are using inline Swagger to define your API so you can substitute the current region in the x-amazon-apigateway-integration section.</li> 
<li>Most of the Swagger template covers CORS to allow you to test this from a browser.</li> 
<li>You are also using substitution to populate the environment variable used by the “Hello World” method with the region into which it is being deployed.</li> 
<p>The Swagger allows you to use the same SAM template in both regions.</p> 
<p>You can only use SAM from the AWS CLI, so do the following from the command prompt. First, deploy the SAM template in us-east-1 with the following commands, replacing “&lt;your bucket in us-east-1&gt;” with a bucket in your account:</p> 
<code class="lang-bash">
&gt; cd helloworld-api
&gt; aws cloudformation package --template-file helloworld-sam.yaml --output-template-file /tmp/cf-helloworld-sam.yaml --s3-bucket &lt;your bucket in us-east-1&gt; --region us-east-1
&gt; aws cloudformation deploy --template-file /tmp/cf-helloworld-sam.yaml --stack-name multiregionhelloworld --capabilities CAPABILITY_IAM --region us-east-1
</code> 
<p>Second, do the same in us-west-2:</p> 
<code class="lang-bash">
&gt; aws cloudformation package --template-file helloworld-sam.yaml --output-template-file /tmp/cf-helloworld-sam.yaml --s3-bucket &lt;your bucket in us-west-2&gt; --region us-west-2
&gt; aws cloudformation deploy --template-file /tmp/cf-helloworld-sam.yaml --stack-name multiregionhelloworld --capabilities CAPABILITY_IAM --region us-west-2
</code> 
<p>The API was created with the default endpoint type of <strong>Edge Optimized</strong>. Switch it to <strong>Regional</strong>. In the Amazon API Gateway console, select the API that you just created and choose the wheel-icon to edit it.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-edit-api.png" /></p> 
<p>In the edit screen, select the <strong>Regional</strong> endpoint type and save the API. Do the same in both regions.</p> 
<p>Grab the URL for the API in the console by navigating to the method in the <strong>prod</strong> stage.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-endpoint-link.png" /></p> 
<p>You can now test this with curl:</p> 
<code class="lang-bash">
&gt; curl https://2wkt1cxxxx.execute-api.us-west-2.amazonaws.com/prod/helloworld
{&quot;message&quot;: &quot;Hello from us-west-2&quot;}
</code> 
<p>Write down the domain name for the URL in each region (for example, <em>2wkt1cxxxx.execute-api.us-west-2.amazonaws.com</em>), as you need that later when you deploy the Route 53 setup.</p> 
<h4>Create the custom domain name</h4> 
<p>Next, create an Amazon API Gateway custom domain name endpoint. As part of using this feature, you must have a hosted zone and domain available to use in Route 53 as well as an SSL certificate that you use with your specific domain name.</p> 
<p>You can create the SSL certificate by using AWS Certificate Manager. In the ACM console, choose <strong>Get started</strong> (if you have no existing certificates) or <strong>Request a certificate</strong>. Fill out the form with the domain name to use for the custom domain name endpoint, which is the same across the two regions:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/acm-request-certificate.png" /></p> 
<p>Go through the remaining steps and <a title="Validate ACM certificate" href="http://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate.html" target="_blank" rel="noopener noreferrer">validate the certificate</a> for each region before moving on.</p> 
<p>You are now ready to create the endpoints. In the Amazon API Gateway console, choose <strong>Custom Domain Names</strong>, <strong>Create Custom Domain Name</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-custom-domain-name.png" /></p> 
<p>A few things to highlight:</p> 
<li>The domain name is the same as what you requested earlier through ACM.</li> 
<li>The endpoint configuration should be regional.</li> 
<li>Select the ACM Certificate that you created earlier.</li> 
<li>You need to create a base path mapping that connects back to your earlier API Gateway endpoint. Set the base path to v1 so you can version your API, and then select the API and the prod stage.</li> 
<p>Choose <strong>Save</strong>. You should see your newly created custom domain name:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/api-gateway-custom-domain-save.png" /></p> 
<p>Note the value for <strong>Target Domain Name</strong> as you need that for the next step. Do this for both regions.</p> 
<h4>Deploy Route 53 setup</h4> 
<p>Use the global Route 53 service to provide DNS lookup for the Rest API, distributing the traffic in an active-active setup based on latency. You can find the full CloudFormation template in the <a title="Multi region serverless application GitHub sample" href="https://github.com/aws-samples/blog-multi-region-serverless-service/blob/master/helloworld-dns/helloworld-dns.yaml" target="_blank" rel="noopener noreferrer">blog-multi-region-serverless-service GitHub repo</a>.</p> 
<p>The template sets up health checks, for example, for us-east-1:</p> 
<code class="lang-yaml">
HealthcheckRegion1:
Type: &quot;AWS::Route53::HealthCheck&quot;
Properties:
HealthCheckConfig:
Port: &quot;443&quot;
Type: &quot;HTTPS_STR_MATCH&quot;
SearchString: &quot;ok&quot;
ResourcePath: &quot;/prod/healthcheck&quot;
FullyQualifiedDomainName: !Ref Region1HealthEndpoint
RequestInterval: &quot;30&quot;
FailureThreshold: &quot;2&quot;
</code> 
<p>Use the health check when you set up the record set and the latency routing, for example, for us-east-1:</p> 
<code class="lang-yaml">
Region1EndpointRecord:
Type: AWS::Route53::RecordSet
Properties:
Region: us-east-1
HealthCheckId: !Ref HealthcheckRegion1
SetIdentifier: &quot;endpoint-region1&quot;
HostedZoneId: !Ref HostedZoneId
Name: !Ref MultiregionEndpoint
Type: CNAME
TTL: 60
ResourceRecords:
- !Ref Region1Endpoint
</code> 
<p>You can create the stack by using the following link, copying in the domain names from the previous section, your existing hosted zone name, and the main domain name that is created (for example, hellowordapi.replacewithyourcompanyname.com): <a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=multiregiondns&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/building-a-multi-region-serverless-application/helloworld-dns.yaml" target="_blank" rel="noopener noreferrer"><img src="https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png" /></a></p> 
<p>The following screenshot shows what the parameters might look like:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/route-53-healthcheck-setup.png" /></p> 
<p>Specifically, the domain names that you collected earlier would map according to following:</p> 
<li>The domain names from the API Gateway “prod”-stage go into Region1HealthEndpoint and Region2HealthEndpoint.</li> 
<li>The domain names from the custom domain name’s target domain name goes into Region1Endpoint and Region2Endpoint.</li> 
<h4>Using the Rest API from server-side applications</h4> 
<p>You are now ready to use your setup. First, demonstrate the use of the API from server-side clients. You can demonstrate this by using curl from the command line:</p> 
<code class="lang-bash">
&gt; curl https://hellowordapi.replacewithyourcompanyname.com/v1/helloworld/
{&quot;message&quot;: &quot;Hello from us-east-1&quot;}
</code> 
<h4>Testing failover of Rest API in browser</h4> 
<p>Here’s how you can use this from the browser and test the failover. Find all of the files for this test in the <a title="Browser client" href="https://github.com/aws-samples/blog-multi-region-serverless-service/blob/master/browser-client/" target="_blank" rel="noopener noreferrer">browser-client</a> folder of the blog-multi-region-serverless-service GitHub repo.</p> 
<p>Use this html file:</p> 
<code class="lang-html">
&lt;!DOCTYPE HTML&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;/&gt;
&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;/&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;/&gt;
&lt;title&gt;Multi-Region Client&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div&gt;
&lt;b&gt;Test Client&lt;/b&gt;
&lt;p id=&quot;client_result&quot;&gt;
&lt;/p&gt;
&lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;settings.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;client.js&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code> 
<p>The html file uses this JavaScript file to repeatedly call the API and print the history of messages:</p> 
<code class="lang-js">
var messageHistory = &quot;&quot;;
(function call_service() {
$.ajax({
url: helloworldMultiregionendpoint+'v1/helloworld/',
dataType: &quot;json&quot;,
cache: false,
success: function(data) {
messageHistory+=&quot;&lt;p&gt;&quot;+data['message']+&quot;&lt;/p&gt;&quot;;
$('#client_result').html(messageHistory);
},
complete: function() {
// Schedule the next request when the current one's complete
setTimeout(call_service, 10000);
},
error: function(xhr, status, error) {
$('#client_result').html('ERROR: '+status);
}
});
})();
</code> 
<p>Also, make sure to update the settings in settings.js to match with the API Gateway endpoints for the DNS-proxy and the multi-regional endpoint for the Hello World API: <code>var helloworldMultiregionendpoint = &quot;https://hellowordapi.replacewithyourcompanyname.com/&quot;;</code></p> 
<p>You can now open the HTML file in the browser (you can do this directly from the file system) and you should see something like the following screenshot:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/browser-test-client-1.png" /></p> 
<p>You can test failover by changing the environment variable in your health check Lambda function. In the Lambda console, select your health check function and scroll down to the <strong>Environment variables</strong> section. For the <strong>STATUS</strong> key, modify the value to <strong>fail</strong>.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/lambda-change-environment-variable.png" /></p> 
<p>You should see the region switch in the test client:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/11/08/browser-test-client-2.png" /></p> 
<p>During an emulated failure like this, the browser might take some additional time to switch over due to connection keep-alive functionality. If you are using a browser like Chrome, you can <a title="Chrome kill connections" href="https://support.google.com/chrome/a/answer/6271171?hl=en" target="_blank" rel="noopener noreferrer">kill all the connections</a> to see a more immediate fail-over: <code>chrome://net-internals/#sockets</code></p> 
<h3>Summary</h3> 
<p>You have implemented a simple way to do multi-regional serverless applications that fail over seamlessly between regions, either being accessed from the browser or from other applications/services. You achieved this by using the capabilities of Amazon Route 53 to do latency based routing and health checks for fail-over. You unlocked the use of these features in a serverless application by leveraging the new regional endpoint feature of Amazon API Gateway.</p> 
<p>The setup was fully scripted using CloudFormation, the AWS Serverless Application Model (SAM), and the AWS CLI, and it can be integrated into deployment tools to push the code across the regions to make sure it is available in all the needed regions. For more information about cross-region deployments, see <a title="Cross-region code deployment" href="https://aws.amazon.com/blogs/devops/building-a-cross-regioncross-account-code-deployment-solution-on-aws/" target="_blank" rel="noopener noreferrer">Building a Cross-Region/Cross-Account Code Deployment Solution on AWS on the AWS DevOps blog</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3143');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure1-756x630.jpeg" /> 
<b class="lb-b blog-post-title" property="name headline">Bringing Datacenter-Scale Hardware-Software Co-design to the Cloud with FireSim and Amazon EC2 F1 Instances</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Mia Champion</span></span> | on 
<time property="datePublished" datetime="2017-10-25T10:09:51+00:00">25 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/bringing-datacenter-scale-hardware-software-co-design-to-the-cloud-with-firesim-and-amazon-ec2-f1-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3096" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3096&amp;disqus_title=Bringing+Datacenter-Scale+Hardware-Software+Co-design+to+the+Cloud+with+FireSim+and+Amazon+EC2+F1+Instances&amp;disqus_url=https://aws.amazon.com/blogs/compute/bringing-datacenter-scale-hardware-software-co-design-to-the-cloud-with-firesim-and-amazon-ec2-f1-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3096');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>The recent addition of Xilinx FPGAs to AWS Cloud compute offerings is one way that AWS is enabling global growth in the areas of advanced analytics, deep learning and AI. The customized F1 servers use pooled accelerators, enabling interconnectivity of up to 8 FPGAs, each one including 64 GiB DDR4 ECC protected memory, with a dedicated PCIe x16 connection. That makes this a powerful engine with the capacity to process advanced analytical applications at scale, at a significantly faster rate. For example, AWS commercial partner <a href="http://www.edicogenome.com/dragen-on-amazon-web-services/">Edico Genome </a>is able to achieve an approximately 30X speedup in analyzing whole genome sequencing datasets using their DRAGEN platform powered with F1 instances.</em></p> 
<p><em>While the availability of FPGA F1 compute on-demand provides clear accessibility and cost advantages, many mainstream users are still finding that the “threshold to entry” in developing or running FPGA-accelerated simulations is too high. Researchers at the UC Berkeley RISE Lab have developed “FireSim”, powered by Amazon FPGA F1 instances as an open-source resource, FireSim lowers that entry bar and makes it easier for everyone to leverage the power of an FPGA-accelerated compute environment. Whether you are part of a small start-up development team or working at a large datacenter scale, hardware-software co-design enables faster time-to-deployment, lower costs, and more predictable performance. We are excited to feature FireSim in this post from Sagar Karandikar and his colleagues at UC-Berkeley.</em></p> 
<p>―Mia Champion, Sr. Data Scientist, AWS</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure1.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure1.jpeg" /></a> 
<p class="wp-caption-text">Mapping an 8-node FireSim cluster simulation to Amazon EC2 F1</p> 
<p style="text-align: left">As traditional hardware scaling nears its end, the data centers of tomorrow are trending towards heterogeneity, employing custom hardware accelerators and increasingly high-performance interconnects. Prototyping new hardware at scale has traditionally been either extremely expensive, or very slow. In this post, I introduce <a href="https://fires.im/">FireSim</a>, a new hardware simulation platform under development in the computer architecture research group at UC Berkeley that enables fast, scalable hardware simulation using <a href="https://aws.amazon.com/ec2/instance-types/f1/">Amazon EC2 F1 instances</a>.</p> 
<p>FireSim benefits both hardware and software developers working on new rack-scale systems: software developers can use the simulated nodes with new hardware features as they would use a real machine, while hardware developers have full control over the hardware being simulated and can run real software stacks while hardware is still under development. In conjunction with this post, we’re releasing the first public demo of FireSim, which lets you deploy your own 8-node simulated cluster on an F1 Instance and run benchmarks against it. This demo simulates a pre-built “vanilla” cluster, but demonstrates FireSim’s high performance and usability.</p> 
<h3>Why FireSim + F1?</h3> 
<p>FPGA-accelerated hardware simulation is by no means a new concept. However, previous attempts to use FPGAs for simulation have been fraught with usability, scalability, and cost issues. FireSim takes advantage of EC2 F1 and open-source hardware to address the traditional problems with FPGA-accelerated simulation:<br /> <em>Problem #1: FPGA-based simulations have traditionally been expensive, difficult to deploy, and difficult to reproduce.</em><br /> FireSim uses public-cloud infrastructure like F1, which means no upfront cost to purchase and deploy FPGAs. Developers and researchers can distribute pre-built AMIs and AFIs, as in this public demo (more details later in this post), to make experiments easy to reproduce. FireSim also automates most of the work involved in deploying an FPGA simulation, essentially enabling one-click conversion from new RTL to deploying on an FPGA cluster.</p> 
<p><em>Problem #2: FPGA-based simulations have traditionally been difficult (and expensive) to scale.</em><br /> Because FireSim uses F1, users can scale out experiments by spinning up additional EC2 instances, rather than spending hundreds of thousands of dollars on large FPGA clusters.</p> 
<p><em>Problem #3: Finding open hardware to simulate has traditionally been difficult.</em> <em>Finding open hardware that can run real software stacks is even harder.</em><br /> FireSim simulates <a href="https://github.com/freechipsproject/rocket-chip">RocketChip</a>, an open, silicon-proven, <a href="https://riscv.org/">RISC-V</a>-based processor platform, and adds peripherals like a NIC and disk device to build up a realistic system. Processors that implement RISC-V automatically support real operating systems (such as <a href="https://github.com/riscv/riscv-linux">Linux</a>) and even support applications like Apache and Memcached. We provide a custom Buildroot-based FireSim Linux distribution that runs on our simulated nodes and includes many popular developer tools.</p> 
<p><em>Problem #4: Writing hardware in traditional HDLs is time-consuming.</em><br /> Both FireSim and RocketChip use the <a href="https://chisel.eecs.berkeley.edu/">Chisel</a> HDL, which brings modern programming paradigms to hardware description languages. Chisel greatly simplifies the process of building large, highly parameterized hardware components.</p> 
<h3>How to use FireSim for hardware/software co-design</h3> 
<p>FireSim drastically improves the process of co-designing hardware and software by acting as a push-button interface for collaboration between hardware developers and systems software developers. The following diagram describes the workflows that hardware and software developers use when working with FireSim.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure2-1.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure2-1.jpeg" /></a> 
<p class="wp-caption-text">Figure 2. The FireSim custom hardware development workflow.</p> 
<p>The hardware developer’s view:</p> 
<ol> 
<li>Write custom RTL for your accelerator, peripheral, or processor modification in a productive language like Chisel.</li> 
<li>Run a software simulation of your hardware design in standard gate-level simulation tools for early-stage debugging.</li> 
<li>Run FireSim build scripts, which automatically build your simulation, run it through the Vivado toolchain/AWS shell scripts, and publish an AFI.</li> 
<li>Deploy your simulation on EC2 F1 using the generated simulation driver and AFI</li> 
<li>Run real software builds released by software developers to benchmark your hardware</li> 
</ol> 
<p>The software developer’s view:</p> 
<ol> 
<li>Deploy the AMI/AFI generated by the hardware developer on an F1 instance to simulate a cluster of nodes (or scale out to many F1 nodes for larger simulated core-counts).</li> 
<li>Connect using SSH into the simulated nodes in the cluster and boot the Linux distribution included with FireSim. This distribution is easy to customize, and already supports many standard software packages.</li> 
<li>Directly prototype your software using the same exact interfaces that the software will see when deployed on the real future system you’re prototyping, with the same performance characteristics as observed from software, even at scale.</li> 
</ol> 
<p><strong>FireSim demo v1.0</strong></p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure3.jpeg"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/25/figure3.jpeg" /></a> 
<p class="wp-caption-text">Figure 3. Cluster topology simulated by FireSim demo v1.0.</p> 
<p style="text-align: left">This first public demo of FireSim focuses on the aforementioned “software-developer’s view” of the custom hardware development cycle. The demo simulates a cluster of 1 to 8 RocketChip-based nodes, interconnected by a functional network simulation. The simulated nodes work just like “real” machines: &nbsp;they boot Linux, you can connect to them using SSH, and you can run real applications on top. The nodes can see each other (and the EC2 F1 instance on which they’re deployed) on the network and communicate with one another. While the demo currently simulates a pre-built “vanilla” cluster, the entire hardware configuration of these simulated nodes can be modified after FireSim is open-sourced.</p> 
<p>In this post, I walk through bringing up a single-node FireSim simulation for experienced EC2 F1 users. For more detailed instructions for new users and instructions for running a larger 8-node simulation, see <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html">FireSim Demo v1.0 on Amazon EC2 F1</a>. Both demos walk you through setting up an instance from a demo AMI/AFI and booting Linux on the simulated nodes. The full demo instructions also walk you through an example workload, running Memcached on the simulated nodes, with YCSB as a load generator to demonstrate network functionality.</p> 
<h3>Deploying the demo on F1</h3> 
<p>In this release, we provide pre-built binaries for driving simulation from the host and a pre-built AFI that contains the FPGA infrastructure necessary to simulate a RocketChip-based node.</p> 
<h3>Starting your F1 instances</h3> 
<p>First, launch an instance using the free <a href="https://aws.amazon.com/marketplace/pp/B0758SR46G">FireSim Demo v1.0 product</a> available on the AWS Marketplace on an f1.2xlarge instance. After your instance has booted, log in using the user name centos. On the first login, you should see the message “FireSim network config completed.” This sets up the necessary tap interfaces and bridge on the EC2 instance to enable communicating with the simulated nodes.</p> 
<h3>AMI contents</h3> 
<p>The AMI contains a variety of tools to help you run simulations and build software for RISC-V systems, including the riscv64 toolchain, a Buildroot-based Linux distribution that runs on the simulated nodes, and the simulation driver program. For more details, see the <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html#ami-contents">AMI Contents</a> section on the FireSim website.</p> 
<h3>Single-node demo</h3> 
<p>First, you need to flash the FPGA with the FireSim AFI. To do so, run:</p> 
<p style="text-align: left"><em>[centos@ip-IP_ADDR ~]$ sudo fpga-load-local-image -S 0 -I agfi-00a74c2d615134b21</em></p> 
<p>To start a simulation, run the following at the command line:</p> 
<p style="text-align: left"><em>[centos@ip-IP_ADDR ~]$ boot-firesim-singlenode</em></p> 
<p>This automatically calls the simulation driver, telling it to load the Linux kernel image and root filesystem for the Linux distro. This produces output similar to the following:</p> 
<p style="text-align: left"><em>Simulations Started. You can use the UART console of each simulated node by attaching to the following screens:</em></p> 
<p style="text-align: left"><em>There is a screen on:</em></p> 
<p style="text-align: left"><em>2492.fsim0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Detached)</em></p> 
<p style="text-align: left"><em>1 Socket in /var/run/screen/S-centos.</em></p> 
<p>You could connect to the simulated UART console by connecting to this screen, but instead opt to use SSH to access the node instead.</p> 
<p>First, ping the node to make sure it has come online. This is currently required because nodes may get stuck at Linux boot if the NIC does not receive any network traffic. For more information, see <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html#troubleshooting-ping">Troubleshooting/Errata</a>. The node is always assigned the IP address 192.168.1.10:</p> 
<p style="text-align: left"><em>[centos@ip-IP_ADDR ~]$ ping 192.168.1.10</em></p> 
<p>This should eventually produce the following output:</p> 
<p><em>PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.</em></p> 
<p><em>From 192.168.1.1 icmp_seq=1 Destination Host Unreachable</em></p> 
<p><em>…</em></p> 
<p><em>64 bytes from 192.168.1.10: icmp_seq=1 ttl=64 time=2017 ms</em></p> 
<p><em>64 bytes from 192.168.1.10: icmp_seq=2 ttl=64 time=1018 ms</em></p> 
<p><em>64 bytes from 192.168.1.10: icmp_seq=3 ttl=64 time=19.0 ms</em></p> 
<p><em>…</em></p> 
<p>At this point, you know that the simulated node is online. You can connect to it using SSH with the user name root and password firesim. It is also convenient to make sure that your TERM variable is set correctly. In this case, the simulation expects TERM=linux, so provide that:</p> 
<p><em>[centos@ip-IP_ADDR ~]$ TERM=linux ssh root@192.168.1.10</em></p> 
<p><em>The authenticity of host ‘192.168.1.10 (192.168.1.10)’ can’t be established.</em></p> 
<p><em>ECDSA key fingerprint is 63:e9:66:d0:5c:06:2c:1d:5c:95:33:c8:36:92:30:49.</em></p> 
<p><em>Are you sure you want to continue connecting (yes/no)? yes</em></p> 
<p><em>Warning: Permanently added ‘192.168.1.10’ (ECDSA) to the list of known hosts.</em></p> 
<p><em>root@192.168.1.10’s password:</em></p> 
<p><em>#</em></p> 
<p>At this point, you’re connected to the simulated node. Run uname -a as an example. You should see the following output, indicating that you’re connected to a RISC-V system:</p> 
<p><em># uname -a</em></p> 
<p><em>Linux buildroot 4.12.0-rc2 #1 Fri Aug 4 03:44:55 UTC 2017 riscv64 GNU/Linux</em></p> 
<p>Now you can run programs on the simulated node, as you would with a real machine. For an example workload (running YCSB against Memcached on the simulated node) or to run a larger 8-node simulation, see the full <a href="https://fires.im/2017/08/29/firesim-demo-v1.0.html">FireSim Demo v1.0 on Amazon EC2 F1 demo instructions</a>.</p> 
<p>Finally, when you are finished, you can shut down the simulated node by running the following command from within the simulated node:</p> 
<p style="text-align: left"><em># poweroff</em></p> 
<p>You can confirm that the simulation has ended by running screen -ls, which should now report that there are no detached screens.</p> 
<h3>Future plans</h3> 
<p>At Berkeley, we’re planning to keep improving the FireSim platform to enable our own research in future data center architectures, like <a href="https://www.usenix.org/sites/default/files/conference/protected-files/fast14_asanovic.pdf">FireBox</a>. The FireSim platform will eventually support more sophisticated processors, custom accelerators (such as <a href="http://hwacha.org/">Hwacha</a>), network models, and peripherals, in addition to scaling to larger numbers of FPGAs. In the future, we’ll open source the entire platform, including Midas, the tool used to transform RTL into FPGA simulators, allowing users to modify any part of the hardware/software stack. Follow <a href="https://twitter.com/firesimproject">@firesimproject</a> on Twitter to stay tuned to future FireSim updates.</p> 
<h3>Acknowledgements</h3> 
<p>FireSim is the joint work of many students and faculty at Berkeley: Sagar Karandikar, Donggyu Kim, Howard Mao, David Biancolin, Jack Koenig, Jonathan Bachrach, and Krste Asanović. This work is partially funded by AWS through the RISE Lab, by the Intel Science and Technology Center for Agile HW Design, and by ASPIRE Lab sponsors and affiliates Intel, Google, HPE, Huawei, NVIDIA, and SK hynix.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3096');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/1-topology.png" /> 
<b class="lb-b blog-post-title" property="name headline">Automating Security Group Updates with AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ian Scofield</span></span> | on 
<time property="datePublished" datetime="2017-10-24T13:38:16+00:00">24 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/management-tools/amazon-cloudwatch/" title="View all posts in Amazon CloudWatch*"><span property="articleSection">Amazon CloudWatch*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/auto-scaling/" title="View all posts in Auto Scaling*"><span property="articleSection">Auto Scaling*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/" title="View all posts in Security, Identity, &amp; Compliance*"><span property="articleSection">Security, Identity, &amp; Compliance*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/automating-security-group-updates-with-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2861" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2861&amp;disqus_title=Automating+Security+Group+Updates+with+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/automating-security-group-updates-with-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2861');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Customers often use public endpoints to perform cross-region replication or other application layer communication to remote regions. But a common problem is how do you protect these endpoints? It can be tempting to open up the security groups to the world due to the complexity of keeping security groups in sync across regions with a dynamically changing infrastructure.</p> 
<p>Consider a situation where you are running large clusters of instances in different regions that all require internode connectivity. One approach would be to use a VPN tunnel between regions to provide a secure tunnel over which to send your traffic. A good example of this is the <a title="undefined" href="https://aws.amazon.com/answers/networking/transit-vpc/" target="null">Transit VPC Solution</a>, which is a published AWS solution to help customers quickly get up and running. However, this adds additional cost and complexity to your solution due to the newly required additional infrastructure.</p> 
<p>Another approach, which I’ll explore in this post, is to restrict access to the nodes by whitelisting the public IP addresses of your hosts in the opposite region. Today, I’ll outline a solution that allows for cross-region security group updates, can handle remote region failures, and supports external actions such as manually terminating instances or adding instances to an existing <a title="undefined" href="https://aws.amazon.com/autoscaling" target="null">Auto Scaling</a> group.<span id="more-2861"></span></p> 
<b>Solution overview</b> 
<p>The overview of this solution is diagrammed below. Although this post covers limiting access to your instances, you should still implement encryption to protect your data in transit.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/1-topology.png" /></p> 
<p>If your entire infrastructure is running in a single region, you can reference a security group as the source, allowing your IP addresses to change without any updates required. However, if you’re going across the public internet between regions to perform things like application-level traffic or cross-region replication, this is no longer an option. Security groups are regional. When you go across regions it can be tempting to drop security to enable this communication.</p> 
<p>Although using an Elastic IP address can provide you with a static IP address that you can define as a source for your security groups, this may not always be feasible, especially when automatic scaling is desired.</p> 
<p>In this example scenario, you have a distributed database that requires full internode communication for replication. If you place a cluster in us-east-1 and us-west-2, you must provide a secure method of communication between the two. Because the database uses cloud best practices, you can add or remove nodes as the load varies.</p> 
<p>To start the process of updating your security groups, you must know when an instance has come online to trigger your workflow. Auto Scaling groups have the concept of <a title="undefined" href="http://docs.aws.amazon.com/autoscaling/latest/userguide/lifecycle-hooks.html" target="null">lifecycle hooks</a> that enable you to perform custom actions as the group launches or terminates instances.</p> 
<p>When Auto Scaling begins to launch or terminate an instance, it puts the instance into a wait state (<code>Pending:Wait</code> or <code>Terminating:Wait</code>). The instance remains in this state while you perform your various actions until either you tell Auto Scaling to <code>Continue</code>, <code>Abandon</code>, or the timeout period ends. A lifecycle hook can trigger a CloudWatch event, publish to an Amazon SNS topic, or send to an Amazon SQS queue. For this example, you use <a title="undefined" href="https://aws.amazon.com/cloudwatch" target="null">CloudWatch Events</a> to trigger an <a title="undefined" href="https://aws.amazon.com/lambda" target="null">AWS Lambda</a> function that updates an <a title="undefined" href="https://aws.amazon.com/dynamodb" target="null">Amazon DynamoDB</a> table.</p> 
<b>Component breakdown</b> 
<p>Here’s a quick breakdown of the components involved in this solution:</p> 
<p style="padding-left: 30px">• Lambda function<br /> • CloudWatch event<br /> • DynamoDB table</p> 
<h3>Lambda function</h3> 
<p>The Lambda function automatically updates your security groups, in the following way:</p> 
<p style="padding-left: 30px">1. Determines whether a change was triggered by your Auto Scaling group lifecycle hook or manually invoked for a “true up” functionality, which I discuss later in this post.<br /> 2. Describes the instances in the Auto Scaling group and obtain public IP addresses for each instance.<br /> 3. Updates both local and remote DynamoDB tables.<br /> 4. Compares the list of public IP addresses for both local and remote clusters with what’s already in the local region security group. Update the security group.<br /> 5. Compares the list of public IP addresses for both local and remote clusters with what’s already in the remote region security group. Update the security group<br /> 6. Signals <code>CONTINUE</code> back to the lifecycle hook.</p> 
<h3>CloudWatch event</h3> 
<p>The CloudWatch event triggers when an instance passes through either the <code>launching</code> or <code>terminating</code> states. When the Lambda function gets invoked, it receives an event that looks like the following:</p> 
<code class="lang-json">{
&quot;account&quot;: &quot;123456789012&quot;,
&quot;region&quot;: &quot;us-east-1&quot;,
&quot;detail&quot;: {
&quot;LifecycleHookName&quot;: &quot;hook-launching&quot;,
&quot;AutoScalingGroupName&quot;: &quot;&quot;,
&quot;LifecycleActionToken&quot;: &quot;33965228-086a-4aeb-8c26-f82ed3bef495&quot;,
&quot;LifecycleTransition&quot;: &quot;autoscaling:EC2_INSTANCE_LAUNCHING&quot;,
&quot;EC2InstanceId&quot;: &quot;i-017425ec54f22f994&quot;
},
&quot;detail-type&quot;: &quot;EC2 Instance-launch Lifecycle Action&quot;,
&quot;source&quot;: &quot;aws.autoscaling&quot;,
&quot;version&quot;: &quot;0&quot;,
&quot;time&quot;: &quot;2017-05-03T02:20:59Z&quot;,
&quot;id&quot;: &quot;cb930cf8-ce8b-4b6c-8011-af17966eb7e2&quot;,
&quot;resources&quot;: [
&quot;arn:aws:autoscaling:us-east-1:123456789012:autoScalingGroup:d3fe9d96-34d0-4c62-b9bb-293a41ba3765:autoScalingGroupName/&quot;
]
}</code> 
<h3>DynamoDB table</h3> 
<p>You use DynamoDB to store lists of remote IP addresses in a local table that is updated by the opposite region as a failsafe source of truth. Although you can describe your Auto Scaling group for the local region, you must maintain a list of IP addresses for the remote region.</p> 
<p>To minimize the number of describe calls and prevent an issue in the remote region from blocking your local scaling actions, we keep a list of the remote IP addresses in a local DynamoDB table. Each Lambda function in each region is responsible for updating the public IP addresses of its Auto Scaling group for both the local and remote tables.</p> 
<p>As with all the infrastructure in this solution, there is a DynamoDB table in both regions that mirror each other. For example, the following screenshot shows a sample DynamoDB table. The Lambda function in <code>us-east-1</code> would update the DynamoDB entry for <code>us-east-1</code> in both tables in both regions.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/2-ddb.png" /></p> 
<p>By updating a DynamoDB table in both regions, it allows the local region to gracefully handle issues with the remote region, which would otherwise prevent your ability to scale locally. If the remote region becomes inaccessible, you have a copy of the latest configuration from the table that you can use to continue to sync with your security groups. When the remote region comes back online, it pushes its updated public IP addresses to the DynamoDB table. The security group is updated to reflect the current status by the remote Lambda function.</p> 
<p>&nbsp;</p> 
<b>Walkthrough</b> 
<p><strong>Note: All of the following steps are performed in both regions. The Launch Stack buttons will default to the us-east-1 region.</strong></p> 
<p>Here’s a quick overview of the steps involved in this process:</p> 
<p style="padding-left: 30px">1. An instance is launched or terminated, which triggers an Auto Scaling group lifecycle hook, triggering the Lambda function via CloudWatch Events.<br /> 2. The Lambda function retrieves the list of public IP addresses for all instances in the local region Auto Scaling group.<br /> 3. The Lambda function updates the local and remote region DynamoDB tables with the public IP addresses just received for the local Auto Scaling group.<br /> 4. The Lambda function updates the local region security group with the public IP addresses, removing and adding to ensure that it mirrors what is present for the local and remote Auto Scaling groups.<br /> 5. The Lambda function updates the remote region security group with the public IP addresses, removing and adding to ensure that it mirrors what is present for the local and remote Auto Scaling groups.</p> 
<h3>Prerequisites</h3> 
<p>To deploy this solution, you need to have Auto Scaling groups, launch configurations, and a base security group in both regions. To expedite this process, <a title="undefined" href="https://github.com/awslabs/aws-automating-security-group-updates/blob/master/templates/prerequisites.yml" target="null">this CloudFormation template</a> can be launched in both regions.</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Prerequisites&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/automate-sg-lambda/prerequisites.yml" target="_blank" rel="noopener noreferrer"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/2-launch.png" /></a></p> 
<h3>Step 1: Launch the AWS SAM template in the first region</h3> 
<p>To make the deployment process easy, I’ve created an <a title="undefined" href="https://github.com/awslabs/serverless-application-model" target="null">AWS Serverless Application Model</a> (AWS SAM) template, which is a new specification that makes it easier to manage and deploy serverless applications on AWS. This template creates the following resources:</p> 
<p style="padding-left: 30px">• A Lambda function, to perform the various security group actions<br /> • A DynamoDB table, to track the state of the local and remote Auto Scaling groups<br /> • Auto Scaling group lifecycle hooks for instance launching and terminating<br /> • A CloudWatch event, to track the <code>EC2 Instance-Launch Lifecycle-Action</code> and <code>EC2 Instance-terminate Lifecycle-Action</code> events<br /> • A pointer from the CloudWatch event to the Lambda function, and the necessary permissions</p> 
<p>Download the template from <a title="undefined" href="https://github.com/awslabs/aws-automating-security-group-updates/blob/master/templates/master.yml" target="null">here</a> or click to launch.</p> 
<p><a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=Master&amp;templateURL=https://s3.amazonaws.com/computeblog-us-east-1/automate-sg-lambda/master.yml" target="_blank" rel="noopener noreferrer"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/17/2-launch.png" /></a></p> 
<p>Upon launching the template, you’ll be presented with a list of parameters which includes the remote/local names for your Auto Scaling Groups, AWS region, Security Group IDs, DynamoDB table names, as well as where the code for the Lambda function is located. Because this is the first region you’re launching the stack in, fill out all the parameters except for the <em>RemoteTable</em> parameter as it hasn’t been created yet (you fill this in later).</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/3-cfn.png" /></p> 
<h3>Step 2: Test the local region</h3> 
<p>After the stack has finished launching, you can test the local region. Open the EC2 console and find the Auto Scaling group that was created when launching the prerequisite stack. Change the desired number of instances from 0 to 1.</p> 
<p>For both regions, check your security group to verify that the public IP address of the instance created is now in the security group.</p> 
<p>Local region:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/4-local-sg.png" /></p> 
<p>Remote region:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/5-remote-sg.png" /></p> 
<p>Now, change the desired number of instances for your group back to 0 and verify that the rules are properly removed.</p> 
<p>Local region:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/6-local-sg.png" /></p> 
<p>Remote region:</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/7-remote-sg.png" /></p> 
<h3>Step 3: Launch in the remote region</h3> 
<p>When you deploy a Lambda function using CloudFormation, the Lambda zip file needs to reside in the same region you are launching the template. Once you choose your remote region, create an Amazon S3 bucket and upload the <a href="https://s3.amazonaws.com/computeblog-us-east-1/automate-sg-lambda/auto-sg-updater.zip">Lambda zip file</a> there. Next, go to the remote region and launch the same SAM template as before, but make sure you update the <strong>CodeBucket</strong> and <strong>CodeKey</strong> parameters. Also, because this is the second launch, you now have all the values and can fill out all the parameters, specifically the <em>RemoteTable</em> value.</p> 
<p>&nbsp;</p> 
<h3>Step 4: Update the local region Lambda environment variable</h3> 
<p>When you originally launched the template in the local region, you didn’t have the name of the DynamoDB table for the remote region, because you hadn’t created it yet. Now that you have launched the remote template, you can perform a CloudFormation stack update on the initial SAM template. This populates the remote DynamoDB table name into the initial Lambda function’s environment variables.</p> 
<p>In the CloudFormation console in the initial region, select the stack. Under <strong>Actions</strong>, choose <strong>Update Stack</strong>, and select the SAM template used for both regions. Under <strong>Parameters</strong>, populate the remote DynamoDB table name, as shown below. Choose <strong>Next</strong> and let the stack update complete. This updates your Lambda function and completes the setup process.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/15/8-CFN.png" /></p> 
<p>&nbsp;</p> 
<h3>Step 5: Final testing</h3> 
<p>You now have everything fully configured and in place to trigger security group changes based on instances being added or removed to your Auto Scaling groups in both regions. Test this by changing the desired capacity of your group in both regions.</p> 
<p>True up functionality<br /> If an instance is manually added or removed from the Auto Scaling group, the lifecycle hooks don’t get triggered. To account for this, the Lambda function supports a “true up” functionality in which the function can be manually invoked. If you paste in the following JSON text for your test event, it kicks off the entire workflow. For added peace of mind, you can also have this function fire via a CloudWatch event with a CRON expression for nearly continuous checking.</p> 
<code class="lang-json">{
&quot;detail&quot;: {
&quot;AutoScalingGroupName&quot;: &quot;&lt;your ASG name&gt;&quot;
},
&quot;trueup&quot;:true
}</code> 
<b>Extra credit</b> 
<p>Now that all the resources are created in both regions, go back and break down the policy to incorporate resource-level permissions for specific security groups, Auto Scaling groups, and the DynamoDB tables.</p> 
<p>Although this post is centered around using public IP addresses for your instances, you could instead use a VPN between regions. In this case, you would still be able to use this solution to scope down the security groups to the cluster instances. However, the code would need to be modified to support private IP addresses.</p> 
<p>&nbsp;</p> 
<b>Conclusion</b> 
<p>At this point, you now have a mechanism in place that captures when a new instance is added to or removed from your cluster and updates the security groups in both regions. This ensures that you are locking down your infrastructure securely by allowing access only to other cluster members.</p> 
<p>Keep in mind that this architecture (lifecycle hooks, CloudWatch event, Lambda function, and DynamoDB table) requires that the infrastructure to be deployed in both regions, to have synchronization going both ways.</p> 
<p>Because this Lambda function is modifying security group rules, it’s important to have an audit log of what has been modified and who is modifying them. The out-of-the-box function provides logs in CloudWatch for what IP addresses are being added and removed for which ports. As these are all API calls being made, they are logged in CloudTrail and can be traced back to the IAM role that you created for your lifecycle hooks. This can provide historical data that can be used for troubleshooting or auditing purposes.</p> 
<p>Security is paramount at AWS. We want to ensure that customers are protecting access to their resources. This solution helps you keep your security groups in both regions automatically in sync with your Auto Scaling group resources. Let us know if you have any questions or other solutions you’ve come up with!</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2861');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/LambdaTrigger-626x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Implementing Default Directory Indexes in Amazon S3-backed Amazon CloudFront Origins Using Lambda@Edge</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ronnie Eichler</span></span> | on 
<time property="datePublished" datetime="2017-10-18T07:35:23+00:00">18 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/networking-content-delivery/amazon-cloudfront/" title="View all posts in Amazon CloudFront*"><span property="articleSection">Amazon CloudFront*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/storage/amazon-simple-storage-services-s3/" title="View all posts in Amazon Simple Storage Services (S3)*"><span property="articleSection">Amazon Simple Storage Services (S3)*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2895" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2895&amp;disqus_title=Implementing+Default+Directory+Indexes+in+Amazon+S3-backed+Amazon+CloudFront+Origins+Using+Lambda%40Edge&amp;disqus_url=https://aws.amazon.com/blogs/compute/implementing-default-directory-indexes-in-amazon-s3-backed-amazon-cloudfront-origins-using-lambdaedge/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2895');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>With the recent launch of <a href="https://aws.amazon.com/lambda/edge/">Lambda@Edge</a>, it’s now possible for you to provide even more robust functionality to your static websites. <a href="https://aws.amazon.com/cloudfront">Amazon CloudFront</a> is a content distribution network service. In this post, I show how you can use Lambda@Edge along with the CloudFront origin access identity (OAI) for Amazon S3 and still provide simple URLs (such as www.example.com/about/ instead of www.example.com/about/index.html).<span id="more-2895"></span></p> 
<h3>Background</h3> 
<p>Amazon S3 is a great platform for <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html">hosting a static website</a>. You don’t need to worry about managing servers or underlying infrastructure—you just publish your static to content to an S3 bucket. S3 provides a DNS name such as &lt;<strong>bucket-name</strong>&gt;.s3-website-&lt;<strong>AWS-region</strong>&gt;.amazonaws.com. Use this name for your website by creating a CNAME record in your domain’s DNS environment (or Amazon Route 53) as follows:</p> 
<blockquote> 
<p>www.example.com -&gt; &lt;<strong>bucket-name</strong>&gt;.s3-website-&lt;<strong>AWS-region</strong>&gt;.amazonaws.com</p> 
</blockquote> 
<p>You can also put CloudFront in front of S3 to further scale the performance of your site and cache the content closer to your users. CloudFront can enable HTTPS-hosted sites, by either using a custom Secure Sockets Layer (SSL) certificate or a managed certificate from <a href="https://aws.amazon.com/blogs/aws/new-aws-certificate-manager-deploy-ssltls-based-apps-on-aws/">AWS Certificate Manager</a>. In addition, CloudFront also offers integration with <a href="https://aws.amazon.com/waf/">AWS WAF</a>, a web application firewall. As you can see, it’s possible to achieve some robust functionality by using S3, CloudFront, and other managed services and not have to worry about maintaining underlying infrastructure.</p> 
<p>One of the key concerns that you might have when implementing any type of WAF or CDN is that you want to force your users to go through the CDN. If you implement CloudFront in front of S3, you can achieve this by using an <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html">OAI</a>. However, in order to do this, you cannot use the HTTP endpoint that is exposed by S3’s static website hosting feature. Instead, CloudFront must use the S3 REST endpoint to fetch content from your origin so that the request can be authenticated using the OAI. This presents some challenges in that the REST endpoint does not support redirection to a default index page.</p> 
<p>CloudFront does allow you to specify a default root object (index.html), but it only works on the root of the website (such as http://www.example.com &gt; http://www.example.com/index.html). It does not work on any subdirectory (such as http://www.example.com/about/). If you were to attempt to request this URL through CloudFront, CloudFront would do a S3 GetObject API call against a key that does not exist.</p> 
<p>Of course, it is a bad user experience to expect users to always type index.html at the end of every URL (or even know that it should be there). Until now, there has not been an easy way to provide these simpler URLs (equivalent to the DirectoryIndex Directive in an Apache Web Server configuration) to users through CloudFront. Not if you still want to be able to restrict access to the S3 origin using an OAI. However, with the release of <a href="https://aws.amazon.com/lambda/edge/">Lambda@Edge</a>, you can use a JavaScript function running on the CloudFront edge nodes to look for these patterns and request the appropriate object key from the S3 origin.</p> 
<h3>Solution</h3> 
<p>In this example, you use the compute power at the CloudFront edge to inspect the request as it’s coming in from the client. Then re-write the request so that CloudFront requests a default index object (index.html in this case) for any request URI that ends in ‘/’.</p> 
<p>When a request is made against a web server, the client specifies the object to obtain in the request. You can use this URI and apply a regular expression to it so that these URIs get resolved to a default index object before CloudFront requests the object from the origin. Use the following code:</p> 
<code class="lang-js">'use strict';
exports.handler = (event, context, callback) =&gt; {
// Extract the request from the CloudFront event that is sent to Lambda@Edge 
var request = event.Records[0].cf.request;
// Extract the URI from the request
var olduri = request.uri;
// Match any '/' that occurs at the end of a URI. Replace it with a default index
var newuri = olduri.replace(/\/$/, '\/index.html');
// Log the URI as received by CloudFront and the new URI to be used to fetch from origin
console.log(&quot;Old URI: &quot; + olduri);
console.log(&quot;New URI: &quot; + newuri);
// Replace the received URI with the URI that includes the index page
request.uri = newuri;
// Return to CloudFront
return callback(null, request);
};
</code> 
<p>To get started, create an S3 bucket to be the origin for CloudFront:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/17/Picture1-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/17/Picture1-1-921x1024.png" /></a></p> 
<p>On the other screens, you can just accept the defaults for the purposes of this walkthrough. If this were a production implementation, I would recommend enabling bucket logging and specifying an existing S3 bucket as the destination for access logs. These logs can be useful if you need to troubleshoot issues with your S3 access.</p> 
<p>Now, put some content into your S3 bucket. For this walkthrough, create two simple webpages to demonstrate the functionality: &nbsp;A page that resides at the website root, and another that is in a subdirectory.</p> 
<p><strong>&lt;s3bucketname&gt;/index.html</strong></p> 
<code class="lang-html">&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Root home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the root directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code> 
<p><strong>&lt;s3bucketname&gt;/subdirectory/index.html</strong></p> 
<code class="lang-html">&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Subdirectory home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the /subdirectory/ directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code> 
<p>When uploading the files into S3, you can accept the defaults. You add a bucket policy as part of the CloudFront distribution creation that allows CloudFront to access the S3 origin. You should now have an S3 bucket that looks like the following:</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3root.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3root.png" /></a> 
<p class="wp-caption-text">Root of bucket</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3subdirectory.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/S3subdirectory.png" /></a> 
<p class="wp-caption-text">Subdirectory in bucket</p> 
<p>Next, create a CloudFront distribution that your users will use to access the content. Open the CloudFront console, and choose <strong>Create Distribution</strong>. For <strong>Select a delivery method for your content</strong>, under <strong>Web</strong>, choose <strong>Get Started</strong>.</p> 
<p>On the next screen, you set up the distribution. Below are the options to configure:</p> 
<li><strong>Origin Domain Name</strong>: &nbsp;Select the S3 bucket that you created earlier.</li> 
<li><strong>Restrict Bucket Access:&nbsp;</strong>Choose&nbsp;<strong>Yes</strong>.</li> 
<li><strong>Origin Access Identity:&nbsp;</strong>Create a new identity.</li> 
<li><strong>Grant Read Permissions on Bucket</strong>: Choose&nbsp;<strong>Yes, Update Bucket Policy</strong>.</li> 
<li><strong>Object Caching</strong>: Choose&nbsp;<strong>Customize</strong> (I am changing the behavior to avoid having CloudFront cache objects, as this could affect your ability to troubleshoot while implementing the Lambda code). 
<li><strong>Minimum TTL</strong>: 0</li> 
<li><strong>Maximum TTL</strong>: 0</li> 
<li><strong>Default TTL</strong>: 0</li> 
</ul> </li> 
<p>You can accept all of the other defaults. Again, this is a proof-of-concept exercise. After you are comfortable that the CloudFront distribution is working properly with the origin and Lambda code, you can re-visit the preceding values and make changes before implementing it in production.</p> 
<p>CloudFront distributions can take several minutes to deploy (because the changes have to propagate out to all of the edge locations). After that’s done, test the functionality of the S3-backed static website. Looking at the distribution, you can see that CloudFront assigns a domain name:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/CfDistribution-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/CfDistribution-1.png" /></a></p> 
<p>Try to access the website using a combination of various URLs:</p> 
<p>http://&lt;domainname&gt;/: &nbsp;<strong>Works</strong></p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/
*   Trying 54.192.192.214...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.214) port 80 (#0)
&gt; GET / HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; ETag: &quot;cb7e2634fe66c1fd395cf868087dd3b9&quot;
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: Miss from cloudfront
&lt; X-Amz-Cf-Id: -D2FSRwzfcwyKZKFZr6DqYFkIf4t7HdGw2MkUF5sE6YFDxRJgi0R1g==
&lt; Content-Length: 209
&lt; Content-Type: text/html
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:16 GMT
&lt; Via: 1.1 6419ba8f3bd94b651d416054d9416f1e.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Root home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the root directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>This is because CloudFront is configured to request a default root object (index.html) from the origin.</p> 
<p>http://&lt;domainname&gt;/subdirectory/: &nbsp;<strong>Doesn’t work</strong></p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/subdirectory/
*   Trying 54.192.192.214...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.214) port 80 (#0)
&gt; GET /subdirectory/ HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; ETag: &quot;d41d8cd98f00b204e9800998ecf8427e&quot;
&lt; x-amz-server-side-encryption: AES256
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: Miss from cloudfront
&lt; X-Amz-Cf-Id: Iqf0Gy8hJLiW-9tOAdSFPkL7vCWBrgm3-1ly5tBeY_izU82ftipodA==
&lt; Content-Length: 0
&lt; Content-Type: application/x-directory
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:24 GMT
&lt; Via: 1.1 6419ba8f3bd94b651d416054d9416f1e.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>If you use a tool such like cURL to test this, you notice that CloudFront and S3 are returning a blank response. The reason for this is that the subdirectory does exist, but it does not resolve to an S3 object. Keep in mind that S3 is an object store, so there are no real directories. User interfaces such as the S3 console present a hierarchical view of a bucket with folders based on the presence of forward slashes, but behind the scenes the bucket is just a collection of keys that represent stored objects.</p> 
<p>http://&lt;domainname&gt;/subdirectory/index.html: &nbsp;<strong>Works</strong></p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/subdirectory/index.html
*   Trying 54.192.192.130...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.130) port 80 (#0)
&gt; GET /subdirectory/index.html HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Date: Thu, 20 Jul 2017 20:35:15 GMT
&lt; ETag: &quot;ddf87c487acf7cef9d50418f0f8f8dae&quot;
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: RefreshHit from cloudfront
&lt; X-Amz-Cf-Id: bkh6opXdpw8pUomqG3Qr3UcjnZL8axxOH82Lh0OOcx48uJKc_Dc3Cg==
&lt; Content-Length: 227
&lt; Content-Type: text/html
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:45 GMT
&lt; Via: 1.1 3f2788d309d30f41de96da6f931d4ede.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Subdirectory home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the /subdirectory/ directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>This request works as expected because you are referencing the object directly. Now, you implement the Lambda@Edge function to return the default index.html page for any subdirectory. Looking at the example JavaScript code, here’s where the magic happens:</p> 
<code class="lang-js">var newuri = olduri.replace(/\/$/, '\/index.html');
</code> 
<p>You are going to use a JavaScript regular expression to match any ‘/’ that occurs at the end of the URI and replace it with ‘/index.html’. This is the equivalent to what S3 does on its own with static website hosting. However, as I mentioned earlier, you can’t rely on this if you want to use a policy on the bucket to restrict it so that users must access the bucket through CloudFront. That way, all requests to the S3 bucket must be authenticated using the S3 REST API. Because of this, you implement a Lambda@Edge function that takes any client request ending in ‘/’ and append a default ‘index.html’ to the request before requesting the object from the origin.</p> 
<p>In the Lambda console, choose <strong>Create function</strong>. On the next screen, skip the blueprint selection and choose <strong>Author from scratch</strong>, as you’ll use the sample code provided.</p> 
<p>Next, configure the trigger. Choosing the empty box shows a list of available triggers. Choose <strong>CloudFront</strong> and select your CloudFront distribution ID (created earlier). For this example, leave <strong>Cache Behavior</strong> as <strong>*</strong> and <strong>CloudFront Event</strong> as <strong>Origin Request</strong>. Select the <strong>Enable trigger and replicate box</strong> and choose <strong>Next</strong>.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/LambdaTrigger.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/20/LambdaTrigger.png" /></a></p> 
<p>Next, give the function a name and a description. Then, copy and paste the following code:</p> 
<code class="lang-js">'use strict';
exports.handler = (event, context, callback) =&gt; {
// Extract the request from the CloudFront event that is sent to Lambda@Edge 
var request = event.Records[0].cf.request;
// Extract the URI from the request
var olduri = request.uri;
// Match any '/' that occurs at the end of a URI. Replace it with a default index
var newuri = olduri.replace(/\/$/, '\/index.html');
// Log the URI as received by CloudFront and the new URI to be used to fetch from origin
console.log(&quot;Old URI: &quot; + olduri);
console.log(&quot;New URI: &quot; + newuri);
// Replace the received URI with the URI that includes the index page
request.uri = newuri;
// Return to CloudFront
return callback(null, request);
};
</code> 
<p>Next, define a role that grants permissions to the Lambda function. For this example, choose <strong>Create new role from template</strong>, <strong>Basic Edge Lambda permissions</strong>. This creates a new IAM role for the Lambda function and grants the following permissions:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;,
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:logs:*:*:*&quot;
]
}
]
}
</code> 
<p>In a nutshell, these are the permissions that the function needs to create the necessary CloudWatch log group and log stream, and to put the log events so that the function is able to write logs when it executes.</p> 
<p>After the function has been created, you can go back to the browser (or cURL) and re-run the test for the subdirectory request that failed previously:</p> 
<code class="lang-xml">› curl -v http://d3gt20ea1hllb.cloudfront.net/subdirectory/
*   Trying 54.192.192.202...
* TCP_NODELAY set
* Connected to d3gt20ea1hllb.cloudfront.net (54.192.192.202) port 80 (#0)
&gt; GET /subdirectory/ HTTP/1.1
&gt; Host: d3gt20ea1hllb.cloudfront.net
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Date: Thu, 20 Jul 2017 21:18:44 GMT
&lt; ETag: &quot;ddf87c487acf7cef9d50418f0f8f8dae&quot;
&lt; Accept-Ranges: bytes
&lt; Server: AmazonS3
&lt; X-Cache: Miss from cloudfront
&lt; X-Amz-Cf-Id: rwFN7yHE70bT9xckBpceTsAPcmaadqWB9omPBv2P6WkIfQqdjTk_4w==
&lt; Content-Length: 227
&lt; Content-Type: text/html
&lt; Last-Modified: Wed, 19 Jul 2017 19:21:45 GMT
&lt; Via: 1.1 3572de112011f1b625bb77410b0c5cca.cloudfront.net (CloudFront), 1.1 iad6-proxy-3.amazon.com:80 (Cisco-WSA/9.1.2-010)
&lt; Connection: keep-alive
&lt;
&lt;!doctype html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Subdirectory home page&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Hello, this page resides in the /subdirectory/ directory.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
* Curl_http_done: called premature == 0
* Connection #0 to host d3gt20ea1hllb.cloudfront.net left intact
</code> 
<p>You have now configured a way for CloudFront to return a default index page for subdirectories in S3!</p> 
<h3>Summary</h3> 
<p>In this post, you used <a href="http://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html">Lambda@Edge</a> to be able to use CloudFront with an S3 origin access identity and serve a default root object on subdirectory URLs. To find out some more about this use-case, see <a href="http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html">Lambda@Edge integration with CloudFront</a>&nbsp;in&nbsp;our documentation.</p> 
<p>If you have questions or suggestions, feel free to comment below. For troubleshooting or implementation help, check out the Lambda <a href="https://forums.aws.amazon.com/forum.jspa?forumID=186">forum</a>.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2895');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/withoutcold-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Analyzing Performance for Amazon Rekognition Apps Written on AWS Lambda Using AWS X-Ray</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Bharath Kumar</span></span> | on 
<time property="datePublished" datetime="2017-10-12T11:30:41+00:00">12 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/developer-tools/aws-x-ray/" title="View all posts in AWS X-Ray*"><span property="articleSection">AWS X-Ray*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/" title="View all posts in Compute*"><span property="articleSection">Compute*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/analyzing-performance-for-amazon-rekognition-apps-written-on-aws-lambda-using-aws-x-ray/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2975" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2975&amp;disqus_title=Analyzing+Performance+for+Amazon+Rekognition+Apps+Written+on+AWS+Lambda+Using+AWS+X-Ray&amp;disqus_url=https://aws.amazon.com/blogs/compute/analyzing-performance-for-amazon-rekognition-apps-written-on-aws-lambda-using-aws-x-ray/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2975');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://aws.amazon.com/xray/"> AWS X-Ray </a> helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. It allows you to view, filter, and gain insights to identify issues and opportunities for optimization.</p> 
<p>X-Ray helps you to both understand the performance of your app and to annotate important parameters. In this post, I look at a sample <a href="https://aws.amazon.com/rekognition/">Amazon Rekognition</a> app that stores images in <a href="https://aws.amazon.com/s3/">Amazon S3</a>. The app calls the&nbsp;<a href="http://docs.aws.amazon.com/rekognition/latest/dg/API_RecognizeCelebrities.html"><code class="lang-json">RecognizeCelebrities</code></a> action from an <a href="https://aws.amazon.com/lambda/"> AWS Lambda </a> function. The function uses X-Ray features to analyze the app’s performance in recognizing any celebrity in a single image or in multiple images. These features include:</p> 
<li>Annotations and traces</li> 
<li>Histogram</li> 
<li>Metadata</li> 
<li>Exceptions</li> 
<p><span id="more-2975"></span></p> 
<b>Sample app overview</b> 
<p>In the <a href="https://github.com/awslabs/aws-xray-rekognition-lambda-sample">sample app</a>, you can upload images and store them in the <strong>imagestoragexray</strong> bucket. Then you can choose <strong>Recognize faces</strong> for an individual image. This calls a Lambda function, which then calls <code class="lang-json">RecognizeCelebrities</code> to recognize the celebrities in the image. The Lambda function returns to the client a comma-separated list of celebrities that it found in the image.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/architecture-1024x576-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/architecture-1024x576-1.png" /></a> 
<p class="wp-caption-text">Overview of the sample app’s architecture</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/servicemap.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/servicemap-1024x472.png" /></a> 
<p class="wp-caption-text">X-Ray service map from the sample app</p> 
<p>To search for a celebrity in the collection of images, you can type the celebrity’s name in the text box and choose <strong>Search celeb</strong>. This calls another Lambda function that runs <code class="lang-json">RecognizeCelebrities</code> on all the images in the <strong>imagestoragexray</strong> bucket. The Lambda function then returns the number of times that the celebrity was found in all the images.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/Picture3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/Picture3.png" /></a> 
<p class="wp-caption-text">Example of a sample app webpage with stored images</p> 
<b>Annotations and traces</b> 
<p>Annotations are indexed for grouping traces in the X-Ray console, based on parameters stored in the annotations. Annotations help you determine the performance of particular images.</p> 
<p>Using annotations to record the celebrity image file name, you can filter to see traces for performance of your app on specific images. For example, all the celebrities recognized in the analyzed images are annotated with the average confidence with which Amazon Rekognition matched the celebrity. The number of faces found in an image is also annotated.</p> 
<p>Resourceful annotations such as these provide you with a comprehensive outlook about your app’s performance.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/annotations.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/annotations-1024x388.png" /></a> 
<p class="wp-caption-text">X-Ray annotation in the sample app</p> 
<h3>Understanding Lambda function cold starts</h3> 
<p>When a Lambda function is invoked for the first time, or after it’s updated, Lambda launches a container based on the configuration settings that you provided. It takes time to set up a container and do the necessary bootstrapping, also known as a cold start.</p> 
<p>The latency of a Lambda function is the amount of time between when a request starts and when it completes. Due to the cold start behavior, Lambda function executions may take longer on the first invocation or after a function has been updated. For subsequent invocations, Lambda tries to reuse the container to reduce the latency.</p> 
<p>X-Ray helps you trace the cold start time as well as the overall latency for a Lambda function. In the following screenshot showing the 3.5-second duration of the Lambda function, you see that it took 473 milliseconds to initialize the function. For more information about execution time, see <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html">AWS Lambda: How It Works</a>.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/coldstart.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/coldstart-1024x441.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing initialization time for the Lambda function</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/withoutcold-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/withoutcold-1-1024x389.png" /></a> 
<p class="wp-caption-text">X-Ray trace on subsequent invocations of the Lambda function without the cold start</p> 
<h3>Determining the time for image analysis on celebrities</h3> 
<p>You can determine that the time Amazon Rekognition takes differs with the number of celebrity images in the photo. For example, a photo without a celebrity takes a shorter time to process, compared to a photo with a celebrity that Amazon Rekognition was able to recognize. In the following screenshots, you can see that the Lambda function that calls Amazon Rekognition to find images with Steve Jobs took 1.7 seconds to complete. This is compared to 858 milliseconds, using the same Lambda function for an image with a random landscape</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobslandscape.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobslandscape-1024x189.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the time that a Lambda function takes to call Amazon Rekognition and return</p> 
<p>By choosing the particular traces that show up in the trace list for your annotation filter, you can investigate further. Specifically, you can look at the performance of your Amazon Rekognition call.</p> 
<p>Amazon Rekognition took 1.3 seconds to analyze and provide results for the image with Steve Jobs. However, it took only 829 milliseconds to analyze and provide results for the random landscape image.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobstrace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/stevejobstrace-1024x424.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the time that Amazon Rekognition takes to analyze the Steve Jobs image</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/landscapetrace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/landscapetrace-1024x409.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the time that Amazon Rekognition takes to analyze the random landscape image</p> 
<h3>Determining the time for image analysis on multiple faces</h3> 
<p>X-Ray can also help you to look at the performance of your app for images with multiple faces. Using the annotations feature, you can record the number of faces that Amazon Rekognition recognized in the image. Through this annotation, you can filter traces for images with a specific number of face counts that were recognized or not recognized in Amazon Rekognition, as shown in the following screenshot.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/multiplefaces-facecount.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/multiplefaces-facecount-1024x221.png" /></a> 
<p class="wp-caption-text">X-Ray annotations for face counts in images recognized by Amazon Rekognition</p> 
<p>In the images below, notice that Amazon Rekognition takes more time to analyze images that contain more faces. For example, Amazon Rekognition took 8.9 seconds to analyze an image with 15 faces compared to only 6.0 seconds for an image with 5 faces. You can use the filter expression <em>Annotation.Facecount &gt; “5”</em> to view requests for which Amazon Rekognition recognized more than 5 faces.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount15trace-1.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount15trace-1-1024x405.png" /></a> 
<p class="wp-caption-text">X-Ray traces for an image with 15 face counts</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount5trace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/facecount5trace-1024x430.png" /></a> 
<p class="wp-caption-text">X-Ray traces for an image with 5 face counts</p> 
<b>Histograms</b> 
<p>When you select a node or edge on an X-Ray service map, the console shows a latency distribution histogram. It shows duration on the x-axis, and the percentage of requests that match each duration on the y-axis. Using this histogram, you can look at calls that have a high latency and try to improve their performance.</p> 
<h3>Using the latency histogram</h3> 
<p>In your service map, you can choose individual nodes to look at the response latency for calls. When you choose the Amazon Rekognition node AWS::rekognition in the service map, you can see the corresponding response latency from Amazon Rekognition, as shown in the following screenshot.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/xraylatency.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/xraylatency-1024x783.png" /></a> 
<p class="wp-caption-text">X-Ray response distribution histogram for response latency</p> 
<p>Notice that the response latency increases with the number of face counts in the image. As you noted previously, image analysis time also increases with the face count increase.</p> 
<p>For example, the random landscape image’s response latency is less than 1 second, whereas calls to Amazon Rekognition for images with one face count have a response latency between 1 second and 1.5 seconds. Images with multiple face counts have a much higher response latency, greater than 1.5 seconds.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/between1and15.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/between1and15-1024x485.png" /></a> 
<p class="wp-caption-text">X-Ray traces for images with a response latency between 1 second and 1.5 seconds</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/highlatencyxraytraces.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/highlatencyxraytraces-1024x784.png" /></a> 
<p class="wp-caption-text">X-Ray response distribution histogram for a higher response latency</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/histogram.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/histogram-1024x490.png" /></a> 
<p class="wp-caption-text">X-Ray traces for images with multiple face counts that have a higher response latency</p> 
<b>Metadata</b> 
<p>The metadata feature in X-Ray enables you to store app information for later reference. However, you cannot filter out traces based on the metadata. The sample app stores the response from Amazon Rekognition in metadata for your reference.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/metadata.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/10/metadata-1024x549.png" /></a> 
<p class="wp-caption-text">X-Ray metadata in the sample app</p> 
<b>Exceptions</b> 
<p>X-Ray also provides information on exceptions from other AWS services in the trace. For the sample app, Amazon Rekognition responded back with an exception when the RecognizeCelebrities call was made on a non-image file. You can also filter requests that had an exception in Amazon Rekognition, using the following expression:<br /> <code>service(&quot;rekognition&quot;) { error = true }</code></p> 
<p>For more information, see <a href="http://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html">Searching for Traces in the AWS X-Ray Console with Filter Expressions</a>.</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptiontrace.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptiontrace-1024x448.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing an error from Amazon Rekognition</p> 
<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptioninfo.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/exceptioninfo.png" /></a> 
<p class="wp-caption-text">X-Ray trace showing the Amazon Rekognition exception in more detail</p> 
<b>Summary</b> 
<p>X-Ray helps you analyze your app and its performance. Through the annotations feature, you can index and filter specific traces. Annotating specific parameters in your app helps you get a comprehensive overview of your app’s performance specific to these parameters. The histogram helps you analyze calls with high response latency and gives you visibility into the time that individual services take. The metadata feature helps you store any relevant information that might be useful to improve your app’s performance. X-Ray also provides information on exceptions that happened on a particular trace to an AWS service.</p> 
<p>Overall, by using X-Ray you can both improve performance of your app and understand the underlying services that your app uses. Use this information to troubleshoot and improve specific portions of your app, and save both time and money.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2975');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Clean up Your Container Images with Amazon ECR Lifecycle Policies</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-10-11T16:33:49+00:00">11 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/clean-up-your-container-images-with-amazon-ecr-lifecycle-policies/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-3025" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=3025&amp;disqus_title=Clean+up+Your+Container+Images+with+Amazon+ECR+Lifecycle+Policies&amp;disqus_url=https://aws.amazon.com/blogs/compute/clean-up-your-container-images-with-amazon-ecr-lifecycle-policies/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3025');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>This post comes from the desk of Brent Langston.</em></p> 
<p>—</p> 
<p>Starting today, customers can keep their container image repositories tidy by automatically removing old or unused images using <strong>lifecycle policies</strong>, now available as part of <a href="https://aws.amazon.com/ecr">Amazon EC2 Container Registry (Amazon ECR)</a>.</p> 
<p>Amazon ECR is a fully managed Docker container registry that makes it easy to store manage and deploy Docker container images without worrying about the typical challenges of scaling a service to handle pulling hundreds of images at one time. This scale means that development teams using Amazon ECR actively often find that their repositories fill up with many container image versions. This makes it difficult to find the code changes that matter and incurs unnecessary storage costs. Previously, cleaning up your repository meant spending time to manually delete old images, or writing and executing scripts.</p> 
<p>Now, lifecycle policies allow you to define a set of rules to remove old container images automatically. You can also preview rules to see exactly which container images are affected when the rule runs. This allows repositories to be better organized, makes it easier to find the code revisions that matter, and lowers storage costs.</p> 
<p>Let’s take a look&nbsp;at how lifecycle policies work.</p> 
<p><span id="more-3025"></span></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/10/11/ecr-lifecyclepolicies.png" /></p> 
<b>Ground&nbsp;Rules</b> 
<p>One of the biggest benefits of deploying code in containers is the ability to quickly and easily roll back to a previous version. You can deploy with less risk because, if something goes wrong, it is easy to revert back to the previous container version and know that your application will run like it did before the failed deployment. Most people probably never roll back past a few versions. If your situation is similar, then one simple lifecycle rule might be to just keep the last 30 images.</p> 
<h3>Last 30 Images</h3> 
<p>In your ECR registry, choose <strong>Dry-Run Lifecycle Rules, Add</strong>.</p> 
<li>For <strong>Image Status</strong>, select <strong>Untagged</strong>.</li> 
<li>Under <strong>Match criteria</strong>, for <strong>Count Type</strong>, enter <strong>Image Count More Than</strong>.</li> 
<li>For <strong>Count Number</strong>, enter <strong>30</strong>.</li> 
<li>For <strong>Rule action</strong>, choose <strong>expire</strong>.</li> 
<p>Choose <strong>Save</strong>. To see which images would be cleaned up, <strong>Save and dry-run rules</strong>.</p> 
<p>Of course, there are teams who, for compliance reasons, might prefer to keep certain images for a period of time, rather than keeping by count. For that situation, you can choose to clean up images older than 90 days.</p> 
<h3>Last 90 Days</h3> 
<p>Select the rule that you just created and choose Edit. Change the parameters to keep only 90 days of untagged images:</p> 
<li>Under <strong>Match criteria</strong>, for <strong>Count Type</strong>, enter <strong>Since Image Pushed</strong></li> 
<li>For <strong>Count Number</strong>, enter <strong>90</strong>.</li> 
<li>For <strong>Count Unit</strong>, enter <strong>days</strong>.</li> 
<b>Tags</b> 
<p>Certainly 90 days is an arbitrary timeframe, and your team might have policies in place that would require a longer timeframe for certain kinds of images. If that’s the case, but you still want to continue with the spring cleaning, you can consider getting rid of images that are tag prefixed.</p> 
<p>Here is the list of rules I came up with to groom untagged, development, staging, and production images:</p> 
<li>Remove untagged images over 90 days old</li> 
<li>Remove development tagged images over 90 days old</li> 
<li>Remove staging tagged images over 180 days old</li> 
<li>Remove production tagged images over 1 year old</li> 
<p>As you can see, the new Amazon ECR lifecycle policies are powerful, and help you easily keep the images you need, while cleaning out images you may never use again. This feature is available starting today, in all <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">regions where Amazon ECR is available</a>, at no extra charge. For more information, see <a href="http://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html">Amazon ECR Lifecycle Policies</a> in the AWS&nbsp;technical documentation.</p> 
<p>— Brent<br /> <a href="https://twitter.com/brentContained">@brentContained</a></p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-3025');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture1-3-629x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Improved Testing on the AWS Lambda Console</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Orr Weinstein</span></span> | on 
<time property="datePublished" datetime="2017-10-02T11:10:46+00:00">02 OCT 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/improved-testing-on-the-aws-lambda-console/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2928" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2928&amp;disqus_title=Improved+Testing+on+the+AWS+Lambda+Console&amp;disqus_url=https://aws.amazon.com/blogs/compute/improved-testing-on-the-aws-lambda-console/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2928');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>(This post has been written by Chris Tate, SDE on the Lambda Console team)</p> 
<p>Today, <a href="https://aws.amazon.com/lambda/">AWS Lambda</a> released three console enhancements:</p> 
<li>A quicker creation flow that lets you quickly create a function with the minimum working configuration, so that you can start iterating faster.</li> 
<li>A streamlined configuration page with Lambda function settings logically grouped into cards, which makes locating and making changes much easier.</li> 
<li>Persisting multiple events to help test your function.</li> 
<p>This post focuses on persisting test events, and I discuss how I’ve been using this new feature. Now when you are testing on the Lambda console, you can save up to 10 test events per function, and each event can be up to 6 megabytes in size, the maximum payload size for synchronous invocations. The events are saved for the logged-in user, so that two different users in the same account have their own set of events.</p> 
<b>Testing Lambda functions</b> 
<p>As a Lambda console developer, when I work on side projects at home, I sometimes use our development server. I’ve been using this new feature to test a Lambda function in one of my projects. The function is probably more complicated than it should be, because it can be triggered by an Alexa skill, Amazon CloudWatch schedule, or an Amazon API Gateway API. If you have had a similarly complicated function, you may have run into the same problem I did: &nbsp;How do you test?</p> 
<p>For quick testing, I used the console but the console used to save only one test event at a time. To work around this, my solution was a text file with three different JSON events, one for each trigger. I would copy whatever event I needed into the Lambda console, tweak it, and choose Test. This would become particularly annoying when I wanted to quickly test all three.</p> 
<p>I also switch between my laptop and desktop depending on my mood. For that reason, I needed to make sure this text file with the events were shared in some way, as the console only locally saved one test event to the current browser. But now you don’t have to worry about any of that.</p> 
<b>Walkthrough</b> 
<p>In the Lambda console, go to the detail page of any function, and select <strong>Configure test events</strong> from the test events dropdown (the dropdown beside the orange test button). In the dialog box, you can manage 10 test events for your function. First, paste your Alexa trigger event in the dialog box and type an event name, such as AlexaTrigger.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture2-2.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture2-2.png" /></a></p> 
<p>Choose <strong>Create</strong>. After it saves, you see AlexaTrigger in the Test list.</p> 
<p>When you open the dialog box again by choosing <strong>Configure test events</strong>, you are in edit mode.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture1-3.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Picture1-3.png" /></a></p> 
<p>To add another event, choose <strong>Create new test event</strong>. Now you can choose from a list of templates or any of your previously saved test events. This is very useful for a couple of reasons:</p> 
<li>First, when you want to slightly tweak one of your existing events and still keep the earlier version intact.</li> 
<li>Second, when you are not sure how to structure a particular event from an event source. You can use one of the sample event templates and tweak them to your needs. Skip it when you know what your event should be.</li> 
<p>Paste in your CloudWatch schedule event, give it a name, and choose <strong>Create</strong>. Repeat for API Gateway.</p> 
<p>Now that you have three events saved, you can quickly switch between them and repeatedly test. Furthermore, if you’re on your desktop but you created the test events on your laptop, there’s no problem. You can still see all your events and you can switch back and forth seamlessly between different computers.</p> 
<b>Conclusion</b> 
<p>This feature should allow you to more easily test your Lambda functions through the console. If you have more suggestions, add a comment to this post or submit feedback through the console. We actually read the feedback, believe it!</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2928');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Creating a Cost-Efficient Amazon ECS Cluster for Scheduled Tasks</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-09-28T10:44:39+00:00">28 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-registry/" title="View all posts in Amazon EC2 Container Registry*"><span property="articleSection">Amazon EC2 Container Registry*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2-container-service/" title="View all posts in Amazon EC2 Container Service*"><span property="articleSection">Amazon EC2 Container Service*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/creating-a-cost-efficient-amazon-ecs-cluster-for-scheduled-tasks/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2947" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2947&amp;disqus_title=Creating+a+Cost-Efficient+Amazon+ECS+Cluster+for+Scheduled+Tasks&amp;disqus_url=https://aws.amazon.com/blogs/compute/creating-a-cost-efficient-amazon-ecs-cluster-for-scheduled-tasks/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2947');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/28/mperi_profile.jpeg" /></td> 
</tr> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><b>Madhuri Peri</b><br /> Sr. DevOps Consultant</td> 
</tr> 
</tbody> 
</table> 
<p>When you use Amazon Relational Database Service (<a href="https://aws.amazon.com/rds/">Amazon RDS</a>), depending on the logging levels on the RDS instances and the volume of transactions, you could generate a lot of log data. To ensure that everything is running smoothly, many customers search for log error patterns using different log aggregation and visualization systems, such as Amazon Elasticsearch Service, Splunk, or other tool of their choice. A module needs to periodically retrieve the RDS logs using the SDK, and then send them to Amazon S3. From there, you can stream them to your log aggregation tool.</p> 
<p>One option is writing an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function to retrieve the log files. However, because of the time that this function needs to execute, depending on the volume of log files retrieved and transferred, it is possible that Lambda could time out on many instances.&nbsp; Another approach is launching an Amazon EC2 instance that runs this job periodically. However, this would require you to run an EC2 instance continuously, not an optimal use of time or money.</p> 
<p>Using&nbsp;the new Amazon CloudWatch integration with Amazon EC2 Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>), you&nbsp;can&nbsp;trigger this job to run in a container on an existing Amazon ECS cluster. Additionally, this would allow you to improve costs by running containers on a fleet of Spot Instances.</p> 
<p>In this post, I will show you how to use the new scheduled tasks (<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduled_tasks.html">cron</a>) feature in Amazon ECS and launch tasks&nbsp;using CloudWatch events, while leveraging Spot Fleet to maximize availability and cost optimization for containerized workloads.</p> 
<p><span id="more-2947"></span></p> 
<b>Architecture</b> 
<p>The following diagram shows how the various components described schedule a task that retrieves log files from&nbsp;Amazon RDS database instances, and deposits the logs into an S3 bucket.</p> 
<p>Amazon ECS cluster container instances are using Spot Fleet, which is a perfect match for the workload that needs to run when it can. This improves cluster costs.</p> 
<p>The task definition defines which Docker image to retrieve from the Amazon&nbsp;EC2 Container Registry (Amazon ECR)&nbsp;repository and run on the Amazon ECS cluster.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/917_scheduledtasks-arch-1024x790.png" /></p> 
<p>The container image has Python code functions to make AWS API calls using boto3. It iterates over the RDS database instances, retrieves the logs, and deposits them in the S3 bucket. Many customers choose these logs to be delivered to their centralized log-store. CloudWatch Events defines the schedule for when the container task has to be launched.</p> 
<b>Walkthrough</b> 
<p>To provide the basic framework, we have built an AWS CloudFormation <a href="https://github.com/awslabs/aws-ecs-scheduled-tasks/blob/master/cloudformation/ecs-spot-fleet.yaml">template</a> that creates the following resources:</p> 
<li>Amazon ECR repository for storing the Docker image to be used in the task definition</li> 
<li>S3 bucket that holds the transferred logs</li> 
<li>Task definition, with image name and S3 bucket as environment variables provided via input parameter</li> 
<li>CloudWatch Events rule</li> 
<li>Amazon ECS cluster</li> 
<li>Amazon ECS container instances using Spot Fleet</li> 
<li>IAM roles required for the container instance profiles</li> 
<h3>Before you begin</h3> 
<p>Ensure that <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git</a>, <a href="https://docs.docker.com/engine/installation/">Docker</a>, and the <a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">AWS CLI</a> are installed on your computer.</p> 
<p>In your AWS account, instantiate one Amazon Aurora instance using the console. For more information, see <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.CreateInstance.html">Creating an Amazon Aurora DB Cluster</a>.</p> 
<h3>Implementation Steps</h3> 
<ol> 
<li>Clone the code from GitHub that performs RDS API calls to retrieve the log files.<br /> <code>git clone https://github.com/awslabs/aws-ecs-scheduled-tasks.git</code></li> 
<li>Build and tag the image.<br /> <code>cd aws-ecs-scheduled-tasks/container-code/src &amp;&amp; ls</code><p></p> <code class="lang-bash">Dockerfile		rdslogsshipper.py	requirements.txt</code> <p><code>docker build -t rdslogsshipper .</code></p> <code class="lang-bash">Sending build context to Docker daemon 9.728 kB
Step 1 : FROM python:3
---&gt; 41397f4f2887
Step 2 : WORKDIR /usr/src/app
---&gt; Using cache
---&gt; 59299c020e7e
Step 3 : COPY requirements.txt ./
---&gt; 8c017e931c3b
Removing intermediate container df09e1bed9f2
Step 4 : COPY rdslogsshipper.py /usr/src/app
---&gt; 099a49ca4325
Removing intermediate container 1b1da24a6699
Step 5 : RUN pip install --no-cache-dir -r requirements.txt
---&gt; Running in 3ed98b30901d
Collecting boto3 (from -r requirements.txt (line 1))
Downloading boto3-1.4.6-py2.py3-none-any.whl (128kB)
Collecting botocore (from -r requirements.txt (line 2))
Downloading botocore-1.6.7-py2.py3-none-any.whl (3.6MB)
Collecting s3transfer&lt;0.2.0,&gt;=0.1.10 (from boto3-&gt;-r requirements.txt (line 1))
Downloading s3transfer-0.1.10-py2.py3-none-any.whl (54kB)
Collecting jmespath&lt;1.0.0,&gt;=0.7.1 (from boto3-&gt;-r requirements.txt (line 1))
Downloading jmespath-0.9.3-py2.py3-none-any.whl
Collecting python-dateutil&lt;3.0.0,&gt;=2.1 (from botocore-&gt;-r requirements.txt (line 2))
Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
Collecting docutils&gt;=0.10 (from botocore-&gt;-r requirements.txt (line 2))
Downloading docutils-0.14-py3-none-any.whl (543kB)
Collecting six&gt;=1.5 (from python-dateutil&lt;3.0.0,&gt;=2.1-&gt;botocore-&gt;-r requirements.txt (line 2))
Downloading six-1.10.0-py2.py3-none-any.whl
Installing collected packages: six, python-dateutil, docutils, jmespath, botocore, s3transfer, boto3
Successfully installed boto3-1.4.6 botocore-1.6.7 docutils-0.14 jmespath-0.9.3 python-dateutil-2.6.1 s3transfer-0.1.10 six-1.10.0
---&gt; f892d3cb7383
Removing intermediate container 3ed98b30901d
Step 6 : COPY . .
---&gt; ea7550c04fea
Removing intermediate container b558b3ebd406
Successfully built ea7550c04fea</code> </li> 
<li>Run the <a href="https://github.com/awslabs/aws-ecs-scheduled-tasks.git">CloudFormation stack</a> and get the names for the Amazon ECR repo and S3 bucket. In the stack, choose <b>Outputs</b>.<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/917_scheduledtasks-cf.png" /></li> 
<li>Open the ECS console and choose <b>Repositories</b>. The rdslogs repo has been created. Choose <b>View Push Commands</b> and follow the instructions to connect to the repository and push the image for the code that you built in Step 2. The screenshot shows the final result:<br /> <img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/917_scheduledtasks-cf2.png" /></li> 
<li>Associate the CloudWatch scheduled task with the created Amazon ECS Task Definition, using a new CloudWatch event rule that is scheduled to run at intervals. The following rule is scheduled to run every 15 minutes:<br /> <code>aws --profile default --region us-west-2 events put-rule --name demo-ecs-task-rule&nbsp; --schedule-expression &quot;rate(15 minutes)&quot;</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;RuleArn&quot;: &quot;arn:aws:events:us-west-2:12345678901:rule/demo-ecs-task-rule&quot;
}</code> </li> 
<li>CloudWatch requires IAM permissions to place a task on the Amazon ECS cluster when the CloudWatch event rule is executed, in addition to an IAM role that can be assumed by CloudWatch Events. This is done in three steps: 
<ol> 
<li>Create the IAM role to be assumed by CloudWatch.<br /> <code>aws --profile default --region us-west-2 iam create-role --role-name Test-Role --assume-role-policy-document file://event-role.json</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;Role&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;AssumeRolePolicyDocument&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Version&quot;: &quot;2012-10-17&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Statement&quot;: [
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Action&quot;: &quot;sts:AssumeRole&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Effect&quot;: &quot;Allow&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Principal&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &quot;Service&quot;: &quot;events.amazonaws.com&quot;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]
&nbsp; &nbsp; &nbsp; &nbsp; },&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;RoleId&quot;: &quot;AROAIRYYLDCVZCUACT7FS&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;CreateDate&quot;: &quot;2017-07-14T22:44:52.627Z&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;RoleName&quot;: &quot;Test-Role&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Path&quot;: &quot;/&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Arn&quot;: &quot;arn:aws:iam::12345678901:role/Test-Role&quot;
&nbsp; &nbsp; }
}</code> <p>The following is an example of the event-role.json&nbsp;file used earlier:</p> <code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;Service&quot;: &quot;events.amazonaws.com&quot;
},
&quot;Action&quot;: &quot;sts:AssumeRole&quot;
}
]
}</code> </li> 
<li>Create the IAM policy defining the ECS cluster and task definition. You need to get these values from the CloudFormation outputs and resources.<br /> <code>aws --profile default --region us-west-2 iam create-policy --policy-name test-policy --policy-document file://event-policy.json</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;Policy&quot;: {
&nbsp; &nbsp; &nbsp; &nbsp; &quot;PolicyName&quot;: &quot;test-policy&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;CreateDate&quot;: &quot;2017-07-14T22:51:20.293Z&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;AttachmentCount&quot;: 0,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;IsAttachable&quot;: true,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;PolicyId&quot;: &quot;ANPAI7XDIQOLTBUMDWGJW&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;DefaultVersionId&quot;: &quot;v1&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Path&quot;: &quot;/&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;Arn&quot;: &quot;arn:aws:iam::123455678901:policy/test-policy&quot;,&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &quot;UpdateDate&quot;: &quot;2017-07-14T22:51:20.293Z&quot;
&nbsp; &nbsp; }
}</code> <p>The following is an example of the event-policy.json&nbsp;file used earlier:</p> <code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ecs:RunTask&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:ecs:*::task-definition/&quot;
],
&quot;Condition&quot;: {
&quot;ArnLike&quot;: {
&quot;ecs:cluster&quot;: &quot;arn:aws:ecs:*::cluster/&quot;
}
}
}
]
}</code> </li> 
<li>Attach the IAM policy to the role.<br /> <code>aws --profile default --region us-west-2 iam attach-role-policy --role-name Test-Role --policy-arn arn:aws:iam::1234567890:policy/test-policy</code></li> 
</ol> </li> 
<li>Associate the CloudWatch rule created earlier to place the task on the ECS cluster. The following command shows an example. Replace the AWS account ID and region with your settings.<br /> <code>aws events put-targets --rule demo-ecs-task-rule --targets &quot;Id&quot;=&quot;1&quot;,&quot;Arn&quot;=&quot;arn:aws:ecs:us-west-2:12345678901:cluster/test-cwe-blog-ecsCluster-15HJFWCH4SP67&quot;,&quot;EcsParameters&quot;={&quot;TaskDefinitionArn&quot;=&quot;arn:aws:ecs:us-west-2:12345678901:task-definition/test-cwe-blog-taskdef:8&quot;},&quot;RoleArn&quot;=&quot;arn:aws:iam::12345678901:role/Test-Role&quot;</code><p></p> <code class="lang-bash">{
&nbsp; &nbsp; &quot;FailedEntries&quot;: [],&nbsp;
&nbsp; &nbsp; &quot;FailedEntryCount&quot;: 0
}</code> </li> 
</ol> 
<p>That’s it. The logs now run based on the defined schedule.</p> 
<p>To test this, open the Amazon ECS console, select the Amazon ECS cluster that you created, and then choose <b>Tasks, Run New Task</b>. Select the task definition created by the CloudFormation template, and the cluster should be selected automatically. As this runs, the S3 bucket should be populated with the RDS logs for the instance.</p> 
<b>Conclusion</b> 
<p>In this post, you’ve seen that the choices for workloads that need to run at a scheduled time include Lambda with CloudWatch events or EC2 with cron. However, sometimes the job could run outside of Lambda execution time limits or be not cost-effective for an EC2 instance.</p> 
<p>In such cases, you can schedule the tasks on an ECS cluster using CloudWatch rules. In addition, you can use a Spot Fleet cluster with Amazon ECS for cost-conscious workloads that do not have hard requirements on execution time or instance availability in the Spot Fleet. For more information, see <a href="https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/">Powering your Amazon ECS Cluster with Amazon EC2 Spot Instances and Scheduled Events</a>.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<p>— Madhuri</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2947');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using Enhanced Request Authorizers in Amazon API Gateway</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Stefano Buliani</span></span> | on 
<time property="datePublished" datetime="2017-09-27T10:05:56+00:00">27 SEP 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/application-services/amazon-api-gateway-application-services/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/security-identity-compliance/" title="View all posts in Security, Identity, &amp; Compliance*"><span property="articleSection">Security, Identity, &amp; Compliance*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/using-enhanced-request-authorizers-in-amazon-api-gateway/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2917" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2917&amp;disqus_title=Using+Enhanced+Request+Authorizers+in+Amazon+API+Gateway&amp;disqus_url=https://aws.amazon.com/blogs/compute/using-enhanced-request-authorizers-in-amazon-api-gateway/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2917');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Recently, AWS introduced a new type of authorizer in <a href="https://aws.amazon.com/api-gateway/" target="_blank" rel="noopener noreferrer">Amazon API Gateway</a>, <a href="https://aws.amazon.com/about-aws/whats-new/2017/09/amazon-api-gateway-now-supports-enhanced-request-authorizers/" target="_blank" rel="noopener noreferrer">enhanced request authorizers</a>. Previously, custom authorizers received only the bearer token included in the request and the ARN of the API Gateway method being called. Enhanced request authorizers receive all of the headers, query string, and path parameters as well as the request context. This enables you to make more sophisticated authorization decisions based on parameters such as the client IP address, user agent, or a query string parameter alongside the client bearer token.</p> 
<h3>Enhanced request authorizer configuration</h3> 
<p>From the API Gateway console, you can declare a new enhanced request authorizer by selecting the <strong>Request</strong> option as the AWS Lambda event payload:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-10.13.18-AM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-10.13.18-AM-215x300.png" /></a></p> 
<p>&nbsp;</p> 
<p>Just like normal custom authorizers, API Gateway can cache the policy returned by your Lambda function. With enhanced request authorizers, however, you can also specify the values that form the unique key of a policy in the cache. For example, if your authorization decision is based on both the bearer token and the IP address of the client, both values should be part of the unique key in the policy cache. The identity source parameter lets you specify these values as <a href="http://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html" target="_blank" rel="noopener noreferrer">mapping expressions</a>:</p> 
<li>The bearer token appears in the Authorization header</li> 
<li>The client IP address is stored in the sourceIp parameter of the request context.</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-1.54.04-PM.png"><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/09/27/Screen-Shot-2017-09-13-at-1.54.04-PM-300x163.png" /></a></p> 
<p>&nbsp;</p> 
<h3>Using enhanced request authorizers with Swagger</h3> 
<p>You can also define enhanced request authorizers in your Swagger (<a href="https://www.openapis.org" target="_blank" rel="noopener noreferrer">Open API</a>) definitions. In the following example, you can see that all of the options configured in the API Gateway console are available as custom extensions in the API definition. For example, the <i>identitySource</i> field is a comma-separated list of mapping expressions.</p> 
<code class="lang-yaml">securityDefinitions:
IpAuthorizer:
type: &quot;apiKey&quot;
name: &quot;IpAuthorizer&quot;
in: &quot;header&quot;
x-amazon-apigateway-authtype: &quot;custom&quot;
x-amazon-apigateway-authorizer:
authorizerResultTtlInSeconds: 300
identitySource: &quot;method.request.header.Authorization, context.identity.sourceIp&quot;
authorizerUri: &quot;arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:XXXXXXXXXX:function:py-ip-authorizer/invocations&quot;
type: &quot;request&quot;
</code> 
<p>After you have declared your authorizer in the security definitions section, you can use it in your API methods:</p> 
<code class="lang-yaml">---
swagger: &quot;2.0&quot;
info:
title: &quot;request-authorizer-demo&quot;
basePath: &quot;/dev&quot;
paths:
/hello:
get:
security:
- IpAuthorizer: []
...
</code> 
<h3>Enhanced request authorizer Lambda functions</h3> 
<p>Enhanced request authorizer <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">Lambda</a> functions receive an event object that is similar to proxy integrations. It contains all of the information about a request, excluding the body.</p> 
<code class="lang-json">{
&quot;methodArn&quot;: &quot;arn:aws:execute-api:us-east-1:XXXXXXXXXX:xxxxxx/dev/GET/hello&quot;,
&quot;resource&quot;: &quot;/hello&quot;,
&quot;requestContext&quot;: {
&quot;resourceId&quot;: &quot;xxxx&quot;,
&quot;apiId&quot;: &quot;xxxxxxxxx&quot;,
&quot;resourcePath&quot;: &quot;/hello&quot;,
&quot;httpMethod&quot;: &quot;GET&quot;,
&quot;requestId&quot;: &quot;9e04ff18-98a6-11e7-9311-ef19ba18fc8a&quot;,
&quot;path&quot;: &quot;/dev/hello&quot;,
&quot;accountId&quot;: &quot;XXXXXXXXXXX&quot;,
&quot;identity&quot;: {
&quot;apiKey&quot;: &quot;&quot;,
&quot;sourceIp&quot;: &quot;58.240.196.186&quot;
},
&quot;stage&quot;: &quot;dev&quot;
},
&quot;queryStringParameters&quot;: {},
&quot;httpMethod&quot;: &quot;GET&quot;,
&quot;pathParameters&quot;: {},
&quot;headers&quot;: {
&quot;cache-control&quot;: &quot;no-cache&quot;,
&quot;x-amzn-ssl-client-hello&quot;: &quot;AQACJAMDAAAAAAAAAAAAAAAAAAAAAAAAAAAA…&quot;,
&quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,
&quot;X-Forwarded-For&quot;: &quot;54.240.196.186, 54.182.214.90&quot;,
&quot;Accept&quot;: &quot;*/*&quot;,
&quot;User-Agent&quot;: &quot;PostmanRuntime/6.2.5&quot;,
&quot;Authorization&quot;: &quot;hello&quot;
},
&quot;stageVariables&quot;: {},
&quot;path&quot;: &quot;/hello&quot;,
&quot;type&quot;: &quot;REQUEST&quot;
}
</code> 
<p>The following enhanced request authorizer snippet is written in Python and compares the source IP address against a list of valid IP addresses. The comments in the code explain what happens in each step.</p> 
<code class="lang-python">...
VALID_IPS = [&quot;58.240.195.186&quot;, &quot;201.246.162.38&quot;]
def lambda_handler(event, context):
# Read the client’s bearer token.
jwtToken = event[&quot;headers&quot;][&quot;Authorization&quot;]
# Read the source IP address for the request form 
# for the API Gateway context object.
clientIp = event[&quot;requestContext&quot;][&quot;identity&quot;][&quot;sourceIp&quot;]
# Verify that the client IP address is allowed.
# If it’s not valid, raise an exception to make sure
# that API Gateway returns a 401 status code.
if clientIp not in VALID_IPS:
raise Exception('Unauthorized')
# Only allow hello users in!
if not validate_jwt(userId):
raise Exception('Unauthorized')
# Use the values from the event object to populate the 
# required parameters in the policy object.
policy = AuthPolicy(userId, event[&quot;requestContext&quot;][&quot;accountId&quot;])
policy.restApiId = event[&quot;requestContext&quot;][&quot;apiId&quot;]
policy.region = event[&quot;methodArn&quot;].split(&quot;:&quot;)[3]
policy.stage = event[&quot;requestContext&quot;][&quot;stage&quot;]
# Use the scopes from the bearer token to make a 
# decision on which methods to allow in the API.
policy.allowMethod(HttpVerb.GET, '/hello')
# Finally, build the policy.
authResponse = policy.build()
return authResponse
...
</code> 
<h3>Conclusion</h3> 
<p>API Gateway customers build complex APIs, and authorization decisions often go beyond the simple properties in a JWT token. For example, users may be allowed to call the “list cars” endpoint but only with a specific subset of filter parameters. With enhanced request authorizers, you have access to all request parameters. You can centralize all of your application’s access control decisions in a Lambda function, making it easier to manage your application security.</p> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2917');
});
</script> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
