<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/computeblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Compute Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li class="active"><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="computeblogs1.html">Page 1</a>|<a href="computeblogs2.html">Page 2</a>|<a href="computeblogs3.html">Page 3</a>|<a href="computeblogs4.html">Page 4</a</p>
<br>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/nginx_reverse_proxy-1133x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Deploying an NGINX Reverse Proxy Sidecar Container on Amazon ECS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Peck</span></span> | on 
<time property="datePublished" datetime="2017-08-03T14:54:02+00:00">03 AUG 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/nginx-reverse-proxy-sidecar-container-on-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2525" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2525&amp;disqus_title=Deploying+an+NGINX+Reverse+Proxy+Sidecar+Container+on+Amazon+ECS&amp;disqus_url=https://aws.amazon.com/blogs/compute/nginx-reverse-proxy-sidecar-container-on-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2525');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Reverse proxies are a powerful software architecture primitive for fetching resources from a server on behalf of a client. They serve a number of purposes, from protecting servers from unwanted traffic to offloading some of the heavy lifting of HTTP traffic processing.</p> 
<p>This post explains the benefits of a reverse proxy, and explains how to use <a href="https://www.nginx.com/resources/wiki/">NGINX</a> and <a href="https://aws.amazon.com/ecs/">Amazon EC2 Container Service (Amazon ECS)</a> to easily implement and deploy a reverse proxy for your containerized application.</p> 
<b>Components</b> 
<p>NGINX is a high performance HTTP server that has achieved significant adoption because of its asynchronous event driven architecture. It can serve thousands of concurrent requests with a low memory footprint. This efficiency also makes it ideal as a reverse proxy.</p> 
<p>Amazon ECS is a highly scalable, high performance container management service that supports Docker containers. It allows you to run applications easily on a managed cluster of Amazon EC2 instances. Amazon ECS helps you get your application components running on instances according to a specified configuration. It also helps scale out these components across an entire fleet of instances.</p> 
<p>Sidecar containers are a common software pattern that has been embraced by engineering organizations. It’s a way to keep server side architecture easier to understand by building with smaller, modular containers that each serve a simple purpose. Just like an application can be powered by multiple microservices, each microservice can also be powered by multiple containers that work together. A sidecar container is simply a way to move part of the core responsibility of a service out into a containerized module that is deployed alongside a core application container.<span id="more-2525"></span></p> 
<p>The following diagram shows how an NGINX reverse proxy sidecar container operates alongside an application server container:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/nginx_reverse_proxy.png"><img class="aligncenter wp-image-2526 size-large" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/nginx_reverse_proxy-1024x570.png" alt="" width="640" height="356" /></a></p> 
<p>In this architecture, Amazon ECS has deployed two copies of an application stack that is made up of an NGINX reverse proxy side container and an application container. Web traffic from the public goes to an Application Load Balancer, which then distributes the traffic to one of the NGINX reverse proxy sidecars. The NGINX reverse proxy then forwards the request to the application server and returns its response to the client via the load balancer.</p> 
<b>Reverse proxy for security</b> 
<p>Security is one reason for using a reverse proxy in front of an application container. Any web server that serves resources to the public can expect to receive lots of unwanted traffic every day. Some of this traffic is relatively benign scans by researchers and tools, such as Shodan or nmap:</p> 
<pre><code class="lang-bash">[18/May/2017:15:10:10 +0000] &quot;GET /YesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScanningForResearchPurposePleaseHaveALookAtTheUserAgentTHXYesThisIsAReallyLongRequestURLbutWeAreDoingItOnPurposeWeAreScann HTTP/1.1&quot; 404 1389 - Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36
[18/May/2017:18:19:51 +0000] &quot;GET /clientaccesspolicy.xml HTTP/1.1&quot; 404 322 - Cloud mapping experiment. Contact research@pdrlabs.net</code></pre> 
<p>But other traffic is much more malicious. For example, here is what a web server sees while being scanned by the hacking tool <a href="https://en.wikipedia.org/wiki/ZmEu_(vulnerability_scanner)">ZmEu</a>, which scans web servers trying to find PHPMyAdmin installations to exploit:</p> 
<pre><code class="lang-bash">[18/May/2017:16:27:39 +0000] &quot;GET /mysqladmin/scripts/setup.php HTTP/1.1&quot; 404 391 - ZmEu
[18/May/2017:16:27:39 +0000] &quot;GET /web/phpMyAdmin/scripts/setup.php HTTP/1.1&quot; 404 394 - ZmEu
[18/May/2017:16:27:39 +0000] &quot;GET /xampp/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 396 - ZmEu
[18/May/2017:16:27:40 +0000] &quot;GET /apache-default/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 405 - ZmEu
[18/May/2017:16:27:40 +0000] &quot;GET /phpMyAdmin-2.10.0.0/scripts/setup.php HTTP/1.1&quot; 404 397 - ZmEu
[18/May/2017:16:27:40 +0000] &quot;GET /mysql/scripts/setup.php HTTP/1.1&quot; 404 386 - ZmEu
[18/May/2017:16:27:41 +0000] &quot;GET /admin/scripts/setup.php HTTP/1.1&quot; 404 386 - ZmEu
[18/May/2017:16:27:41 +0000] &quot;GET /forum/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 396 - ZmEu
[18/May/2017:16:27:41 +0000] &quot;GET /typo3/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 396 - ZmEu
[18/May/2017:16:27:42 +0000] &quot;GET /phpMyAdmin-2.10.0.1/scripts/setup.php HTTP/1.1&quot; 404 399 - ZmEu
[18/May/2017:16:27:44 +0000] &quot;GET /administrator/components/com_joommyadmin/phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 418 - ZmEu
[18/May/2017:18:34:45 +0000] &quot;GET /phpmyadmin/scripts/setup.php HTTP/1.1&quot; 404 390 - ZmEu
[18/May/2017:16:27:45 +0000] &quot;GET /w00tw00t.at.blackhats.romanian.anti-sec:) HTTP/1.1&quot; 404 401 - ZmEu</code></pre> 
<p>In addition, servers can also end up receiving unwanted web traffic that is intended for another server. In a cloud environment, an application may end up reusing an IP address that was formerly connected to another service. It’s common for misconfigured or misbehaving DNS servers to send traffic intended for a different host to an IP address now connected to your server.</p> 
<p>It’s the responsibility of anyone running a web server to handle and reject potentially malicious traffic or unwanted traffic. Ideally, the web server can reject this traffic as early as possible, before it actually reaches the core application code. A reverse proxy is one way to provide this layer of protection for an application server. It can be configured to reject these requests before they reach the application server.</p> 
<b>Reverse proxy for performance</b> 
<p>Another advantage of using a reverse proxy such as NGINX is that it can be configured to offload some heavy lifting from your application container. For example, every HTTP server should support gzip. Whenever a client requests gzip encoding, the server compresses the response before sending it back to the client. This compression saves network bandwidth, which also improves speed for clients who now don’t have to wait as long for a response to fully download.</p> 
<p>NGINX can be configured to accept a plaintext response from your application container and gzip encode it before sending it down to the client. This allows your application container to focus 100% of its CPU allotment on running business logic, while NGINX handles the encoding with its efficient gzip implementation.</p> 
<p>An application may have security concerns that require SSL termination at the instance level instead of at the load balancer. NGINX can also be configured to terminate SSL before proxying the request to a local application container. Again, this also removes some CPU load from the application container, allowing it to focus on running business logic. It also gives you a cleaner way to patch any SSL vulnerabilities or update SSL certificates by updating the NGINX container without needing to change the application container.</p> 
<b>NGINX configuration</b> 
<p>Configuring NGINX for both traffic filtering and gzip encoding is shown below:</p> 
<pre><code class="lang-bash">http {
&nbsp; # NGINX will handle gzip compression of responses from the app server
&nbsp; gzip on;
&nbsp; gzip_proxied any;
&nbsp; gzip_types text/plain application/json;
&nbsp; gzip_min_length 1000;
&nbsp;
&nbsp; server {
&nbsp;&nbsp;&nbsp; listen 80;
&nbsp;
&nbsp;&nbsp;&nbsp; # NGINX will reject anything not matching /api
&nbsp;&nbsp;&nbsp; location /api {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Reject requests with unsupported HTTP method
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if ($request_method !~ ^(GET|POST|HEAD|OPTIONS|PUT|DELETE)$) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return 405;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Only requests matching the whitelist expectations will
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # get sent to the application server
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_pass http://app:3000;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_http_version 1.1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header Upgrade $http_upgrade;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header Connection 'upgrade';
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header Host $host;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; proxy_cache_bypass $http_upgrade;
&nbsp;&nbsp;&nbsp; }
&nbsp; }
}</code></pre> 
<p>The above configuration only accepts traffic that matches the expression /api and has a recognized HTTP method. If the traffic matches, it is forwarded to a local application container accessible at the local hostname app. If the client requested gzip encoding, the plaintext response from that application container is gzip-encoded.</p> 
<b>Amazon ECS configuration</b> 
<p>Configuring ECS to run this NGINX container as a sidecar is also simple. ECS uses a core primitive called the task definition. Each task definition can include one or more containers, which can be linked to each other:</p> 
<pre><code class="lang-json">&nbsp;{
&nbsp;&nbsp;&quot;containerDefinitions&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;name&quot;: &quot;nginx&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;image&quot;: &quot;&lt;NGINX reverse proxy image URL here&gt;&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;memory&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;cpu&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;essential&quot;: true,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;portMappings&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;containerPort&quot;: &quot;80&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;protocol&quot;: &quot;tcp&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;links&quot;: [
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;app&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]
&nbsp;&nbsp;&nbsp;&nbsp; },
&nbsp;&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;name&quot;: &quot;app&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;image&quot;: &quot;&lt;app image URL here&gt;&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;memory&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;cpu&quot;: &quot;256&quot;,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &quot;essential&quot;: true
&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp; ],
&nbsp;&nbsp; &quot;networkMode&quot;: &quot;bridge&quot;,
&nbsp;&nbsp; &quot;family&quot;: &quot;application-stack&quot;
}
</code></pre> 
<p>This task definition causes ECS to start both an NGINX container and an application container on the same instance. Then, the NGINX container is linked to the application container. This allows the NGINX container to send traffic to the application container using the hostname app.</p> 
<p>The NGINX container has a port mapping that exposes port 80 on a publically accessible port but the application container does not. This means that the application container is not directly addressable. The only way to send it traffic is to send traffic to the NGINX container, which filters that traffic down. It only forwards to the application container if the traffic passes the whitelisted rules.</p> 
<b>Conclusion</b> 
<p>Running a sidecar container such as NGINX can bring significant benefits by making it easier to provide protection for application containers. Sidecar containers also improve performance by freeing your application container from various CPU intensive tasks. Amazon ECS makes it easy to run sidecar containers, and automate their deployment across your cluster.</p> 
<p>To see the full code for this NGINX sidecar reference, or to try it out yourself, you can check out the open source <a href="https://github.com/awslabs/ecs-nginx-reverse-proxy/tree/master/reverse-proxy">NGINX reverse proxy reference architecture on GitHub</a>.</p> 
<p>– Nathan<br /> <a href="https://twitter.com/nathankpeck" target="_blank"><img class="alignnone wp-image-2597 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/08/03/twitter_16.png" alt="" width="16" height="16" /></a>&nbsp;<a href="https://twitter.com/nathankpeck" target="_blank" rel="noopener noreferrer">@nathankpeck</a></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/nginx/" rel="tag">nginx</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2525');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Deploying Java Microservices on Amazon EC2 Container Service</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-07-11T09:12:17+00:00">11 JUL 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/deploying-java-microservices-on-amazon-ec2-container-service/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2489" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2489&amp;disqus_title=Deploying+Java+Microservices+on+Amazon+EC2+Container+Service&amp;disqus_url=https://aws.amazon.com/blogs/compute/deploying-java-microservices-on-amazon-ec2-container-service/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2489');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post and accompanying code graciously contributed by:</p> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><img class="aligncenter wp-image-2449 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/huynhz.jpeg" alt="" width="120" height="160" /></td> 
<td style="padding: 0px 30px 0px 0px"><img class="aligncenter wp-image-2466 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/magnusb.jpeg" alt="" width="119" height="160" /></td> 
</tr> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><b>Huy Huynh</b><br /> Sr. Solutions Architect</td> 
<td style="padding: 0px 30px 0px 0px"><b>Magnus Bjorkman</b><br /> Solutions Architect</td> 
</tr> 
</tbody> 
</table> 
<p>Java is a popular language used by many enterprises today. To simplify and accelerate Java application development, many companies are moving from a monolithic to microservices architecture. For some, it has become a strategic imperative. <a href="https://aws.amazon.com/containers/">Containerization</a> technology, such as <a href="https://aws.amazon.com/docker/">Docker</a>, lets enterprises build scalable, robust microservice architectures without major code rewrites.</p> 
<p>In this post, I cover how to containerize a monolithic Java application to run on Docker. Then, I show how to deploy it on AWS using Amazon EC2 Container Service (<a href="https://aws.amazon.com/ecs/">Amazon ECS</a>), a high-performance container management service. Finally, I show how to break the monolith into multiple services, all running in containers on Amazon ECS.</p> 
<p><span id="more-2489"></span></p> 
<b>Application Architecture</b> 
<p>For this example, I use the <a href="https://github.com/spring-projects/spring-petclinic">Spring Pet Clinic</a>, a monolithic Java application for managing a veterinary practice. It is a simple REST&nbsp;API, which allows the client to manage and view Owners, Pets, Vets, and Visits.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_1.png"><img class="aligncenter wp-image-2498 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_1.png" alt="" width="975" height="394" /></a></p> 
<p>It is a simple three-tier architecture:</p> 
<li><strong>Client</strong><br /> You simulate this by using curl commands.</li> 
<li><strong>Web/app server</strong><br /> This is the Java and Spring-based application that you run using the embedded Tomcat. As part of this post, you run this within Docker containers.</li> 
<li><strong>Database server</strong><br /> This is the relational database for your application that stores information about owners, pets, vets, and visits. For this post, use MySQL RDS.</li> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_2.png"><img class="aligncenter size-medium wp-image-2499" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/PetClinicApp_2-288x300.png" alt="" width="288" height="300" /></a></p> 
<p>I decided to not put the database inside a container as containers were designed for applications and are transient in nature. The choice was made even easier because you have a fully managed database service available with <a href="https://aws.amazon.com/rds">Amazon RDS</a>.</p> 
<p>RDS manages the work involved in setting up a relational database, from provisioning the infrastructure capacity that you request to installing the database software. After your database is up and running, RDS automates common administrative tasks, such as performing backups and patching the software that powers your database. With optional <a href="https://aws.amazon.com/rds/faqs/#36">Multi-AZ deployments</a>, Amazon RDS also manages synchronous data replication across Availability Zones with automatic failover.</p> 
<b>Walkthrough</b> 
<p>You can find the code for the example covered in this post at <a href="https://github.com/awslabs/amazon-ecs-java-microservices/">amazon-ecs-java-microservices</a> on GitHub.</p> 
<b>Prerequisites</b> 
<p>You need the following to walk through this solution:</p> 
<li>An AWS account</li> 
<li>An access key and secret key for a user in the account</li> 
<li>The AWS CLI installed</li> 
<p>Also, install the latest versions of the following:</p> 
<li>Java</li> 
<li>Maven</li> 
<li>Python</li> 
<li>Docker</li> 
<b>Step 1: Move the existing Java Spring application to a container deployed using Amazon ECS</b> 
<p>First, move the existing monolith application to a container and deploy it using Amazon ECS. This is a great first step before breaking the monolith apart because you still get some benefits before breaking apart the monolith:</p> 
<li>An improved pipeline. The container also allows an engineering organization to create a standard pipeline for the application lifecycle.</li> 
<li>No mutations to machines.</li> 
<p>You can find the monolith example at <a href="https://github.com/awslabs/amazon-ecs-java-microservices/tree/master/1_ECS_Java_Spring_PetClinic">1_ECS_Java_Spring_PetClinic</a>.</p> 
<h3>Container deployment overview</h3> 
<p>The following diagram is an overview of what the setup looks like for Amazon ECS and related services:</p> 
<p><img class="aligncenter wp-image-2497 size-medium" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/ecs-spring-monolithic-containers-1.png" alt="" width="262" height="300" /></p> 
<p><strong>This setup consists of the following resources:</strong></p> 
<li>The client application that makes a request to the load balancer.</li> 
<li>The load balancer that distributes requests across all available ports and instances registered in the application’s target group using round-robin.</li> 
<li>The target group that is updated by Amazon ECS to always have an up-to-date list of all the service containers in the cluster. This includes the port on which they are accessible.</li> 
<li>One Amazon ECS cluster that hosts the container for the application.</li> 
<li>A VPC network to host the Amazon ECS cluster and associated security groups.</li> 
<p>Each container has a single application process that is bound to port 8080 within its namespace. In reality, all the containers are exposed on a different, randomly assigned port on the host.</p> 
<p>The architecture is containerized but still monolithic because each container has all the same features of the rest of the containers</p> 
<p><strong>The following is also part of the solution but not depicted in the above diagram:</strong></p> 
<li>One <a href="https://aws.amazon.com/ecr/">Amazon EC2 Container Registry (Amazon ECR)</a>&nbsp;repository for the application.</li> 
<li>A service/task definition that spins up containers on the instances of the Amazon ECS cluster.</li> 
<li>A MySQL RDS instance that hosts the applications schema. The information about the MySQL RDS instance is sent in through environment variables to the containers, so that the application can connect to the MySQL RDS instance.</li> 
<p>I have automated setup with the <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/ecs-cluster.cf">1_ECS_Java_Spring_PetClinic/ecs-cluster.cf</a> AWS CloudFormation template and a <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/setup.py">Python script</a>.</p> 
<p>The Python script calls the CloudFormation template for the initial setup of the VPC, Amazon ECS cluster, and RDS instance. It then extracts the outputs from the template and uses those for API calls to create Amazon ECR repositories, tasks, services, Application Load Balancer, and target groups.</p> 
<h3>Environment variables and Spring properties binding</h3> 
<p>As part of the Python script, you pass in a number of environment variables to the container as part of the task/container definition:</p> 
<pre><code class="lang-json">'environment': [
{
'name': 'SPRING_PROFILES_ACTIVE',
'value': 'mysql'
},
{
'name': 'SPRING_DATASOURCE_URL',
'value': my_sql_options['dns_name']
},
{
'name': 'SPRING_DATASOURCE_USERNAME',
'value': my_sql_options['username']
},
{
'name': 'SPRING_DATASOURCE_PASSWORD',
'value': my_sql_options['password']
}
],</code></pre> 
<p>The preceding environment variables work in concert with the Spring property system. The value in the variable SPRING_PROFILES_ACTIVE, makes Spring use the MySQL version of the <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/src/main/resources/application-mysql.properties">application property file</a>. The other environment files override the following properties in that file:</p> 
<li><code class="lang-sql">spring.datasource.url</code></li> 
<li><code class="lang-sql">spring.datasource.username</code></li> 
<li><code class="lang-sql">spring.datasource.password</code></li> 
<p>Optionally, you can also encrypt sensitive values by using <a href="https://aws.amazon.com/ec2/systems-manager/parameter-store/">Amazon EC2 Systems Manager Parameter Store. Instead of handing in the password, you pass in a reference to the parameter and fetch the value as part of the container startup. For more information, see </a><a href="https://aws.amazon.com/blogs/compute/managing-secrets-for-amazon-ecs-applications-using-parameter-store-and-iam-roles-for-tasks/">Managing Secrets for Amazon ECS Applications Using Parameter Store and IAM Roles for Tasks</a><a href="https://aws.amazon.com/ec2/systems-manager/parameter-store/">.</a></p> 
<h3>Spotify Docker Maven plugin</h3> 
<p>Use the <a href="https://github.com/spotify/docker-maven-plugin">Spotify Docker Maven plugin</a> to create the image and push it directly to Amazon ECR. This allows you to do this as part of the regular Maven build. It also integrates the image generation as part of the overall build process. Use an explicit <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/1_ECS_Java_Spring_PetClinic/src/main/docker/Dockerfile">Dockerfile</a> as input to the plugin.</p> 
<pre><code class="lang-sql">FROM frolvlad/alpine-oraclejdk8:slim
VOLUME /tmp
ADD spring-petclinic-rest-1.7.jar app.jar
RUN sh -c 'touch /app.jar'
ENV JAVA_OPTS=&quot;&quot;
ENTRYPOINT [ &quot;sh&quot;, &quot;-c&quot;, &quot;java $JAVA_OPTS -Djava.security.egd=file:/dev/./urandom -jar /app.jar&quot; ]</code></pre> 
<p>The Python script discussed earlier uses the <a href="https://aws.amazon.com/cli">AWS CLI</a> to authenticate you with AWS. The script places the token in the appropriate location so that the plugin can work directly against the Amazon ECR repository.</p> 
<h3>Test setup</h3> 
<p>You can test the setup by running the Python script:<br /> <code class="lang-python">python setup.py -m setup -r &lt;your region&gt;</code></p> 
<p>After the script has successfully run, you can test by querying an endpoint:<br /> <code class="lang-python">curl &lt;your endpoint from output above&gt;/owner</code></p> 
<p>You can clean this up before going to the next section:<br /> <code class="lang-python">python setup.py -m cleanup -r &lt;your region&gt;</code></p> 
<b>Step 2: Converting the monolith into microservices running on Amazon ECS</b> 
<p>The second step is to convert the monolith into microservices. For a real application, you would likely not do this as one step, but re-architect an application piece by piece. You would continue to run your monolith but it would keep getting smaller for each piece that you are breaking apart.</p> 
<p>By migrating microservices, you would get four benefits associated with microservices:</p> 
<li><b>Isolation of crashes</b><br /> If one microservice in your application is crashing, then only that part of your application goes down. The rest of your application continues to work properly.</li> 
<li><b>Isolation of security</b><br /> When microservice best practices are followed, the result is that if an attacker compromises one service, they only gain access to the resources of that service. They can’t horizontally access other resources from other services without breaking into those services as well.</li> 
<li><b>Independent scaling</b><br /> When features are broken out into microservices, then the amount of infrastructure and number of instances of each microservice class can be scaled up and down independently.</li> 
<li><b>Development velocity</b><br /> In a monolith, adding a new feature can potentially impact every other feature that the monolith contains. On the other hand, a proper microservice architecture has new code for a new feature going into a new service. You can be confident that any code you write won’t impact the existing code at all, unless you explicitly write a connection between two microservices.</li> 
<p>Find the monolith example at <a href="https://github.com/awslabs/amazon-ecs-java-microservices/tree/master/2_ECS_Java_Spring_PetClinic_Microservices">2_ECS_Java_Spring_PetClinic_Microservices</a>.<br /> You break apart the Spring Pet Clinic application by creating a microservice for each REST&nbsp;API operation, as well as creating one for the system services.</p> 
<h3>Java code changes</h3> 
<p>Comparing the project structure between the monolith and the microservices version, you can see that each service is now its own separate build.<br /> <strong>First, the monolith version:</strong><br /> <img class="aligncenter size-full wp-image-2500" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/PetClinicApp_3.png" alt="" width="379" height="377" /><br /> You can clearly see how each API operation is its own subpackage under the <i>org.springframework.samples.petclinic</i> package, all part of the same monolithic application.<br /> <strong>This changes as you break it apart in the microservices version:</strong><br /> <img class="aligncenter size-full wp-image-2501" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/11/PetClinicApp_4.png" alt="" width="392" height="477" /><br /> Now, each API operation is its own separate build, which you can build independently and deploy. You have also duplicated some code across the different microservices, such as the classes under the model subpackage. This is intentional as you don’t want to introduce artificial dependencies among the microservices and allow these to evolve differently for each microservice.</p> 
<p>Also, make the dependencies among the API operations more loosely coupled. In the monolithic version, the components are tightly coupled and use object-based invocation.</p> 
<p>Here is an example of this from the OwnerController operation, where the class is directly calling PetRepository to get information about pets. PetRepository is the Repository class (Spring data access layer) to the Pet table in the RDS instance for the Pet API:</p> 
<pre><code class="lang-java">@RestController
class OwnerController {
@Inject
private PetRepository pets;
@Inject
private OwnerRepository owners;
private static final Logger logger = LoggerFactory.getLogger(OwnerController.class);
@RequestMapping(value = &quot;/owner/{ownerId}/getVisits&quot;, method = RequestMethod.GET)
public ResponseEntity&lt;List&lt;Visit&gt;&gt; getOwnerVisits(@PathVariable int ownerId){
List&lt;Pet&gt; petList = this.owners.findById(ownerId).getPets();
List&lt;Visit&gt; visitList = new ArrayList&lt;Visit&gt;();
petList.forEach(pet -&gt; visitList.addAll(pet.getVisits()));
return new ResponseEntity&lt;List&lt;Visit&gt;&gt;(visitList, HttpStatus.OK);
}
}</code></pre> 
<p>In the microservice version, call the Pet API operation and not PetRepository directly. Decouple the components by using interprocess communication; in this case, the Rest API. This provides for fault tolerance and disposability.</p> 
<pre><code class="lang-java">@RestController
class OwnerController {
@Value(&quot;#{environment['SERVICE_ENDPOINT'] ?: 'localhost:8080'}&quot;)
private String serviceEndpoint;
@Inject
private OwnerRepository owners;
private static final Logger logger = LoggerFactory.getLogger(OwnerController.class);
@RequestMapping(value = &quot;/owner/{ownerId}/getVisits&quot;, method = RequestMethod.GET)
public ResponseEntity&lt;List&lt;Visit&gt;&gt; getOwnerVisits(@PathVariable int ownerId){
List&lt;Pet&gt; petList = this.owners.findById(ownerId).getPets();
List&lt;Visit&gt; visitList = new ArrayList&lt;Visit&gt;();
petList.forEach(pet -&gt; {
logger.info(getPetVisits(pet.getId()).toString());
visitList.addAll(getPetVisits(pet.getId()));
});
return new ResponseEntity&lt;List&lt;Visit&gt;&gt;(visitList, HttpStatus.OK);
}
private List&lt;Visit&gt; getPetVisits(int petId){
List&lt;Visit&gt; visitList = new ArrayList&lt;Visit&gt;();
RestTemplate restTemplate = new RestTemplate();
Pet pet = restTemplate.getForObject(&quot;http://&quot;+serviceEndpoint+&quot;/pet/&quot;+petId, Pet.class);
logger.info(pet.getVisits().toString());
return pet.getVisits();
}
}</code></pre> 
<p>You now have an additional method that calls the API. You are also handing in the service endpoint that should be called, so that you can easily inject dynamic endpoints based on the current deployment.</p> 
<h3>Container deployment overview</h3> 
<p>Here is an overview of what the setup looks like for Amazon ECS and the related services:<br /> <img class="aligncenter size-large wp-image-2496" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/07/10/ecs-spring-microservice-containers.png" alt="" width="640" height="731" /><br /> <b>This setup consists of the following resources:</b></p> 
<li>The client application that makes a request to the load balancer.</li> 
<li>The Application Load Balancer that inspects the client request. Based on routing rules, it directs the request to an instance and port from the target group that matches the rule.</li> 
<li>The Application Load Balancer that has a target group for each microservice. The target groups are used by the corresponding services to register available container instances. Each target group has a path, so when you call the path for a particular microservice, it is mapped to the correct target group. This allows you to use one Application Load Balancer to serve all the different microservices, accessed by the path. For example, https:///owner/* would be mapped and directed to the Owner microservice.</li> 
<li>One Amazon ECS cluster that hosts the containers for each microservice of the application.</li> 
<li>A VPC network to host the Amazon ECS cluster and associated security groups.</li> 
<p>Because you are running multiple containers on the same instances, use dynamic port mapping to avoid port clashing. By using dynamic port mapping, the container is allocated an anonymous port on the host to which the container port (8080) is mapped. The anonymous port is registered with the Application Load Balancer and target group so that traffic is routed correctly.</p> 
<p><strong>The following is also part of the solution but not depicted in the above diagram:</strong></p> 
<li>One Amazon ECR repository for each microservice.</li> 
<li>A service/task definition per microservice that spins up containers on the instances of the Amazon ECS cluster.</li> 
<li>A MySQL RDS instance that hosts the applications schema. The information about the MySQL RDS instance is sent in through environment variables to the containers. That way, the application can connect to the MySQL RDS instance.</li> 
<p>I have again automated setup with the <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/2_ECS_Java_Spring_PetClinic_Microservices/ecs-cluster.cf">2_ECS_Java_Spring_PetClinic_Microservices/ecs-cluster.cf</a> CloudFormation template and a <a href="https://github.com/awslabs/amazon-ecs-java-microservices/blob/master/2_ECS_Java_Spring_PetClinic_Microservices/setup.py">Python script</a>.</p> 
<p>The CloudFormation template remains the same as in the previous section. In the Python script, you are now building five different Java applications, one for each microservice (also includes a system application). There is a separate Maven POM file for each one. The resulting Docker image gets pushed to its own Amazon ECR repository, and is deployed separately using its own service/task definition. This is critical to get the benefits described earlier for microservices.</p> 
<p>Here is an example of the POM file for the Owner microservice:</p> 
<pre><code class="lang-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt;
&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
&lt;groupId&gt;org.springframework.samples&lt;/groupId&gt;
&lt;artifactId&gt;spring-petclinic-rest&lt;/artifactId&gt;
&lt;version&gt;1.7&lt;/version&gt;
&lt;parent&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
&lt;version&gt;1.5.2.RELEASE&lt;/version&gt;
&lt;/parent&gt;
&lt;properties&gt;
&lt;!-- Generic properties --&gt;
&lt;java.version&gt;1.8&lt;/java.version&gt;
&lt;docker.registry.host&gt;${env.docker_registry_host}&lt;/docker.registry.host&gt;
&lt;/properties&gt;
&lt;dependencies&gt;
&lt;dependency&gt;
&lt;groupId&gt;javax.inject&lt;/groupId&gt;
&lt;artifactId&gt;javax.inject&lt;/artifactId&gt;
&lt;version&gt;1&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- Spring and Spring Boot dependencies --&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-data-rest&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
&lt;!-- Databases - Uses HSQL by default --&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.hsqldb&lt;/groupId&gt;
&lt;artifactId&gt;hsqldb&lt;/artifactId&gt;
&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;mysql&lt;/groupId&gt;
&lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;!-- caching --&gt;
&lt;dependency&gt;
&lt;groupId&gt;javax.cache&lt;/groupId&gt;
&lt;artifactId&gt;cache-api&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.ehcache&lt;/groupId&gt;
&lt;artifactId&gt;ehcache&lt;/artifactId&gt;
&lt;/dependency&gt;
&lt;!-- end of webjars --&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;
&lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;
&lt;plugins&gt;
&lt;plugin&gt;
&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
&lt;/plugin&gt;
&lt;plugin&gt;
&lt;groupId&gt;com.spotify&lt;/groupId&gt;
&lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;
&lt;version&gt;0.4.13&lt;/version&gt;
&lt;configuration&gt;
&lt;imageName&gt;${env.docker_registry_host}/${project.artifactId}&lt;/imageName&gt;
&lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt;
&lt;useConfigFile&gt;true&lt;/useConfigFile&gt;
&lt;registryUrl&gt;${env.docker_registry_host}&lt;/registryUrl&gt;
&lt;!--dockerHost&gt;https://${docker.registry.host}&lt;/dockerHost--&gt;
&lt;resources&gt;
&lt;resource&gt;
&lt;targetPath&gt;/&lt;/targetPath&gt;
&lt;directory&gt;${project.build.directory}&lt;/directory&gt;
&lt;include&gt;${project.build.finalName}.jar&lt;/include&gt;
&lt;/resource&gt;
&lt;/resources&gt;
&lt;forceTags&gt;false&lt;/forceTags&gt;
&lt;imageTags&gt;
&lt;imageTag&gt;${project.version}&lt;/imageTag&gt;
&lt;/imageTags&gt;
&lt;/configuration&gt;
&lt;/plugin&gt;
&lt;/plugins&gt;
&lt;/build&gt;
&lt;/project&gt;</code></pre> 
<h3>Test setup</h3> 
<p>You can test this by running the Python script:</p> 
<p><code class="lang-python">python setup.py -m setup -r &lt;your region&gt;</code></p> 
<p>After the script has successfully run, you can test by querying an endpoint:</p> 
<p><code class="lang-python">curl &lt;your endpoint from output above&gt;/owner</code></p> 
<b>Conclusion</b> 
<p>Migrating a monolithic application to a containerized set of microservices can seem like a daunting task. Following the steps outlined in this post, you can begin to containerize monolithic Java apps, taking advantage of the container runtime environment, and beginning the process of re-architecting into microservices. On the whole, containerized microservices are faster to develop, easier to iterate on, and more cost effective to maintain and secure.</p> 
<p>This post focused on the first steps of microservice migration. You can learn more about optimizing and scaling your microservices with components such as service discovery, blue/green deployment, circuit breakers, and configuration servers at <a href="http://aws.amazon.com/containers">http://aws.amazon.com/containers</a>.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2489');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Manage Kubernetes Clusters on AWS Using Kops</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Arun Gupta</span></span> | on 
<time property="datePublished" datetime="2017-07-06T13:52:28+00:00">06 JUL 2017</time> | 
<a href="https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2475" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2475&amp;disqus_title=Manage+Kubernetes+Clusters+on+AWS+Using+Kops&amp;disqus_url=https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2475');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Any containerized application typically consists of multiple containers. There are containers for the application itself, a database, possibly a web server, and so on. During development, it’s normal to build and test this multi-container application on a single host. This approach works fine during early dev and test cycles but becomes a single point of failure for production, when application availability is critical.</p> 
<p>In such cases, a multi-container application can be deployed on multiple hosts. Customers may need an external tool to manage such multi-container, multi-host deployments. Container orchestration frameworks provides the capability of cluster management, scheduling containers on different hosts, service discovery and load balancing, crash recovery, and other related functionalities. There are multiple options for container orchestration on Amazon Web Services: <a href="https://aws.amazon.com/ecs/">Amazon ECS</a>, <a href="https://docs.docker.com/docker-for-aws/">Docker for AWS</a>, and <a href="https://dcos.io/">DC/OS</a>.</p> 
<p>Another popular option for container orchestration on AWS is <a href="https://kubernetes.io/">Kubernetes</a>. There are multiple ways to run a <a href="https://github.com/kubernetes/community/blob/master/sig-aws/kubernetes-on-aws.md">Kubernetes cluster on AWS</a>. This multi-part blog series provides a brief overview and explains some of these approaches in detail. This first post explains how to create a Kubernetes cluster on AWS using <a href="https://github.com/kubernetes/kops">kops</a>.<span id="more-2475"></span></p> 
<b>Kubernetes and Kops overview</b> 
<p>Kubernetes is an open source, container orchestration platform. Applications packaged as Docker images can be easily deployed, scaled, and managed in a Kubernetes cluster. Some of the key features of Kubernetes are:</p> 
<li><strong>Self-healing</strong><br /> Failed containers are restarted to ensure that the desired state of the application is maintained. If a node in the cluster dies, then the containers are rescheduled on a different node. Containers that do not respond to application-defined health check are terminated, and thus rescheduled.</li> 
<li><strong>Horizontal scaling</strong><br /> Number of containers can be easily scaled up and down automatically based upon CPU utilization, or manually using a command.</li> 
<li><strong>Service discovery and load balancing</strong><br /> Multiple containers can be grouped together discoverable using a DNS name. The service can be load balanced with integration to the native LB provided by the cloud provider.</li> 
<li><strong>Application upgrades and rollbacks</strong><br /> Applications can be upgraded to a newer version without an impact to the existing one. If something goes wrong, Kubernetes rolls back the change.</li> 
<p>Kops, short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud. A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons. After the cluster is created, the usual <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/">kubectl CLI</a> can be used to manage resources in the cluster.</p> 
<b>Download Kops and Kubectl</b> 
<p>There is no need to download the Kubernetes binary distribution for creating a cluster using kops. However, you do need to download the kops CLI. It then takes care of downloading the right Kubernetes binary in the cloud, and provisions the cluster.</p> 
<p>The different download options for kops are explained at <a href="https://github.com/kubernetes/kops#installing">github.com/kubernetes/kops#installing</a>. On MacOS, the easiest way to install kops is using the <a href="https://brew.sh/">brew</a> package manager.</p> 
<pre>brew update &amp;&amp; brew install kops</pre> 
<p>The version of kops can be verified using the kops version command, which shows:</p> 
<pre>Version 1.6.1</pre> 
<p>In addition, download kubectl. This is required to manage the Kubernetes cluster. The latest version of kubectl can be downloaded using the following command:</p> 
<pre>curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl</pre> 
<p>Make sure to include the directory where kubectl is downloaded in your <code>PATH</code>.</p> 
<b>IAM user permission</b> 
<p>The IAM user to create the Kubernetes cluster must have the following permissions:</p> 
<li><code>AmazonEC2FullAccess</code></li> 
<li><code>AmazonRoute53FullAccess</code></li> 
<li><code>AmazonS3FullAccess</code></li> 
<li><code>IAMFullAccess</code></li> 
<li><code>AmazonVPCFullAccess</code></li> 
<p>Alternatively, a new IAM user may be created and the policies attached as explained at <a href="http://github.com/kubernetes/kops/blob/master/docs/aws.md#setup-iam-user">github.com/kubernetes/kops/blob/master/docs/aws.md#setup-iam-user</a>.</p> 
<b>Create an Amazon S3 bucket for the Kubernetes state store</b> 
<p>Kops needs a “state store” to store configuration information of the cluster. &nbsp;For example, how many nodes, instance type of each node, and Kubernetes version. The state is stored during the initial cluster creation. Any subsequent changes to the cluster are also persisted to this store as well. As of publication, Amazon S3 is the only supported storage mechanism. Create a S3 bucket and pass that to the kops CLI during cluster creation.</p> 
<p>This post uses the bucket name <code>kubernetes-aws-io</code>. Bucket names must be unique; you have to use a different name. Create an S3 bucket:</p> 
<pre>aws s3api create-bucket --bucket kubernetes-aws-io</pre> 
<p>I strongly recommend versioning this bucket in case you ever need to revert or recover a previous version of the cluster. This can be enabled using the AWS CLI as well:</p> 
<pre>aws s3api put-bucket-versioning --bucket kubernetes-aws-io --versioning-configuration Status=Enabled</pre> 
<p>For convenience, you can also define <code>KOPS_STATE_STORE</code> environment variable pointing to the S3 bucket. For example:</p> 
<pre>export KOPS_STATE_STORE=s3://kubernetes-aws-io</pre> 
<p>This environment variable is then used by the kops CLI.</p> 
<b>DNS configuration</b> 
<p>As of Kops 1.6.1, a top-level domain or a subdomain is required to create the cluster. This domain allows the worker nodes to discover the master and the master to discover all the etcd servers. This is also needed for kubectl to be able to talk directly with the master.</p> 
<p>This domain may be registered with AWS, in which case a Route 53 hosted zone is created for you. Alternatively, this domain may be at a different registrar. In this case, create a Route 53 hosted zone. Specify the name server (NS) records from the created zone as NS records with the domain registrar.</p> 
<p>This post uses a <code>kubernetes-aws.io</code> domain registered at a third-party registrar.</p> 
<p>Generate a Route 53 hosted zone using the AWS CLI. Download <a href="https://github.com/stedolan/jq/wiki/Installation">jq</a> to run this command:</p> 
<pre>ID=$(uuidgen) &amp;&amp; \
aws route53 create-hosted-zone \
--name cluster.kubernetes-aws.io \
--caller-reference $ID \
| jq .DelegationSet.NameServers</pre> 
<p>This shows an output such as the following:</p> 
<pre>[
&quot;ns-94.awsdns-11.com&quot;,
&quot;ns-1962.awsdns-53.co.uk&quot;,
&quot;ns-838.awsdns-40.net&quot;,
&quot;ns-1107.awsdns-10.org&quot;
]</pre> 
<p>Create NS records for the domain with your registrar. Different options on how to configure DNS for the cluster are explained at <a href="http://github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns">github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns</a>.</p> 
<p>Experimental&nbsp;support to create a gossip-based cluster was added in Kops 1.6.2. This post uses a DNS-based approach, as that is more mature and well tested.</p> 
<b>Create the Kubernetes cluster</b> 
<p>The Kops CLI can be used to create a highly available cluster, with multiple master nodes spread across multiple Availability Zones. Workers can be spread across multiple zones as well. Some of the tasks that happen behind the scene during cluster creation are:</p> 
<li>Provisioning EC2 instances</li> 
<li>Setting up AWS resources such as networks, Auto Scaling groups, IAM users, and security groups</li> 
<li>Installing Kubernetes.</li> 
<p>Start the Kubernetes cluster using the following command:</p> 
<pre>kops create cluster \
--name cluster.kubernetes-aws.io \
--zones us-west-2a \
--state s3://kubernetes-aws-io \
--yes</pre> 
<p>In this command:</p> 
<li><code>--zones</code><br /> Defines the zones in which the cluster is going to be created. Multiple comma-separated zones can be specified to span the cluster across multiple zones.</li> 
<li><code>--name</code><br /> Defines the cluster’s name.</li> 
<li><code>--state</code><br /> Points to the S3 bucket that is the state store.</li> 
<li><code>--yes</code><br /> Immediately creates the cluster. Otherwise, only the cloud resources are created and the cluster needs to be started explicitly using the command <code>kops update --yes</code>. If the cluster needs to be edited, then the <code>kops edit cluster</code> command can be used.</li> 
<p>This starts a single master and two worker node Kubernetes cluster. The master is in an Auto Scaling group and the worker nodes are in a separate group. By default, the master node is <code>m3.medium</code> and the worker node is <code>t2.medium</code>. Master and worker nodes are assigned separate IAM roles as well.</p> 
<p>Wait for a few minutes for the cluster to be created. The cluster can be verified using the command <code>kops validate cluster --state=s3://kubernetes-aws-io</code>. It shows the following output:</p> 
<pre>Using cluster from kubectl context: cluster.kubernetes-aws.io
Validating cluster cluster.kubernetes-aws.io
INSTANCE GROUPS
NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;ROLE&nbsp;&nbsp;&nbsp;  &nbsp;MACHINETYPE&nbsp;&nbsp;&nbsp;&nbsp;MIN&nbsp;&nbsp;&nbsp;&nbsp;MAX&nbsp;&nbsp;&nbsp;&nbsp;SUBNETS
master-us-west-2a&nbsp;&nbsp;&nbsp;&nbsp;Master&nbsp;&nbsp;&nbsp;&nbsp;m3.medium&nbsp;&nbsp;&nbsp;&nbsp;  1&nbsp;&nbsp;&nbsp;   1&nbsp;&nbsp;&nbsp;&nbsp;  us-west-2a
nodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    &nbsp;Node&nbsp;&nbsp;&nbsp;  &nbsp;t2.medium&nbsp;&nbsp;&nbsp;&nbsp;  2&nbsp;&nbsp;&nbsp; &nbsp; 2&nbsp;&nbsp;&nbsp;&nbsp;  us-west-2a
NODE STATUS
NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                   ROLE&nbsp;&nbsp;&nbsp;&nbsp;  READY
ip-172-20-38-133.us-west-2.compute.internal&nbsp;&nbsp;&nbsp;&nbsp;node&nbsp;&nbsp;&nbsp;&nbsp;  True
ip-172-20-38-177.us-west-2.compute.internal&nbsp;&nbsp;&nbsp;&nbsp;master&nbsp;&nbsp;&nbsp;&nbsp;True
ip-172-20-46-33.us-west-2.compute.internal&nbsp;&nbsp; &nbsp;&nbsp;node&nbsp;&nbsp;&nbsp;  &nbsp;True
Your cluster cluster.kubernetes-aws.io is ready</pre> 
<p>It shows the different instances started for the cluster, and their roles. If multiple cluster states are stored in the same bucket, then <code>--name &lt;NAME&gt;</code> can be used to specify the exact cluster name.</p> 
<p>Check all nodes in the cluster using the command kubectl get nodes:</p> 
<pre>NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AGE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VERSION
ip-172-20-38-133.us-west-2.compute.internal&nbsp;&nbsp; Ready,node&nbsp;&nbsp;&nbsp;&nbsp; 14m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v1.6.2
ip-172-20-38-177.us-west-2.compute.internal&nbsp;&nbsp; Ready,master&nbsp;&nbsp; 15m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v1.6.2
ip-172-20-46-33.us-west-2.compute.internal&nbsp;&nbsp;&nbsp;&nbsp;Ready,node&nbsp;&nbsp;&nbsp;&nbsp; 14m&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; v1.6.2</pre> 
<p>Again, the internal IP address of each node, their current status (master or node), and uptime are shown. The key information here is the Kubernetes version for each node in the cluster, 1.6.2 in this case.</p> 
<p>The kubectl value included in the PATH earlier is configured to manage this cluster. Resources such as pods, replica sets, and services can now be created in the usual way.</p> 
<p>Some of the common options that can be used to override the default cluster creation are:</p> 
<li><code>--kubernetes-version</code><br /> The version of Kubernetes cluster. The exact versions supported are defined at <a href="http://github.com/kubernetes/kops/blob/master/channels/stable">github.com/kubernetes/kops/blob/master/channels/stable</a>.</li> 
<li><code>--master-size and --node-size</code><br /> Define the instance of master and worker nodes.</li> 
<li><code>--master-count and --node-count</code><br /> Define the number of master and worker nodes. By default, a master is created in each zone specified by <code>--master-zones</code>. Multiple master nodes can be created by a higher number using <code>--master-count</code> or specifying multiple Availability Zones in <code>--master-zones</code>.</li> 
<p>A three-master and five-worker node cluster, with master nodes spread across different Availability Zones, can be created using the following command:</p> 
<pre>kops create cluster \
--name cluster2.kubernetes-aws.io \
--zones us-west-2a,us-west-2b,us-west-2c \
--node-count 5 \
--state s3://kubernetes-aws-io \
--yes</pre> 
<p>Both the clusters are sharing the same state store but have different names. This also requires you to create an additional Amazon Route 53 hosted zone for the name.</p> 
<p>By default, the resources required for the cluster are directly created in the cloud. The <code>--target</code> option can be used to generate the AWS CloudFormation scripts instead. These scripts can then be used by the AWS CLI to create resources at your convenience.</p> 
<p>Get a complete list of options for cluster creation with <code>kops create cluster --help</code>.</p> 
<p>More details about the cluster can be seen using the command <code>kubectl cluster-info</code>:</p> 
<pre>Kubernetes master is running at https://api.cluster.kubernetes-aws.io
KubeDNS is running at https://api.cluster.kubernetes-aws.io/api/v1/proxy/namespaces/kube-system/services/kube-dns
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</pre> 
<p>Check the client and server version using the command <code>kubectl version</code>:</p> 
<pre>Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4&quot;, GitCommit:&quot;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-05-19T18:44:27Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;darwin/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.2&quot;, GitCommit:&quot;477efc3cbe6a7effca06bd1452fa356e2201e1ee&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-19T20:22:08Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}</pre> 
<p>Both client and server version are 1.6 as shown by the Major and Minor attribute values.</p> 
<b>Upgrade the Kubernetes cluster</b> 
<p>Kops can be used to create a Kubernetes 1.4.x, 1.5.x, or an older version of the 1.6.x cluster using the <code>--kubernetes-version</code> option. The exact versions supported are defined at <a href="https://github.com/kubernetes/kops/blob/master/channels/stable">github.com/kubernetes/kops/blob/master/channels/stable</a>.</p> 
<p>Or, you may have used kops to create a cluster a while ago, and now want to upgrade to the latest recommended version of Kubernetes. Kops supports rolling cluster upgrades where the master and worker nodes are upgraded one by one.</p> 
<p>As of kops 1.6.1, upgrading a cluster is a three-step process.</p> 
<p>First, check and apply the latest recommended Kubernetes update.</p> 
<pre>kops upgrade cluster \
--name cluster2.kubernetes-aws.io \
--state s3://kubernetes-aws-io \
--yes</pre> 
<p>The <code>--yes</code> option immediately applies the changes. Not specifying the <code>--yes</code> option shows only the changes that are applied.</p> 
<p>Second, update the state store to match the cluster state. This can be done using the following command:</p> 
<pre>kops update cluster \
--name cluster2.kubernetes-aws.io \
--state s3://kubernetes-aws-io \
--yes</pre> 
<p>Lastly, perform a rolling update for all cluster nodes using the <code>kops rolling-update</code> command:</p> 
<pre>kops rolling-update cluster \
--name cluster2.kubernetes-aws.io \
--state s3://kubernetes-aws-io \
--yes</pre> 
<p>Previewing the changes before updating the cluster can be done using the same command but without specifying the <code>--yes</code> option. This shows the following output:</p> 
<pre>NAME&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     STATUS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NEEDUPDATE&nbsp;&nbsp;&nbsp;&nbsp;READY&nbsp;&nbsp;&nbsp;&nbsp;MIN&nbsp;&nbsp;&nbsp;&nbsp;MAX&nbsp;&nbsp;&nbsp;&nbsp;NODES
master-us-west-2a&nbsp;&nbsp;&nbsp;&nbsp;NeedsUpdate&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;0&nbsp;&nbsp;&nbsp;    &nbsp;1&nbsp;&nbsp;&nbsp;  &nbsp;1&nbsp;&nbsp;&nbsp;  &nbsp;1
nodes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    &nbsp;&nbsp;&nbsp;NeedsUpdate&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     &nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;    &nbsp;2&nbsp;&nbsp;&nbsp;  &nbsp;2&nbsp;&nbsp;&nbsp;  &nbsp;2</pre> 
<p>Using <code>--yes</code> updates all nodes in the cluster, first master and then worker. There is a 5-minute delay between restarting master nodes, and a 2-minute delay between restarting nodes. These values can be altered using <code>--master-interval</code> and <code>--node-interval</code> options, respectively.</p> 
<p>Only the worker nodes may be updated by using the <code>--instance-group</code> node option.</p> 
<b>Delete the Kubernetes cluster</b> 
<p>Typically, the Kubernetes cluster is a long-running cluster to serve your applications. After its purpose is served, you may delete it. It is important to delete the cluster using the kops command. This ensures that all resources created by the cluster are appropriately cleaned up.</p> 
<p>The command to delete the Kubernetes cluster is:</p> 
<pre>kops delete cluster --state=s3://kubernetes-aws-io --yes</pre> 
<p>If multiple clusters have been created, then specify the cluster name as in the following command:</p> 
<pre>kops delete cluster&nbsp;cluster2.kubernetes-aws.io --state=s3://kubernetes-aws-io --yes</pre> 
<b>Conclusion</b> 
<p>This post explained how to manage a Kubernetes cluster on AWS using kops. Kubernetes on AWS users provides a self-published list of companies using Kubernetes on AWS.</p> 
<p>Try starting a cluster, create a few Kubernetes resources, and then tear it down. Kops on AWS provides a more comprehensive tutorial for setting up Kubernetes clusters. Kops docs are also helpful for understanding the details.</p> 
<p>In addition, the Kops team hosts <a href="https://github.com/kubernetes/kops#office-hours">office hours</a> to help you get started, from guiding you with your first pull request. You can always join the <a href="http://slack.k8s.io/">#kops channel on Kubernetes slack</a> to ask questions. If nothing works, then file an issue at <a href="https://github.com/kubernetes/kops/issues">github.com/kubernetes/kops/issues</a>.</p> 
<p>Future posts in this series will explain other ways of creating and running a Kubernetes cluster on AWS.</p> 
<p>—&nbsp;<a href="http://twitter.com/arungupta">Arun</a></p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/containers/" rel="tag">Containers</a>, <a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/kubernetes/" rel="tag">Kubernetes</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2475');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Blue/Green Deployments with Amazon EC2 Container Service</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> | on 
<time property="datePublished" datetime="2017-06-30T15:26:24+00:00">30 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/bluegreen-deployments-with-amazon-ecs/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2448" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2448&amp;disqus_title=Blue%2FGreen+Deployments+with+Amazon+EC2+Container+Service&amp;disqus_url=https://aws.amazon.com/blogs/compute/bluegreen-deployments-with-amazon-ecs/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2448');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post and accompanying code was generously contributed by:</p> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/jicowan.jpeg"><img class="aligncenter wp-image-2449 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/jicowan.jpeg" alt="" width="120" height="160" /></a></td> 
<td style="padding: 0px 30px 0px 0px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/anshrma.jpeg"><img class="aligncenter wp-image-2466 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/anshrma.jpeg" alt="" width="119" height="160" /></a></td> 
<td style="padding: 0px 30px 0px 0px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/dalbhanj.jpeg"><img class="aligncenter wp-image-2467 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/dalbhanj.jpeg" alt="" width="119" height="160" /></a></td> 
</tr> 
<tr> 
<td style="padding: 0px 30px 0px 0px"><b>Jeremy Cowan</b><br /> Solutions Architect</td> 
<td style="padding: 0px 30px 0px 0px"><b>Anuj Sharma</b><br /> DevOps Cloud Architect</td> 
<td style="padding: 0px 30px 0px 0px"><b>Peter Dalbhanjan</b><br /> Solutions Architect</td> 
</tr> 
</tbody> 
</table> 
<p>Deploying software updates in traditional non-containerized environments is hard and fraught with risk. When you write your deployment package or script, you have to assume that the target machine is in a particular state. If your staging environment is not an exact mirror image of your production environment, your deployment could fail. These failures frequently cause outages that persist until you re-deploy the last known good version of your application. If you are an Operations Manager, this is what keeps you up at night.</p> 
<p>Increasingly, customers want to do testing in production environments without exposing customers to the new version until the release has been vetted. Others want to expose a small percentage of their customers to the new release to gather feedback about a feature before it’s released to the broader population. This is often referred to as canary analysis or canary testing. In this post, I introduce patterns to implement blue/green and canary deployments using <a href="https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/">Application Load Balancers</a> and <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html">target groups</a>.</p> 
<p>If you’d like to try this approach to blue/green deployments, we have open sourced the code and AWS CloudFormation templates in the <a href="https://github.com/awslabs/ecs-blue-green-deployment">ecs-blue-green-deployment GitHub repo</a>. The workflow builds an <a href="https://github.com/awslabs/ecs-refarch-continuous-deployment">automated CI/CD pipeline</a> that deploys your service onto an ECS cluster and offers a controlled process to swap target groups when you’re ready to promote the latest version of your code to production. You can quickly set up the environment in three steps&nbsp;and see the blue/green swap in action. We’d love for you to try it and send us your feedback!</p> 
<p><span id="more-2448"></span></p> 
<b>Benefits of blue/green</b> 
<p>Blue/green deployments are a type of immutable deployment that help you deploy software updates with less risk. The risk is reduced by creating separate environments for the current running or “blue” version of your application, and the new or “green” version of your application.</p> 
<p>This type of deployment gives you an opportunity to test features in the green environment without impacting the current running version of your application. When you’re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment by modifying DNS. By following this method, you can update and roll back features with near zero downtime.</p> 
<table> 
<tbody> 
<tr> 
<td><img class="wp-image-2450" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/0617_bluegreenecs_1-245x300.png" alt="" width="200" height="245" /></td> 
<td><i>A typical blue/green deployment involves shifting traffic between 2 distinct environments.</i></td> 
</tr> 
</tbody> 
</table> 
<p>This ability to quickly roll traffic back to the still-operating blue environment is one of the key benefits of blue/green deployments. With blue/green, you should be able to roll back to the blue environment at any time during the deployment process. This limits downtime to the time it takes to realize there’s an issue in the green environment and shift the traffic back to the blue environment. Furthermore, the impact of the outage is limited to the portion of traffic going to the green environment, not all traffic. If the blast radius of deployment errors is reduced, so is the overall deployment risk.</p> 
<b>Containers make it simpler</b> 
<p>Historically, blue/green deployments were not often used to deploy software on-premises because of the cost and complexity associated with provisioning and managing multiple environments. Instead, applications were upgraded in place.</p> 
<p>Although this approach worked, it had several flaws, including the ability to roll back quickly from failures. Rollbacks typically involved re-deploying a previous version of the application, which could affect the length of an outage caused by a bad release. Fixing the issue took precedence over the need to debug, so there were fewer opportunities to learn from your mistakes.</p> 
<p>Containers can ease the adoption of blue/green deployments because they’re easily packaged and behave consistently as they’re moved between environments. This consistency comes partly from their immutability. To change the configuration of a container, update its Dockerfile and rebuild and re-deploy the container rather than updating the software in place.</p> 
<p>Containers also provide process and namespace isolation for your applications, which allows you to run multiple versions of them side by side on the same Docker host without conflicts. Given their small sizes relative to virtual machines, you can binpack more containers per host than VMs. This lets you make more efficient use of your computing resources, reducing the cost of blue/green deployments.</p> 
<b>Fully Managed Updates with Amazon&nbsp;ECS</b> 
<p><a href="https://aws.amazon.com/ecs/">Amazon EC2 Container Service</a> (ECS)&nbsp;performs rolling updates when you update an existing Amazon ECS service. A rolling update involves replacing the current running version of the container with the latest version. The number of containers Amazon ECS adds or removes from service during a rolling update is controlled by adjusting the minimum and maximum number of healthy tasks allowed during service deployments.</p> 
<p>When you update your service’s task definition with the latest version of your container image, Amazon ECS automatically starts replacing the old version of your container with the latest version. During a deployment, Amazon ECS drains connections from the current running version and registers your new containers with the <a href="https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/">Application Load Balancer</a> as they come online.</p> 
<b>Target groups</b> 
<p>A target group is a logical construct that allows you to run multiple services behind the same Application Load Balancer. This is possible because each target group has its own listener.</p> 
<p>When you create an Amazon ECS service that’s fronted by an Application Load Balancer, you have to designate a target group for your service. Ordinarily, you would create a target group for each of your Amazon ECS services. However, the approach we’re going to explore here involves creating two target groups: one for the blue version of your service, and one for the green version of your service. We’re also using a different listener port for each target group so that you can test the green version of your service using the same path as the blue service.</p> 
<p>With this configuration, you can run both environments in parallel until you’re ready to cut over to the green version of your service. You can also do things such as restricting access to the green version to testers on your internal network, using security group rules and placement constraints. For example, you can target the green version of your service to only run on instances that are accessible from your corporate network.</p> 
<b>Swapping Over</b> 
<p>When you’re ready to replace the old blue service with the new green service, call the <a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/APIReference/API_ModifyListener.html">ModifyListener API</a> operation to swap the listener’s rules for the target group rules. The change happens instantaneously. Afterward, the green service is running in the target group with the port 80 listener and the blue service is running in the target group with the port 8080 listener. The diagram below is an illustration of the approach described.</p> 
<h4>Scenario</h4> 
<p>Two services are defined, each with their own target group registered to the same Application Load Balancer but listening on different ports. Deployment is completed by swapping the listener rules between the two target groups.</p> 
<p>The second service is deployed with a new target group listening on a different port but registered to the same Application Load Balancer.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/0617_bluegreenecs_2.png"><img class="aligncenter wp-image-2451" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/0617_bluegreenecs_2-300x214.png" alt="" width="400" height="285" /></a></p> 
<p>By using 2 listeners, requests to blue services are directed to the target group with the port 80 listener, while requests to the green services are directed to target group with the port 8080 listener.</p> 
<p>After automated or manual testing, the deployment can be completed by swapping the listener rules on the Application Load Balancer and sending traffic to the green service.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/0617_bluegreenecs_3.png"><img class="aligncenter wp-image-2452" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/30/0617_bluegreenecs_3-300x221.png" alt="" width="401" height="295" /></a></p> 
<b>Caveats</b> 
<p>There are a few caveats to be mindful of when using this approach. This method:</p> 
<li><strong>Assumes that your application code is completely stateless</strong>. Store state outside of the container.</li> 
<li><strong>Doesn’t gracefully drain connections</strong>. The swapping of target groups is sudden and abrupt. Therefore, be cautious about using this approach if your service has long-running transactions.</li> 
<li><strong>Doesn’t allow you to perform canary deployments</strong>. While the method gives you the ability to quickly switch between different versions of your service, it does not allow you to divert a portion of the production traffic to a canary or control the rate at which your service is deployed across the cluster.</li> 
<b>Canary testing</b> 
<p>While this type of deployment automates much of the heavy lifting associated with rolling deployments, it doesn’t allow you to interrupt the deployment if you discover an issue midstream. Rollbacks using the standard Amazon ECS deployment require updating the service’s task definition with the last known good version of the container. Then, you wait for Amazon ECS to schedule and deploy it across the cluster. If the latest version introduces a breaking change that went undiscovered during testing, this might be too slow.</p> 
<p>With canary testing, if you discover the green environment is not operating as expected, there is no impact on the blue environment. You can route traffic back to it, minimizing impaired operation or downtime, and limiting the blast radius of impact.</p> 
<p>This type of deployment is particularly useful for A/B testing where you want to expose a new feature to a subset of users to get their feedback before making it broadly available.</p> 
<p>For canary style deployments, you can use a variation of the blue/green swap that involves deploying the blue and the green service to the same target group. Although this method is not as fast as the swap, it allows you to control the rate at which your containers are replaced by adjusting the task count for each service. Furthermore, it gives you the ability to roll back by adjusting the number of tasks for the blue and green services respectively. Unlike the swap approach described above, connections to your containers are drained gracefully. We plan to address canary style deployments for&nbsp;Amazon ECS in a future post.</p> 
<b>Conclusion</b> 
<p>With AWS, you can operationalize your blue/green deployments using Amazon ECS, an Application Load Balancer, and target groups. I encourage you to adapt the code published to the <a href="https://github.com/awslabs/ecs-blue-green-deployment">ecs-blue-green-deployment GitHub repo</a> for your use cases and look forward to reading your feedback.</p> 
<p>If you’re interested in learning more, I encourage you to read the <a href="https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf">Blue/Green Deployments on AWS</a> and <a href="https://d0.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf">Practicing Continuous Integration and Continuous Delivery on AWS</a> whitepapers.</p> 
<p>If you have questions or suggestions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/deployment/" rel="tag">deployment</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2448');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Synchronizing Amazon S3 Buckets Using AWS Step Functions</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andy Katz</span></span> | on 
<time property="datePublished" datetime="2017-06-25T22:26:59+00:00">25 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/application-services/aws-step-functions/" title="View all posts in AWS Step Functions*"><span property="articleSection">AWS Step Functions*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/synchronizing-amazon-s3-buckets-using-aws-step-functions/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2404" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2404&amp;disqus_title=Synchronizing+Amazon+S3+Buckets+Using+AWS+Step+Functions&amp;disqus_url=https://aws.amazon.com/blogs/compute/synchronizing-amazon-s3-buckets-using-aws-step-functions/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2404');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/glez.jpg"><img class="alignnone size-full wp-image-2406" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/glez.jpg" alt="" width="120" height="160" /></a></p> 
<p><strong>Constantin Gonzalez is a Principal Solutions Architect at AWS</strong></p> 
<p>In my free time, I run a small blog that uses Amazon S3 to host static content and Amazon CloudFront to distribute it world-wide. I use a home-grown, static website generator to create and upload my blog content onto S3.</p> 
<p>My blog uses two S3 buckets: one for staging and testing, and one for production. As a website owner, I want to update the production bucket with all changes from the staging bucket in a reliable and efficient way, without having to create and populate a new bucket from scratch. Therefore, to synchronize files between these two buckets, I use <a href="https://aws.amazon.com/lambda" target="_blank" rel="noopener noreferrer">AWS Lambda</a> and <a href="https://aws.amazon.com/step-functions" target="_blank" rel="noopener noreferrer">AWS Step Functions</a>.</p> 
<p>In this post, I show how you can use Step Functions to build a scalable synchronization engine for S3 buckets and learn some common patterns for designing Step Functions state machines while you do so.<span id="more-2404"></span></p> 
<b id="toc_1">Step Functions overview</b> 
<p>Step Functions makes it easy to coordinate the components of distributed applications and microservices using visual workflows. Building applications from individual components that each perform a discrete function lets you scale and change applications quickly.</p> 
<p>While this particular example focuses on synchronizing objects between two S3 buckets, it can be generalized to any other use case that involves coordinated processing of any number of objects in S3 buckets, or other, similar data processing patterns.</p> 
<b id="toc_2">Bucket replication options</b> 
<p>Before I dive into the details on how this particular example works, take a look at some alternatives for copying or replicating data between two Amazon S3 buckets:</p> 
<li>The <a href="https://aws.amazon.com/cli">AWS CLI</a> provides customers with a powerful <a href="http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html" target="_blank" rel="noopener noreferrer">aws s3 sync</a> command that can synchronize the contents of one bucket with another.</li> 
<li><a href="http://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html">S3DistCP</a> is a powerful tool for users of <a href="https://aws.amazon.com/emr/" target="_blank" rel="noopener noreferrer">Amazon EMR</a> that can efficiently load, save, or copy large amounts of data between S3 buckets and HDFS.</li> 
<li>The S3 <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html" target="_blank" rel="noopener noreferrer">cross-region replication</a> functionality enables automatic, asynchronous copying of objects across buckets in different AWS regions.</li> 
<p>In this use case, you are looking for a slightly different bucket synchronization solution that:</p> 
<li>Works within the same region</li> 
<li>Is more scalable than a CLI approach running on a single machine</li> 
<li>Doesn’t require managing any servers</li> 
<li>Uses a more finely grained cost model than the hourly based Amazon EMR approach</li> 
<p>You need a scalable, serverless, and customizable bucket synchronization utility.</p> 
<b id="toc_3">Solution architecture</b> 
<p>Your solution needs to do three things:</p> 
<ol> 
<li>Copy all objects from a source bucket into a destination bucket, but leave out objects that are already present, for efficiency.</li> 
<li>Delete all “orphaned” objects from the destination bucket that aren’t present on the source bucket, because you don’t want obsolete objects lying around.</li> 
<li>Keep track of all objects for #1 and #2, regardless of how many objects there are.</li> 
</ol> 
<p>In the beginning, you read in the source and destination buckets as parameters and perform basic parameter validation. Then, you operate two separate, independent loops, one for copying missing objects and one for deleting obsolete objects. Each loop is a sequence of Step Functions states that read in chunks of S3 object lists and use the continuation token to decide in a choice state whether to continue the loop or not.</p> 
<p>This solution is based on the following architecture that uses Step Functions, Lambda, and two S3 buckets:</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_Arch-1.png"><img class="aligncenter size-medium wp-image-2417" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_Arch-1-300x250.png" alt="" width="300" height="250" /></a></p> 
<p>As you can see, this setup involves no servers, just two main building blocks:</p> 
<li>Step Functions manages the overall flow of synchronizing the objects from the source bucket with the destination bucket.</li> 
<li>A set of Lambda functions carry out the individual steps necessary to perform the work, such as validating input, getting lists of objects from source and destination buckets, copying or deleting objects in batches, and so on.</li> 
<p>To understand the synchronization flow in more detail, look at the Step Functions state machine diagram for this example.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_State-Machine-1.png"><img class="aligncenter size-medium wp-image-2415" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/21/Step_Functions_S3_Bucket_State-Machine-1-267x300.png" alt="" width="267" height="300" /></a></p> 
<b id="toc_4">Walkthrough</b> 
<p>Here’s a detailed discussion of how this works.</p> 
<p>To follow along, use the code in the <a href="https://github.com/awslabs/sync-buckets-state-machine" target="_blank" rel="noopener noreferrer">sync-buckets-state-machine</a> GitHub repo. The code comes with a ready-to-run deployment script in Python that takes care of all the IAM roles, policies, Lambda functions, and of course the Step Functions state machine deployment using <a href="https://aws.amazon.com/cloudformation" target="_blank" rel="noopener noreferrer">AWS CloudFormation</a>, as well as instructions on how to use it.</p> 
<h3 id="toc_5">Fine print: Use at your own risk</h3> 
<p>Before I start, here are some disclaimers:</p> 
<li><strong>Educational purposes only.</strong> <p>The following example and code are intended for educational purposes only. Make sure that you customize, test, and review it on your own before using any of this in production.</p></li> 
<li><strong>S3 object deletion.</strong> <p>In particular, using the code included below may delete objects on S3 in order to perform synchronization. Make sure that you have backups of your data. In particular, consider using the <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html" target="_blank" rel="noopener noreferrer">Amazon S3 Versioning feature</a> to protect yourself against unintended data modification or deletion.</p></li> 
<p>Step Functions execution starts with an initial set of parameters that contain the source and destination bucket names in JSON:</p> 
<pre><code class="language-none">{
&quot;source&quot;:       &quot;my-source-bucket-name&quot;,
&quot;destination&quot;:  &quot;my-destination-bucket-name&quot;
}</code></pre> 
<p>Armed with this data, Step Functions execution proceeds as follows.</p> 
<h3 id="toc_6">Step 1: Detect the bucket region</h3> 
<p>First, you need to know the regions where your buckets reside. In this case, take advantage of the Step Functions <em>Parallel</em> state. This allows you to use a Lambda function <code>get_bucket_location.py</code> inside two different, parallel branches of <em>task</em> states:</p> 
<li><code>FindRegionForSourceBucket</code></li> 
<li><code>FindRegionForDestinationBucket</code></li> 
<p>Each task state receives one bucket name as an input parameter, then detects the region corresponding to “their” bucket. The output of these functions is collected in a result array containing one element per parallel function.</p> 
<h3 id="toc_7">Step 2: Combine the parallel states</h3> 
<p>The output of a parallel state is a list with all the individual branches’ outputs. To combine them into a single structure, use a Lambda function called <code>combine_dicts.py</code> in its own <code>CombineRegionOutputs</code> <em>task</em> state. The function combines the two outputs from step 1 into a single JSON dict that provides you with the necessary region information for each bucket.</p> 
<h3 id="toc_8">Step 3: Validate the input</h3> 
<p>In this walkthrough, you only support buckets that reside in the same region, so you need to decide if the input is valid or if the user has given you two buckets in different regions. To find out, use a Lambda function called <code>validate_input.py</code> in the <code>ValidateInput</code> <em>task</em> state that tests if the two regions from the previous step are equal. The output is a Boolean.</p> 
<h3 id="toc_9">Step 4: Branch the workflow</h3> 
<p>Use another type of Step Functions state, a <em>Choice</em> state, which branches into a <em>Failure</em> state if the comparison in step 3 yields false, or proceeds with the remaining steps if the comparison was successful.</p> 
<h3 id="toc_10">Step 5: Execute in parallel</h3> 
<p>The actual work is happening in another <em>Parallel</em> state. Both branches of this state are very similar to each other and they re-use some of the Lambda function code.</p> 
<p>Each parallel branch implements a looping pattern across the following steps:</p> 
<ol> 
<li>Use a <em>Pass</em> state to inject either the string value <code>&quot;source&quot;</code> (<code>InjectSourceBucket</code>) or <code>&quot;destination&quot;</code> (<code>InjectDestinationBucket</code>) into the <code>listBucket</code> attribute of the state document.The next step uses either the source or the destination bucket, depending on the branch, while executing the same, generic Lambda function. You don’t need two Lambda functions that differ only slightly. This step illustrates how to use <em>Pass</em> states as a way of injecting constant parameters into your state machine and as a way of controlling step behavior while re-using common step execution code.</li> 
<li>The next step <code>UpdateSourceKeyList/UpdateDestinationKeyList</code> lists objects in the given bucket. <p>Remember that the previous step injected either <code>&quot;source&quot;</code> or <code>&quot;destination&quot;</code> into the state document’s <code>listBucket</code> attribute. This step uses the same <code>list_bucket.py</code> Lambda function to list objects in an S3 bucket. The <code>listBucket</code> attribute of its input decides which bucket to list. In the left branch of the main parallel state, use the list of source objects to work through copying missing objects. The right branch uses the list of destination objects, to check if they have a corresponding object in the source bucket and eliminate any orphaned objects. Orphans don’t have a source object of the same S3 key.</p></li> 
<li>This step performs the actual work. In the left branch, the <code>CopySourceKeys</code> step uses the <code>copy_keys.py</code> Lambda function to go through the list of source objects provided by the previous step, then copies any missing object into the destination bucket. Its sister step in the other branch, <code>DeleteOrphanedKeys</code>, uses its destination bucket key list to test whether each object from the destination bucket has a corresponding source object, then deletes any orphaned objects.</li> 
<li>The S3 <code>ListObjects</code> API action is designed to be scalable across many objects in a bucket. Therefore, it returns object lists in chunks of configurable size, along with a continuation token. If the API result has a continuation token, it means that there are more objects in this list. You can work from token to token to continue getting object list chunks, until you get no more continuation tokens.</li> 
</ol> 
<p>By breaking down large amounts of work into chunks, you can make sure each chunk is completed within the timeframe allocated for the Lambda function, and within the maximum input/output data size for a Step Functions state.</p> 
<p>This approach comes with a slight tradeoff: the more objects you process at one time in a given chunk, the faster you are done. There’s less overhead for managing individual chunks. On the other hand, if you process too many objects within the same chunk, you risk going over time and space limits of the processing Lambda function or the Step Functions state so the work cannot be completed.</p> 
<p>In this particular case, use a Lambda function that maximizes the number of objects listed from the S3 bucket that can be stored in the input/output state data. This is currently up to 32,768 bytes, assuming (based on some experimentation) that the execution of the <code>COPY/DELETE</code> requests in the processing states can always complete in time.</p> 
<p>A more sophisticated approach would use the Step Functions <em>retry/catch</em> state attributes to account for any time limits encountered and adjust the list size accordingly through some list site adjusting.</p> 
<h3 id="toc_11">Step 6: Test for completion</h3> 
<p>Because the presence of a continuation token in the S3 ListObjects output signals that you are not done processing all objects yet, use a <em>Choice</em> state to test for its presence. If a continuation token exists, it branches into the <code>UpdateSourceKeyList</code> step, which uses the token to get to the next chunk of objects. If there is no token, you’re done. The state machine then branches into the <code>FinishCopyBranch/FinishDeleteBranch</code> state.</p> 
<p>By using <em>Choice</em> states like this, you can create loops exactly like the old times, when you didn’t have for statements and used branches in assembly code instead!</p> 
<h3 id="toc_12">Step 7: Success!</h3> 
<p>Finally, you’re done, and can step into your final <em>Success</em> state.</p> 
<b id="toc_13">Lessons learned</b> 
<p>When implementing this use case with Step Functions and Lambda, I learned the following things:</p> 
<li>Sometimes, it is necessary to manipulate the JSON state of a Step Functions state machine with just a few lines of code that hardly seem to warrant their own Lambda function. This is ok, and the cost is actually pretty low given Lambda’s 100 millisecond billing granularity. The upside is that functions like these can be helpful to make the data more palatable for the following steps or for facilitating <em>Choice</em> states. An example here would be the <code>combine_dicts.py</code> function.</li> 
<li><em>Pass</em> states can be useful beyond debugging and tracing, they can be used to inject arbitrary values into your state JSON and guide generic Lambda functions into doing specific things.</li> 
<li><em>Choice</em> states are your friend because you can build while-loops with them. This allows you to reliably grind through large amounts of data with the patience of an engine that currently supports execution times of up to 1 year. <p>Currently, there is an execution history limit of 25,000 events. Each Lambda task state execution takes up 5 events, while each choice state takes 2 events for a total of 7 events per loop. This means you can loop about 3500 times with this state machine. For even more scalability, you can split up work across multiple Step Functions executions through object key sharding or similar approaches.</p></li> 
<li>It’s not necessary to spend a lot of time coding exception handling within your Lambda functions. You can delegate all exception handling to Step Functions and instead simplify your functions as much as possible.</li> 
<li>Step Functions are great replacements for shell scripts. This could have been a shell script, but then I would have had to worry about where to execute it reliably, how to scale it if it went beyond a few thousand objects, etc. Think of Step Functions and Lambda as tools for scripting at a cloud level, beyond the boundaries of servers or containers. “Serverless” here also means “boundary-less”.</li> 
<b id="toc_14">Summary</b> 
<p>This approach gives you scalability by breaking down any number of S3 objects into chunks, then using Step Functions to control logic to work through these objects in a scalable, serverless, and fully managed way.</p> 
<p>To take a look at the code or tweak it for your own needs, use the code in the <a href="https://github.com/awslabs/sync-buckets-state-machine" target="_blank" rel="noopener noreferrer">sync-buckets-state-machine</a> GitHub repo.</p> 
<p>To see more examples, please visit the <a href="https://aws.amazon.com/step-functions/getting-started/" target="_blank" rel="noopener noreferrer">Step Functions Getting Started</a> page.</p> 
<p>Enjoy!</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-storage-service/" rel="tag">Amazon Simple Storage Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/aws-lambda/" rel="tag">AWS Lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/s3/" rel="tag">S3</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a>, <a href="https://aws.amazon.com/blogs/compute/tag/step-functions/" rel="tag">Step Functions</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2404');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Kotlin and Groovy JVM Languages with AWS Lambda</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Juan Villa</span></span> | on 
<time property="datePublished" datetime="2017-06-23T07:21:46+00:00">23 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/kotlin-and-groovy-jvm-languages-with-aws-lambda/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2430" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2430&amp;disqus_title=Kotlin+and+Groovy+JVM+Languages+with+AWS+Lambda&amp;disqus_url=https://aws.amazon.com/blogs/compute/kotlin-and-groovy-jvm-languages-with-aws-lambda/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2430');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/23/jcv.jpeg"><img class="alignnone size-full wp-image-2442" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/23/jcv.jpeg" alt="" width="119" height="160" /></a><br /> Juan Villa – Partner Solutions Architect</p> 
<p>&nbsp;</p> 
<p>When most people hear “Java” they think of Java the programming language. Java is a lot more than a programming language, it also implies a larger ecosystem including the Java Virtual Machine (JVM). Java, the programming language, is just one of the many languages that can be compiled to run on the JVM. Some of the most popular JVM languages, other than Java, are Clojure, Groovy, Scala, Kotlin, JRuby, and Jython (see <a href="https://en.wikipedia.org/wiki/List_of_JVM_languages">this link</a> for a list of more JVM languages).</p> 
<p>Did you know that you can compile and subsequently run all these languages on AWS Lambda?</p> 
<p><a href="https://aws.amazon.com/lambda">AWS Lambda</a> supports the Java 8 runtime, but this does not mean you are limited to the Java language. The Java 8 runtime is capable of running JVM languages such as Kotlin and Groovy once they have been compiled and packaged as a “fat” JAR (a JAR file containing all necessary dependencies and classes bundled in).</p> 
<p>In this blog post we’ll work through building AWS Lambda functions in both <a href="https://kotlinlang.org/">Kotlin</a> and <a href="http://www.groovy-lang.org/">Groovy</a> programming languages. To compile and package our projects we will use <a href="https://gradle.org/">Gradle</a> build tool.</p> 
<p><span id="more-2430"></span></p> 
<p>To follow along, please clone the Git repository available at GitHub <a href="https://github.com/awslabs/lambda-kotlin-groovy-example">here</a>. Also, I recommend using an Integrated Development Environment (IDE) such as JetBrain’s <a href="https://www.jetbrains.com/idea/">IntelliJ IDEA</a>, this is the IDE I used while working on these projects.</p> 
<h4>Kotlin</h4> 
<p><a href="https://kotlinlang.org/">Kotlin</a> is a statically-typed JVM language designed and developed by <a href="https://www.jetbrains.com/">JetBrains</a> (one of our <a href="https://aws.amazon.com/partners/">Amazon Partner Network</a> Technology partners) and the open source community. Compared to Java the programming language, Kotlin has additional powerful language features such as: <a href="https://kotlinlang.org/docs/reference/data-classes.html">Data Classes</a>, <a href="https://kotlinlang.org/docs/reference/functions.html#default-arguments">Default Arguments</a>, <a href="https://kotlinlang.org/docs/reference/extensions.html">Extensions</a>, <a href="https://kotlinlang.org/docs/reference/null-safety.html">Elvis Operator</a>, and <a href="https://kotlinlang.org/docs/reference/multi-declarations.html">Destructuring Declarations</a>. This is a just a short list of Kotlin’s powerful language features. For a more thorough list of features, and how to use them, refer to the <a href="https://kotlinlang.org/docs/reference/">full documentation</a> of the Kotlin language.</p> 
<p>Let’s jump right into the code and see what an AWS Lambda function looks like in Kotlin.</p> 
<pre><code class="lang-kotlin">package com.aws.blog.jvmlangs.kotlin
import java.io.*
import com.fasterxml.jackson.module.kotlin.*
data class HandlerInput(val who: String)
data class HandlerOutput(val message: String)
class Main {
&nbsp;&nbsp;&nbsp; val mapper = jacksonObjectMapper()
&nbsp;&nbsp;&nbsp; fun handler(input: InputStream, output: OutputStream): Unit {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val inputObj = mapper.readValue&lt;HandlerInput&gt;(input)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mapper.writeValue(output, HandlerOutput(&quot;Hello ${inputObj.who}&quot;))
&nbsp;&nbsp;&nbsp; }
}</code></pre> 
<p>The above example is a very simple Hello World application that accepts as an input a JSON object containing a key called “who” and returns a JSON object containing a key called “message” with a value of “Hello {who}”.</p> 
<p>AWS Lambda does not support serializing JSON objects into Kotlin data classes, but don’t worry! AWS Lambda supports passing an input object as a Stream, and also supports an output Stream for returning a result (see <a href="http://docs.aws.amazon.com/lambda/latest/dg/java-handler-io-type-stream.html">this link</a> for more information). Combined with the Input/Output Stream form of the handler function, we are using the <a href="https://github.com/FasterXML/jackson">Jackson</a> library with a Kotlin extension function to support serialization and deserialization of Kotlin data class types.</p> 
<p>To get started with this example, let’s first compile and package the Kotlin project.</p> 
<pre><code class="lang-bash">git clone https://github.com/awslabs/lambda-kotlin-groovy-example
cd lambda-kotlin-groovy-example/kotlin
./gradlew shadowJar</code></pre> 
<p>Once packaged, a JAR file containing all necessary dependencies will be available at “build/libs/ jvmlangs-kotlin-1.0-SNAPSHOT-all.jar”. Now let’s deploy this package to AWS Lambda.</p> 
<p>To deploy the lambda function, we will be using the AWS Command Line Interface (CLI). You can find information on how to set up the AWS CLI <a href="https://aws.amazon.com/cli/">here</a>. This tool allows you to set up and manage AWS services via the command line.</p> 
<pre><code class="lang-bash">aws lambda create-function --region us-east-1 --function-name kotlin-hello \
--zip-file fileb://build/libs/jvmlangs-kotlin-1.0-SNAPSHOT-all.jar \
--role arn:aws:iam::&lt;account_id&gt;:role/lambda_basic_execution \
--handler com.aws.blog.jvmlangs.kotlin.Main::handler --runtime java8 \
--timeout 15 --memory-size 128</code></pre> 
<p>Once deployed, we can test the function by invoking the lambda function from the CLI.</p> 
<pre><code class="lang-bash">aws lambda invoke --function-name kotlin-hello --payload '{&quot;who&quot;: &quot;AWS Fan&quot;}' output.txt
cat output.txt</code></pre> 
<p>If successful, you’ll see an output of <code class="lang-json">“{&quot;message&quot;:&quot;Hello AWS Fan&quot;}”</code>.</p> 
<h4>Groovy</h4> 
<p><a href="http://groovy-lang.org/">Groovy</a> is an optionally typed JVM language with both dynamic and static typing capabilities. Groovy is currently being supported by the <a href="http://www.apache.org/">Apache Software Foundation</a>. Like Kotlin, Groovy also packs a lot of powerful features such as: <a href="http://docs.groovy-lang.org/latest/html/documentation/#_closures">Closures</a>, <a href="http://docs.groovy-lang.org/latest/html/documentation/#_dynamic_vs_static">Dynamic Typing</a>, <a href="http://docs.groovy-lang.org/latest/html/documentation/#_collection_literal_type_inference">Collection Literals</a>, <a href="http://docs.groovy-lang.org/latest/html/documentation/#_string_interpolation">String Interpolation</a>, and <a href="http://docs.groovy-lang.org/latest/html/documentation/#_elvis_operator">Elvis Operator</a>. This is just a short list, see the <a href="http://docs.groovy-lang.org/latest/html/documentation/">full documentation</a> for a list of features and how to use them.</p> 
<p>Once again, let’s jump right into the code.</p> 
<pre><code class="lang-groovy">package com.aws.blog.jvmlangs.groovy
class HandlerInput {
&nbsp;&nbsp;&nbsp; String who
}
class HandlerOutput {
&nbsp;&nbsp;&nbsp; String message
}
class Main {
&nbsp;&nbsp;&nbsp; def handler(HandlerInput input) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return new HandlerOutput(message: &quot;Hello ${input.who}&quot;)
&nbsp;&nbsp;&nbsp; }
}</code></pre> 
<p>Just like the Kotlin example, we have defined a function that takes a simple JSON object containing a “who” key value and build a response containing a “message” key. Note that in this case we are not using the Input/Output Stream form of the handler function, but rather we are letting AWS Lambda serialize the input JSON object into the type HandlerInput. To accomplish this, AWS Lambda uses the Jackson library and handles the serialization for us.</p> 
<p>Let’s go ahead and compile and package this Groovy example.</p> 
<pre><code class="lang-bash">git clone https://github.com/awslabs/lambda-kotlin-groovy-example
cd lambda-kotlin-groovy-example/groovy
./gradlew shadowJar</code></pre> 
<p>Once packaged, a JAR file containing all necessary dependencies will be available at “build/libs/ jvmlangs-groovy-1.0-SNAPSHOT-all.jar”. Now let’s deploy this package to AWS Lambda.</p> 
<pre><code class="lang-bash">aws lambda create-function --region us-east-1 --function-name groovy-hello \
--zip-file fileb://build/libs/jvmlangs-groovy-1.0-SNAPSHOT-all.jar \
--role arn:aws:iam::&lt;account_id&gt;:role/lambda_basic_execution \
--handler com.aws.blog.jvmlangs.groovy.Main::handler --runtime java8 \
--timeout 15 --memory-size 128</code></pre> 
<p>Once deployed, we can test the function by invoking the lambda function from the CLI.</p> 
<pre><code class="lang-bash">aws lambda invoke --function-name groovy-hello --payload '{&quot;who&quot;: &quot;AWS Fan&quot;}' output.txt
cat output.txt</code></pre> 
<p>If successful, you’ll see an output of <code class="lang-json">“{&quot;message&quot;:&quot;Hello AWS Fan&quot;}”</code>.</p> 
<h4>Gradle Build Tool</h4> 
<p>Finally, let’s touch up on how we built the JAR package from the Kotlin and Groovy sources above. To build the JARs we used the <a href="https://gradle.org/">Gradle</a> build tool. Gradle builds a project by reading instructions from a file called “build.gradle”. This is a file written in Gradle’s Groovy Domain Specific Langauge (DSL). You can find more information on the gradle build file by looking at their <a href="https://gradle.org/docs">documentation</a>. Let’s take a look at the Gradle build files we used for this post.</p> 
<p>For the Kotlin example, this is the build file we used.</p> 
<pre><code class="lang-gradle">buildscript {
&nbsp;&nbsp;&nbsp; repositories {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mavenCentral()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; jcenter()
&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; dependencies {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classpath &quot;org.jetbrains.kotlin:kotlin-gradle-plugin:$kotlin_version&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classpath &quot;com.github.jengelman.gradle.plugins:shadow:1.2.3&quot;
&nbsp;&nbsp;&nbsp; }
}
group 'com.aws.blog.jvmlangs.kotlin'
version '1.0-SNAPSHOT'
apply plugin: 'kotlin'
apply plugin: 'com.github.johnrengelman.shadow'
repositories {
&nbsp;&nbsp;&nbsp; mavenCentral()
}
dependencies {
&nbsp;&nbsp;&nbsp; compile &quot;org.jetbrains.kotlin:kotlin-stdlib:$kotlin_version&quot;
&nbsp;&nbsp;&nbsp; compile &quot;com.fasterxml.jackson.module:jackson-module-kotlin:2.8.2&quot;
}</code></pre> 
<p>For the Groovy example this is the build file we used.</p> 
<pre><code class="lang-gradle">buildscript {
&nbsp;&nbsp;&nbsp; repositories {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; jcenter()
&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; dependencies {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; classpath 'com.github.jengelman.gradle.plugins:shadow:1.2.3'
&nbsp;&nbsp;&nbsp; }
}
group 'com.aws.blog.jvmlangs.groovy'
version '1.0-SNAPSHOT'
apply plugin: 'groovy'
apply plugin: 'com.github.johnrengelman.shadow'
repositories {
&nbsp;&nbsp;&nbsp; mavenCentral()
}
dependencies {
&nbsp;&nbsp;&nbsp; compile 'org.codehaus.groovy:groovy-all:2.3.11'
&nbsp;&nbsp;&nbsp; testCompile group: 'junit', name: 'junit', version: '4.11'
}</code></pre> 
<p>As you can see, the build files for both Kotlin and Groovy files are very similar. For the Kotlin project we define a dependency on the Jackson Kotlin module. Also, for each respective language we include the language supporting libraries (kotlin-stdlib and groovy-all respectively).</p> 
<p>In addition, you will notice that we are using a plugin called “shadow”. We use this plugin to package all the project dependencies into one JAR by using the Gradle task “shadowJar”. You can find more information on Shadow in their <a href="http://imperceptiblethoughts.com/shadow/">documentation</a>.</p> 
<h4>Final Words</h4> 
<p>Don’t stop here though! Take a look at other JVM languages and get them running on AWS Lambda with the Java 8 runtime. Maybe start with <a href="http://clojure.org/">Clojure</a>? or <a href="http://www.scala-lang.org/">Scala</a>?</p> 
<p>Also take a look <a href="https://github.com/aws/aws-lambda-java-libs">AWS Lambda Java libraries</a> provided by AWS. They provide interfaces and models to make handling events from <a href="http://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html">event sources</a> easier to handle.</p> 
<footer> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2430');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Building Loosely Coupled, Scalable, C# Applications with Amazon SQS and Amazon SNS</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2017-06-20T12:56:00+00:00">20 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2369" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2369&amp;disqus_title=Building+Loosely+Coupled%2C+Scalable%2C+C%23+Applications+with+Amazon+SQS+and+Amazon+SNS&amp;disqus_url=https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2369');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h5>&nbsp;<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/Stephen_Liedig.png"><img class="alignnone size-full wp-image-2371" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/Stephen_Liedig.png" alt="" width="203" height="160" /></a><br /> Stephen Liedig, Solutions Architect</h5> 
<p>&nbsp;</p> 
<p>One of the many challenges professional software architects and developers face is how to make cloud-native applications scalable, fault-tolerant, and highly available.</p> 
<p>Fundamental to your project success is understanding the importance of making systems highly cohesive and loosely coupled. That means considering the multi-dimensional facets of system coupling to support the distributed nature of the applications that you are building for the cloud.</p> 
<p>By that, I mean addressing not only the application-level coupling (managing incoming and outgoing dependencies), but also considering the impacts of of platform, spatial, and temporal coupling of your systems. Platform coupling relates to the interoperability (or lack thereof) of heterogeneous systems components. Spatial coupling deals with managing components at a network topology level or protocol level. Temporal, or runtime coupling, refers to the ability of a component within your system to do any kind of meaningful work while it is performing a synchronous, blocking operation.</p> 
<p>The AWS messaging services, <a href="https://aws.amazon.com/sqs/">Amazon SQS</a> and <a href="https://aws.amazon.com/sns/">Amazon SNS</a>, help you deal with these forms of coupling by providing mechanisms for:</p> 
<li>Reliable, durable, and fault-tolerant delivery of messages between application components</li> 
<li>Logical decomposition of systems and increased autonomy of components</li> 
<li>Creating unidirectional, non-blocking operations, temporarily decoupling system components at runtime</li> 
<li>Decreasing the dependencies that components have on each other through standard communication and network channels</li> 
<p>Following on the recent topic, <a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a>, in this post, I look at some of the ways you can introduce SQS and SNS into your architectures to decouple your components, and show how you can implement them using C#.<span id="more-2369"></span></p> 
<h3>Walkthrough</h3> 
<p>To illustrate some of these concepts, consider a web application that processes customer orders. As good architects and developers, you have followed best practices and made your application scalable and highly available. Your solution included implementing load balancing, dynamic scaling across multiple Availability Zones, and persisting orders in a Multi-AZ Amazon RDS database instance, as in the following diagram.</p> 
<p><img class="aligncenter wp-image-2373 size-large" title="Order_Processing_Application" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/WebApplicationDiagram1-1024x552.png" alt="" width="640" height="345" /><br /> In this example, the application is responsible for handling and persisting the order data, as well as dealing with increases in traffic for popular items.</p> 
<p>One potential point of vulnerability in the order processing workflow is in saving the order in the database. The business expects that every order has been persisted into the database. However, any potential deadlock, race condition, or network issue could cause the persistence of the order to fail. Then, the order is lost with no recourse to restore the order.</p> 
<p>With good logging capability, you may be able to identify when an error occurred and which customer order failed. This wouldn’t allow you to “restore” the transaction, and by that stage, your customer is no longer your customer.</p> 
<p>As illustrated in the following diagram, introducing an SQS queue helps improve your ordering application. Using the queue isolates the processing logic into its own component and runs it in a separate process from the web application. This, in turn, allows the system to be more resilient to spikes in traffic, while allowing work to be performed only as fast as necessary in order to manage costs.</p> 
<p><img class="aligncenter wp-image-2379 size-large" title="Order_Processing_SQS" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/CustomOrderQueue-1024x632.png" alt="" width="640" height="395" /><br /> In addition, you now have a mechanism for persisting orders as messages (with the queue acting as a temporary database), and have moved the scope of your transaction with your database further down the stack. In the event of an application exception or transaction failure, this ensures that the order processing can be retired or redirected to the Amazon SQS Dead Letter Queue (DLQ), for re-processing at a later stage. (See the recent post, <a href="https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/">Using Amazon SQS Dead-Letter Queues to Control Message Failure,</a> for more information on dead-letter queues.)</p> 
<h3>Scaling the order processing nodes</h3> 
<p>This change allows you now to scale the web application frontend independently from the processing nodes. The frontend application can continue to scale based on metrics such as CPU usage, or the number of requests hitting the load balancer. Processing nodes can scale based on the number of orders in the queue. Here is an example of scale-in and scale-out alarms that you would associate with the scaling policy.</p> 
<p><strong class="lang-bash">Scale-out Alarm</strong></p> 
<pre><code class="lang-powershell">aws cloudwatch put-metric-alarm --alarm-name AddCapacityToCustomerOrderQueue --metric-name ApproximateNumberOfMessagesVisible --namespace &quot;AWS/SQS&quot; 
--statistic Average --period 300 --threshold 3 --comparison-operator GreaterThanOrEqualToThreshold --dimensions Name=QueueName,Value=customer-orders
--evaluation-periods 2 --alarm-actions &lt;arn of the scale-out autoscaling policy&gt;
</code></pre> 
<p><strong>Scale-in Alarm</strong></p> 
<pre><code class="lang-powershell">aws cloudwatch put-metric-alarm --alarm-name RemoveCapacityFromCustomerOrderQueue --metric-name ApproximateNumberOfMessagesVisible --namespace &quot;AWS/SQS&quot; 
--statistic Average --period 300 --threshold 1 --comparison-operator LessThanOrEqualToThreshold --dimensions Name=QueueName,Value=customer-orders
--evaluation-periods 2 --alarm-actions &lt;arn of the scale-in autoscaling policy&gt;
</code></pre> 
<p>In the above example, use the A<em>pproximateNumberOfMessagesVisible</em> metric to discover the queue length and drive the scaling policy of the Auto Scaling group. Another useful metric is <em>ApproximateAgeOfOldestMessage</em>, when applications have time-sensitive messages and developers need to ensure that messages are processed within a specific time period.</p> 
<h3>Scaling the order processing implementation</h3> 
<p>On top of scaling at an infrastructure level using Auto Scaling, make sure to take advantage of the processing power of your <a href="https://aws.amazon.com/ec2">Amazon EC2</a> instances by using as many of the available threads as possible. There are several ways to implement this. In this post, we build a Windows service that uses the <strong>BackgroundWorker</strong> class to process the messages from the queue.</p> 
<p>Here’s a closer look at the implementation. In the first section of the consuming application, use a loop to continually poll the queue for new messages, and construct a <strong>ReceiveMessageRequest</strong> variable.</p> 
<pre><code class="lang-csharp">public static void PollQueue()
{
while (_running)
{
Task&lt;ReceiveMessageResponse&gt; receiveMessageResponse;
// Pull messages off the queue
using (var sqs = new AmazonSQSClient())
{
const int maxMessages = 10;  // 1-10
//Receiving a message
var receiveMessageRequest = new ReceiveMessageRequest
{
// Get URL from Configuration
QueueUrl = _queueUrl, 
// The maximum number of messages to return. 
// Fewer messages might be returned. 
MaxNumberOfMessages = maxMessages, 
// A list of attributes that need to be returned with message.
AttributeNames = new List&lt;string&gt; { &quot;All&quot; },
// Enable long polling. 
// Time to wait for message to arrive on queue.
WaitTimeSeconds = 5 
};
receiveMessageResponse = sqs.ReceiveMessageAsync(receiveMessageRequest);
}
</code></pre> 
<p>The<strong> WaitTimeSeconds</strong> property of the <strong>ReceiveMessageRequest</strong> specifies the duration (in seconds) that the call waits for a message to arrive in the queue before returning a response to the calling application. There are a few benefits to using long polling:</p> 
<li>It reduces the number of empty responses by allowing SQS to wait until a message is available in the queue before sending a response.</li> 
<li>It eliminates false empty responses by querying all (rather than a limited number) of the servers.</li> 
<li>It returns messages as soon any message becomes available.</li> 
<p>For more information, see <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html">Amazon SQS Long Polling</a>.</p> 
<p>After you have returned messages from the queue, you can start to process them by looping through each message in the response and invoking a new <strong>BackgroundWorker</strong> thread.</p> 
<pre><code class="lang-csharp">// Process messages
if (receiveMessageResponse.Result.Messages != null)
{
foreach (var message in receiveMessageResponse.Result.Messages)
{
Console.WriteLine(&quot;Received SQS message, starting worker thread&quot;);
// Create background worker to process message
BackgroundWorker worker = new BackgroundWorker();
worker.DoWork += (obj, e) =&gt; ProcessMessage(message);
worker.RunWorkerAsync();
}
}
else
{
Console.WriteLine(&quot;No messages on queue&quot;);
}
</code></pre> 
<p>The event handler, <strong>ProcessMessage</strong>, is where you implement business logic for processing orders. It is important to have a good understanding of how long a typical transaction takes so you can set a message <strong>VisibilityTimeout</strong> that is long enough to complete your operation. If order processing takes longer than the specified timeout period, the message becomes visible on the queue. Other nodes may pick it and process the same order twice, leading to unintended consequences.</p> 
<h3>Handling Duplicate Messages</h3> 
<p>In order to manage duplicate messages, seek to make your processing application idempotent. In mathematics, idempotent describes a function that produces the same result if it is applied to itself:</p> 
<p style="text-align: center">f(x) = f(f(x))</p> 
<p>No matter how many times you process the same message, the end result is the same (definition from Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions, Hohpe and Wolf, 2004).</p> 
<p>There are several strategies you could apply to achieve this:</p> 
<li>Create messages that have inherent idempotent characteristics. That is, they are non-transactional in nature and are unique at a specified point in time. Rather than saying “place new order for Customer A,” which adds a duplicate order to the customer, use “place order &lt;orderid&gt; on &lt;timestamp&gt; for Customer A,” which creates a single order no matter how often it is persisted.</li> 
<li>Deliver your messages via an <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html">Amazon SQS FIFO queue</a>, which provides the benefits of message sequencing, but also mechanisms for content-based deduplication. You can deduplicate using the <strong>MessageDeduplicationId</strong> property on the <strong>SendMessage</strong> request or by enabling content-based deduplication on the queue, which generates a hash for <strong>MessageDeduplicationId</strong>, based on the content of the message, not the attributes.</li> 
<pre><code class="lang-csharp">var sendMessageRequest = new SendMessageRequest
{
QueueUrl = _queueUrl,
MessageBody = JsonConvert.SerializeObject(order),
MessageGroupId = Guid.NewGuid().ToString(&quot;N&quot;),
MessageDeduplicationId = Guid.NewGuid().ToString(&quot;N&quot;)
};
</code></pre> 
<li>If using SQS FIFO queues is not an option, keep a message log of all messages attributes processed for a specified period of time, as an alternative to message deduplication on the receiving end. Verifying the existence of the message in the log before processing the message adds additional computational overhead to your processing. This can be minimized through low latency persistence solutions such as Amazon DynamoDB. Bear in mind that this solution is dependent on the successful, distributed transaction of the message and the message log.</li> 
<h3>Handling exceptions</h3> 
<p>Because of the distributed nature of SQS queues, it does not automatically delete the message. Therefore, you must explicitly delete the message from the queue after processing it, using the message <strong>ReceiptHandle</strong> property (see the following code example).</p> 
<p>However, if at any stage you have an exception, avoid handling it as you normally would. The intention is to make sure that the message ends back on the queue, so that you can gracefully deal with intermittent failures. Instead, log the exception to capture diagnostic information, and swallow it.</p> 
<p>By not explicitly deleting the message from the queue, you can take advantage of the <strong>VisibilityTimeout</strong> behavior described earlier. Gracefully handle the message processing failure and make the unprocessed message available to other nodes to process.</p> 
<p><img class="aligncenter wp-image-2378 size-large" title="SQS_queue_settings" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/ConfigureCustomerOrders-1024x732.png" alt="" width="640" height="458" /></p> 
<p>In the event that subsequent retries fail, SQS automatically moves the message to the configured DLQ after the configured number of receives has been reached. You can further investigate why the order process failed. Most importantly, the order has not been lost, and your customer is still your customer.</p> 
<pre><code class="lang-csharp">private static void ProcessMessage(Message message)
{
using (var sqs = new AmazonSQSClient())
{
try
{
Console.WriteLine(&quot;Processing message id: {0}&quot;, message.MessageId);
// Implement messaging processing here
// Ensure no downstream resource contention (parallel processing)
// &lt;your order processing logic in here…&gt;
Console.WriteLine(&quot;{0} Thread {1}: {2}&quot;, DateTime.Now.ToString(&quot;s&quot;), Thread.CurrentThread.ManagedThreadId, message.MessageId);
// Delete the message off the queue. 
// Receipt handle is the identifier you must provide 
// when deleting the message.
var deleteRequest = new DeleteMessageRequest(_queueName, message.ReceiptHandle);
sqs.DeleteMessageAsync(deleteRequest);
Console.WriteLine(&quot;Processed message id: {0}&quot;, message.MessageId);
}
catch (Exception ex)
{
// Do nothing.
// Swallow exception, message will return to the queue when 
// visibility timeout has been exceeded.
Console.WriteLine(&quot;Could not process message due to error. Exception: {0}&quot;, ex.Message);
}
}
}
</code></pre> 
<h3>Using SQS to adapt to changing business requirements</h3> 
<p>One of the benefits of introducing a <a href="https://aws.amazon.com/message-queue/">message queue</a> is that you can accommodate new business requirements without dramatically affecting your application.</p> 
<p>If, for example, the business decided that all orders placed over $5000 are to be handled as a priority, you could introduce a new “priority order” queue. The way the orders are processed does not change. The only significant change to the processing application is to ensure that messages from the “priority order” queue are processed before the “standard order” queue.</p> 
<p>The following diagram shows how this logic could be isolated in an “order dispatcher,” whose only purpose is to route order messages to the appropriate queue based on whether the order exceeds $5000. Nothing on the web application or the processing nodes changes other than the target queue to which the order is sent. The rates at which orders are processed can be achieved by modifying the poll rates and scalability settings that I have already discussed.</p> 
<p><img class="aligncenter wp-image-2377 size-large" title="Order_Dispatcher" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/OrderDispatcher-1024x534.png" alt="" width="640" height="334" /></p> 
<h3>Extending the design pattern with Amazon SNS</h3> 
<p>Amazon SNS supports reliable <a href="https://aws.amazon.com/pub-sub-messaging/">publish-subscribe (pub-sub) scenarios</a> and push notifications to known endpoints across a wide variety of protocols. It eliminates the need to periodically check or poll for new information and updates. SNS supports:</p> 
<li>Reliable storage of messages for immediate or delayed processing</li> 
<li>Publish / subscribe – direct, broadcast, targeted “push” messaging</li> 
<li>Multiple subscriber protocols</li> 
<li>Amazon SQS, HTTP, HTTPS, email, SMS, mobile push, AWS Lambda</li> 
<p>With these capabilities, you can provide parallel asynchronous processing of orders in the system and extend it to support any number of different business use cases without affecting the production environment. This is commonly referred to as a “fanout” scenario.</p> 
<p>Rather than your web application pushing orders to a queue for processing, send a notification via SNS. The SNS messages are sent to a topic and then replicated and pushed to multiple SQS queues and Lambda functions for processing.</p> 
<p><img class="aligncenter wp-image-2376 size-large" title="Order_Dispatcher_Fanout" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/19/OrderDispatcher2-1024x718.png" alt="" width="640" height="449" /></p> 
<p>As the diagram above shows, you have the development team consuming “live” data as they work on the next version of the processing application, or potentially using the messages to troubleshoot issues in production.</p> 
<p>Marketing is consuming all order information, via a Lambda function that has subscribed to the SNS topic, inserting the records into an Amazon Redshift warehouse for analysis.</p> 
<p>All of this, of course, is happening without affecting your order processing application.</p> 
<h3>Summary</h3> 
<p>While I haven’t dived deep into the specifics of each service, I have discussed how these services can be applied at an architectural level to build loosely coupled systems that facilitate multiple business use cases. I’ve also shown you how to use infrastructure and application-level scaling techniques, so you can get the most out of your EC2 instances.</p> 
<p>One of the many benefits of using these managed services is how quickly and easily you can implement powerful messaging capabilities in your systems, and lower the capital and operational costs of managing your own messaging middleware.</p> 
<p>Using Amazon SQS and Amazon SNS together can provide you with a powerful mechanism for decoupling application components. This should be part of design considerations as you architect for the cloud.</p> 
<p>For more information, see the <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/Welcome.html">Amazon SQS Developer Guide</a> and <a href="http://docs.aws.amazon.com/sns/latest/dg/welcome.html">Amazon SNS Developer Guide</a>. You’ll find tutorials on all the concepts covered in this post, and more. To can get started using the AWS console or SDK of your choice visit:</p> 
<li><a href="https://aws.amazon.com/sqs/getting-started/">Getting Started with Amazon SQS</a></li> 
<li><a href="https://aws.amazon.com/sns/getting-started/">Getting Started with Amazon SNS</a></li> 
<p>Happy messaging!</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-notification-service/" rel="tag">Amazon Simple Notification Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-queue-service/" rel="tag">Amazon Simple Queue Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sns/" rel="tag">Amazon SNS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sqs/" rel="tag">Amazon SQS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloud-messaging/" rel="tag">cloud messaging</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-queues/" rel="tag">message queues</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-topics/" rel="tag">message topics</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/notifications/" rel="tag">notifications</a>, <a href="https://aws.amazon.com/blogs/compute/tag/pubsub/" rel="tag">pub/sub</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2369');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Secure API Access with Amazon Cognito Federated Identities, Amazon Cognito User Pools, and Amazon API Gateway</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ed Lima</span></span> | on 
<time property="datePublished" datetime="2017-06-19T06:31:15+00:00">19 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-api-gateway/" title="View all posts in Amazon API Gateway*"><span property="articleSection">Amazon API Gateway*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/mobile-services/amazon-cognito/" title="View all posts in Amazon Cognito*"><span property="articleSection">Amazon Cognito*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/secure-api-access-with-amazon-cognito-federated-identities-amazon-cognito-user-pools-and-amazon-api-gateway/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image002-1.png"><img class="alignnone size-full wp-image-2274" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image002-1.png" alt="" width="90" height="120" /></a></p> 
<p>Ed Lima, Solutions Architect</p> 
<p>&nbsp;</p> 
<p>Our identities are what define us as human beings. Philosophical discussions aside, it also applies to our day-to-day lives. For instance, I need my work badge to get access to my office building or my passport to travel overseas. My identity in this case is attached to my work badge or passport. As part of the system that checks my access, these documents or objects help define whether I have access to get into the office building or travel internationally.</p> 
<p>This exact same concept can also be applied to cloud applications and APIs. To provide secure access to your application users, you define who can access the application resources and what kind of access can be granted. <span id="more-2229"></span>Access is based on identity controls that can confirm authentication (AuthN) and authorization (AuthZ), which are different concepts. According to <a href="https://en.wikipedia.org/wiki/Authentication">Wikipedia</a>:</p> 
<p>&nbsp;</p> 
<blockquote> 
<p>“<em>The process of authorization is distinct from that of authentication. Whereas authentication is the process of verifying that “you are who you say you are,” authorization is the process of verifying that “you are permitted to do what you are trying to do.” This does not mean authorization presupposes authentication; an anonymous agent could be authorized to a limited action set.</em>”</p> 
</blockquote> 
<p><a href="https://aws.amazon.com/cognito">Amazon Cognito</a> allows building, securing, and scaling a solution to handle user management and authentication, and to sync across platforms and devices. In this post, I discuss the different ways that you can use Amazon Cognito to authenticate API calls to <a href="https://aws.amazon.com/apigateway">Amazon API Gateway</a> and secure access to your own API resources.</p> 
<p>&nbsp;</p> 
<b>Amazon Cognito Concepts</b> 
<p>&nbsp;</p> 
<p>It’s important to understand that Amazon Cognito provides three different services:</p> 
<li><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html">Amazon Cognito Federated Identities </a></li> 
<li><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html">Amazon Cognito User Pools</a></li> 
<li><a href="http://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html">Amazon Cognito Sync</a></li> 
<p>Today, I discuss the use of the first two. One service doesn’t need the other to work; however, they can be configured to work together.<br /> &nbsp;</p> 
<h3>Amazon Cognito Federated Identities</h3> 
<p>&nbsp;<br /> To use Amazon Cognito Federated Identities in your application, create an identity pool. An identity pool is a store of user data specific to your account. It can be configured to require an identity provider (IdP) for user authentication, after you enter details such as app IDs or keys related to that specific provider.</p> 
<p>After the user is validated, the provider sends an identity token to Amazon Cognito Federated Identities. In turn, Amazon Cognito Federated Identities contacts the <a href="http://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html">AWS Security Token Service</a> (AWS STS) to retrieve temporary AWS credentials based on a configured, authenticated IAM role linked to the identity pool. The role has appropriate IAM policies attached to it and uses these policies to provide access to other AWS services.</p> 
<p>Amazon Cognito Federated Identities currently supports the IdPs listed in the following graphic.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/18/CognitoDiagram.png"><img class="aligncenter wp-image-2309 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/18/CognitoDiagram.png" alt="" width="1275" height="714" /></a><br /> 
<!--more--><br /> &nbsp;</p> 
<p>In a nutshell, Amazon Cognito Federated Identities can be compared to a token vending machine that uses STS as a backend. The simplified user authentication flow for a given provider is:</p> 
<ol> 
<li>App sends user credentials to provider, usually user name and password.</li> 
<li>After the user is authenticated, the provider sends a valid token back to the application.</li> 
<li>The application sends the token to the identity pool associated with it.</li> 
<li>Amazon Cognito Federated Identities validates the token with the IdP.</li> 
<li>If the token is valid, Amazon Cognito Federated Identities contacts STS to retrieve temporary access credentials (access key, secret key, and session token) based on the authenticated IAM role associated with the identity pool.</li> 
<li>App contacts the specific AWS service with the temporary credentials.</li> 
</ol> 
<p>For more information about the different authentication flows, see the <a href="http://docs.aws.amazon.com/cognito/latest/developerguide/authentication-flow.html">Authentication Flow</a> topic and the <a href="https://aws.amazon.com/blogs/mobile/understanding-amazon-cognito-authentication-part-4-enhanced-flow/">Understanding Amazon Cognito Authentication Part 4: Enhanced Flow</a> blog post.</p> 
<p>If you don’t want to use an IdP, Amazon Cognito Federated Identities can also support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an IdP. If your application allows customers to connect as a guest user without logging in, you can enable access for unauthenticated identities. In that case, STS sends temporary credentials based on a specific unauthenticated IAM role with appropriate policies. In these cases, AWS strongly recommends that you stick with the principle of the least privilege, only allowing access to perform a certain task and nothing else.<br /> &nbsp;</p> 
<h3>Amazon Cognito User Pools</h3> 
<p>&nbsp;<br /> Amazon Cognito User Pools, on other hand is a full-fledged IdP that you can use to maintain a user directory and add sign-up and sign-in support to your mobile or web application. It uses JSON Web Tokens (JWTs) to authenticate and validate users. JWT is an open standard that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed.</p> 
<p>Amazon Cognito User Pools and identity pools can be used in conjunction to provide access to your application. An Amazon Cognito User Pools user authenticated with a user name and password can send a JWT to an associated identity pool. In turn, the identity pool sends temporary AWS credentials back to the application to access other AWS services. There’s no difference with the authentication flow mentioned above for other IdPs.</p> 
<p>To use a metaphor, Amazon Cognito User Pools provided you with a passport (JWT) that you can use at the airport counter to retrieve a boarding pass (access credentials) from an identity pool. With your valid boarding pass, you are then allowed to go to the airport gate and board your flight to the AWS Cloud, which is a fitting analogy for this post. The boarding pass is only valid for a specific time. In application terms, the token (passport) authenticates the user and the issued temporary credentials (boarding pass) authorize the access to connect (board).</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image005.png"><img class="aligncenter wp-image-2276 size-medium" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image005-300x175.png" alt="" width="300" height="175" /></a></p> 
<p>&nbsp;</p> 
<b>A Practical Example – Integrating Amazon Cognito with API Gateway</b> 
<p>&nbsp;</p> 
<p>To demonstrate the different ways that Amazon Cognito User Pools and Amazon Cognito Federated Identities can be used to authorize access to your API Gateway API, use a simple AngularV4 single page web application:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image007.png"><img class="aligncenter wp-image-2277 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image007.png" alt="" width="940" height="449" /></a></p> 
<p>&nbsp;</p> 
<p>Here’s the basic concept. You have a sample application that authenticates its users with three different IdPs. You also have an API Gateway API with three different resources (paths), one for each type of user. Each provider can only access one API-specific resource/path and cannot access any other resource allocated to other providers.</p> 
<p>After authentication, the user retrieves specific user attributes such as first name, last name, and email details from their provider and sends a request to the API resource (POST) with the data. After access is granted, an <a href="https://aws.amazon.com/lambda">AWS Lambda</a> function is invoked to add the details of the specific user to an <a href="https://aws.amazon.com/dynamodb">Amazon DynamoDB</a> table.</p> 
<p>You are using a single identity pool and a single API Gateway API to demonstrate that you can secure API access using multiple providers and multiple AuthN/AuthZ options in different ways, but sharing the same resources.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image009.png"><img class="aligncenter wp-image-2278 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image009.png" alt="" width="939" height="342" /></a></p> 
<p>&nbsp;</p> 
<p>Both “/google” and “/cip” resources GET/POST methods are configured and secured with IAM authorization. The GET/POST methods in “/cup”, on the other hand, are secured with a user pool authorizer.</p> 
<p>You can find the application code and a SAM template with instructions to deploy all the backend services in the <strong><a href="https://github.com/awslabs/aws-cognito-apigw-angular-auth">aws-cognito-apigw-angular-auth</a></strong> GitHub repository.</p> 
<p>There’s yet another way to authenticate API calls with Amazon Cognito: using a Lambda custom authorizer. I’m not covering it in this post; however, the <a href="https://aws.amazon.com/blogs/mobile/integrating-amazon-cognito-user-pools-with-api-gateway/">Integrating Amazon Cognito User Pools with API Gateway</a> post showcases this specific use case.</p> 
<p>For the first provider, use a public IdP, such as Google. After authenticating with Google credentials, the application collects the user details and then issue a POST call to your API.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image011.png"><img class="aligncenter wp-image-2279 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image011.png" alt="" width="939" height="489" /></a></p> 
<p>&nbsp;</p> 
<p>The DynamoDB table is updated with the latest user details and you can retrieve the data about the user making the API call. Notice the ID in blue in the user information card.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image013.png"><img class="aligncenter wp-image-2280 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image013.png" alt="" width="941" height="405" /></a></p> 
<p>&nbsp;</p> 
<p>After Amazon Cognito validates a user, it creates a unique identity ID for that user and links it with the specific IdP.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image015.png"><img class="aligncenter size-full wp-image-2281" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image015.png" alt="" width="572" height="347" /></a></p> 
<p>&nbsp;</p> 
<p>API Gateway is able to identify the identity ID based on the IAM credentials sent to the API method using the variable <em>$context.identity.cognitoIdentityId</em> in the integration request. A body mapping template adds the ID data automatically to the payload sent to the Lambda function, as follows:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">#set($inputRoot = $input.path('$'))
{
&quot;operation&quot;: &quot;create&quot;,
&quot;payload&quot;: {
&quot;Item&quot; : {
&quot;userId&quot; : &quot;$context.identity.cognitoIdentityId&quot;,
&quot;name&quot; : &quot;$inputRoot.name&quot;,
&quot;surname&quot; : &quot;$inputRoot.surname&quot;,
&quot;email&quot; : &quot;$inputRoot.email&quot;,
&quot;provider&quot;: &quot;Google&quot;
}
}
}
</code></pre> 
<p>&nbsp;</p> 
<p>Lambda processes the data and sends it in a proper format to DynamoDB, which uses the identity ID as a hash key. Due to the random and unique nature of the ID, it’s perfect to be used for querying the table.</p> 
<p>To guarantee that the Google users can only access their specific allocated API resources, attach the following authenticated role to the identity pool. The users assume this IAM role when validated and it explicitly allows only GET (read) and POST (write) access to the API resource “/google”:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;execute-api:Invoke&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/GET/google&quot;,
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/POST/google&quot;
]
}
]
}
</code></pre> 
<p>&nbsp;</p> 
<p>It’s time to sign in with the second user. This user is authenticated using Amazon Cognito User Pools as IdP. A JWT is used to retrieve temporary AWS credentials from the associated identity pool, same as is used by the Google provider.</p> 
<p>Using the Test Access card, you can issue API calls to each one of the three different resources to test the credentials. For instance, if the user tries to make an API call to “/cip” using the IAM credentials provided by Amazon Cognito, the access is granted (Status 200). However, if you try to access the “/google” resource, you get access denied (Status 403). You can confirm using the browser developer tools.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image017.png"><img class="aligncenter size-full wp-image-2282" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image017.png" alt="" width="941" height="552" /></a></p> 
<p>&nbsp;</p> 
<p>Even if you have the same identity pool in use by different providers (Google and Amazon Cognito User Pools), you can associate another IAM role to a group with Amazon Cognito User Pools. Because the user “jdoe” is part of that group, it inherits the IAM role association.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image019.png"><img class="aligncenter size-full wp-image-2283" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image019.png" alt="" width="454" height="331" /></a></p> 
<p>&nbsp;</p> 
<p>The IAM role explicitly allows only GET (read) and POST (write) access to the API resource “/cip”, nothing else.</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;execute-api:Invoke&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/GET/cip&quot;,
&quot;arn:aws:execute-api:us-east-1:123412341234:abcde12345/*/POST/cip&quot;
]
}
]
}
</code></pre> 
<p>&nbsp;</p> 
<p>The role details are passed in the JWT itself. For that reason, the identity pool allows the role to be assumed. The default authenticated role for the identity pool needs to be DENIED or else your user would also have access to the “/google” resource.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image021.png"><img class="aligncenter size-full wp-image-2284" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image021.png" alt="" width="796" height="512" /></a></p> 
<p>&nbsp;</p> 
<p>For the third and final user, skip Amazon Cognito Federated Identities altogether and authenticate the user from the Amazon Cognito User Pool directly to API Gateway using a Cognito user pool authorizer. You can find more information on my previous blog post, <a href="https://aws.amazon.com/blogs/compute/authorizing-access-through-a-proxy-resource-to-amazon-api-gateway-and-aws-lambda-using-amazon-cognito-user-pools/">Authorizing Access Through a Proxy Resource to Amazon API Gateway and AWS Lambda Using Amazon Cognito User Pools</a>.</p> 
<p>The user can only access the API resource assigned to the user pool and it’s denied for the other API paths:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image023-1.png"><img class="aligncenter size-full wp-image-2285" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image023-1.png" alt="" width="940" height="624" /></a></p> 
<p>&nbsp;</p> 
<p>The ID Token (JWT) is sent directly to API Gateway and there’s no IAM role involved in the user validation. The ID in the user information card looks different compared to the previous users. As no cognito identity ID is generated in this scenario, you need another type of unique ID for the user.</p> 
<p>Test the JWT generated in this particular user session in the API Gateway console, and retrieve user attributes and claims:</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image025.png"><img class="aligncenter size-full wp-image-2286" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image025.png" alt="" width="926" height="423" /></a></p> 
<p>&nbsp;</p> 
<p>The “sub” attribute is a unique identifier generated by Amazon Cognito User Pools and can be used to track each user from the user pool. You use this attribute as a hash key for the user pool users in the DynamoDB table. Use the variable <em>$context.authorizer.claims.sub</em> in the integration request, and the following body mapping template adds the ID data automatically to the payload sent to the Lambda function:</p> 
<p>&nbsp;</p> 
<pre><code class="lang-json">#set($inputRoot = $input.path('$'))
{
&quot;operation&quot;: &quot;create&quot;,
&quot;payload&quot;: {
&quot;Item&quot; : {
&quot;userId&quot; : &quot;$context.authorizer.claims.sub&quot;,
&quot;name&quot; : &quot;$context.authorizer.claims.given_name&quot;,
&quot;surname&quot; : &quot;$context.authorizer.claims.family_name&quot;,
&quot;email&quot; : &quot;$context.authorizer.claims.email&quot;,
&quot;provider&quot; : &quot;Cognito User Pools&quot;
}
}
}
</code></pre> 
<p>&nbsp;</p> 
<p>Check the DynamoDB console and confirm that all users were successfully added to the table with the API calls. As each user can only add or retrieve data from their specific API resource and only using their own UserID, the data is unavailable to other users.</p> 
<p>&nbsp;</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image027.png"><img class="aligncenter size-full wp-image-2287" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/image027.png" alt="" width="684" height="346" /></a></p> 
<p>&nbsp;</p> 
<b>Summary</b> 
<p>&nbsp;</p> 
<p>Amazon Cognito provide a rich set of features to authenticate and authorize users. You can take advantage of the built-in flexibility and different options to secure access to your API Gateway API integrating with multiple IdPs.</p> 
<p>There are distinct and specific use cases to authenticate, secure, and provide access to your API leveraging the integration between Amazon Cognito and API Gateway, depending on the work load and implementation of your application. I covered a couple of different scenarios in this post:</p> 
<li><strong>Your users are defined and authenticated by an external or public IdP.</strong></li> 
<p>You used Google for your sample application but it could be Facebook, Twitter, Amazon, OpenID Connect, your own developer IdP, or even an enterprise-level SAML provider, such as Active Directory. Use any IdP that can seamlessly integrate with Amazon Cognito Federated Identities linked with AWS Identity and Access Management roles.</p> 
<li><strong>Your users are defined in your own IdP powered by Amazon Cognito User Pools, leveraging aditional secure access with IAM permissions.</strong></li> 
<p>You want to have additional access granularity and security by integrating the authentication with IAM role-based access controls and Amazon Cognito User Pools groups.</p> 
<li><strong>Your users are defined in your own IdP powered by Amazon Cognito User Pools, leveraging secure JWTs for authentication.</strong></li> 
<p>You want to authenticate your API calls directly and seamlessly using these tokens and you are not interested in using IAM integration to authorize access with IAM roles.</p> 
<p>With Amazon Cognito User Pools, there’s no need to worry about the undifferentiated heavy lifting of maintaining your own IdP servers, allowing you to focus on application logic instead. It also gives you more control around your users, groups, applications, and attributes with no dependency on an external public provider.</p> 
<p>It’s a great balance between control and ease of use as it integrates seamlessly with identity pools for specific scenarios that require the additional use of IAM roles for granular security but can also work directly with JWTs. In addition to all these advantages, you can also integrate Lambda triggers for different sign-in, sign-up, confirmation, validation, and verification workflows to customize and adapt the user identification process even further.</p> 
<p>The integration between Amazon Cognito and API Gateway allows great flexibility such that you can implement these different authentication scenarios separately or even use all of them in conjunction (as demonstrated with the sample web application). You can provide access to the different API users and resources based on a specific IdP or multiple providers of your choice.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/apigateway/" rel="tag">apigateway</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cognito/" rel="tag">cognito</a>, <a href="https://aws.amazon.com/blogs/compute/tag/dynamodb/" rel="tag">DynamoDB</a>, <a href="https://aws.amazon.com/blogs/compute/tag/lambda/" rel="tag">lambda</a></span> 
</footer> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using Amazon SQS Dead-Letter Queues to Control Message Failure</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Tara Van Unen</span></span> | on 
<time property="datePublished" datetime="2017-06-07T13:18:10+00:00">07 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/compute/amazon-ec2/" title="View all posts in Amazon EC2*"><span property="articleSection">Amazon EC2*</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/compute/aws-lambda/" title="View all posts in AWS Lambda*"><span property="articleSection">AWS Lambda*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2216" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2216&amp;disqus_title=Using+Amazon+SQS+Dead-Letter+Queues+to+Control+Message+Failure&amp;disqus_url=https://aws.amazon.com/blogs/compute/using-amazon-sqs-dead-letter-queues-to-control-message-failure/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2216');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<h5><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/headshot.jpg"><img class="alignnone wp-image-2218 size-thumbnail" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/07/headshot-150x150.jpg" alt="" width="150" height="150" /></a><strong><br /> Michael G. Khmelnitsky, Senior Programmer Writer</strong></h5> 
<p>&nbsp;</p> 
<p>Sometimes, messages can’t be processed because of a variety of possible issues, such as erroneous conditions within the producer or consumer application. For example, if a user places an order within a certain number of minutes of creating an account, the producer might pass a message with an empty string instead of a customer identifier. Occasionally, producers and consumers might fail to interpret aspects of the protocol that they use to communicate, causing message corruption or loss. Also, the consumer’s hardware errors might corrupt message payload. For these reasons, messages that can’t be processed in a timely manner are delivered to a <em>dead-letter message queue</em>.<span id="more-2216"></span></p> 
<p>The recent post <a href="https://aws.amazon.com/blogs/compute/building-scalable-applications-and-microservices-adding-messaging-to-your-toolbox/">Building Scalable Applications and Microservices: Adding Messaging to Your Toolbox</a> gives an overview of messaging in the microservice architecture of modern applications. This post explains how and when you should use dead-letter queues to gain better control over message handling in your applications. It also offers some resources for configuring a dead-letter <a href="https://aws.amazon.com/message-queue/">message queue in Amazon Simple Queue Service (SQS)</a>.</p> 
<h3>What are the benefits of dead-letter queues?</h3> 
<p>The main task of a dead-letter queue is handling message failure. A dead-letter queue lets you set aside and isolate messages that can’t be processed correctly to determine why their processing didn’t succeed. Setting up a dead-letter queue allows you to do the following:</p> 
<li>Configure an alarm for any messages delivered to a dead-letter queue.</li> 
<li>Examine logs for exceptions that might have caused messages to be delivered to a dead-letter queue.</li> 
<li>Analyze the contents of messages delivered to a dead-letter queue to diagnose software or the producer’s or consumer’s hardware issues.</li> 
<li>Determine whether you have given your consumer sufficient time to process messages.</li> 
<h3>How do high-throughput, unordered queues handle message failure?</h3> 
<p>High-throughput, unordered queues (sometimes called <em>standard</em> or <em>storage queues</em>) keep processing messages until the expiration of the retention period. This helps ensure continuous processing of messages, which minimizes the chances of your queue being blocked by messages that can’t be processed. It also ensures fast recovery for your queue.</p> 
<p>In a system that processes thousands of messages, having a large number of messages that the consumer repeatedly fails to acknowledge and delete might increase costs and place extra load on the hardware. Instead of trying to process failing messages until they expire, it is better to move them to a dead-letter queue after a few processing attempts.</p> 
<p><strong>Note:</strong> This queue type often allows a high number of in-flight messages. If the majority of your messages can’t be consumed and aren’t sent to a dead-letter queue, your rate of processing valid messages can slow down. Thus, to maintain the efficiency of your queue, you must ensure that your application handles message processing correctly.</p> 
<h3>How do FIFO queues handle message failure?</h3> 
<p>FIFO (first-in-first-out) queues (sometimes called <em>service bus queues</em>) help ensure exactly-once processing by consuming messages in sequence from a <em>message group</em>. Thus, although the consumer can continue to retrieve ordered messages from another message group, the first message group remains unavailable until the message blocking the queue is processed successfully.</p> 
<p><strong>Note:</strong> This queue type often allows a lower number of in-flight messages. Thus, to help ensure that your FIFO queue doesn’t get blocked by a message, you must ensure that your application handles message processing correctly.</p> 
<h3>When should I use a dead-letter queue?</h3> 
<li><strong>Do</strong> use dead-letter queues with high-throughput, unordered queues. You should always take advantage of dead-letter queues when your applications don’t depend on ordering. Dead-letter queues can help you troubleshoot incorrect message transmission operations. <strong>Note:</strong> Even when you use dead-letter queues, you should continue to monitor your queues and retry sending messages that fail for transient reasons.</li> 
<li><strong>Do</strong> use dead-letter queues to decrease the number of messages and to reduce the possibility of exposing your system to <em>poison-pill messages</em> (messages that can be received but can’t be processed).</li> 
<li><strong>Don’t</strong> use a dead-letter queue with high-throughput, unordered queues when you want to be able to keep retrying the transmission of a message indefinitely. For example, don’t use a dead-letter queue if your program must wait for a dependent process to become active or available.</li> 
<li><strong>Don’t</strong> use a dead-letter queue with a FIFO queue if you don’t want to break the exact order of messages or operations. For example, don’t use a dead-letter queue with instructions in an Edit Decision List (EDL) for a video editing suite, where changing the order of edits changes the context of subsequent edits.</li> 
<h3>How do I get started with dead-letter queues in Amazon SQS?</h3> 
<p>Amazon SQS is a fully managed service that offers reliable, highly scalable hosted queues for exchanging messages between applications or microservices. Amazon SQS moves data between distributed application components and helps you decouple these components. It supports both <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html">standard queues</a> and <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html">FIFO queues</a>. To configure a queue as a dead-letter queue, you can use the <a href="http://console.aws.amazon.com/sqs/home">AWS Management Console</a> or the Amazon SQS <code class="lang-setqueueattributes">SetQueueAttributes</code> API action.</p> 
<p>To get started with dead-letter queues in Amazon SQS, see the following topics in the <em>Amazon SQS Developer Guide</em>:</p> 
<li><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/Welcome.html">What is Amazon SQS?</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">Using Amazon SQS Dead-Letter Queues</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/MonitorSQSwithCloudWatch.html">Monitoring Amazon SQS Using CloudWatch</a></li> 
<p>To start working with dead-letter queues programmatically, see the following resources:</p> 
<li>Java: <a href="http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-configure-dead-letter-queue.html#configure-dead-letter-queue-java">Configure a Dead-Letter Queue with the Amazon SQS API</a></li> 
<li>Java: <a href="https://aws.amazon.com/blogs/developer/using-amazon-sqs-dead-letter-queues-2/">Using Amazon SQS Dead-Letter Queues</a></li> 
<li>C#: <a href="https://aws.amazon.com/blogs/developer/using-amazon-sqs-dead-letter-queues/">Using Amazon SQS Dead-Letter Queues</a></li> 
<li>Lambda: <a href="https://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/">Robust Serverless Application Design with AWS Lambda Dead-Letter Queues</a></li> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/amazon-simple-queue-service/" rel="tag">Amazon Simple Queue Service</a>, <a href="https://aws.amazon.com/blogs/compute/tag/amazon-sqs/" rel="tag">Amazon SQS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/cloud-messaging/" rel="tag">cloud messaging</a>, <a href="https://aws.amazon.com/blogs/compute/tag/decoupling/" rel="tag">decoupling</a>, <a href="https://aws.amazon.com/blogs/compute/tag/lambda/" rel="tag">lambda</a>, <a href="https://aws.amazon.com/blogs/compute/tag/message-queues/" rel="tag">message queues</a>, <a href="https://aws.amazon.com/blogs/compute/tag/microservices/" rel="tag">microservices</a>, <a href="https://aws.amazon.com/blogs/compute/tag/serverless/" rel="tag">serverless</a>, <a href="https://aws.amazon.com/blogs/compute/tag/sqs/" rel="tag">sqs</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2216');
});
</script> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Powering your Amazon ECS Cluster with Amazon EC2 Spot Instances</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nathan Taber</span></span> and 
<span property="author" typeof="Person"><span property="name">Sebastian Dreisch</span></span> | on 
<time property="datePublished" datetime="2017-06-06T16:21:58+00:00">06 JUN 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/compute/category/amazon-ec2-spot/" title="View all posts in Amazon EC2 Spot"><span property="articleSection">Amazon EC2 Spot</span></a>, <a href="https://aws.amazon.com/blogs/compute/category/amazon-ecs/" title="View all posts in Amazon ECS"><span property="articleSection">Amazon ECS</span></a></span> | 
<a href="https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/" property="url">Permalink</a> | 
<a id="aws-comment-trigger-2071" href="https://commenting.awsblogs.com/embed.html?disqus_shortname=aws-compute&amp;disqus_identifier=2071&amp;disqus_title=Powering+your+Amazon+ECS+Cluster+with+Amazon+EC2+Spot+Instances&amp;disqus_url=https://aws.amazon.com/blogs/compute/powering-your-amazon-ecs-cluster-with-amazon-ec2-spot-instances/" data-sandbox-attr="allow-forms allow-scripts allow-popups allow-same-origin" property="discussionUrl"><i class="icon-comment"></i> Comments</a> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2071');
});
</script> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This post was graciously contributed by:</p> 
<table> 
<tbody> 
<tr> 
<td style="padding: 0px 15px 0px 0px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ChadS.jpeg"><img class="size-full wp-image-2197" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ChadS.jpeg" alt="Chad Schmutzer, Solutions Architect" width="120" height="160" /></a></td> 
<td style="padding: 0px 0px 0px 15px"><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ShawnOConner.jpeg"><img class="wp-image-2198 size-full" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/ShawnOConner.jpeg" alt="Shawn O'Conner, Enterprise Solutions Architect" width="119" height="160" /></a></td> 
</tr> 
<tr> 
<td style="padding: 0px 15px 0px 0px"><b>Chad Schmutzer</b><br /> Solutions Architect</td> 
<td style="padding: 0px 0px 0px 15px"><b>Shawn O’Connor</b><br /> Solutions Architect</td> 
</tr> 
</tbody> 
</table> 
<p>Today <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-ecs-adds-console-support-for-spot-fleet-creation/">we are excited to announce</a> that Amazon EC2 Container Service (<a href="https://aws.amazon.com/ecs">Amazon ECS</a>) now supports the ability to launch your ECS cluster on <a href="https://aws.amazon.com/ec2/spot">Amazon EC2 Spot Instances</a> directly from the ECS console.</p> 
<p>Spot Instances allow you to bid on spare Amazon EC2 compute capacity. Spot Instances typically cost 50-90% less than On-Demand Instances. Powering your ECS cluster with Spot Instances lets you reduce the cost of running your existing containerized workloads, or increase your compute capacity by two to ten times while keeping the same budget. Or you could do a combination of both!</p> 
<p><span id="more-2071"></span></p> 
<p>Using Spot Instances, you specify the price you are willing to pay per instance-hour. Your Spot Instance runs whenever your bid exceeds the current Spot&nbsp;price. If your instance is reclaimed due to an increase in the Spot price, you are not charged for the partial hour that your instance has run.</p> 
<p>The ECS console uses <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html">Spot Fleet</a> to deploy your Spot Instances. Spot Fleet attempts to deploy the target capacity you request (expressed in terms of instances or a vCPU count) for your containerized application by launching Spot Instances that result in the best prices for you. If your Spot Instances are reclaimed due to a change in Spot prices or available capacity, Spot Fleet also attempts to maintain its target capacity.</p> 
<p>Containers are a natural fit for the diverse pool of resources that Spot Fleet thrives on. Spot Fleet enable you to provision capacity across multiple Spot Instance pools (combinations of instance types and Availability Zones), which helps improve your application’s availability and reduce operating costs of the fleet over time. Combining the extensible and flexible container placement system provided by ECS with Spot Fleet allows you to efficiently deploy containerized workloads and easily manage clusters at any scale for a fraction of the cost.</p> 
<p>Previously, deploying your ECS cluster on Spot Instances was a manual process. In this post, we show you how to achieve high availability, scalability, and cost savings for your container workloads by using the new Spot Fleet integration in the ECS console. We also show you how to build your own ECS cluster on Spot Instances using AWS CloudFormation.</p> 
<b>Creating an ECS cluster running on Spot Instances</b> 
<p>You can create an ECS cluster using the AWS Management Console.</p> 
<ol> 
<li>Open the Amazon ECS console at <a href="https://console.aws.amazon.com/ecs/">https://console.aws.amazon.com/ecs/</a>.</li> 
<li>In the navigation pane, choose&nbsp;<strong>Clusters</strong>.</li> 
<li>On the&nbsp;<strong>Clusters</strong>&nbsp;page, choose&nbsp;<strong>Create Cluster</strong>.</li> 
<li>For&nbsp;<strong>Cluster name</strong>, enter a name.</li> 
<li>In <strong>Instance configuration</strong>, for <strong>Provisioning model</strong>, choose <strong>Spot</strong>.</li> 
</ol> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-1.png"><img class="aligncenter wp-image-2186" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-1.png" alt="ECS Create Cluster - Spot Fleet" width="700" height="345" /></a></p> 
<b>Choosing an allocation strategy</b> 
<p>The two available Spot Fleet allocation strategies are&nbsp;Diversified and Lowest price.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-2.png"><img class="aligncenter size-full wp-image-2187" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-2.png" alt="ECS Spot Allocation Strategies" width="572" height="122" /></a></p> 
<p>The allocation strategy you choose for your Spot Fleet determines how it fulfills your Spot Fleet request from the possible Spot Instance pools. When you use the diversified strategy, the Spot Instances are distributed across all pools. When you use the lowest price strategy, the Spot Instances come from the pool with the lowest price specified in your request.</p> 
<p>Remember that each instance type (the instance size within each instance family, e.g., c4.4xlarge), in each Availability Zone, in every region, is a separate pool of capacity, and therefore a separate Spot market. By diversifying across as many different instance types and Availability Zones as possible, you can improve the availability of your fleet. You also make your fleet less sensitive to increases in the Spot price in any one pool over time.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-3.png"><img class="aligncenter size-full wp-image-2188" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-3.png" alt="Spot Fleet Market" width="366" height="448" /></a></p> 
<p>You can select up to six EC2 instance types to use for your Spot Fleet. In this example, we’ve selected m3, m4, c3, c4, r3, and r4 instance types of size xlarge.</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-4.png"><img class="aligncenter wp-image-2189" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/06.06-Spot-4.png" alt="Spot Instance Selection" width="400" height="302" /></a></p> 
<p>You need to enter a bid for your instances. Typically, bidding at or near the On-Demand Instance price is a good starting point. Your bid is the maximum price that you are willing to pay for instance types in that Spot pool. While the Spot&nbsp;price is at or below your bid, you pay the Spot&nbsp;price. Bidding lower ensures that you have lower costs, while bidding higher reduces the probability of interruption.</p> 
<p>Configure the number of instances to have in your cluster. Spot Fleet attempts to launch the number of Spot Instances that are required to meet the target capacity specified in your request. The Spot Fleet also attempts to maintain its target capacity if your Spot Instances are reclaimed due to a change in Spot prices or available capacity.</p> 
<p>The latest <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html">ECS–optimized AMI</a> is used for the instances when they are launched.</p> 
<p>Configure your storage and network settings. To enable diversification and high availability, be sure to select subnets in multiple Availability Zones. You can’t select multiple subnets in the same Availability Zone in a single Spot Fleet.</p> 
<p>The ECS container agent makes calls to the ECS API actions on your behalf. Container instances that run the agent require the ecsInstanceRole IAM policy and role for the service to know that the agent belongs to you. If you don’t have the ecsInstanceRole already, you can create one using the ECS console.</p> 
<p>If you create a managed compute environment that uses Spot Fleet, you must create a role that grants the Spot Fleet permission to bid on, launch, and terminate instances on your behalf. You can also create this role using the ECS console.</p> 
<p>That’s it! In the ECS console, choose <strong>Create</strong> to spin up your new ECS cluster running on Spot Instances.</p> 
<b>Using AWS CloudFormation to deploy your ECS cluster on Spot Instances</b> 
<p>We have also published a reference architecture AWS CloudFormation template that demonstrates how to easily launch a CloudFormation stack and deploy your ECS cluster on Spot Instances.</p> 
<p>The CloudFormation template includes the Spot Instance termination notice script mentioned earlier, as well as some additional logging and other example features to get you started quickly. You can find the CloudFormation template in the <a href="https://github.com/awslabs/ec2-spot-labs/tree/master/ecs-ec2-spot-fleet">Amazon EC2 Spot Instances GitHub repo</a>.</p> 
<p>Give it a try and customize it as needed for your environment!</p> 
<p><a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/0606-Spot-5.jpg"><img class="size-large wp-image-2194 aligncenter" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/0606-Spot-5-1024x709.jpg" alt="Spot Fleet Architecture" width="640" height="443" /></a></p> 
<b>Handling termination</b> 
<p>With Spot Instances, you never pay more than the price you specified. If the Spot price exceeds your bid price for a given instance, it is terminated automatically for you.</p> 
<p>The best way to protect against Spot Instance interruption is to architect your containerized application to be fault-tolerant. In addition, you can take advantage of a feature called Spot Instance termination notices, which provides a two-minute warning before EC2 must terminate your Spot Instance.</p> 
<p>This warning is made available to the applications on your Spot Instance using an item in the instance metadata. When you deploy your ECS cluster on Spot Instances using the console, AWS installs a script that checks every 5 seconds for the Spot Instance termination notice. If the notice is detected, the script immediately updates the container instance state to <strong>DRAINING</strong>.</p> 
<p>A simplified version of the Spot Instance termination notice script is as follows:</p> 
<pre><code class="lang-bash">#!/bin/bash
while sleep 5; do
if [ -z $(curl -Isf http://169.254.169.254/latest/meta-data/spot/termination-time) ]; then
/bin/false
else
ECS_CLUSTER=$(curl -s http://localhost:51678/v1/metadata | jq .Cluster | tr -d \&quot;)
CONTAINER_INSTANCE=$(curl -s http://localhost:51678/v1/metadata \
| jq .ContainerInstanceArn | tr -d \&quot;)
aws ecs update-container-instances-state --cluster $ECS_CLUSTER \
--container-instances $CONTAINER_INSTANCE --status DRAINING
fi
done</code></pre> 
<p>When you set a container instance to <strong>DRAINING</strong>, ECS prevents new tasks from being scheduled for placement on the container instance. If the resources are available, replacement service tasks are started on other container instances in the cluster. Container instance draining enables you to remove a container instance from a cluster without impacting tasks in your cluster. Service tasks on the container instance that are in the <strong>PENDING</strong> state are stopped immediately.</p> 
<p>Service tasks on the container instance that are in the <strong>RUNNING</strong> state are stopped and replaced according to the service’s deployment configuration parameters, minimumHealthyPercent and maximumPercent.</p> 
<b>ECS on Spot Instances in action<a href="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/mapboxv1.jpg"><img class="alignright wp-image-2190 size-medium" src="https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2017/06/05/mapboxv1-300x150.jpg" alt="" width="300" height="150" /></a></b> 
<p>Want to see how customers are already powering their ECS clusters on Spot Instances? Our friends at <a href="https://www.mapbox.com/">Mapbox</a> are doing just that.</p> 
<p>Mapbox&nbsp;is a platform for designing and publishing custom maps. The company uses ECS to power their entire batch processing architecture to collect and process over 100 million miles of sensor data per day that they use for powering their maps. They also optimize their batch processing architecture on ECS using Spot Instances.</p> 
<p>The Mapbox platform powers over 5,000 apps and reaches more than 200 million users each month. Its backend runs on ECS, allowing it to serve more than 1.3 billion requests per day. To learn more about their recent migration to ECS, read their recent blog post,&nbsp;<a href="https://www.mapbox.com/blog/switch-to-ecs/">We Switched to Amazon ECS, and You Won’t Believe What Happened Next</a>. Then, in their follow-up blog post, <a href="https://www.mapbox.com/blog/caches-to-cash/">Caches to Cash</a>, learn how they are running their entire platform on Spot Instances, allowing them to save upwards of 50–90% on their EC2 costs.</p> 
<b>Conclusion</b> 
<p>We hope that you are as excited as we are about running your containerized applications at scale and cost effectively using Spot Instances. For more information, see the following pages:</p> 
<li><a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_GetStarted.html">Getting Started with Amazon ECS</a></li> 
<li><a href="https://aws.amazon.com/ec2/spot/getting-started/">Getting Started with Amazon EC2 Spot Instances</a></li> 
<li><a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.html">Getting Started with AWS CloudFormation</a></li> 
<p>If you have comments or suggestions, please comment below.</p> 
<footer> 
TAGS: 
<span property="keywords"><a href="https://aws.amazon.com/blogs/compute/tag/docker/" rel="tag">Docker</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ec2/" rel="tag">EC2</a>, <a href="https://aws.amazon.com/blogs/compute/tag/ecs/" rel="tag">ECS</a>, <a href="https://aws.amazon.com/blogs/compute/tag/spot/" rel="tag">spot</a></span> 
</footer> 
<script>
require(['blog'], function() {
AWS.Blog.commenting('#aws-comment-trigger-2071');
});
</script> 
</article> 
<p>
© 2017, Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
