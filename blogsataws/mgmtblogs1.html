<!DOCTYPE html>
<html lang="en">
  <head>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-593498H');</script>
<!-- End Google Tag Manager -->

    <meta charset="utf-8">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">

    <meta name="msapplication-config" content="/microsoft-tile.xml" />
    <meta name="theme-color" content="#ffffff">

    <meta property="og:url" content="https://bejoycalias.github.io/blogsataws/mgmtblogs1.html" />
    <meta property="og:site_name" content="Bejoy's TechNotes"/>
    <meta property="og:title" content="Bejoy's TechNotes - Kindle Friendly AWS Management Tools Blogs" />
    <meta property="og:image" content="https://bejoycalias.github.io/assets/images/og-image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="1200"/>
    <meta property="og:description" content="" />

    <title>Bejoy's TechNotes - Kindle Friendly AWS Management Tools Blogs</title>
    <link href="../assets/stylesheets/application-832aa42c.css" rel="stylesheet" />

    <!--[if lt IE 9]>
      <script src="../assets/javascripts/ie-compat-c141a02d.js"></script>
    <![endif]-->
    <script src="../assets/javascripts/application-ff53d307.js"></script>

    <!-- Typekit script to import Klavika font -->
    <script src="https://use.typekit.net/wxf7mfi.js"></script>
    <script>try{Typekit.load({ async: true });}catch(e){}</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-100043608-1', 'auto');
  ga('send', 'pageview');

</script>

    
  </head>

  <body id="uh-oh-" class="layout-inner page-uh-oh-">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-593498H"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->


    <div id="header" class="navigation navbar-static-top hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <div class="navbar-header">
              <div class="navbar-brand">
                <a href="/">
                  <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="50" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet" class="logo">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

                </a>
              </div>
              <button class="navbar-toggle" type="button">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="buttons hidden-xs">
              <nav class="navigation-links" role="navigation">
                <ul class="main-links nav navbar-nav navbar-right">
                  <li><a href="../aws.html">AWS</a></li>
                  <li><a href="../linux.html">Linux</a></li>
                  <li><a href="../docker.html">Docker</a></li>
                  <li><a href="../ibmpower.html">IBM Power</a></li>
                  <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="sidebar-overlay"></div>

<aside class="sidebar" role="navigation">
  <div class="sidebar-header">
    <svg version="1.0" xmlns="http://www.w3.org/2000/svg" width="135.000000pt" height="42" viewbox="0 0 135.000000 45.000000" preserveaspectratio="xMidYMid meet">

<g transform="translate(0.000000,45.000000) scale(0.100000,-0.100000)" fill="#000000" stroke="none">
<path class="text" fill="#000000" d="M527 384 c-4 -4 -7 -18 -7 -31 0 -21 4 -24 28 -21 21 2 27 8 27 28 0
18 -6 26 -20 28 -12 2 -24 0 -28 -4z"></path>
<path class="text" fill="#000000" d="M1080 335 c0 -48 2 -55 20 -55 18 0 20 7 20 55 0 48 -2 55 -20 55
-18 0 -20 -7 -20 -55z"></path>
<path class="text" fill="#000000" d="M54 357 c-2 -7 -3 -69 -2 -138 l3 -124 65 -3 c90 -3 130 20 130 77 0
29 -6 44 -20 54 -20 14 -20 15 -3 45 14 25 15 34 5 56 -6 15 -23 31 -38 36
-38 15 -134 13 -140 -3z m110 -33 c19 -7 21 -45 4 -62 -7 -7 -22 -12 -35 -12
-20 0 -23 5 -23 40 0 41 13 50 54 34z m20 -130 c19 -18 19 -20 6 -45 -8 -13
-21 -19 -45 -19 -34 0 -35 1 -35 40 0 38 2 40 29 40 16 0 37 -7 45 -16z"></path>
<path class="text" fill="#000000" d="M319 271 c-23 -24 -29 -38 -29 -74 0 -25 3 -53 6 -62 20 -50 164 -64
164 -15 0 15 -7 17 -45 12 -46 -5 -75 8 -75 34 0 11 16 14 65 14 l65 0 0 35
c0 80 -92 114 -151 56z m99 -33 c3 -15 -4 -18 -37 -18 -43 0 -50 7 -29 28 18
18 62 11 66 -10z"></path>
<path class="text" fill="#000000" d="M520 184 c0 -107 -1 -116 -20 -121 -11 -3 -20 -12 -20 -19 0 -21 76
-19 84 2 3 9 6 69 6 135 l0 119 -25 0 -25 0 0 -116z"></path>
<path class="text" fill="#000000" d="M649 271 c-24 -25 -29 -37 -29 -78 1 -70 34 -103 105 -103 55 0 95
44 95 103 0 60 -7 75 -41 92 -47 25 -96 19 -130 -14z m111 -30 c25 -48 2 -111
-40 -111 -45 0 -69 80 -34 114 21 22 61 20 74 -3z"></path>
<path class="text" fill="#000000" d="M850 287 c0 -8 16 -57 36 -110 28 -76 33 -100 25 -116 -16 -28 -14
-31 18 -31 27 0 29 4 70 123 23 67 41 128 41 135 0 6 -11 12 -24 12 -21 0 -26
-8 -41 -65 -10 -36 -21 -68 -25 -70 -3 -2 -16 27 -29 66 -19 60 -25 69 -47 69
-13 0 -24 -6 -24 -13z"></path>
<path class="text" fill="#000000" d="M1192 284 c-17 -12 -22 -24 -20 -47 2 -26 10 -36 45 -52 49 -23 59
-55 17 -55 -14 0 -34 5 -45 10 -16 9 -19 7 -19 -13 0 -17 7 -27 23 -31 69 -18
117 6 117 60 0 32 -4 37 -45 55 -60 26 -62 54 -4 46 37 -5 40 -3 37 16 -2 18
-11 23 -43 25 -25 2 -49 -3 -63 -14z"></path>
</g>
</svg>

  </div>

  <ul class="nav sidebar-nav">
    <li><a href="../aws.html">AWS</a></li>
    <li><a href="../linux.html">Linux</a></li>
    <li><a href="../docker.html">Docker</a></li>
    <li><a href="../ibmpower.html">IBM Power</a></li>
    <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
  </ul>
</aside>


    <div class="container">
  <div class="row">
    <div id="docs-sidebar" class="col-sm-3 col-md-3 col-xs-12 hidden-print" role="complementary">
      <ul class="nav docs-sidenav">
      <li><a href="blogsataws1.html">Blogs @ AWS</a></li>
      <li><a href="whatsnew1.html">What's New @ AWS</a></li>
      <li class="active"><a href="mgmtblogs1.html">Management Tools Blogs @ AWS</a></li>
      <li><a href="computeblogs1.html">Compute Blogs @ AWS</a></li>
      <li><a href="devopsblogs1.html">DevOps Blogs @ AWS</a></li>
      <li><a href="secblogs1.html">Security Blogs @ AWS</a></li>
      <li><a href="apnblogs1.html">APN Blogs @ AWS</a></li>

    </ul>
    </div>

    <div id="inner" class="col-sm-9 col-md-9 col-xs-12" role="main">
<p></p>
<p><i>Contents of this page is copied directly from AWS blog sites to make it Kindle friendly. Some styles & sections from these pages are removed to render this properly in 'Article Mode' of Kindle e-Reader browser. All the contents of this page is property of AWS.</i></p>
<p><a href="mgmtblogs1.html">Page 1</a>|<a href="mgmtblogs2.html">Page 2</a>|<a href="mgmtblogs3.html">Page 3</a>|<a href="mgmtblogs4.html">Page 4</a></p>
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/02/Screen-Shot-2018-01-02-at-1.27.34-PM-1243x630.png" /> 
<b class="lb-b blog-post-title" property="name headline">Control AWS resources available to your users using AWS Service Catalog</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Kanchan Waikar</span></span> | on 
<time property="datePublished" datetime="2018-01-12T09:32:08+00:00">12 JAN 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-service-catalog/" title="View all posts in AWS Service Catalog*"><span property="articleSection">AWS Service Catalog*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>The <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege" target="_blank" rel="noopener noreferrer">grant least privilege</a> best practice advises you to grant only the permissions that are required to perform a task. To follow this best practice you should determine what your users need to do and then design <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html" target="_blank" rel="noopener noreferrer">IAM policies</a> that let users perform&nbsp;only&nbsp;those tasks. <a href="https://aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">AWS Service Catalog</a> extends the very same best practice. If you are an administrator, it lets you specify the AWS CloudFormation template that your users can launch, while restricting the permissions required to create individual services. If you are an end user of the AWS Service Catalog, you don’t need to worry whether you have enough permissions to launch the stack or whether you are following your organization’s tagging strategy correctly.</p> 
<p>In the first part of this two-part series of blog posts, I explain how you can manage the AWS resources your users can create using AWS Service Catalog. In the <a href="https://aws.amazon.com/blogs/mt/using-aws-lambda-to-decommission-products-provisioned-from-an-aws-service-catalog-portfolio/" target="_blank" rel="noopener noreferrer">second blog post</a>, I show how you can use AWS Lambda to decommission all of the resources that have been provisioned from an AWS Service Catalog portfolio.</p> 
<p><span id="more-2373"></span></p> 
<p>Before I get into how it works, let’s first review a few key AWS Service Catalog concepts:</p> 
<li>A <strong><a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-product" target="_blank" rel="noopener noreferrer">product</a></strong> is an IT service that you want to make available for deployment on AWS. You create a product by importing a <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-concepts.html#w2ab2b5c15b7" target="_blank" rel="noopener noreferrer">CloudFormation template</a>.</li> 
<li>A&nbsp;<strong><a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-provprod" target="_blank" rel="noopener noreferrer">provisioned product</a></strong>&nbsp;is a <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacks.html" target="_blank" rel="noopener noreferrer">CloudFormation stack</a>. When an <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-users" target="_blank" rel="noopener noreferrer">end user</a> launches a product, the AWS Service Catalog provisions the product in form of a CloudFormation stack.</li> 
<li>A&nbsp;<strong><a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-portfolio" target="_blank" rel="noopener noreferrer">portfolio</a></strong>&nbsp;is a collection of&nbsp;<strong>products</strong>, together with the configuration information. You can use portfolios to manage the user access to specific products.</li> 
<li><strong><a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-constraints" target="_blank" rel="noopener noreferrer">Constraints</a>&nbsp;</strong>control the way users can deploy a product. With <strong>launch constraints</strong>, you can specify a role that the AWS Service Catalog can assume to launch a product from the portfolio.</li> 
<p>Now I’ll walk you through a use case to show you how you can control the AWS resources your users can create. In this use case, you want to conduct a training of a web-based product, say WordPress, using your training AWS account. (WordPress is an open-source blogging tool and content management system (CMS) based on PHP and MySQL.) On the day of the training, attendees sign into your training AWS account using <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_identity-management.html#intro-identity-users" target="_blank" rel="noopener noreferrer">IAM user</a> credentials that you created, and create their own AWS CloudFormation stack using a WordPress CloudFormation template. The stack outputs a WordPress endpoint that they use during the training. Here are two of the many things that could go wrong due to a single malicious attendee:</p> 
<li>The user might tweak the CloudFormation template to do unauthorized actions like: 
<li>Launching a high-end EC2 instance.</li> 
<li>Launching an EC2 instance with a wide-open security group.</li> 
<li>Something else!</li> 
</ul> </li> 
<li>The user might never tag any of the resources they created leaving you a number of unknown resources to manually discover and clean-up after the training.</li> 
<p>A simple solution to mitigate these problems is not to give CloudFormation template creation access to the IAM users. Instead, you will use AWS Service Catalog to manage the resource provisioning.</p> 
<b>Solution</b> 
<p>Here are the steps you need to go through to let attendees launch a stack using your predefined CloudFormation template:</p> 
<ol> 
<li>First, <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/getstarted-portfolio.html" target="_blank" rel="noopener noreferrer">create a portfolio</a>.</li> 
<li>Then, upload WordPress product to the portfolio created. The product contains the WordPress CloudFormation template and configuration information.</li> 
<li>Next, create an AWS Service Catalog IAM role (role) and associate it with the WordPress product, as a launch constraint. Later, when users provision the product, AWS Service Catalog will assume the role created.</li> 
<li>Next, <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/getstarted-iamenduser.html" target="_blank" rel="noopener noreferrer">grant the training attendees access</a> to the portfolio that you created.</li> 
<li>Then verify whether training users can provision the product. During the training, users can use the AWS Service Catalog to provision the WordPress product.</li> 
<li>After the training is over, you terminate all provisioned products from the portfolio.</li> 
</ol> 
<p>The following solution diagram illustrates how you can set up a training using AWS Service Catalog.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/02/Screen-Shot-2018-01-02-at-1.27.34-PM.png" /><br /> 
<!--more--></p> 
<p><strong>Note</strong></p> 
<p>Before you begin, you need to do the following:</p> 
<ol> 
<li>Create an IAM user group for the training attendees. To do so, <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html" target="_blank" rel="noopener noreferrer">create one or more IAM users</a> (for distributing to training attendees) in your AWS account and assign no permissions to them. Next, <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_create.html" target="_blank" rel="noopener noreferrer">create an IAM User Group</a> with a group name as <code>Training-Users</code> and then <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_manage_add-remove-users.html" target="_blank" rel="noopener noreferrer">add IAM users</a> created to the <code>Training-Users</code> group.</li> 
<li>Verify that <a href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/" target="_blank" rel="noopener noreferrer">AWS Service Catalog is supported in the AWS Region</a>.</li> 
<li>Verify that you have: 
<li><a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/getstarted-iamadmin.html" target="_blank" rel="noopener noreferrer">AWS Service Catalog Administrator access</a>.</li> 
<li>IAM Permissions to create a new IAM role.</li> 
</ul> </li> 
</ol> 
<p>Then, you need to create an AWS Service Catalog portfolio. You will use the portfolio to provide training attendees an access to the WordPress product.</p> 
<h3>Step 1. Create a portfolio</h3> 
<p>For this use case, you can create a single portfolio with a Training Portfolio name. You will later grant training attendees access to this portfolio. To create a portfolio:</p> 
<ol> 
<li>Sign in to the AWS Management Console and then open&nbsp;<a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a>.</li> 
<li>If you are using the AWS Service Catalog administrator console for the first time, choose&nbsp;<strong>Get started</strong>&nbsp;to start the wizard for configuring a portfolio. Otherwise, choose&nbsp;<strong>Create </strong>portfolio.</li> 
<li>Enter the following values: 
<ol> 
<li><strong>Portfolio name&nbsp;</strong>–&nbsp;<code>Training Portfolio</code></li> 
<li><strong>Description</strong>&nbsp;–&nbsp;<code>Portfolio for conducting a WordPress training.</code></li> 
<li><strong>Owner</strong>&nbsp;–&nbsp;<code>Training Team</code></li> 
</ol> </li> 
<li>Choose&nbsp;<strong>Create</strong>.</li> 
</ol> 
<p>Next, associate the <code>Portfolio-Name</code> <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoptions-using.html" target="_blank" rel="noopener noreferrer">tagOption</a> with the portfolio. This will cause AWS Service Catalog to attach <code>Portfolio-Name</code> tag to all resources created during provisioning of the product from the portfolio. If you are a first-time user, ensure that you have <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoptions-migrate.html" target="_blank" rel="noopener noreferrer">migrated to the TagOption Library</a>. To associate a tag:</p> 
<ol> 
<li>Open&nbsp;<a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a>.</li> 
<li>From the <strong>Service Catalog</strong> drop-down menu, choose&nbsp;<strong>TagOption library</strong>.</li> 
<li>Choose&nbsp;<strong>Create new TagOption</strong>.</li> 
<li>Specify <strong>key</strong> as <code>Portfolio-Name</code>, <strong>value</strong> as <code>Training Portfolio</code>, choose <strong>Save</strong>.</li> 
<li>From the <strong>Service Catalog</strong> drop-down menu, choose <strong>Portfolios list</strong>.</li> 
<li>Select <code>Training Portfolio</code>, expand <strong>TagOptions</strong>, choose <strong>Add TagOption</strong>.</li> 
<li>Select the entry with the <strong>key</strong> as <code>Portfolio-Name</code>, <strong>value</strong> as <code>Training Portfolio</code>, choose <strong>Save</strong>.</li> 
</ol> 
<p>Next, you need to upload the WordPress product to the portfolio created.</p> 
<h3>Step 2. Upload the WordPress product in the portfolio</h3> 
<p>In this step, you upload the CloudFormation Template you want your users to launch, in the form of a product. To upload a product:</p> 
<ol> 
<li>Open&nbsp;<a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a>.</li> 
<li>Click on <code>Training Portfolio</code> to open the portfolio details page, and then choose&nbsp;<strong>Upload new product</strong>.</li> 
<li>On the&nbsp;<strong>Enter product details</strong>&nbsp;page, type the following and then choose&nbsp;<strong>Next</strong>: 
<li><strong>Product name</strong>&nbsp;–&nbsp;<code>WordPress</code></li> 
<li><strong>Description</strong>&nbsp;–&nbsp;<code>This product launches WordPress CloudFormation Stack.</code></li> 
<li><strong>Provided by</strong>&nbsp;–&nbsp;<code>Training Team</code></li> 
</ul> </li> 
<li>On the&nbsp;<strong>Enter support details</strong>&nbsp;page, choose&nbsp;<strong>Next</strong>.</li> 
<li>On the&nbsp;<strong>Version details</strong>&nbsp;page, choose&nbsp;<strong>Specify a URL location for an Amazon CloudFormation template</strong>, type&nbsp;<a href="https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/WordPress_Single_Instance.template" target="_blank" rel="noopener noreferrer">https://s3-us-west-2.amazonaws.com/cloudformation-templates-us-west-2/WordPress_Single_Instance.template</a> in the text box. Specify: 
<li><strong>Version title</strong>&nbsp;–&nbsp;<code>v1.0</code></li> 
<li><strong>Description</strong>&nbsp;–&nbsp;<code>Base Version</code></li> 
</ul> </li> 
<li>Choose&nbsp;<strong>Next</strong>.</li> 
<li>On the&nbsp;<strong>Review</strong>&nbsp;page, choose&nbsp;<strong>Create</strong>.</li> 
</ol> 
<p>Next, you’ll create an IAM role that AWS Service Catalog can assume to allow training attendees to launch a product.</p> 
<h3>Step 3. Create a Service Catalog IAM role and associate it with the WordPress product as a launch constraint</h3> 
<p>Create a role that will have permissions to create a CloudFormation stack as well as to create all resources created by the WordPress CloudFormation template. To create the IAM role:</p> 
<ol> 
<li>Open the IAM console at&nbsp;<a href="https://console.aws.amazon.com/iam/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/iam/</a>.</li> 
<li>In the navigation pane, choose&nbsp;<strong>Policies</strong>. Choose&nbsp;<strong>Create policy</strong>&nbsp;and do the following: 
<ol> 
<li>Choose <strong>JSON</strong> Tab.</li> 
<li>Paste the following policy in the editor: <code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{ 
&quot;Sid&quot;: &quot;LaunchWordPressRole&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;ec2:AuthorizeSecurityGroupEgress&quot;,
&quot;ec2:AuthorizeSecurityGroupIngress&quot;,
&quot;ec2:DeleteSecurityGroup&quot;,
&quot;ec2:TerminateInstances&quot;, 
&quot;ec2:CreateSecurityGroup&quot;, 
&quot;ec2:RunInstances&quot;, 
&quot;ec2:createTags&quot;, 
&quot;catalog-user:*&quot;, 
&quot;cloudformation:CreateStack&quot;, 
&quot;cloudformation:DeleteStack&quot;, 
&quot;cloudformation:DescribeStackEvents&quot;, 
&quot;cloudformation:DescribeStacks&quot;, 
&quot;cloudformation:GetTemplateSummary&quot;, 
&quot;cloudformation:SetStackPolicy&quot;, 
&quot;cloudformation:ValidateTemplate&quot;, 
&quot;cloudformation:UpdateStack&quot;, 
&quot;ec2:describe*&quot;, 
&quot;ec2:get*&quot;, 
&quot;s3:GetObject&quot;, 
&quot;sns:*&quot;
], 
&quot;Resource&quot;: [ 
&quot;*&quot; 
] 
} 
] 
}</code> </li> 
<li>Choose&nbsp;<strong>Review Policy</strong>.</li> 
<li>Provide a name, such as <code>WordPressPolicy</code>, choose <strong>Create Policy</strong>.</li> 
</ol> </li> 
<li>In the navigation pane, choose&nbsp;<strong>Roles</strong>. Choose&nbsp;<strong>Create role</strong>&nbsp;and do the following: 
<ol> 
<li>Choose <strong>AWS service</strong>&nbsp;and then choose&nbsp;<strong>Service Catalog</strong>. Next, choose&nbsp;<strong>Next: Permissions</strong>.</li> 
<li>In Filter panel, type <code>WordPressPolicy</code>.</li> 
<li>Select the check box for the&nbsp;<code>WordPressPolicy</code> policy, and then choose&nbsp;<strong>Next: Review</strong>.</li> 
<li>For&nbsp;<strong>Role name</strong>, type&nbsp;<code>WordPressServiceCatalogLaunchRole</code>.</li> 
<li>Choose&nbsp;<strong>Create role</strong>.</li> 
<li>In the filter panel, type <code>WordPressServiceCatalogLaunchRole</code>.</li> 
<li>Click on the result with <code>WordPressServiceCatalogLaunchRole</code> as the role name.</li> 
<li>Note the <strong>Role ARN</strong> displayed on the summary page.</li> 
</ol> </li> 
</ol> 
<p>Next, you need to associate the IAM role with the product. This allows AWS Service Catalog to assume the role to launch the product. To associate the IAM role with the product:</p> 
<ol> 
<li>Open the AWS Service Catalog console at&nbsp;<a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a>.</li> 
<li>Choose the <code>Training Portfolio.</code></li> 
<li>On the portfolio details page, expand the&nbsp;<strong>Constraints</strong>&nbsp;section, and then choose&nbsp;<strong>Add constraints</strong>.</li> 
<li>For&nbsp;Product, choose&nbsp;WordPress, and for&nbsp;<strong>Constraint type</strong>, choose&nbsp;<strong>Launch</strong>. Next, choose&nbsp;<strong>Continue</strong>.</li> 
<li>On the&nbsp;<strong>Launch constraint</strong>&nbsp;page, for&nbsp;Enter<strong> role </strong>arn, type the role ARN your noted earlier, and then choose&nbsp;<strong>Submit</strong>.</li> 
</ol> 
<p>Next, you need to allow the <code>Training-Users</code> IAM Group to provision a product.</p> 
<h3>Step 4. Grant the training attendees permission to provision a product from the portfolio</h3> 
<p>Since users will be using AWS Service Catalog to provision a CloudFormation Stack, all they need is access to the AWS Service Catalog and the <strong>Cloudformation:ListStackResources</strong> access. In this step, you give users access:</p> 
<ol> 
<li>Open the IAM console at&nbsp;<a href="https://console.aws.amazon.com/iam/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/iam/</a>.</li> 
<li>In the navigation pane, choose&nbsp;<strong>Groups</strong>.</li> 
<li>In the filter panel, type <code>Training-Users</code>, the search result will display a single result, Choose the result.</li> 
<li>In the summary page, choose <strong>permissions</strong> tab.</li> 
<li>Expand inline policies. Console will display a message stating there are no inline policies associated. Choose <strong>click here</strong>.</li> 
<li>Choose <strong>Custom Policy</strong>, then&nbsp;<strong>Select</strong>. 
<ol> 
<li>For&nbsp;<strong>Policy Name</strong>, type&nbsp;<code>SCProvisionProductAccess</code>.</li> 
<li>Paste the following example policy in&nbsp;<strong>Policy Document</strong>: <code class="lang-json">{ 
&quot;Version&quot;: &quot;2012-10-17&quot;, 
&quot;Statement&quot;: [ 
{ 
&quot;Effect&quot;: &quot;Allow&quot;, 
&quot;Action&quot;: [&quot;cloudformation:ListStackResources&quot;, &quot;servicecatalog:ProvisionProduct&quot;], 
&quot;Resource&quot;: &quot;*&quot; 
} 
] 
}</code> </li> 
</ol> </li> 
<li>Choose <strong>Apply Policy</strong>.</li> 
<li>On the <strong>Summary</strong> page, <strong>Permissions</strong> tab, <strong>Managed Policies</strong> section, choose <strong>Attach Policy</strong>.</li> 
<li>In the Filter panel, type <strong>ServiceCatalogEndUserAccess</strong>, the search result will display a single result, choose the result.</li> 
<li>Choose <strong>Attach Policy</strong>.</li> 
</ol> 
<p>Finally, you need to allow <code>Training-Users</code> IAM Group to access the <code>Training Portfolio</code>. To do so:</p> 
<ol> 
<li>Open <a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a> and choose the <code>Training Portfolio</code>.</li> 
<li>On the portfolio details page, expand the&nbsp;<strong>Users, groups and roles&nbsp;</strong>section.</li> 
<li>Choose&nbsp;<strong>Add user, group or role</strong>.</li> 
<li>On the&nbsp;<strong>Groups</strong>&nbsp;tab, select the check box for&nbsp;<code>Training-Users</code>.</li> 
<li>Choose&nbsp;<strong>Add Access</strong>.</li> 
</ol> 
<h3>Step 5. Verification</h3> 
<p>Before you give the account credentials to training attendees, verify whether training users can provision the product.</p> 
<h4><strong>Note</strong></h4> 
<p>The WordPress template requires you to have at least one <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html" target="_blank" rel="noopener noreferrer">EC2 key pair</a> present in your selected AWS Region. For more information about how to create a key pair, see <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair" target="_blank" rel="noopener noreferrer">Creating a Key Pair Using Amazon EC2</a>.</p> 
<p>To verify that the training attendees can launch the <code>WordPress</code> product:</p> 
<ol> 
<li>Sign in to the AWS Management Console with the credentials of any IAM user you added to the <code>Training-Users</code> IAM group. Next, open the AWS Service Catalog console at&nbsp;<a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a>.</li> 
<li>In the&nbsp;<strong>Products</strong>&nbsp;section of the console, choose&nbsp;<code>WordPress</code> and then choose&nbsp;<strong>Launch product</strong>.</li> 
<li>On the&nbsp;<strong>Product version</strong>&nbsp;page, for&nbsp;<strong>Name</strong>, type&nbsp;<code>WordPress</code>.</li> 
<li>In the&nbsp;<strong>Version</strong>&nbsp;table, choose&nbsp;<code>v1.0</code></li> 
<li>Choose&nbsp;<strong>Next</strong>.</li> 
<li>On the&nbsp;<strong>Parameters</strong>&nbsp;page, type the following and choose&nbsp;<strong>Next</strong>: 
<li><strong>KeyName</strong> – Select the KeyName you have access to.</li> 
<li><strong>SSHlocation</strong> – Type a valid CIDR range for the IP address from which you will connect to the instance. This can be the default value (0.0.0.0/0) to allow access from any IP address, your IP address followed by&nbsp;/32&nbsp;to restrict access to your IP address only, or something in between.</li> 
<li><strong>DBPassword, DBUser, DBRootPassword</strong> – Provide alphanumeric characters of your choice.</li> 
<li><strong>InstanceType</strong> – <code>t2.micro</code></li> 
</ul> </li> 
<li>On the <strong>TagOptions</strong> page, choose <strong>Next</strong>.</li> 
<li>On the <strong>Notifications</strong> page, choose <strong>Next</strong>.</li> 
<li>On the&nbsp;<strong>Review</strong>&nbsp;page, review the information that you typed, and then choose&nbsp;<strong>Launch</strong>&nbsp;to launch the stack. The initial status of the product is shown as&nbsp;<strong>Under Change</strong>. After the product is launched, the status becomes <strong>Available</strong>.</li> 
</ol> 
<p>Your environment is ready. You can now distribute the user credentials that you created in <strong>step 1</strong> to all training attendees.</p> 
<h3>Training starts</h3> 
<p>Attendees will be able to provision the WordPress website using the AWS Service Catalog console, but they will not be able launch an EC2 instance or execute any other CloudFormation template from the AWS Management Console. After the training is over, you can easily clean up all resources created by attendees. To see how you can clean up all products attendees provisioned during the training, see <a href="https://aws.amazon.com/blogs/mt/using-aws-lambda-to-decommission-products-provisioned-from-an-aws-service-catalog-portfolio/" target="_blank" rel="noopener noreferrer">Part 2</a> of this series of blog posts.</p> 
<b>Summary</b> 
<p>This blog-post is Part one of a two-part series of blog posts. In Part one I show you how you can use AWS Service Catalog to allow users to launch CloudFormation stacks in your environment, in a controlled manner. AWS Service Catalog also allows you to define template constraints if you have a requirement of allowing users to specify only certain CloudFormation input parameters.</p> 
<p>To see how you can clean up all products provisioned during the training, using AWS Lambda, see <a href="https://aws.amazon.com/blogs/mt/using-aws-lambda-to-decommission-products-provisioned-from-an-aws-service-catalog-portfolio/" target="_blank" rel="noopener noreferrer">Part two</a> of this two-part series of blog posts.</p> 
<p>If you have questions about implementing the solution described in this post, start a new thread on the&nbsp;<a href="https://forums.aws.amazon.com/forum.jspa?forumID=198" target="_blank" rel="noopener noreferrer">AWS Service Catalog Forum</a> or <a href="https://console.aws.amazon.com/support/home" target="_blank" rel="noopener noreferrer">contact AWS Support</a>.</p> 
<h3>About the Author</h3> 
<p><img width="100%" src="https://internal-cdn.amazon.com/badgephotos.amazon.com/?uid=kwwaikar" /> Kanchan Waikar is an AWS Marketplace Solutions Architect at Amazon Web Services. She enjoys helping customers build architectures using AWS, AWS Marketplace products, and AWS Service Catalog.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using AWS Lambda to decommission products provisioned from an AWS Service Catalog portfolio</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Kanchan Waikar</span></span> | on 
<time property="datePublished" datetime="2018-01-12T09:30:11+00:00">12 JAN 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-service-catalog/" title="View all posts in AWS Service Catalog*"><span property="articleSection">AWS Service Catalog*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/using-aws-lambda-to-decommission-products-provisioned-from-an-aws-service-catalog-portfolio/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>This blog-post is part two in a <a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" target="_blank" rel="noopener noreferrer">two-part series</a> of blog posts. <a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" target="_blank" rel="noopener noreferrer">Part one</a> shows you how to use <a href="https://aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">AWS Service Catalog</a> to control AWS resources available to your users. Part two shows you how you can use <a href="https://aws.amazon.com/lambda/" target="_blank" rel="noopener noreferrer">AWS Lambda</a> to decommission all products provisioned from any product of a <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-portfolio" target="_blank" rel="noopener noreferrer">Portfolio</a>.</p> 
<p>Sometimes you might have a business need to terminate all <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-provprod" target="_blank" rel="noopener noreferrer">products provisioned</a> in your AWS account from a specific portfolio. For example, you might want to decommission an old portfolio of products or terminate resources provisioned by your temporary users, as described in the <a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" target="_blank" rel="noopener noreferrer">first blog post</a>. As an <a href="http://docs.aws.amazon.com/servicecatalog/latest/adminguide/what-is_concepts.html#what-is_concepts-users" target="_blank" rel="noopener noreferrer">AWS Service Catalog Administrator</a>, going through the AWS Service Catalog console and figuring out which products to terminate can be a daunting task if you are managing many portfolios. In this blog post, I share a sample Lambda function that you can use to terminate all products provisioned from a specific portfolio.</p> 
<p>The solution explained in this post finds all products of the portfolio, and then for each product, it finds all provisioned products. Finally, it terminates all provisioned products found.</p> 
<p><span id="more-2384"></span></p> 
<p>Before you begin, you need to find out the portfolio-ID of the portfolio where you want to terminate all provisioned products.</p> 
<h3>Find the portfolio ID</h3> 
<ol> 
<li>Open the AWS Service Catalog console at&nbsp;<a href="https://console.aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">https://console.aws.amazon.com/servicecatalog/</a>.</li> 
<li>Choose the&nbsp;portfolio you want to decommission provisioned products from. In this <a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" target="_blank" rel="noopener noreferrer">blog post series</a>, we choose <code>Training Portfolio</code>.</li> 
<li>On the portfolio details page, locate the <strong>Portfolio ID</strong> and make note of it for later use.</li> 
</ol> 
<p>Next, you need to set up the Lambda function which will terminate provisioned products from your portfolio.</p> 
<h3>Deploy the TerminateProvisionedProducts Lambda function</h3> 
<p>To terminate all products provisioned from a portfolio, you need to set up the <code>TerminateProvisionedProducts</code> Lambda function. You can set up the Lambda function by running a CloudFormation template.</p> 
<ol> 
<li>Sign in to the AWS Management Console and go to the AWS CloudFormation console.</li> 
<li>Next, launch the CloudFormation stack by choosing the following <strong>Launch Stack</strong> link.<br /> <a href="https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=TerminateSCProductsProvisioned&amp;templateURL=https://s3-us-west-2.amazonaws.com/management-tools-blog-cf-template-storage/Blog2SetupCFT-ver3.json" target="_blank" rel="noopener noreferrer"><img width="100%" src="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/cloudformation-launch-stack-button.png" /></a></li> 
<li>Choose <strong>Next</strong>.</li> 
<li>On the <strong>Specify Details</strong> page, leave Stack Name as <code>TerminateSCProductsProvisioned</code> and&nbsp;type PortfolioID as the <code>Portfolio ID </code>you noted in the preceding section.</li> 
<li>Choose <strong>Next</strong>.</li> 
<li>On the <strong>Options</strong> page, choose <strong>Next</strong>.</li> 
<li>On the <strong>Review</strong> page, <strong>select</strong> the <strong>check-box</strong> displayed next to the following message. 
<li><em><strong>I acknowledge that AWS CloudFormation might create IAM resources.</strong></em></li> 
</ul> </li> 
<li>Choose <strong>Create</strong>. The CloudFormation template creates the Lambda function and an <a href="https://aws.amazon.com/iam/" target="_blank" rel="noopener noreferrer">AWS Identity and Access Management</a> (IAM) role to run the Lambda function.</li> 
<li>On the <strong>Stacks</strong> page, select <code>TerminateSCProductsProvisioned</code> stack.</li> 
<li>In the detail pane, choose <strong>Outputs</strong>&nbsp;to view the output of your stack.</li> 
</ol> 
<p>After CloudFormation successfully creates a stack, the <strong>Outputs</strong> tab displays the following result:</p> 
<li><code>TerminateProvisionedProductsLambdaFunction</code>: Value displays the name of the <code>TerminateProvisionedProducts</code> Lambda function. Note the same.</li> 
<p>You will run the <code>TerminateProvisionedProducts</code> Lambda function to decommission products provisioned from your portfolio.</p> 
<h3>Run the Lambda function</h3> 
<p>Next, run the <code>TerminateProvisionedProducts</code> Lambda function. This Lambda function finds all products of the portfolio, and then for each product, it finds all provisioned products. Finally, it terminates all provisioned products found.</p> 
<h4>Warning</h4> 
<p>Do not run this Lambda function if you are sharing a product between portfolios. For each product in the portfolio, the Lambda function finds and terminates all provisioned products. Running this function on a portfolio that contains a product shared with another portfolio can lead to unexpected results such as inadvertently deleting the product provisioned from other portfolios.</p> 
<p>To run the Lambda function do the following.</p> 
<ol> 
<li>Sign in to the AWS Management Console and select&nbsp;<strong>Lambda</strong> in the&nbsp;<strong>Services</strong>&nbsp;menu.</li> 
<li>Select <strong>Functions</strong> from the navigation pane.</li> 
<li>In the <strong>Functions</strong> pane, choose the name of the <code>TerminateProvisionedProducts</code> Lambda function you noted earlier.</li> 
<li>Choose the <strong>Select a test event</strong> drop-down list then select <strong>Configure test events</strong>.</li> 
<li>On the <strong>Configure test event</strong> page, select <strong>Create new test event</strong>, specify the event name as <code>TerminateSCProducts</code>.</li> 
<li>Leave the text in the editor as it is. The Lambda function does not require any input.</li> 
<li>Choose <strong>Create</strong>.</li> 
<li>Next, choose <strong>Test</strong>.</li> 
</ol> 
<p>After the function executes, you will see the output containing a list of terminated provisioned products.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/11/Screen-Shot-2018-01-11-at-9.40.05-AM.png" /></p> 
<p>The AWS Service Catalog uses the role specified in the launch constraint to terminate the provisioned product. The Lambda function may not be able to clean-up resources if launch constraints are not properly configured and you would need to discover all resources that did not get cleaned up.</p> 
<h3>Discover resources that did not get cleaned-up</h3> 
<p>In this step, you will use <a href="http://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/resource-groups.html" target="_blank" rel="noopener noreferrer">Resource Groups</a> to discover the resources associated with the portfolio decommissioned in the previous section. In this blog post series (see <a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" target="_blank" rel="noopener noreferrer">Part 1</a>), we use <code>Portfolio-Name</code> as the <strong>Key</strong> and <code>Training Portfolio</code> as the <strong>Value</strong>. If you are cleaning-up some other portfolio, then use tag-option you created on the portfolio you decommissioned in the previous section.</p> 
<ol> 
<li>Sign in to the AWS Management Console. On the navigation bar, choose&nbsp;<strong>Resource Groups</strong>, and then under Classic groups, choose&nbsp;<strong>Create a classic Group</strong>.</li> 
<li>On the&nbsp;<strong>Create a resource group</strong>&nbsp;page, for&nbsp;<strong>Group name</strong>, type <code>Training Portfolio Resources</code>.</li> 
<li>For&nbsp;<strong>Tags</strong>, choose an appropriate key in the first box. In the box, next to the first box, select an appropriate value.</li> 
<li>Choose <strong>Save</strong>.</li> 
<li>The console will list all the resources associated with the tag you provided. Review the items displayed in the navigation pane.</li> 
</ol> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/02/img2-1024x466.png" /></p> 
<p>AWS Service Catalog associated the tag you specified during portfolio creation to all resources provisioned, this means that you can see the resources that did not get cleaned up with the Lambda function in the navigation pane. If you see any resources, clean them up manually.</p> 
<p>Congratulations! You have successfully decommissioned all products provisioned from your portfolio.</p> 
<h3>Summary</h3> 
<p>This blog-post is Part two of a <a href="https://aws.amazon.com/blogs/mt/control-aws-resources-available-to-your-users-using-aws-service-catalog/" target="_blank" rel="noopener noreferrer">two-part series</a> of blog posts. It shows how you can use AWS Lambda to decommission all products provisioned from a portfolio. The series shows how you can use AWS Service Catalog to control what your IAM users can launch, and Part two shares a sample Lambda function that cleans up products provisioned from a portfolio.</p> 
<p>AWS Service Catalog allows you to centrally manage commonly deployed IT services, and helps you achieve consistent governance and meet your compliance requirements, while enabling you to quickly deploy only the approved IT services you need.</p> 
<p>If you have questions about implementing the solution described in this post, start a new thread on the&nbsp;<a href="https://forums.aws.amazon.com/forum.jspa?forumID=198" target="_blank" rel="noopener noreferrer">AWS Service Catalog Forum</a> or <a href="https://console.aws.amazon.com/support/home" target="_blank" rel="noopener noreferrer">contact AWS Support</a>.</p> 
<h3>About the Author</h3> 
<p><img width="100%" src="https://internal-cdn.amazon.com/badgephotos.amazon.com/?uid=kwwaikar" /> Kanchan Waikar is an AWS Marketplace Solutions Architect at Amazon Web Services. She enjoys helping customers build architectures using AWS, AWS Marketplace products, and AWS Service Catalog.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/Ha_EC2-Management-tools_SOCIAL.jpg" /> 
<b class="lb-b blog-post-title" property="name headline">Enable Modular and Reusable Configuration Using Composite AWS Systems Manager Documents</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Ananth Vaidyanathan</span></span> | on 
<time property="datePublished" datetime="2018-01-11T09:53:33+00:00">11 JAN 2018</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/enable-modular-and-reusable-configuration-using-composite-aws-systems-manager-documents/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>By Melonia Mendonca, Software Development Engineer at Amazon Web Services</em></p> 
<p>AWS Systems Manager (SSM) documents enable infrastructure as code that allows you to configure, manage, and automate your AWS and on-premises resources using AWS Systems Manager services. These SSM Documents define the actions that you want to perform on managed instances. Systems Manager offers a variety of pre-defined public documents and provides the ability to customize the documents as well.</p> 
<p>Systems Manager lets you <a href="https://aws.amazon.com/about-aws/whats-new/2017/10/amazon-ec2-systems-manager-now-integrates-with-github/">execute composite documents</a> as part of your configurations. Composite documents are SSM documents that perform the task of executing one or more secondary documents. Using composite documents, you can do the following:</p> 
<li><span style="text-decoration: underline">Enable modularity</span> – Documents can be broken into small common tasks, and then they can be called from composite documents.</li> 
<li><span style="text-decoration: underline">Enable reusability</span> – Having the ability to invoke other documents removes the requirement to duplicate the plugin calls within custom documents.</li> 
<li><span style="text-decoration: underline">Store documents in remote locations</span>– GitHub and Amazon S3 can be used to save documents because these documents in remote locations can be invoked using composite documents.</li> 
<li><span style="text-decoration: underline">Use YAML in place of JSON in documents</span> – AWS Systems Manager also allows the use of documents written in YAML format that are stored in remote locations.</li> 
<p>AWS-RunDocument is a new document you can use to execute documents that are stored in Systems Manager, private or public GitHub, or Amazon S3. This is achieved by using <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-plugins.html#aws-downloadContent">aws:downloadContent</a> and <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-plugins.html#aws-rundocument">aws:runDocument</a> plugins. The aws:runDocument plugin executes documents that reside in Systems Manager or in the local path.</p> 
<p>In this blog post, I’ll show you how to create custom composite documents using the aws:runDocument plugin. I’ll demonstrate how to execute documents created in Systems Manager by invoking them from the composite document. I’ll also walk through the method of executing documents that are in GitHub by using the AWS-RunDocument document.</p> 
<p><span id="more-2450"></span></p> 
<h3>Walkthrough 1 – Compose a custom document using SSM documents</h3> 
<p>In this example, I’ll show you how to compose a document that updates SSM Agent and then runs baseline patch and eventually configures an AWS package. Before the introduction of composite documents, this could be done one of two ways. You could send three separate commands to execute these public documents – AWS-UpdateSSMAgent, AWS-RunPatchBaseline, and AWS-ConfigureAWSPackage. &nbsp;Alternatively, you could add the plugin references in another document, but you would have to duplicate the document and not use the pre-defined, public document provided. With the use of composite documents, these documents are executed by sending a single composite document.</p> 
<p><strong>Step 1: Create the composite document</strong><br /> I’ll use the aws:runDocument plugin to compose plugins in my document. The plugin has the following parameters:</p> 
<li><span style="text-decoration: underline">documentType</span> – This indicates the type of document. The document could be locally available – LocalPath or from SSM – SSMDocument. In this example, this parameter will be SSMDocument.</li> 
<li><span style="text-decoration: underline">documentPath</span> – This is the path to the document. It is either the absolute path to the document on disk, or the name of the document if the type is SSMDocument. In this walkthrough, we will use the path to specify the names of the documents – AWS-UpdateSSMAgent, AWS-RunPatchBaseline, and AWS-ConfigureAWSPackage.</li> 
<li><span style="text-decoration: underline">documentParameters</span> – The parameters for the secondary document. This should be specified as parameter type StringMap with key and value pairs.</li> 
<p>The document appears as follows. I have named it composite-document.json. This document can be executed on Windows.</p> 
<code class="lang-json"><strong>~/tmp/docs/composite-document.json</strong> 
{
&quot;schemaVersion&quot;: &quot;2.2&quot;,
&quot;description&quot;: &quot;Compose SSM documents&quot;,
&quot;parameters&quot;: {},
&quot;mainSteps&quot;: [
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;AgentUpdate&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-UpdateSSMAgent&quot;
}
},
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;ApplyPatchBaseline&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-RunPatchBaseline&quot;,
&quot;documentParameters&quot;: &quot;{\&quot;Operation\&quot;:\&quot;Install\&quot;}&quot;
}
},
{
&quot;action&quot;:&quot;aws:runDocument&quot;,
&quot;name&quot; : &quot;ConfigureAWSPVDriver&quot;,
&quot;inputs&quot;:{
&quot;documentType&quot;:&quot;SSMDocument&quot;,
&quot;documentPath&quot;:&quot;AWS-ConfigureAWSPackage&quot;,
&quot;documentParameters&quot;: &quot;{\&quot;action\&quot;:\&quot;Install\&quot;, \&quot;name\&quot;:\&quot;AWSPVDriver\&quot;}&quot;
}
}
]
}</code> 
<p>You can navigate to the AWS Systems Manager console and paste this document to create a document. You can also do this using the <a href="http://docs.aws.amazon.com/cli/latest/reference/ssm/create-document.html">create-document</a> command in the AWS CLI:</p> 
<code class="lang-bash">aws ssm create-document --name composite-document --content file://~/tmp/docs/composite-document.json --document-type Command</code> 
<p><strong>Step 2: Run the command and check the output</strong></p> 
<p>After the command is sent, each action will be executed to produce the following output.</p> 
<p><strong><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/walkthru1-step2.png" /></strong></p> 
<p>Or you can use the <a href="http://docs.aws.amazon.com/cli/latest/reference/ssm/send-command.html">send-command</a> command from the AWS CLI. I am targeting all my Windows value instances by using tagging. To do this I created a new tag – PlatformType. For more information on using tags when you send commands, see <a href="http://C:\Users\ananva\AppData\Local\Microsoft\Windows\INetCache\Content.Outlook\WYMO4JZE\docs.aws.amazon.com\systems-manager\latest\userguide\send-commands-multiple.html">Sending Commands to a Fleet</a> in the AWS Systems Manager documentation.</p> 
<code class="lang-bash">aws ssm send-command --document-name &quot;composite-document&quot; --targets Key=tag:PlatformType,Values=Windows</code> 
<h3>Walkthrough 2 – Compose an SSM custom document using documents from GitHub</h3> 
<p>In a <a href="https://aws.amazon.com/blogs/mt/run-scripts-stored-in-private-or-public-github-repositories-using-amazon-ec2-systems-manager/">previous blog post</a>, I reviewed the method of executing scripts from GitHub. Today, I’ll demonstrate how to execute Systems Manager documents from GitHub. This method can be used to execute documents from Amazon S3, too. In this composite document, I will update SSM Agent to the latest version and then install Apache and My SQL. The document to install this software resides in GitHub and is written in YAML.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/walkthru-github.png" /></p> 
<p><strong>Step 1:&nbsp; Create composite document</strong><br /> To create a document from a remote resource like GitHub, I’ll use the pre-defined AWS-RunDocument document. This document has the following parameters to help us specify the information to download the document.</p> 
<li><span style="text-decoration: underline">Source Type</span>: Location of the script – GitHub or Amazon S3. For this example, the value here is GitHub.</li> 
<li><span style="text-decoration: underline">Source Info</span>: This parameter provides information for accessing the document. The repository mentioned earlier is public and thus, this should be the owner, repository, and the path to the document.</li> 
<li><span style="text-decoration: underline">Document Parameters</span>: If the document to be executed takes parameters, they can be specified here. In this case, it will be left blank.</li> 
<p>You can use the composite document below to install the SSM Agent, Apache and MySQL on the instance.</p> 
<code class="lang-json"><strong>~/tmp/docs/composite-document-mysql.json</strong> 
{
&quot;schemaVersion&quot;: &quot;2.2&quot;,
&quot;description&quot;: &quot;Composite documents from GitHub&quot;,
&quot;parameters&quot;: {
},
&quot;mainSteps&quot;: [
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;AgentUpdate&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-UpdateSSMAgent&quot;
}
},
{
&quot;action&quot;: &quot;aws:runDocument&quot;,
&quot;name&quot;: &quot;InstallApacheandMySQL&quot;,
&quot;inputs&quot;: {
&quot;documentType&quot;: &quot;SSMDocument&quot;,
&quot;documentPath&quot;: &quot;AWS-RunDocument&quot;,
&quot;documentParameters&quot; : &quot;{\&quot;sourceType\&quot;:\&quot;GitHub\&quot;, \&quot;sourceInfo\&quot;:\&quot;{\\\&quot;owner\\\&quot;:\\\&quot;mmendonca3\\\&quot;,\\\&quot;repository\\\&quot;:\\\&quot;dummy-test-public\\\&quot;, \\\&quot;path\\\&quot;:\\\&quot;documents/bootstrap/StateManagerBootstrap.yml\\\&quot;}\&quot;}&quot;
}
}
]
}</code> 
<p>You can create it using the AWS Systems Manager console or AWS CLI. The command to create this document would be:</p> 
<code class="lang-bash">aws ssm create-document --name composite-document-mysql --content file://~/tmp/docs/composite-document-mysql.json --document-type Command</code> 
<p><strong>Step 2: Run Command and check the output</strong><br /> On sending a command to execute this document, the agent gets updated to the latest version, downloads the document, and then executes it.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2018/01/09/walkthru2-output.png" /></p> 
<p>The command to run this is follows:</p> 
<code class="lang-bash">aws ssm send-command --document-name &quot;composite-document-mysql&quot; --targets Key=tag:PlatformType,Values=AmazonLinux</code> 
<h3>Conclusion</h3> 
<p>In this blog post, I have shown you how to enable management as code by using composite documents. You can create composite documents to reuse your documents easily, and enable modularity. Documents created using AWS Systems Manager can be referenced using composite documents. Those documents on GitHub and Amazon S3, written in JSON or YAML, can be executed using the AWS-RunDocument document.</p> 
<h3>About the Author</h3> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/11/02/melonia.jpg" />Melonia Mendonca is a Software Development Engineer with the Amazon EC2 Systems Manager team. She is a passionate engineer who enjoys the ability to innovate encouraged by Amazon. Outside of work, Melonia likes traveling, playing board games and trying different restaurants/cuisines.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Using AWS OpsWorks for Chef Automate in a federated environment</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Darko Meszaros</span></span> | on 
<time property="datePublished" datetime="2017-12-31T14:34:39+00:00">31 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-ops-works/" title="View all posts in AWS OpsWorks*"><span property="articleSection">AWS OpsWorks*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/devops/" title="View all posts in DevOps*"><span property="articleSection">DevOps*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/using-aws-opsworks-for-chef-automate-in-a-federated-environment/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>Many large enterprises operate on a federated model. That is, they are separated into different business units or organizations, with different goals, procedures, and skill sets. These enterprises typically use a system to manage their infrastructure configuration and changes.</p> 
<p>You might ask, “Can we apply a federated model to configuration management? If so, what are the pros and cons and how does AWS OpsWorks for Chef Automate fit into that model?”</p> 
<p>In this blog post we discuss how to approach configuration management in a. federated enterprise. &nbsp;The main focus is on how you can leverage <a href="https://aws.amazon.com/opsworks/chefautomate/">AWS OpsWorks for Chef Automate</a> to achieve this goal.</p> 
<p><span id="more-2257"></span></p> 
<b>Single Chef server to serve all parts of your organization</b> 
<p>Approaching configuration management in a non-federated way involves using a single Chef server across many different business units.</p> 
<p>But, I hear you say, “Different business units have their own Chef nodes, cookbooks, and environments. How do we separate those?”</p> 
<p>The way you can achieve this with Chef is to use the concept of <a href="https://docs.chef.io/server_orgs.html"><strong>Chef Organizations</strong></a>, which is a core part of the Chef Server software.</p> 
<p>Chef Organizations allows you to separate the following items among different business units, but they still reside on a single Chef server:</p> 
<li>Permissions</li> 
<li>Nodes</li> 
<li>Roles</li> 
<li>Environments</li> 
<li>Cookbooks</li> 
<li>Data bags</li> 
<p>The main benefits of this approach is that each business unit can independently manage its own nodes, access permissions, and can perform maintenance/updates on its infrastructure on its own schedule. Having this type of isolation allows the business units/organizations to have access to their own unique environments, roles, data bags, and cookbooks. Additionally, by hosting all of this on a single server you reduce the amount of effort required to set up and maintain a separate Chef server for each business unit.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/single_chef.png" /></p> 
<p style="text-align: center"><em>Single Chef Server Model</em></p> 
<b>Single Chef server per business unit</b> 
<p>When we speak about a model for a true federated approach to configuration management, we imply that each of the company’s business units/organizations has its own separate Chef server. This brings us the same benefits that we get when we use Chef Organizations, but with even greater isolation between the business units. This brings more benefits when in the cloud.</p> 
<p>“What are these new benefits you speak of?”</p> 
<p>Well, it is very common to have different business units, each with its own respective AWS Account, for all of their resources. This approach is used for billing. It gives us a clear separation of resource usage per business unit. If each business unit uses a Chef server in a centralized AWS account, billing and management of that resource can get out of hand. Another major benefit, which is not strictly cloud related, is that the maintenance of the Chef server and the potential downtime it experiences, does not cause disruptions across all organizations and their infrastructure. By having a Chef server for each business unit you can mitigate the issue of having a single point of failure. If you rely on a single Chef server throughout your enterprise, that server’s failure would cause potential down time across all business units. By using multiple Chef servers you reduce the blast radius of potential Chef server down time.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/Untitled_Diagram.png" /></p> 
<p style="text-align: center"><em>Federated model/Multi Chef Server</em></p> 
<p>Additionally, the concept of Chef Organizations can be fully utilized even in a federated model. For example, if a certain business unit wants to separate even further, it can split its own Chef Automate server into organizations. The nodes from all organizations will be visible on the Chef Automate dashboard because you can filter nodes based on Chef Organizations.</p> 
<p>“But what happens to our cookbooks? Does each business unit need to write its own?” To answer these questions, let’s take a look at <strong>Centralized Cookbook repositories.</strong></p> 
<b>Centralized Cookbook repositories</b> 
<p>A great way to make all of your configurations as consistent as possible is to have a companywide Cookbook repository. This means that teams/business units can use cookbooks created by other parts of the company to configure their infrastructure – on their respective Chef Servers. This, in turn, reduces the effort required to start using Chef as a configuration management system in a business unit because most of the required code base is already present. So, for example, let’s say that business unit A writes its own cookbook for setting up an application, and business unit B also uses the same application in their own environment. Business unit B does not have to write its own cookbook from scratch, but can use the cookbooks already created by business unit A. The main obstacle to having a shared cookbook system is properly writing cookbooks in the first place. Each cookbook must be as dynamic as possible. When you write a cookbook, leverage attributes, templates, and data-bags as much as possible so you can be flexible and help other teams use the same cookbook for, &nbsp;their use cases.</p> 
<p>“So what kind of Cookbook repos can we use?”</p> 
<h3>Git repo</h3> 
<p>The simplest approach to centrally managing your cookbooks is to use &nbsp;Git repositories. Yes, simple as that. You can use the <a href="https://docs.chef.io/berkshelf.html"><strong>Berkshelf</strong></a> tool to manage the cookbooks, but instead of pulling them from the Chef supermarket, it pulls from the Git repo. With Berkshelf, you can specify a certain cookbook version to be pulled. In addition, because Berkshelf supports specifying a repository branch, you can even select a specific branch of your cookbook repo, for example, a certain testing branch to be used in a Dev environment.</p> 
<p><em>Berksfile example:<br /> </em></p> 
<code class="lang-berks">cookbook &quot;buk-cookbook&quot;, &quot;~&gt;; 0.9.3&quot;, git: &quot;https://github.com/example/buk-cookbook.git&quot;
cookbook &quot;jinar-cookbook&quot;, &quot;~&gt;; 0.2.4&quot;, git: &quot;https://github.com/example/jinar-cookbook.git&quot;, branch: &quot;my-test-branch&quot;</code> 
<p>As mentioned before – you can have centralized Cookbook Git repositories across the entire company. Then each team could create its own repositories and push cookbooks – which could, in turn, be used by everyone in the company.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/git_mutlichef_cropped.png" /></p> 
<p style="text-align: center"><em>Common GIT repositories for cookbooks</em></p> 
<p>“But–hey – at the beginning of this blog post you mentioned that one of the benefits of using a single Chef server is not needing to configure and maintain each server separately. Doesn’t having too many Chef servers give us a lot of overhead?”</p> 
<p>Enter – <a href="https://aws.amazon.com/opsworks/chefautomate/">OpsWorks for Chef Automate</a></p> 
<b>How does OpsWorks for Chef automate fit into all of this?</b> 
<p>AWS OpsWorks for Chef Automate (OWCA) provides a fully managed Chef server and suite of automation tools, which compose <a href="https://www.chef.io/automate/">Chef Automate</a>. OpsWorks for Chef Automate offers you a consistent Chef server configuration throughout all deployed servers. All servers are deployed and bootstrapped by OpsWorks, so this means that each new server will be configured in the same way. Additionally, OpsWorks takes care of the Chef server maintenance in the form of server patching, minor Chef updates, and backups. That means that you do not have to spend time and effort in order to set-up and configure a Chef Automate server. You can just let OpsWorks handle that for you, and have your business units focus all their energy on writing or using Chef cookbooks in their environments.</p> 
<p>The fact that OpsWorks handles patching means that you don’t need to connect to the Chef server using SSH. You only &nbsp;need to use knife to manage your Chef server software. With backup and restore in place, you can easily revert back to a working version of your server if something goes wrong.</p> 
<p>A 10 000 foot view of setting up OWCA for a business unit would look something like this:</p> 
<li>Launch an OWCA server in the desired Region with the desired instance size.</li> 
<li>Take the starter kit provided with the Chef Automate server to be used on a Chef Workstation.</li> 
<li>Once the server is fully running, dministrators can use the Chef Workstation to create all the required resources: 
<li>Environments</li> 
<li>Roles</li> 
</ul> </li> 
<li>Set Berkshelf to get the needed Cookbooks from a common repository, and have them installed on the Chef Automate server.</li> 
<li>Finally, bootstrap nodes in the appropriate roles. The usage of AWS API can be applied here, as OpsWorks can handle node bootstrapping for you.</li> 
<p>Simple as that! You don’t need to manually configure Chef servers. You &nbsp;just &nbsp;launch an OpsWorks for Chef Automate server, and the rest is just Cookbook and node management.</p> 
<b>Summary</b> 
<p>Here we have discussed how to implement a federated model for configuration management using Chef and OpsWorks. We described how simple it is to set up and maintain a Chef server with OpsWorks. Many of our customers are actually taking this direction when using OpsWorks for Chef Automate because it allows them to separate their teams but still maintain a common set of configuration rules across the entire enterprise. With the assistance of the AWS API to bootstrap nodes this allows you to spend less time worrying about setting up the configuration management infrastructure and more on creating and using Cookbooks and any other Chef resources you need.</p> 
<p><strong>About the Author</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/10/24/darko-cropped.jpeg" /><br /> Darko Meszaros is a Cloud Support Engineer who supports customers that use various AWS automation tools, such as AWS OpsWorks, AWS CodeDeploy, and AWS CloudFormation. He is a subject matter expert for OpsWorks and CodeDeploy. Outside of work, he loves collecting video games and old computers.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p><em>The screenshots in this post are Copyright Chef Software Inc., and are available at <a href="https://github.com/chef/chef-web-docs">https://github.com/chef/chef-web-docs</a> under the terms of the CC-BY-3.0 license, available here <a href="https://creativecommons.org/licenses/by/3.0/legalcode">https://creativecommons.org/licenses/by/3.0/legalcode</a>.</em></p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">2017 Year in Review: AWS OpsWorks for Chef Automate and Puppet Enterprise</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Rahul Gulati</span></span> | on 
<time property="datePublished" datetime="2017-12-29T01:12:32+00:00">29 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-ops-works/" title="View all posts in AWS OpsWorks*"><span property="articleSection">AWS OpsWorks*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/devops/" title="View all posts in DevOps*"><span property="articleSection">DevOps*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/2017-year-in-review-aws-opsworks-for-chef-automate-and-puppet-enterprise/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS OpsWorks for Chef Automate and AWS OpsWorks for Puppet Enterprise provide fully managed Chef and Puppet servers with a suite of automation tools for infrastructure and application management. Enterprise customers rely on OpsWorks for easy configuration management and secure maintenance as the service automatically patches, updates, and backs up servers. This blog post gives you a quick recap of this year’s announcements and events for AWS OpsWorks for Chef Automate and AWS OpsWorks for Puppet Enterprise.</p> 
<p><span id="more-2218"></span></p> 
<p><strong>Announcing OpsWorks for Puppet Enterprise</strong></p> 
<p>Puppet Enterprise support has been the most popular request for AWS OpsWorks since AWS launched the service. Using OpsWorks for Puppet Enterprise, customers can easily manage hybrid infrastructure as code, register node instances without signing certificate requests on the Puppet master, and run modules from the Puppet Forge community. AWS OpsWorks for Puppet Enterprise is integrated with AWS CloudTrail and AWS CodeCommit. AWS CloudTrail captures all of the AWS OpsWorks for Puppet Enterprise API calls and delivers log files to an Amazon S3 bucket. AWS CodeCommit hosts R10K remote repositories, which are general purpose toolsets for deploying Puppet environments and modules.</p> 
<p><a href="https://aws.amazon.com/blogs/aws/new-aws-opsworks-for-puppet-enterprise/">Launch announcement</a></p> 
<p><strong>AWS OpsWorks for Chef Automate now supports Chef Compliance</strong></p> 
<p>The release of Chef Automate version 1.6 includes the new Compliance view for the Chef Automate UI. With AWS OpsWorks for Chef Automate integrated with compliance, you can track the compliance of your infrastructure based on a predefined policy. This allows you to frequently audit your applications for vulnerabilities and remediate violations.</p> 
<p><a href="https://aws.amazon.com/blogs/mt/aws-opsworks-for-chef-automate-now-supports-compliance/">Launch announcement</a></p> 
<p><strong>re:Invent – Automate and scale configuration management with AWS OpsWorks</strong></p> 
<p>At re:Invent 2017, we hosted a session on automating and scaling configuration management through AWS OpsWorks. We gave an overview of how using the DevOps model to treat infrastructure environments as code enables you to automate and scale development and production environments.</p> 
<p>We showed re:Invent session attendees how OpsWorks lets you use Chef and Puppet to automate the way servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. We walked through how OpsWorks helps you to focus on the core task of configuration management using Puppet and Chef by setting up and maintaining your environment in just a few clicks.</p> 
<p><a href="https://www.slideshare.net/AmazonWebServices/automate-and-scale-configuration-management-with-aws-opsworks-dev331-reinvent-2017">Slides</a></p> 
<p><a href="https://www.youtube.com/watch?v=1Zw4g3o2O0M">Video</a></p> 
<p>Right after re:Invent, we released two more updates.</p> 
<p><strong>Availability in six new Regions</strong></p> 
<p>AWS OpsWorks for Chef Automate and AWS OpsWorks for Puppet Enterprise are available in six new Regions: US East (Ohio), US West (N. California), EU (Frankfurt), Asia Pacific (Singapore), Asia Pacific (Tokyo), and Asia Pacific (Sydney). With this release, this service is available in nine regions, including US East (N. Virginia), US West (Oregon), and EU (Ireland).</p> 
<p>We added these Regions based on customer feedback, adoption of other AWS products, and to provide wider geographical coverage.</p> 
<p><a href="http://docs.aws.amazon.com/general/latest/gr/rande.html#opsworks_region">AWS Regions and Endpoints</a></p> 
<p><strong>Create backups from the console</strong></p> 
<p>With this release, AWS OpsWorks for Chef Automate and Puppet Enterprise customers can also create manual backups through the AWS Management Console. Earlier, manual backups were limited to using a CLI command. A backup – either manual or automated – saves application-level server data, such as cookbooks, modules, users, organizations, and node configurations. Backup excludes any additional files that you store on the server EC2 instance.</p> 
<p><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opscm-chef-backup.html">Back Up an AWS OpsWorks for Chef Automate Server</a></p> 
<p><a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-backup.html">Back Up an AWS OpsWorks for Puppet Enterprise Server</a></p> 
<p><strong>About the Author</strong></p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/22/ragulati.jpeg" />Rahul Gulati is a Product Manager at AWS OpsWorks. He enjoys working with customers and engineering teams to build software products.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">AWS OpsWorks for Puppet Enterprise and an alternate implementation for policy based auto signing</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Andrew Park</span></span> | on 
<time property="datePublished" datetime="2017-12-28T16:21:29+00:00">28 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-ops-works/" title="View all posts in AWS OpsWorks*"><span property="articleSection">AWS OpsWorks*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/aws-opsworks-puppet-enterprise-and-an-alternate-implementation-for-policy-based-auto-signing/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS OpsWorks for Puppet Enterprise was <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/announcing-aws-opsworks-for-puppet-enterprise/">released in November of 2017</a>. It has a secure API (<em>associate node</em>) that provides a secure, convenient, and AWS-integrated method to sign certificates for clients of OpsWorks for Puppet Enterprise. This secure API is ideal for use within a user data script when being used for AWS CloudFormation (which can be found <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-unattend-assoc.html">here</a>) or Auto Scaling Groups.</p> 
<p>However, this blog post is about creating an alternate mechanism that allows end users to implement a different condition-checking mechanism before signing client certifications. This method is effective in environments such as proof of concept (PoC), development, or testing where clients aren’t executing the full AWS provisioning process. For example, this method could be used where customers build bare metal virtual machines (VMs) or containers. It could also be used where the VMs exist in a hybrid cloud environment or in an environment where OpsWorks for Puppet Enterprise is used to govern a customer’s private cloud VMs. It’s important to note that this method is not integrated with AWS CloudTrail, where every <em>associate node</em> API call would be tracked. When you use this alternative method, this tracking is not in effect.<span id="more-2311"></span></p> 
<b>Introduction</b> 
<p>When a VM makes a request to sign in to the Puppet master, the master bases its approval of the client’s request on certain criteria. This is called policy-based auto signing.&nbsp; Due to the nature of Puppet and the possibility of exposing sensitive information, the signing of a client’s certificate must be approved carefully. The conceptual implementation is <a href="https://puppet.com/docs/puppet/5.3/ssl_autosign.html#policy-based-autosigning">documented on the Puppet website</a>.</p> 
<p>This blog post provides one example of policy-based auto signing using the following assumptions:</p> 
<ol> 
<li>The Puppet clients are Amazon EC2 instances.</li> 
<li>These instances reside in the same VPC.</li> 
<li>The Puppet master is equipped with required tools such as the AWS CLI and <a href="https://stedolan.github.io/jq/">jq</a>.</li> 
<li>The Puppet master’s AWS client tool has been configured with sufficient authority to be able to access all the information on the EC2 instances, such as their private DNS names and their tags.</li> 
</ol> 
<b>Set up the necessary components on the Puppet Enterprise master<strong>&nbsp;</strong></b> 
<ol> 
<li>Log in to the PE console and add the following value under puppet_enterprise::profile::master class</li> 
</ol> 
<code class="lang-css">allow_unauthenticated_ca = true</code> 
<ol start="2"> 
<li>Choose Add parameter.</li> 
<li>Choose Commit.</li> 
<li>Log in to the PE master using SSH and execute the following command to apply the configuration change:</li> 
</ol> 
<code class="lang-bash">puppet agent -tov</code> 
<ol start="5"> 
<li>In the same SSH session on the PE master, ensure that the AWS CLI is configured with the proper Access Key ID and the Secret Access Key for the pe-puppet</li> 
</ol> 
<code class="lang-bash">sudo su - pe-puppet --shell /bin/bash
$ aws configure
AWS Access Key ID [None]: ************
AWS Secret Access Key [None]: **********************
Default region name [None]: us-east-1
Default output format [None]: json              
$ exit</code> 
<p style="padding-left: 60px">NOTE: &nbsp;This is an important step for the Puppet Enterprise Puppet user (pe-puppet) because the policy-based signing script must be executable by the pe-puppet user.</p> 
<ol start="6"> 
<li>Edit&nbsp;/etc/puppetlabs/puppet/puppet.conf file and ensure that following line appears under <em><strong>[main]</strong></em> section of the configuration file:</li> 
</ol> 
<code class="lang-bash">autosign = /opt/puppetlabs/autosign/autosign.sh</code> 
<ol start="6"> 
<li>Create an auto sign structure and copy over the script. (You can find this script in Appendix A.) And restart Puppet Master Service:</li> 
</ol> 
<code class="lang-bash">mkdir -p /opt/puppetlabs/autosign
cp autosign.sh /opt/puppetlabs/autosign/autosign.sh
chown -R pe-puppet.pe-puppet /opt/puppetlabs/autosign
chmod 750 /opt/puppetlabs/autosign /opt/puppetlabs/autosign/autosign.sh   
/etc/init.d/pe-puppetserver restart</code> 
<ol start="7"> 
<li>Now the Puppet master is ready to sign based on the policy. (For the explanation of the policy contained in the example script, see Appendix B.) On the Puppet client that has not been signed into a PE master, run the following command to request to be signed in.</li> 
</ol> 
<code class="lang-bash">puppet agent -tov --waitforcert=200</code> 
<ol start="8"> 
<li>To confirm, you should see the following entries in the /var/log/puppetlabs/puppetserver/puppetserver.log file on the PE master:</li> 
</ol> 
<code class="lang-bash">2017-12-18 20:23:21,739 INFO  [qtp2115462635-102] [p.p.certificate-authority] Signed certificate request for ip-my-ip-address.ec2.internal
2017-12-18 20:23:22,636 INFO  [qtp2115462635-364] [puppetserver] mount[pe_packages] allowing * access
2017-12-18 20:23:22,637 INFO  [qtp2115462635-364] [puppetserver] mount[pe_modules] allowing * access</code> 
<p style="padding-left: 30px">On the Puppet client’s command line, you should see something like the following:</p> 
<code class="lang-bash">[root@ip-my-ip-address ~]# puppet agent -tov --waitforcert=200
Info: Creating a new SSL key for ip-my-ip-address.ec2.internal
Info: Caching certificate for ca
Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
Info: Creating a new SSL certificate request for ip-my-ip-address.ec2.internal
Info: Certificate Request fingerprint (SHA256): 
FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF:FF [snip]
Info: Caching certificate for ip-my-ip-address.ec2.internal
Info: Caching certificate_revocation_list for ca
Info: Caching certificate for ca
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Notice: /File[/opt/puppetlabs/puppet/cache/lib/facter]/ensure: created</code> 
<b>Conclusion</b> 
<p>That’s it. The Puppet Enterprise (PE) Master will sign the client certificates based on the policy you have defined in the autosign.sh. To recap, this blog post shows you how to create an alternate mechanism that allows end users to implement different condition-checking before signing client certificates. We walk you through an example of policy-based auto signing configuration and provide an example script.</p> 
<p>To learn more about AWS OpsWorks for Puppet Enterprise, visit <a href="https://aws.amazon.com/opsworks/puppetenterprise/">here</a>.</p> 
<b>Appendix A: autosign.sh</b> 
<code class="lang-bash">#!/bin/sh
# If we do not have aws cli or jq, exit with error immediately
which aws &gt; /dev/null 2&gt;&amp;1 || exit 2
which jq &gt; /dev/null 2&gt;&amp;1 || exit 3
usage()
{
echo &quot;$0 [private dns name of ec2 instance]&quot;
exit 1
}
[ $# -eq 1 ] || usage
clientname=$1
# Criteria 1: the puppet client must be visible in my VPC
[ `aws ec2 describe-instances --filters &quot;Name=private-dns-name,Values=$clientname&quot; --output=text | wc -l | sed 's/ //g'` -eq 0 ] &amp;&amp; exit 1
# Criteria 2: the puppet client must have proper tags
UNIQUETAG=&quot;puppetclient&quot;
INSTANCEID=`aws ec2 describe-instances --filters &quot;Name=private-dns-name,Values=$clientname&quot; --output=json | jq .Reservations[].Instances[].InstanceId`
[ &quot;`aws ec2 describe-tags --filters &quot;Name=resource-id,Values=$INSTANCEID&quot; --output json  | jq '.Tags[] | select(.Key==&quot;myuniquetag&quot;).Value' | sed 's/&quot;//g'`&quot; != &quot;$UNIQUETAG&quot; ] &amp;&amp; exit 1
# We passed everything, give it a thumbs up
exit 0</code> 
<b>Appendix B: Policy description</b> 
<p>The provided autosign.sh script uses two criteria to validate legitimacy of the puppet client’s certificate signing request.</p> 
<ol> 
<li>The Amazon EC2 VM must exist in the same VPC. (This is verified using the aws ec2 describe-instances)</li> 
<li>The unique tag created during the instantiation phase of the Amazon EC2 VM must exist and match what the Puppet master expects (configurable/adjustable in the sh script).</li> 
</ol> 
<b>Related links</b> 
<p>Add Nodes Automatically: <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-unattend-assoc.html">http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-unattend-assoc.html</a></p> 
<p>Puppet’s policy-based auto signing: <a href="https://puppet.com/docs/puppet/5.3/ssl_autosign.html#policy-based-autosigning">https://puppet.com/docs/puppet/5.3/ssl_autosign.html#policy-based-autosigning</a></p> 
<b>About the author</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/andrew_park.jpg" /></p> 
<p>Andrew Park is a Cloud Infrastructure Architect at Amazon Web Services. Prior to AWS, Andrew has served 20+ years as a Linux Solution Engineer, a Linux infrastructure Architect/Administrator and a Cloud Engineer. He has been a key participating member of cloud adoption programs in many different Canadian banks, and he also has participated in many different open source projects like Samba, restricted shell (rssh), not red hat update (Yes, this was an actual project), Red Hat’s Anaconda and Agenda VR3. He works tirelessly on behalf of the clients in order that customers can enjoy economically friendly open source solutions. He is a foodie, loves to converse with people and machines alike, and a firm believer of Open Source philosophy.</p> 
<p>&nbsp;</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">10 reasons why you should try AWS OpsWorks for Puppet Enterprise</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Bill Solominsky</span></span> | on 
<time property="datePublished" datetime="2017-12-28T13:48:08+00:00">28 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-ops-works/" title="View all posts in AWS OpsWorks*"><span property="articleSection">AWS OpsWorks*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/10-reasons-why-you-should-try-aws-opsworks-for-puppet-enterprise/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p><em>By Ryan Coleman, Director of Product Management at <a href="https://puppet.com/" target="_blank" rel="noopener noreferrer">Puppet</a></em></p> 
<p>We’re really excited to see <a href="https://aws.amazon.com/opsworks/puppetenterprise/" target="_blank" rel="noopener noreferrer">AWS OpsWorks for Puppet Enterprise</a> out in the wild. Over 600 people attended our joint session at AWS re:Invent 2017 (check out the video <a href="https://www.youtube.com/watch?v=1Zw4g3o2O0M" target="_blank" rel="noopener noreferrer">here</a>) and many of you stopped by our booth to find out more. This post is for everyone we didn’t talk to directly. If you’re an open source Puppet user and you’d like to try some of the Puppet Enterprise features you’ve been hearing about, you can deploy OpsWorks for Puppet Enterprise in less than 20 minutes with no upfront costs. If you’re running a small team and you have a big cloud or automation initiative on the table for 2018, then OpsWorks for Puppet Enterprise is a perfect starting point.<span id="more-2283"></span></p> 
<p>And, because it’s that time of year for making lists, we’re bringing you the top 10 reasons why you should try OpsWorks for Puppet Enterprise in the new year.</p> 
<ol> 
<li>You can have a fully configured Puppet master up and running on AWS in less than 20 minutes. We developed a handy <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/gettingstarted-opspup.html" target="_blank" rel="noopener noreferrer">User Guide</a> and <a href="http://docs.aws.amazon.com/opsworks/latest/userguide/opspup-starterkit.html" target="_blank" rel="noopener noreferrer">Starter Kit</a> to help you get started.</li> 
<li>Backups, upgrades, and restorations are automatically handled for you based on maintenance windows that you define.</li> 
<li>You can take advantage of hourly billing and only pay for what you consume.</li> 
<li>You can deploy wherever your customers are—OpsWorks just added support for <a href="https://aws.amazon.com/about-aws/whats-new/2017/12/aws-opsworks-is-now-available-in-nine-regions/" target="_blank" rel="noopener noreferrer">6 new AWS Regions</a>. In addition to these Regions: US East (N. Virginia), US West (Oregon), and EU (Ireland), it’s now available in in these Regions: US East (Ohio), US West (N. California), EU (Frankfurt), Asia Pacific (Singapore), Asia Pacific (Tokyo), and Asia Pacific (Sydney).</li> 
<li>Need to use up some of your AWS credits? You can apply them to your AWS bill and offset part of your OpsWorks for Puppet Enterprise usage!</li> 
<li>Workflows tailored to the cloud make it easy to bring your cloud resources under management. For example, Puppet Enterprise in OpsWorks can securely register nodes on the Puppet master without intervention.</li> 
<li>Connect to your existing module code repository. If you don’t have an existing code repo, we provide an AWS-managed source control repository to securely store your infrastructure code.</li> 
<li>Manage your AWS and on-premises infrastructure with a single tool. If you’re going fully cloud native, OpsWorks for Puppet Enterprise is for you. If you still have on-premises infrastructure to manage, you can bring that, too.</li> 
<li>Are you running open source Puppet or an older version of Puppet Enterprise? Try out the latest features available in Puppet Enterprise 2017.3 like <a href="https://puppet.com/products/capabilities/task-management" target="_blank" rel="noopener noreferrer">Task Management</a>, which lets you run ad hoc tasks across tens of thousands of nodes with full governance and auditability.</li> 
<li>Accelerate migration to the cloud and get the benefits of Day 2 management.</li> 
</ol> 
<p>Together with AWS, we look forward to helping you move to the cloud and scale automation across your organization in 2018. Give it a try and let us know what you think. If you’d like to learn more about OpsWorks for Puppet Enterprise, you can get more info <a href="https://aws.amazon.com/opsworks/puppetenterprise/" target="_blank" rel="noopener noreferrer">here</a>.</p> 
<p><em>The content and opinions in this post are those of the third-party author and AWS is not responsible for the content or accuracy of this post.</em></p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<meta property="image" content="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/image2-1.png" /> 
<b class="lb-b blog-post-title" property="name headline">Integrating AWS CloudFormation with AWS Systems Manager Parameter Store</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Anuradha Garg</span></span> | on 
<time property="datePublished" datetime="2017-12-28T11:40:24+00:00">28 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/amazon-ec2-systems-manager/" title="View all posts in Amazon EC2 Systems Manager*"><span property="articleSection">Amazon EC2 Systems Manager*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-cloudformation/" title="View all posts in AWS CloudFormation*"><span property="articleSection">AWS CloudFormation*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>AWS CloudFormation has always allowed you to customize your templates by using parameters for runtime input values. Parameters make your template code dynamically configurable, improving the reusability of your code. Previously, the only ways you could specify values for these parameters were to pass the plaintext values as arguments to the CloudFormation API, or hard code them in the template using ‘default’ values. This posed the following limitations:</p> 
<ol class="incremental" type="1"> 
<li>There was no centralized place to define/update your parameters (which may contain secrets, configuration data, etc.) and then import them into CloudFormation.</li> 
<li>When changing parameters, you had to either slightly rewrite your template code or pass new parameter values when doing stack update operation.</li> 
</ol> 
<p>We are pleased to announce the integration of CloudFormation with <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html">AWS Systems Manager Parameter Store</a> which provides solutions to the above restrictions. In this blog, I explain how you can use Systems Manager parameters in your CloudFormation templates to simplify stack updates involving parameters and achieve consistency by using values stored in Parameter Store. With this integration, your code remains untouched while the stack update operation automatically picks up the latest parameter store value.</p> 
<p><span id="more-2268"></span></p> 
<b id="how-systems-manager-parameters-work-in-cloudformation">How Systems Manager parameters work in CloudFormation</b> 
<p>You can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation. If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p> 
<p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html">Parameters</a> section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p> 
<b id="new-systems-manager-parameter-types-supported-in-cloudformation">New Systems Manager parameter types supported in CloudFormation</b> 
<p>CloudFormation parameters already support certain AWS specific types. SSM parameter types will be an addition to these types. New parameter types introduced in CloudFormation are:</p> 
<ul class="incremental"> 
<li>AWS::SSM::Parameter::Name</li> 
<li>AWS::SSM::Parameter::Value&lt;String&gt;</li> 
<li>AWS::SSM::Parameter::Value&lt;List&lt;String&gt;&gt;</li> 
<li>AWS::SSM::Parameter::Value&lt;Any AWS type&gt;</li> 
<p>The first one in the list is used to pass the name of the parameter key as-is. CloudFormation will not fetch the value stored against it. For example, you can use this type to validate that the parameter exists in Parameter Store. For all others, the value is fetched from Systems Manager with the type defined in the trailing angle brackets &lt;&gt;. For now, you can only use plaintext strings or list of strings. CloudFormation will support the Parameter Store ‘SecureString’ type in a later release.</p> 
<b id="how-to-use-ssm-types-in-cloudformation">How to Use SSM types in CloudFormation</b> 
<p>As mentioned earlier, SSM parameter types are used in template to reference existing Systems Manager parameters. Let’s look at a couple example scenarios.</p> 
<h3 id="example-1">Example 1:</h3> 
<p>Let’s say you are deploying a CloudFormation stack in both Development and Production. You are planning to use smaller instance types in Development. You don’t want to remember the instance types to be used in your CloudFormation template whenever you redeploy from one environment to another. The data (parameter value for the instance type parameter) can be maintained separately from the code (template). This example shows the end-to-end steps for using Systems Manger parameters for this use-case.</p> 
<code># Create a parameters for Dev and Prod environments in Systems Manager Parameter Store
aws ssm put-parameter --name myEC2TypeDev --type String --value “t2.small”
aws ssm put-parameter --name myEC2TypeProd --type String --value “m4.large”
</code> 
<code class="yaml"># Reference/use existing Systems Manager Parameter in CloudFormation
Parameters:
InstanceType :
Type : 'AWS::SSM::Parameter::Value&lt;String&gt;'
Default: myEC2TypeDev
KeyName :
Type : 'AWS::SSM::Parameter::Value&lt;AWS::EC2::KeyPair::KeyName&gt;'
Default: myEC2Key
AmiId:
Type: 'AWS::EC2::Image::Id'
Default: 'ami-60b6c60a'
Resources :
Instance :
Type : 'AWS::EC2::Instance'
Properties :
Type : !Ref InstanceType
KeyName : !Ref KeyName
ImageId : !Ref AmiId 
</code> 
<code># Call create-stack for Dev environment by passing SSM parameter key as template parameter value
aws cloudformation create-stack --stack-name S1 --template-body &lt;above-template&gt;
# Call create-stack for Prod environment by passing SSM parameter key as template parameter value
aws cloudformation create-stack --stack-name S1 --template-body &lt;above-template&gt; --parameters ParameterKey=InstanceType,ParameterValue=myEC2TypeProd</code> 
<p>Systems Manager console – All SSM parameters under this account are displayed.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/27/image1.png" /></p> 
<p>CloudFormation console – Stack list page where selected stack is using SSM parameters.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/image2-1.png" /></p> 
<p>CloudFormation console – Stack detail page for stack using SSM parameters.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/image3.png" /></p> 
<h3 id="example-2">Example 2:</h3> 
<p>Consider the use case of updating Amazon Machine Image (AMI) IDs for the EC2 instances in your CloudFormation templates. Normally, you might map AMI IDs to specific instance types and Regions. Then to update these, you would manually change them in each of your templates. Or you would be using a custom resource with an AWS Lambda function that gets the IDs of the latest AMIs for the Region and instance type that you’re using. Arguably, neither of these methods is very convenient.</p> 
<p>Systems Manger Parameter Store team recently launched <a href="https://aws.amazon.com/blogs/mt/query-for-the-latest-windows-ami-using-systems-manager-parameter-store/">an easy way to retrieve the latest AMI IDs</a> for your template. <a href="http://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-paramstore-walk.html#sysman-paramstore-walk-hierarchies">Hierarchical</a> public parameters supported by Parameter Store can also be referenced in a CloudFormation template similar to the one used in Example 1. You can use the public parameter variable for the Windows AMI ID in your template. You don’t need to worry about how to fetch the latest AMI IDs. This SSM parameter will be updated whenever there is a newer version available. Whenever you decide to update the EC2 instances in your CloudFormation template to use the new AMI ID, you just call update-stack API on the stack. It will automatically fetch the latest value from Parameter Store. Also, note that for hierarchical parameters, you need to provide the full path of the parameter name.</p> 
<code class="yaml"># Use public Systems Manager Parameter
Parameters :
LatestAmiId :
Type : 'AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;'
Default: ‘/aws/service/ami-windows-latest/Windows_Server-2016-English-Core-Containers’
Resources :
Instance :
Type : 'AWS::EC2::Instance'
Properties :
ImageId : !Ref LatestAmiId</code> 
<code># Create-stack CLI call
aws cloudformation create-stack --stack-name S1 --template-body &lt;above-template&gt;
# Describe stack output’s ‘Parameters’ section for this stack
aws cloudformation describe-stacks --stack-name S1</code> 
<code class="json">…
&quot;Parameters&quot;: [
{
&quot;ParameterValue&quot;: &quot;/aws/service/ami-windows-latest/Windows_Server-2016-English-Core-Containers&quot;,
&quot;ResolvedValue&quot;: &quot;ami-ba9c05c0&quot;,
&quot;ParameterKey&quot;: &quot;LatestAmiId&quot;
}
]
…</code> 
<b id="summary">Summary</b> 
<p>By leveraging the CloudFormation integration with Systems Manager Parameter Store, you can make your templates more reusable and generic by storing and managing your runtime configuration and parameters centrally and securely. This, in turn, can make your stack operations simpler and more consistent, by clearly delineating the separation of infrastructure configuration and infrastructure code from each other.</p> 
<p>The feature is now available in all AWS Regions!</p> 
<hr /> 
<b>About the Author</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/Anuradha.jpg" /> Anuradha Garg is a software developer on the AWS CloudFormation team where she works on developing new features for the service. Outside work, she is a travel enthusiast and loves exploring new things.</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
<p>&nbsp;</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">How to Manage Credentials in AWS OpsWorks for Puppet Enterprise using Hiera-eyaml</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Nick Alteen</span></span> | on 
<time property="datePublished" datetime="2017-12-28T11:13:04+00:00">28 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-ops-works/" title="View all posts in AWS OpsWorks*"><span property="articleSection">AWS OpsWorks*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/devops/" title="View all posts in DevOps*"><span property="articleSection">DevOps*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/how-to-manage-credentials-in-aws-opsworks-for-puppet-enterprise-using-hiera-eyaml/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>For customers new to configuration management with AWS OpsWorks for Puppet Enterprise (OWPE), a frequently-asked question is “How do I store sensitive data, such as database passwords, for use in my manifests?” Hiera allows you to manage and access data from various backends (data storage locations). By default, Hiera 5 supports YAML, JSON, and HOCON backends and only plaintext data files and values.</p> 
<p>With some additional configuration, Hiera 5 supports the eyaml backend, which allows administrators to define backend data files with encrypted data (without having to encrypt the file in its entirety). This provides security of data at rest while allowing fast lookup and ease of use/review by administrators. Additionally, plaintext values can be included in the same file. Hiera-eyaml supports encrypted arrays, hashes, and nested arrays/hashes.</p> 
<p><span id="more-2299"></span></p> 
<h3>Enabling Hiera-eyaml</h3> 
<p>The eyaml command line tool is made available for working with encrypted data files. To install and use hiera-eyaml, follow these instructions.</p> 
gem install hiera-eyaml 
<p>Import the following modules from the Puppet Forge to your Master.</p> 
<li>https://forge.puppet.com/puppet/hiera</li> 
<li>https://forge.puppet.com/puppetlabs/puppetserver_gem</li> 
<li>https://forge.puppet.com/puppetlabs/inifile</li> 
<li>https://forge.puppet.com/puppetlabs/stdlib</li> 
<p>The following example Puppetfile includes the required modules.</p> 
forge &quot;http://forge.puppetlabs.com&quot;
# Modules from the Puppet Forge
mod &quot;puppetlabs/concat&quot;
mod &quot;puppetlabs/ntp&quot;
mod &quot;puppetlabs/stdlib&quot;
mod &quot;puppet/staging&quot;
mod &quot;puppet-logrotate&quot;
mod &quot;puppet/nginx&quot;
mod &quot;puppetlabs/inifile&quot;
mod &quot;puppetlabs/puppetserver_gem&quot;
mod &quot;puppet/hiera&quot; 
<p>Create a profile manifest to configure Hiera. This will enable three hierarchies (Virtual yaml, Nodes yaml, and Default yaml file). In the starter kit provided when creating an AWS OpsWorks for Puppet Enterprise instance, this would be located in <code>[STARTER_KIT]/site/profile/manifests/hiera.pp</code>.</p> 
class profile::hiera {
class { 'hiera':
hiera_version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '5',
hiera5_defaults&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; {&quot;datadir&quot; =&gt; &quot;/etc/puppetlabs/code/environments/%{::environment}/hieradata&quot;, &quot;data_hash&quot; =&gt; &quot;yaml_data&quot;},
hierarchy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; [
{&quot;name&quot; =&gt; &quot;Virtual yaml&quot;, &quot;path&quot;&nbsp; =&gt;&nbsp; &quot;virtual/%{::virtual}.yaml&quot;},
{&quot;name&quot; =&gt; &quot;Nodes yaml&quot;, &quot;paths&quot; =&gt;&nbsp; ['nodes/%{::trusted.certname}.yaml', 'nodes/%{::osfamily}.yaml']},
{&quot;name&quot; =&gt; &quot;Default yaml file&quot;, &quot;path&quot; =&gt;&nbsp; &quot;common.yaml&quot;},
],
eyaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
&nbsp;&nbsp; eyaml_gpg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
&nbsp;&nbsp; eyaml_gpg_recipients =&gt;&nbsp; 'mark@example.com,chris@example.com',
&nbsp;&nbsp; &nbsp;create_keys&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; false,
&nbsp;&nbsp;&nbsp; keysdir&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '/etc/puppetlabs/code-staging/keys',
&nbsp;&nbsp;&nbsp; provider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; puppetserver_gem,
}
} 
<p>Ensuring that you are following best practices, you can then add the newly created hiera profile to a role for our Puppet Master. This would be placed in <code>[STARTER_KIT]/site/role/manifests/puppet_master.pp</code>.</p> 
class role::puppet_master {
include profile::hiera
} 
<p>After steps 2-4 have been completed, and the control repository has been synchronized with the Puppet master (via webhook and puppet-code deploy), the role should be available for classification on the Puppet Master. This can be done in the Classification console under “All Nodes” -&gt; “PE Infrastructure” -&gt; “PE Master”. If for some reason the role is not appearing as a classification option, ensure that you have refreshed class definitions.</p> 
<p>To quickly synchronize these changes, you can manually execute an Agent run on the Master using the PE console. At this point, the necessary configuration to support hiera-eyaml has been configured. The next steps would be to install encryption keys securely on the Puppet Master.</p> 
<h3>Secure Key Storage</h3> 
<p>After eyaml has been configured on the Master, &nbsp;you need to generate and securely store the asymmetric key pairs used for encryption and decryption of sensitive data. This can be done with the <code>eyaml createkeys</code> command from your workstation. The private key should be stored in a secure location that can only be accessed by the Puppet Master and any authorized users. In this case, you can leverage an Amazon S3 bucket with a strict access policy that allows only the OWPE instance’s IAM profile and administrators to our AWS account. The sample policy that follows demonstrates this. With this policy, it is important to ensure that the <code>s3:GetObject</code> and <code>s3:ListBucket</code> permissions are added to the <code>aws-opsworks-cm-ec2</code> role. After the policy has been applied and the OWPE Amazon EC2 role has been updated, have an administrator upload the public and private keys to the bucket.</p> 
{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Id&quot;: &quot;HieraKeysPolicy&quot;,
&nbsp; &quot;Statement&quot;: [
{
&quot;Sid&quot;: &quot;AllowOpsWorksCMInstanceRole&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;AWS&quot;: &quot;arn:aws:iam::ACCOUNT_ID:role/service-role/aws-opsworks-cm-ec2-role&quot;
},
&quot;Action&quot;: [
&quot;s3:GetObject&quot;,
&quot;s3:ListBucket&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:s3:::BUCKET_NAME/*&quot;,
&quot;arn:aws:s3:::BUCKET_NAME&quot;
]
},
{
&quot;Sid&quot;: &quot;AllowAdminAccess&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Principal&quot;: {
&quot;AWS&quot;: &quot;arn:aws:iam::ACCOUNT_ID:user/ADMIN_USER&quot;
},
&quot;Action&quot;: &quot;s3:*&quot;,
&quot;Resource&quot;: [
&quot;arn:aws:s3:::BUCKET_NAME/*&quot;,
&quot;arn:aws:s3:::BUCKET_NAME&quot;
]
}
]
} 
<p>After this policy has been applied, public and private keys can be generated with the <code>eyaml createkeys</code> command. This will automatically place the key files in a <code>./keys</code> directory on your workstation. The public/private keys can then be uploaded to the S3 bucket, and you can modify the <code>hiera.pp</code> manifest to include importing these files to <code>/etc/puppetlabs/code-staging/keys</code>. This directory is configured as part of the Hiera profile created previously, and is included in the backup/restore procedures.</p> 
<p>Since these files are not created with in-line content, or from files/templates stored within the module itself, you need to make use of the exec Puppet resource. Since the AWS CLI is installed by default on OWPE Masters, no additional configuration/installation is required. The benefit of this approach is that the AWS CLI will make use of instance profiles to determine access permissions to S3. You need to confirm that the role assigned to nodes in your environment has appropriate S3 permissions (specifically <code>s3:ListBucket</code> on the bucket itself, and <code>s3:GetObject</code> for any key files).</p> 
<p>First, update the Hiera profile to include importing the public/private key files.</p> 
class profile::hiera {
class { 'hiera':
hiera_version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '5',
hiera5_defaults&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; {&quot;datadir&quot; =&gt; &quot;/etc/puppetlabs/code/environments/%{::environment}/hieradata&quot;, &quot;data_hash&quot; =&gt; &quot;yaml_data&quot;},
hierarchy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; [
{&quot;name&quot; =&gt; &quot;Virtual yaml&quot;, &quot;path&quot;&nbsp; =&gt;&nbsp; &quot;virtual/%{::virtual}.yaml&quot;},
{&quot;name&quot; =&gt; &quot;Nodes yaml&quot;, &quot;paths&quot; =&gt;&nbsp; ['nodes/%{::trusted.certname}.yaml', 'nodes/%{::osfamily}.yaml']},
{&quot;name&quot; =&gt; &quot;Default yaml file&quot;, &quot;path&quot; =&gt;&nbsp; &quot;common.yaml&quot;},
],
eyaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg_recipients =&gt;&nbsp; 'mark@example.com,chris@example.com',
create_keys&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; false,
keysdir&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&gt;&nbsp; '/etc/puppetlabs/code-staging/keys',
provider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; puppetserver_gem,
}
exec { 'get-private-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/private/key.pem /etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
}
exec { 'get-public-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/public/key.pem /etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
}
exec { 'update-key-perms':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'chown -R pe-puppet:pe-puppet /etc/puppetlabs/code-staging/keys &amp;&amp; chmod -R 0500 /etc/puppetlabs/code-staging/keys &amp;&amp; chmod 0400 /etc/puppetlabs/code-staging/keys/*.pem',
require =&gt; [
Exec['get-private-key'],
Exec['get-public-key'],
],
}
} 
<p>After this is complete and deployed to your control repository, run Puppet Agent on the master and deploy the changes with <code>puppet-code deploy --wait --all --config-file .config/puppet-code.conf --token-file .config/puppetlabs/token</code> (executed from the root of your starter kit). To verify the modifications work as intended, test the puppet lookup tool to verify you are able to retrieve data from <code>common.yaml</code>.</p> 
$ puppet lookup message --explain
Searching for &quot;message&quot;
&nbsp; Global Data Provider (hiera configuration version 5)
&nbsp;&nbsp;&nbsp; Using configuration &quot;/etc/puppetlabs/puppet/hiera.yaml&quot;
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Virtual yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/virtual/xenhvm.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;virtual/%{::virtual}.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Path not found
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Nodes yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/nodes/opsworks-1703-ohq1bhd0cetefkis.us-east-1.opsworks-cm.io.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;nodes/%{::trusted.certname}.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path not found
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/nodes/RedHat.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;nodes/%{::osfamily}.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path not found
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Default yaml file&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/common.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;common.yaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Found key: &quot;message&quot; value: &quot;This node is using common data&quot; 
<p>Now, you can update the Hiera data within your control repository to include encrypted values. Note that this will require separate additions to the hierarchy specified in the Hiera class. Thus, you must first update <code>hiera.pp</code> to include the additional hierarchy layer.</p> 
class profile::hiera {
class { 'hiera':
hiera_version&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '5',
hiera5_defaults&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; {&quot;datadir&quot; =&gt; &quot;/etc/puppetlabs/code/environments/%{::environment}/hieradata&quot;, &quot;data_hash&quot; =&gt; &quot;yaml_data&quot;},
hierarchy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; [
{&quot;name&quot; =&gt; &quot;Secret Data&quot;, &quot;lookup_key&quot; =&gt; &quot;eyaml_lookup_key&quot;, &quot;paths&quot; =&gt; ['common.eyaml'], &quot;options&quot; =&gt; { &quot;pkcs7_private_key&quot; =&gt; &quot;/etc/puppetlabs/code/keys/private_key.pkcs7.pem&quot;, &quot;pkcs7_public_key&quot; =&gt; &quot;/etc/puppetlabs/code/keys/public_key.pkcs7.pem&quot; } },
{&quot;name&quot; =&gt; &quot;Virtual yaml&quot;, &quot;path&quot;&nbsp; =&gt;&nbsp; &quot;virtual/%{::virtual}.yaml&quot;},
{&quot;name&quot; =&gt; &quot;Nodes yaml&quot;, &quot;paths&quot; =&gt;&nbsp; ['nodes/%{::trusted.certname}.yaml', 'nodes/%{::osfamily}.yaml']},
{&quot;name&quot; =&gt; &quot;Default yaml file&quot;, &quot;path&quot; =&gt;&nbsp; &quot;common.yaml&quot;},
],
eyaml&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; true,
eyaml_gpg_recipients =&gt;&nbsp; 'mark@example.com,chris@example.com',
create_keys&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; false,
keysdir&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; '/etc/puppetlabs/code-staging/keys',
provider&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&gt;&nbsp; puppetserver_gem,
}
exec { 'get-private-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/private/key.pem /etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/private_key.pkcs7.pem',
}
exec { 'get-public-key':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'aws s3api get-object --bucket BUCKET_NAME --key path/to/public/key.pem /etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
creates =&gt; '/etc/puppetlabs/code-staging/keys/public_key.pkcs7.pem',
}
exec { 'update-key-perms':
path&nbsp;&nbsp;&nbsp; =&gt; ['/usr/bin','/usr/sbin','/bin'],
cwd&nbsp;&nbsp;&nbsp;&nbsp; =&gt; '/etc/puppetlabs/code-staging',
command =&gt; 'chown -R pe-puppet:pe-puppet /etc/puppetlabs/code-staging/keys &amp;&amp; chmod -R 0500 /etc/puppetlabs/code-staging/keys &amp;&amp; chmod 0400 /etc/puppetlabs/code-staging/keys/*.pem',
require =&gt; [
Exec['get-private-key'],
Exec['get-public-key'],
],
}
} 
<p>After this is complete, add an encrypted YAML file with some test data. Note that there is a specific format required when adding encrypted values, as seen in the following snippet.</p> 
# From the same directory as ./keys/
eyaml edit CONTROL_REPO/hieradata/common.eyaml 
<p>For testing purposes, we created a new value.</p> 
---
encryptedmessage: DEC::PKCS7['Hello, World!']! 
<p>After this is complete, you can commit and push the changes to your control repository, sync them with the Puppet Master, and run Puppet Agent on the master to implement the updated Hiera configuration. To validate that this works as expected, output the file contents, and compare them to the output of puppet lookup.</p> 
$ cat /etc/puppetlabs/code/environments/production/hieradata/common.eyaml
---
encryptedmessage: ENC[PKCS7,ENCRYPTED_STRING]
$ puppet lookup encryptedmessage
Searching for &quot;encryptedmessage&quot;
&nbsp; Global Data Provider (hiera configuration version 5)
&nbsp;&nbsp;&nbsp; Using configuration &quot;/etc/puppetlabs/puppet/hiera.yaml&quot;
&nbsp;&nbsp;&nbsp; Hierarchy entry &quot;Secret Data&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Path &quot;/etc/puppetlabs/code/environments/production/hieradata/common.eyaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Original path: &quot;common.eyaml&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Found key: &quot;encryptedmessage&quot; value: &quot;'Hello, World!'&quot; 
<p>At this point, you can add additional hierarchies for environment or fact-based data, and manage sensitive information within. From your manifest code, you can simply query Hiera with the <code>lookup()</code> function as normal. You need to create and maintain encryption keys, and distribute them to administrators who need to manage Hiera data.</p> 
<h3>About the Author</h3> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/28/internal-cdn.amazon.com_.jpeg" /></p> 
<p>&nbsp;</p> 
<p>Nick Alteen is a Lab Development Engineer at Amazon Web Services, previously a Cloud Support Engineer. In both roles, he enjoys developing training and providing guidance to customers for usage of AWS services to fit their needs.</p> 
</article> 
<article class="blog-post" vocab="http://schema.org/" typeof="TechArticle"> 
<meta property="inLanguage" content="en-US" /> 
<b class="lb-b blog-post-title" property="name headline">Tracking AWS Service Catalog products provisioned by individual SAML users</b> 
<footer class="blog-post-meta" data-lb-comp="aws-blog:share-dialog">
by 
<span property="author" typeof="Person"><span property="name">Remek Hetman</span></span> | on 
<time property="datePublished" datetime="2017-12-19T11:48:17+00:00">19 DEC 2017</time> | in 
<span class="blog-post-categories"><a href="https://aws.amazon.com/blogs/mt/category/management-tools/aws-service-catalog/" title="View all posts in AWS Service Catalog*"><span property="articleSection">AWS Service Catalog*</span></a>, <a href="https://aws.amazon.com/blogs/mt/category/management-tools/" title="View all posts in Management Tools*"><span property="articleSection">Management Tools*</span></a></span> | 
<a href="https://aws.amazon.com/blogs/mt/tracking-aws-service-catalog-products-provisioned-by-individual-saml-users/" property="url">Permalink</a> | 
<a href="#" role="button" data-share-dialog-toggle=""><span class="span icon-share"></span>&nbsp;Share</a> 
</footer> 
<p>To manage access to the AWS Cloud, many companies prefer <a title="undefined" href="https://aws.amazon.com/iam/details/manage-federation/" target="_blank" rel="noopener noreferrer">Enterprise Federation</a> over AWS Identity and Access Management (IAM) users. Identity federation provides single sign-on (SSO) to access AWS accounts using credentials from the corporate directory. This method of accessing AWS allows companies to utilize their existing identity solutions, such as Active Directory (AD) or Active Directory Federation Services, by mapping users to IAM roles.</p> 
<p>Another option for managing access to AWS is to use <a title="undefined" href="https://aws.amazon.com/servicecatalog/" target="_blank" rel="noopener noreferrer">AWS Service Catalog</a>. In this blog I’ll show you how to set up AWS Service Catalog to grant users IAM roles for launching AWS resources.</p> 
<p>AWS Service Catalog allows an organization to create a portfolio of products that can be provisioned by users. This method mitigates the need to grant user permissions to AWS resources and only grants permissions to the service catalog and specific products.</p> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/19/Diagram1-1024x304.png"></span><br /> This diagram shows how users can access products through AWS Service Catalog after they have access to an appropriate IAM role. However, we need a way to distinguish users because multiple users can belong to the same AD group.</p> 
<p>One way to identify each user is to add the user name parameter to the product template. But this method doesn’t guarantee that the value entered by the user will be correct or match the user name in Active Directory.</p> 
<p>A better way to accomplish this is to programmatically add the user name to each product template. Let’s take a look at how to accomplish this using AWS Service Catalog.</p> 
<b>Solution Overview</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/19/Diagram2-1024x404.png" /></p> 
<p>This Solution Overview diagram shows the architecture of the proposed solution:</p> 
<ol>
1. The user provisions a product after authenticating to AWS Service Catalog. 
</ol> 
<ol>
2. AWS Service Catalog launches an AWS CloudFormation template in response to the user’s request. 
</ol> 
<ol>
3. An AWS Lambda function is invoked based on the Amazon CloudWatch rule triggered by the CloudFormation CreateStack event. 
</ol> 
<ol>
4. The Lambda function reads the Active Directory User Name and CloudFormation stack ID from the event record and stores this information in an Amazon DynamoDB database. 
</ol> 
<ol>
5. The CloudFormation template provisions a custom resource that invokes the AWS Lambda function. 
</ol> 
<ol>
6. The Lambda function reads the user name from the Amazon DynamoDB record associated with the CloudFormation stack ID and returns this information back to the CloudFormation template. 
</ol> 
<b>Prerequisites</b> 
<p>Before you begin implementing this solution, be sure to do the following:</p> 
<ol> 
<li style="list-style-type: none"> 
<ol>
1. Install the AWS CLI: 
</ol> </li> 
</ol> 
<p><a title="undefined" href="https://aws.amazon.com/cli/" target="_blank" rel="noopener noreferrer">https://aws.amazon.com/cli/</a></p> 
<ol>
. 
</ol> 
<ol>
2. Install Python 2.7 (including pip). 
</ol> 
<b>Implementation</b> 
<h3>Step 1: Create an Amazon DynamoDB table</h3> 
<p>An Amazon DynamoDB table will be used as central location to store the user name and CloudFormation stack ID.</p> 
<code>aws dynamodb create-table --table-name sc-track-user --attribute-definitions AttributeName=CFStackid,AttributeType=S --key-schema AttributeName=CFStackid,KeyType=HASH --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5</code> 
<h3>Step 2: Create an IAM role</h3> 
<p>Create an IAM role that will be associated with both AWS Lambda functions to grant permission to Amazon DynamoDB table.</p> 
<code>aws iam create-role --role-name sc-lambda-role --assume-role-policy-document &quot;{\&quot;Version\&quot;: \&quot;2012-10-17\&quot;,\&quot;Statement\&quot;:[{\&quot;Effect\&quot;: \&quot;Allow\&quot;,\&quot;Principal\&quot;:{\&quot;Service\&quot;: \&quot;lambda.amazonaws.com\&quot;},\&quot;Action\&quot;: \&quot;sts:AssumeRole\&quot;}]}&quot;</code> 
<h3>Step 3: Create an IAM policy</h3> 
<p>1. Create new file name called lambda-access-policy.json and add following context to the file:</p> 
<code class="lang-json">{
&quot;Version&quot;: &quot;2012-10-17&quot;,
&quot;Statement&quot;: [
{
&quot;Sid&quot;: &quot;AccessDynamoDB&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;dynamodb:GetItem&quot;,
&quot;dynamodb:PutItem&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:dynamodb:us-east-1:{your AWS Account No}:table/sc-track-user&quot;
]
},
{
&quot;Sid&quot;: &quot;CreateCWLogs&quot;,
&quot;Effect&quot;: &quot;Allow&quot;,
&quot;Action&quot;: [
&quot;logs:CreateLogGroup&quot;,
&quot;logs:CreateLogStream&quot;
],
&quot;Resource&quot;: &quot;*&quot;
},
{
&quot;Sid&quot;: &quot;WriteCWLog&quot;,
&quot;Action&quot;: [
&quot;logs:PutLogEvents&quot;
],
&quot;Resource&quot;: [
&quot;arn:aws:logs:us-east-1: {AWS Account No}:log-group:/aws/lambda/sc-add-user-id:*:*&quot;,
&quot;arn:aws:logs:us-east-1: {AWS Account No}:log-group:/aws/lambda/sc-get-user-id:*:*&quot;
],
&quot;Effect&quot;: &quot;Allow&quot;
}
]
}
</code> 
<p>2. Replace {AWS Account No} with your AWS account number.<br /> 3. Execute the following CLI command:</p> 
<code>aws iam put-role-policy --role-name sc-lambda-role --policy-name Lambda-DynamoDB-CloudWatch --policy-document file://lambda-access-policy.json</code> 
<h3>Step 4: Create an AWS Lambda function</h3> 
<p>Create a Lambda function to store the user name and CloudFormation stack ID.<br /> 1. Create a new file name called adduser.py.<br /> 2. Add the following code to the file:</p> 
<code class="lang-python">import boto3
def lambda_handler(event, context):
dynamodb = boto3.resource('dynamodb',region_name='us-east-1')
table = dynamodb.Table('sc-track-user')
stackId = event['detail']['responseElements']['stackId']
Id = (stackId.split('/'))[-1]
UserArn = event['detail']['userIdentity']['arn']
UserAID = (UserArn.split('/'))[-1]
table.put_item(
Item={
'CFStackid' : Id,
'User' : UserAID
})
return ''
</code> 
<p>3. Zip file as adduser.zip.<br /> 4. Run the following command:</p> 
<code>aws lambda create-function --function-name sc-add-user-id --runtime python2.7 --role arn:aws:iam::{AWS Account No}:role/sc-lambda-role --handler adduser.lambda_handler --timeout 30 --zip-file &quot;fileb://adduser.zip&quot;</code> 
<p><strong>Note</strong>: Before running this command change {AWS Account No} to the correct account number.</p> 
<h3>Step 5: Create an Amazon CloudWatch Event</h3> 
<p>An Amazon CloudWatch Event will invoke a Lambda function each time a new CloudFormation stack is created.</p> 
<p>1. Create the CloudWatch Event:</p> 
<code>aws events put-rule --name &quot;sc-add-user&quot; --event-pattern &quot;{\&quot;source\&quot;:[\&quot;aws.cloudformation\&quot;],\&quot;detail-type\&quot;:[\&quot;AWS API Call via CloudTrail\&quot;],\&quot;detail\&quot;:{\&quot;eventSource\&quot;:[\&quot;cloudformation.amazonaws.com\&quot;],\&quot;eventName\&quot;:[\&quot;CreateStack\&quot;]}}&quot;</code> 
<p>2. Add the Lambda function as the target to the event.</p> 
<code>aws events put-targets --rule sc-add-user --targets &quot;Id&quot;=&quot;LambdaFunction&quot;,&quot;Arn&quot;=&quot;arn:aws:lambda:us-east-1:{AWS Account No}:function:sc-add-user-id&quot;</code> 
<p>3. Grant permission for your event to invoke the Lambda function.</p> 
<code>aws lambda add-permission --function-name sc-add-user-id --statement-id LamdaPermission-10 --action lambda:InvokeFunction --principal events.amazonaws.com --source-arn arn:aws:events:us-east-1: {AWS Account No}:rule/sc-add-user</code> 
<p><strong>Note</strong>: Before running commands change {AWS Account No} to the correct account number, where you need to.</p> 
<h3>Step 6: Call the AWS Lambda function</h3> 
<p>This Lambda function will be called by CloudFormation template to retrieve user name from DynamoDB table</p> 
<p>1. Create a new file name: getuser.py<br /> 2. Add the following code to the file:</p> 
<code class="lang-python">import json
import requests
import os
import boto3
from botocore.exceptions import ClientError
import time
from requests.auth import HTTPBasicAuth
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
dynamodb = boto3.resource('dynamodb')
###########################################################################
# Lambda Handler
def lambda_handler(event, context):
stackId = event['ResourceProperties']['stackId']
Id = (stackId.split('/'))[-1]
responseStatus = 'SUCCESS'
responseData = {}
responseData[&quot;Id&quot;]  = get_user_aid(Id)
sendResponse(event, context, responseStatus, responseData)
#Send Response back to CF
def sendResponse(event, context, responseStatus, responseData):
responseBody = {'Status': responseStatus,
'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
'PhysicalResourceId': responseData[&quot;Id&quot;],
'StackId': event['StackId'],
'RequestId': event['RequestId'],
'LogicalResourceId': event['LogicalResourceId'],
'Data': responseData}
try:
req = requests.put(event['ResponseURL'], data=json.dumps(responseBody))
if req.status_code != 200:
print(req.text)
raise Exception('Received non 200 response while sending response to CFN.')
return
except requests.exceptions.RequestException as e:
print(e)
raise
###########################################################################
# Get User AID
def get_user_aid(stackId):
IValue = ''
table = dynamodb.Table('sc-track-user')
counter = 0
while not IValue:
try:
response = table.get_item(
Key={
'CFStackid': stackId
}
)
if 'Item' in response:
IValue = response['Item']['User']
else:
time.sleep(5)
except ClientError as e:
print(e.response['Error']['Message'])
return(IValue)
</code> 
<p>3. The code for our Lambda function requires a requests Python module that is not included in the standard Python library. To install the requests module, use the command that follows. Note that $PWD is the location of the getuser.py file..:</p> 
<code>pip install -t &quot;$PWD&quot; requests</code> 
<p>4. Zip getuser.py along with all modules installed in the previous step as getuser.zip.<br /> 5. Execute the following command:</p> 
<code>aws lambda create-function --function-name sc-get-user-id --runtime python2.7 --role arn:aws:iam::{AWS Account No}:role/sc-lambda-role --handler getuser.lambda_handler --timeout 120 --zip-file &quot;fileb://getuser.zip</code> 
<h3>Step 7: Modify the CloudFormation template</h3> 
<p>In the final step, we need to add a custom resource to product CloudFormation templates to retrieve the name of the user who provisions the product.</p> 
<p>Here is an example of a CloudFormation template for a product:</p> 
<code class="lang-json">{
&quot;AWSTemplateFormatVersion&quot;: &quot;2010-09-09&quot;,
&quot;Description&quot;: &quot;test-get-user-aid&quot;,
&quot;Resources&quot;: {
&quot;UserAID&quot;: {
&quot;Type&quot;: &quot;Custom::SCUserAID&quot;,
&quot;Properties&quot;: {
&quot;ServiceToken&quot;: {
&quot;Fn::Join&quot;: [ &quot;&quot;,  
[ &quot;arn:aws:lambda:&quot;, { &quot;Ref&quot;: &quot;AWS::Region&quot; }, &quot;:&quot;, { &quot;Ref&quot;: &quot;AWS::AccountId&quot; }, &quot;:function:sc-get-user-id&quot; ]
] },
&quot;stackId&quot;: {
&quot;Ref&quot;: &quot;AWS::StackId&quot;
}
}
}
},
&quot;Outputs&quot;: {
&quot;Param&quot;: {
&quot;Value&quot;: {
&quot;Fn::GetAtt&quot;: [
&quot;UserAID&quot;,
&quot;Id&quot;
]
}
}
}
}
</code> 
<p>The user name returned by our Lambda function can now be used to tag AWS resources launched through this process.</p> 
<b>Use cases</b> 
<p>Beside proper tagging resource with correct user name there are examples where described solution come handy. Here are couple of examples:</p> 
<h3>Custom DNS</h3> 
<p>In many cases, provisioning products using the AWS Service Catalog requires access over DNS rather than an IP address– for example an HTTPS connection. Since multiple users can launch the same product, the DNS name cannot be hard coded in the CloudFormation template but should be generated dynamically during product provisioning. In such cases, the user ID could be used as a prefix to the DNS name.</p> 
<h3>AWS Service Catalog product access control</h3> 
<p>When users authenticate to AWS through federation, access to AWS Service Catalog is managed through an IAM role. In such cases, an organization might require excluding access to AWS Service Catalog products) for certain users. This can be accomplished by creating an Amazon DynamoDB table with a list of users who have access to products and then modifying the sc-get-user-id Lambda function so the function will query the table and return status FAILED when user doesn’t have permission to provision a given product. The name of the product can be passed to the Lambda function as additional parameter directly from CloudFormation template.</p> 
<b>About the Author</b> 
<p><img width="100%" src="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2017/12/19/remek.jpg" /><br /> Remek Hetman is a Senior Cloud Infrastructure Architect with the Amazon Web Services ProServe team. He works with AWS Enterprise customers providing technical guidance and assistance for Infrastructure, DevOps, and big data to help them make the best use of AWS services. Outside of work he enjoys spending time actively as well as pursing his passion – astronomy.</p> 
</article> 
<p>
© 2018 Amazon Web Services, Inc. or its affiliates. All rights reserved.
</p>

    </div>
  </div>
</div>


    <div id="footer" class="navigation hidden-print">
      <div class="container">
        <div class="row">
          <div class="col-xs-12">
            <ul class="footer-links nav navbar-nav">
              <li><a href="../aws.html">AWS</a></li>
              <li><a href="../linux.html">Linux</a></li>
              <li><a href="../docker.html">Docker</a></li>
              <li><a href="../ibmpower.html">IBM Power</a></li>
              <li><a href="blogsataws1.html">Blogs@AWS (Kindle Friendly)</a></li>
            </ul>
            <ul class="footer-links nav navbar-nav navbar-right">
              <li><a href="../comments.html">Comments?</a></li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
